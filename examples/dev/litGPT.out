============================================ START: LABEL default
============================================ START: computation_trc split_forward_backward
# Constructed by Dead Code Elimination (took 4 milliseconds)
import thunder
import thunder.core.dtypes as dtypes
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight):
  # idx: "cuda:0 i64[1, 512]"
  # tos1: "cuda:0 f32[4096, 128]"
  # t_lm_head_weight: "cuda:0 bf16[32000, 4096]"
  # t_sin: "cuda:0 f32[4096, 128]"
  # t_transformer_h_0_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_0_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_0_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_0_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_0_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_1_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_1_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_1_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_2_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_2_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_2_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_3_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_3_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_3_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_4_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_4_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_4_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_5_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_5_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_5_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_6_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_6_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_6_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_7_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_7_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_7_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_8_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_8_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_8_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_9_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_9_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_9_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_10_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_10_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_10_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_11_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_11_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_11_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_12_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_12_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_12_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_13_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_13_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_13_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_14_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_14_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_14_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_15_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_15_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_15_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_ln_f_weight: "cuda:0 bf16[4096]"
  # t_transformer_wte_weight: "cuda:0 bf16[32000, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:85: 	            cos = self.cos[:T]
  cos = ltorch.getitem(tos1, slice(None, 512, None))  # cos: "cuda:0 f32[512, 128]"
    # cos = prims.slice_prim(tos1, [0, 0], [512, 128], [1, 1])  # cos: "cuda:0 f32[512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:86: 	            sin = self.sin[:T]
  sin = ltorch.getitem(t_sin, slice(None, 512, None))  # sin: "cuda:0 f32[512, 128]"
    # sin = prims.slice_prim(t_sin, [0, 0], [512, 128], [1, 1])  # sin: "cuda:0 f32[512, 128]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py:190: 	        return F.embedding(
  x = ltorch.embedding(idx, t_transformer_wte_weight, None, None, 2.0, False, False)  # x: "cuda:0 bf16[1, 512, 4096]"
    # t16 = ltorch.reshape(idx, [512])  # t16: "cuda:0 i64[512]"
      # t16 = prims.reshape(idx, (512,))  # t16: "cuda:0 i64[512]"
    # t17 = prims.take(t_transformer_wte_weight, t16, 0)  # t17: "cuda:0 bf16[512, 4096]"
    # x = ltorch.reshape(t17, [1, 512, 4096])  # x: "cuda:0 bf16[1, 512, 4096]"
      # x = prims.reshape(t17, (1, 512, 4096))  # x: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  a = prims.convert_element_type(x, dtypes.float32)  # a: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  result = ltorch.mul(a, a)  # result: "cuda:0 f32[1, 512, 4096]"
    # result = prims.mul(a, a)  # result: "cuda:0 f32[1, 512, 4096]"
  norm_x = ltorch.mean(result, -1, True, dtype=None)  # norm_x: "cuda:0 f32[1, 512, 1]"
    # t24 = prims.sum(result, (2,))  # t24: "cuda:0 f32[1, 512]"
    # t25 = prims.broadcast_in_dim(t24, [1, 512, 1], [0, 1])  # t25: "cuda:0 f32[1, 512, 1]"
    # norm_x = ltorch.true_divide(t25, 4096)  # norm_x: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # norm_x = prims.div(t25, 4096.0)  # norm_x: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t28 = ltorch.add(norm_x, 1e-05, alpha=None)  # t28: "cuda:0 f32[1, 512, 1]"
    # t28 = prims.add(norm_x, 1e-05)  # t28: "cuda:0 f32[1, 512, 1]"
  b = ltorch.rsqrt(t28)  # b: "cuda:0 f32[1, 512, 1]"
    # b = prims.rsqrt(t28)  # b: "cuda:0 f32[1, 512, 1]"
  x_normed = ltorch.mul(a, b)  # x_normed: "cuda:0 f32[1, 512, 4096]"
    # t30 = prims.broadcast_in_dim(b, (1, 512, 4096), (0, 1, 2))  # t30: "cuda:0 f32[1, 512, 4096]"
    # x_normed = prims.mul(a, t30)  # x_normed: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t32 = ltorch.to(x_normed, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t32: "cuda:0 bf16[1, 512, 4096]"
    # t32 = prims.convert_element_type(x_normed, dtypes.bfloat16)  # t32: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  input = ltorch.mul(t32, t_transformer_h_0_norm_1_weight)  # input: "cuda:0 bf16[1, 512, 4096]"
    # t38 = prims.broadcast_in_dim(t_transformer_h_0_norm_1_weight, (1, 512, 4096), (2,))  # t38: "cuda:0 bf16[1, 512, 4096]"
    # t39 = prims.convert_element_type(t32, dtypes.float32)  # t39: "cuda:0 f32[1, 512, 4096]"
    # t40 = prims.convert_element_type(t38, dtypes.float32)  # t40: "cuda:0 f32[1, 512, 4096]"
    # t41 = prims.mul(t39, t40)  # t41: "cuda:0 f32[1, 512, 4096]"
    # input = prims.convert_element_type(t41, dtypes.bfloat16)  # input: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  qkv = ltorch.linear(input, t_transformer_h_0_attn_attn_weight, None)  # qkv: "cuda:0 bf16[1, 512, 12288]"
    # qkv = prims.linear(input, t_transformer_h_0_attn_attn_weight, None)  # qkv: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t51 = ltorch.view(qkv, 1, 512, 32, 3, 128)  # t51: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t51 = ltorch.reshape(qkv, (1, 512, 32, 3, 128))  # t51: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t51 = prims.reshape(qkv, (1, 512, 32, 3, 128))  # t51: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t52 = ltorch.permute(t51, 0, 2, 3, 1, 4)  # t52: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t52 = prims.transpose(t51, (0, 2, 3, 1, 4))  # t52: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (res, k, v) = ltorch.split(t52, (1, 1, 1), 2)
    # res = prims.slice_prim(t52, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # res: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # k = prims.slice_prim(t52, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # k: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # v = prims.slice_prim(t52, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # v: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  q = ltorch.reshape(res, 1, -1, 512, 128)  # q: "cuda:0 bf16[1, 32, 512, 128]"
    # q = prims.reshape(res, (1, 32, 512, 128))  # q: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t57 = ltorch.reshape(k, 1, -1, 512, 128)  # t57: "cuda:0 bf16[1, 32, 512, 128]"
    # t57 = prims.reshape(k, (1, 32, 512, 128))  # t57: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t58 = ltorch.reshape(v, 1, -1, 512, 128)  # t58: "cuda:0 bf16[1, 32, 512, 128]"
    # t58 = prims.reshape(v, (1, 32, 512, 128))  # t58: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t60 = ltorch.getitem(q, (..., slice(None, 128, None)))  # t60: "cuda:0 bf16[1, 32, 512, 128]"
    # t60 = prims.slice_prim(q, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t60: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  x1 = ltorch.getitem(t60, (..., slice(None, 64, None)))  # x1: "cuda:0 bf16[1, 32, 512, 64]"
    # x1 = prims.slice_prim(t60, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # x1: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  x2 = ltorch.getitem(t60, (..., slice(64, None, None)))  # x2: "cuda:0 bf16[1, 32, 512, 64]"
    # x2 = prims.slice_prim(t60, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # x2: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t65 = ltorch.neg(x2)  # t65: "cuda:0 bf16[1, 32, 512, 64]"
    # t63 = prims.convert_element_type(x2, dtypes.float32)  # t63: "cuda:0 f32[1, 32, 512, 64]"
    # t64 = prims.neg(t63)  # t64: "cuda:0 f32[1, 32, 512, 64]"
    # t65 = prims.convert_element_type(t64, dtypes.bfloat16)  # t65: "cuda:0 bf16[1, 32, 512, 64]"
  rotated = ltorch.cat((t65, x1), -1)  # rotated: "cuda:0 bf16[1, 32, 512, 128]"
    # rotated = prims.cat((t65, x1), -1)  # rotated: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t69 = ltorch.mul(t60, cos)  # t69: "cuda:0 f32[1, 32, 512, 128]"
    # t67 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t67: "cuda:0 f32[1, 32, 512, 128]"
    # t68 = prims.convert_element_type(t60, dtypes.float32)  # t68: "cuda:0 f32[1, 32, 512, 128]"
    # t69 = prims.mul(t68, t67)  # t69: "cuda:0 f32[1, 32, 512, 128]"
  t72 = ltorch.mul(rotated, sin)  # t72: "cuda:0 f32[1, 32, 512, 128]"
    # t70 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t70: "cuda:0 f32[1, 32, 512, 128]"
    # t71 = prims.convert_element_type(rotated, dtypes.float32)  # t71: "cuda:0 f32[1, 32, 512, 128]"
    # t72 = prims.mul(t71, t70)  # t72: "cuda:0 f32[1, 32, 512, 128]"
  roped = ltorch.add(t69, t72, alpha=None)  # roped: "cuda:0 f32[1, 32, 512, 128]"
    # roped = prims.add(t69, t72)  # roped: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  q_roped = ltorch.to(roped, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # q_roped: "cuda:0 bf16[1, 32, 512, 128]"
    # q_roped = prims.convert_element_type(roped, dtypes.bfloat16)  # q_roped: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t75 = ltorch.getitem(t57, (..., slice(None, 128, None)))  # t75: "cuda:0 bf16[1, 32, 512, 128]"
    # t75 = prims.slice_prim(t57, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t75: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t76 = ltorch.getitem(t75, (..., slice(None, 64, None)))  # t76: "cuda:0 bf16[1, 32, 512, 64]"
    # t76 = prims.slice_prim(t75, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t76: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  tos = ltorch.getitem(t75, (..., slice(64, None, None)))  # tos: "cuda:0 bf16[1, 32, 512, 64]"
    # tos = prims.slice_prim(t75, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # tos: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t80 = ltorch.neg(tos)  # t80: "cuda:0 bf16[1, 32, 512, 64]"
    # t78 = prims.convert_element_type(tos, dtypes.float32)  # t78: "cuda:0 f32[1, 32, 512, 64]"
    # t79 = prims.neg(t78)  # t79: "cuda:0 f32[1, 32, 512, 64]"
    # t80 = prims.convert_element_type(t79, dtypes.bfloat16)  # t80: "cuda:0 bf16[1, 32, 512, 64]"
  t81 = ltorch.cat((t80, t76), -1)  # t81: "cuda:0 bf16[1, 32, 512, 128]"
    # t81 = prims.cat((t80, t76), -1)  # t81: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t84 = ltorch.mul(t75, cos)  # t84: "cuda:0 f32[1, 32, 512, 128]"
    # t82 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t82: "cuda:0 f32[1, 32, 512, 128]"
    # t83 = prims.convert_element_type(t75, dtypes.float32)  # t83: "cuda:0 f32[1, 32, 512, 128]"
    # t84 = prims.mul(t83, t82)  # t84: "cuda:0 f32[1, 32, 512, 128]"
  t87 = ltorch.mul(t81, sin)  # t87: "cuda:0 f32[1, 32, 512, 128]"
    # t85 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t85: "cuda:0 f32[1, 32, 512, 128]"
    # t86 = prims.convert_element_type(t81, dtypes.float32)  # t86: "cuda:0 f32[1, 32, 512, 128]"
    # t87 = prims.mul(t86, t85)  # t87: "cuda:0 f32[1, 32, 512, 128]"
  t88 = ltorch.add(t84, t87, alpha=None)  # t88: "cuda:0 f32[1, 32, 512, 128]"
    # t88 = prims.add(t84, t87)  # t88: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  k_roped = ltorch.to(t88, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # k_roped: "cuda:0 bf16[1, 32, 512, 128]"
    # k_roped = prims.convert_element_type(t88, dtypes.bfloat16)  # k_roped: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t90 = ltorch.getitem(q, (..., slice(128, None, None)))  # t90: "cuda:0 bf16[1, 32, 512, 0]"
    # t90 = prims.slice_prim(q, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t90: "cuda:0 bf16[1, 32, 512, 0]"
  t91 = ltorch.cat((q_roped, t90), -1)  # t91: "cuda:0 bf16[1, 32, 512, 128]"
    # t91 = prims.cat((q_roped, t90), -1)  # t91: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t92 = ltorch.getitem(t57, (..., slice(128, None, None)))  # t92: "cuda:0 bf16[1, 32, 512, 0]"
    # t92 = prims.slice_prim(t57, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t92: "cuda:0 bf16[1, 32, 512, 0]"
  t93 = ltorch.cat((k_roped, t92), -1)  # t93: "cuda:0 bf16[1, 32, 512, 128]"
    # t93 = prims.cat((k_roped, t92), -1)  # t93: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  y = ltorch.scaled_dot_product_attention(t91, t93, t58, None, 0.0, True, scale=0.08838834764831843)  # y: "cuda:0 bf16[1, 32, 512, 128]"
    # t96 = ltorch.mul(t91, 0.29730177875068026)  # t96: "cuda:0 bf16[1, 32, 512, 128]"
      # t94 = prims.convert_element_type(t91, dtypes.float32)  # t94: "cuda:0 f32[1, 32, 512, 128]"
      # t95 = prims.mul(t94, 0.29730177875068026)  # t95: "cuda:0 f32[1, 32, 512, 128]"
      # t96 = prims.convert_element_type(t95, dtypes.bfloat16)  # t96: "cuda:0 bf16[1, 32, 512, 128]"
    # t97 = ltorch.transpose(t93, -2, -1)  # t97: "cuda:0 bf16[1, 32, 128, 512]"
      # t97 = prims.transpose(t93, (0, 1, 3, 2))  # t97: "cuda:0 bf16[1, 32, 128, 512]"
    # t100 = ltorch.mul(t97, 0.29730177875068026)  # t100: "cuda:0 bf16[1, 32, 128, 512]"
      # t98 = prims.convert_element_type(t97, dtypes.float32)  # t98: "cuda:0 f32[1, 32, 128, 512]"
      # t99 = prims.mul(t98, 0.29730177875068026)  # t99: "cuda:0 f32[1, 32, 128, 512]"
      # t100 = prims.convert_element_type(t99, dtypes.bfloat16)  # t100: "cuda:0 bf16[1, 32, 128, 512]"
    # t101 = ltorch.matmul(t96, t100)  # t101: "cuda:0 bf16[1, 32, 512, 512]"
      # t101 = prims.matmul(t96, t100)  # t101: "cuda:0 bf16[1, 32, 512, 512]"
    # t111 = ltorch.tril(t101, 0, fill_value=-float('inf'))  # t111: "cuda:0 bf16[1, 32, 512, 512]"
      # t102 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t102: "cuda:0 i64[512]"
        # t102 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t102: "cuda:0 i64[512]"
      # t103 = ltorch.unsqueeze(t102, -1)  # t103: "cuda:0 i64[512, 1]"
        # t103 = prims.broadcast_in_dim(t102, [512, 1], [0])  # t103: "cuda:0 i64[512, 1]"
      # t104 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t104: "cuda:0 i64[512]"
        # t104 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t104: "cuda:0 i64[512]"
      # t105 = ltorch.unsqueeze(t104, -2)  # t105: "cuda:0 i64[1, 512]"
        # t105 = prims.broadcast_in_dim(t104, [1, 512], [1])  # t105: "cuda:0 i64[1, 512]"
      # t106 = ltorch.add(t103, 0, alpha=None)  # t106: "cuda:0 i64[512, 1]"
        # t106 = prims.add(t103, 0)  # t106: "cuda:0 i64[512, 1]"
      # t109 = ltorch.ge(t106, t105)  # t109: "cuda:0 b8[512, 512]"
        # t107 = prims.broadcast_in_dim(t106, (512, 512), (0, 1))  # t107: "cuda:0 i64[512, 512]"
        # t108 = prims.broadcast_in_dim(t105, (512, 512), (0, 1))  # t108: "cuda:0 i64[512, 512]"
        # t109 = prims.ge(t107, t108)  # t109: "cuda:0 b8[512, 512]"
      # t111 = ltorch.where(t109, t101, -float('inf'))  # t111: "cuda:0 bf16[1, 32, 512, 512]"
        # t110 = prims.broadcast_in_dim(t109, (1, 32, 512, 512), (2, 3))  # t110: "cuda:0 b8[1, 32, 512, 512]"
        # t111 = prims.where(t110, t101, -float('inf'))  # t111: "cuda:0 bf16[1, 32, 512, 512]"
    # t122 = ltorch._softmax(t111, -1, dtype=None)  # t122: "cuda:0 bf16[1, 32, 512, 512]"
      # t112 = ltorch.to(t111, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t112: "cuda:0 f32[1, 32, 512, 512]"
        # t112 = prims.convert_element_type(t111, dtypes.float32)  # t112: "cuda:0 f32[1, 32, 512, 512]"
      # t114 = ltorch.amax(t112, -1, True)  # t114: "cuda:0 f32[1, 32, 512, 1]"
        # t113 = prims.amax(t112, (3,))  # t113: "cuda:0 f32[1, 32, 512]"
        # t114 = prims.broadcast_in_dim(t113, [1, 32, 512, 1], [0, 1, 2])  # t114: "cuda:0 f32[1, 32, 512, 1]"
      # t116 = ltorch.sub(t112, t114, alpha=None)  # t116: "cuda:0 f32[1, 32, 512, 512]"
        # t115 = prims.broadcast_in_dim(t114, (1, 32, 512, 512), (0, 1, 2, 3))  # t115: "cuda:0 f32[1, 32, 512, 512]"
        # t116 = prims.sub(t112, t115)  # t116: "cuda:0 f32[1, 32, 512, 512]"
      # t117 = ltorch.exp(t116)  # t117: "cuda:0 f32[1, 32, 512, 512]"
        # t117 = prims.exp(t116)  # t117: "cuda:0 f32[1, 32, 512, 512]"
      # t119 = ltorch.sum(t117, -1, True, dtype=None)  # t119: "cuda:0 f32[1, 32, 512, 1]"
        # t118 = prims.sum(t117, (3,))  # t118: "cuda:0 f32[1, 32, 512]"
        # t119 = prims.broadcast_in_dim(t118, [1, 32, 512, 1], [0, 1, 2])  # t119: "cuda:0 f32[1, 32, 512, 1]"
      # t121 = ltorch.true_divide(t117, t119)  # t121: "cuda:0 f32[1, 32, 512, 512]"
        # t120 = prims.broadcast_in_dim(t119, (1, 32, 512, 512), (0, 1, 2, 3))  # t120: "cuda:0 f32[1, 32, 512, 512]"
        # t121 = prims.div(t117, t120)  # t121: "cuda:0 f32[1, 32, 512, 512]"
      # t122 = ltorch.to(t121, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t122: "cuda:0 bf16[1, 32, 512, 512]"
        # t122 = prims.convert_element_type(t121, dtypes.bfloat16)  # t122: "cuda:0 bf16[1, 32, 512, 512]"
    # y = ltorch.matmul(t122, t58)  # y: "cuda:0 bf16[1, 32, 512, 128]"
      # y = prims.matmul(t122, t58)  # y: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t124 = ltorch.transpose(y, 1, 2)  # t124: "cuda:0 bf16[1, 512, 32, 128]"
    # t124 = prims.transpose(y, (0, 2, 1, 3))  # t124: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t125 = ltorch.reshape(t124, 1, 512, 4096)  # t125: "cuda:0 bf16[1, 512, 4096]"
    # t125 = prims.reshape(t124, (1, 512, 4096))  # t125: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  attention_output = ltorch.linear(t125, t_transformer_h_0_attn_proj_weight, None)  # attention_output: "cuda:0 bf16[1, 512, 4096]"
    # attention_output = prims.linear(t125, t_transformer_h_0_attn_proj_weight, None)  # attention_output: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t134 = ltorch.add(attention_output, x, alpha=None)  # t134: "cuda:0 bf16[1, 512, 4096]"
    # t131 = prims.convert_element_type(attention_output, dtypes.float32)  # t131: "cuda:0 f32[1, 512, 4096]"
    # t132 = prims.convert_element_type(x, dtypes.float32)  # t132: "cuda:0 f32[1, 512, 4096]"
    # t133 = prims.add(t131, t132)  # t133: "cuda:0 f32[1, 512, 4096]"
    # t134 = prims.convert_element_type(t133, dtypes.bfloat16)  # t134: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t135 = prims.convert_element_type(t134, dtypes.float32)  # t135: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t136 = ltorch.mul(t135, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
    # t136 = prims.mul(t135, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
  t140 = ltorch.mean(t136, -1, True, dtype=None)  # t140: "cuda:0 f32[1, 512, 1]"
    # t138 = prims.sum(t136, (2,))  # t138: "cuda:0 f32[1, 512]"
    # t139 = prims.broadcast_in_dim(t138, [1, 512, 1], [0, 1])  # t139: "cuda:0 f32[1, 512, 1]"
    # t140 = ltorch.true_divide(t139, 4096)  # t140: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t140 = prims.div(t139, 4096.0)  # t140: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t142 = ltorch.add(t140, 1e-05, alpha=None)  # t142: "cuda:0 f32[1, 512, 1]"
    # t142 = prims.add(t140, 1e-05)  # t142: "cuda:0 f32[1, 512, 1]"
  t143 = ltorch.rsqrt(t142)  # t143: "cuda:0 f32[1, 512, 1]"
    # t143 = prims.rsqrt(t142)  # t143: "cuda:0 f32[1, 512, 1]"
  t145 = ltorch.mul(t135, t143)  # t145: "cuda:0 f32[1, 512, 4096]"
    # t144 = prims.broadcast_in_dim(t143, (1, 512, 4096), (0, 1, 2))  # t144: "cuda:0 f32[1, 512, 4096]"
    # t145 = prims.mul(t135, t144)  # t145: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t146 = ltorch.to(t145, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t146: "cuda:0 bf16[1, 512, 4096]"
    # t146 = prims.convert_element_type(t145, dtypes.bfloat16)  # t146: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t156 = ltorch.mul(t146, t_transformer_h_0_norm_2_weight)  # t156: "cuda:0 bf16[1, 512, 4096]"
    # t152 = prims.broadcast_in_dim(t_transformer_h_0_norm_2_weight, (1, 512, 4096), (2,))  # t152: "cuda:0 bf16[1, 512, 4096]"
    # t153 = prims.convert_element_type(t146, dtypes.float32)  # t153: "cuda:0 f32[1, 512, 4096]"
    # t154 = prims.convert_element_type(t152, dtypes.float32)  # t154: "cuda:0 f32[1, 512, 4096]"
    # t155 = prims.mul(t153, t154)  # t155: "cuda:0 f32[1, 512, 4096]"
    # t156 = prims.convert_element_type(t155, dtypes.bfloat16)  # t156: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(t156, t_transformer_h_0_mlp_fc_1_weight, None)  # x_fc_1: "cuda:0 bf16[1, 512, 11008]"
    # x_fc_1 = prims.linear(t156, t_transformer_h_0_mlp_fc_1_weight, None)  # x_fc_1: "cuda:0 bf16[1, 512, 11008]"
  x_fc_2 = ltorch.linear(t156, t_transformer_h_0_mlp_fc_2_weight, None)  # x_fc_2: "cuda:0 bf16[1, 512, 11008]"
    # x_fc_2 = prims.linear(t156, t_transformer_h_0_mlp_fc_2_weight, None)  # x_fc_2: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t175 = ltorch.silu(x_fc_1, False)  # t175: "cuda:0 bf16[1, 512, 11008]"
    # t166 = prims.convert_element_type(x_fc_1, dtypes.float32)  # t166: "cuda:0 f32[1, 512, 11008]"
    # t167 = prims.neg(t166)  # t167: "cuda:0 f32[1, 512, 11008]"
    # t168 = prims.exp(t167)  # t168: "cuda:0 f32[1, 512, 11008]"
    # t169 = prims.add(1.0, t168)  # t169: "cuda:0 f32[1, 512, 11008]"
    # t170 = prims.reciprocal(t169)  # t170: "cuda:0 f32[1, 512, 11008]"
    # t171 = prims.convert_element_type(t170, dtypes.bfloat16)  # t171: "cuda:0 bf16[1, 512, 11008]"
    # t172 = prims.convert_element_type(x_fc_1, dtypes.float32)  # t172: "cuda:0 f32[1, 512, 11008]"
    # t173 = prims.convert_element_type(t171, dtypes.float32)  # t173: "cuda:0 f32[1, 512, 11008]"
    # t174 = prims.mul(t172, t173)  # t174: "cuda:0 f32[1, 512, 11008]"
    # t175 = prims.convert_element_type(t174, dtypes.bfloat16)  # t175: "cuda:0 bf16[1, 512, 11008]"
  t179 = ltorch.mul(t175, x_fc_2)  # t179: "cuda:0 bf16[1, 512, 11008]"
    # t176 = prims.convert_element_type(t175, dtypes.float32)  # t176: "cuda:0 f32[1, 512, 11008]"
    # t177 = prims.convert_element_type(x_fc_2, dtypes.float32)  # t177: "cuda:0 f32[1, 512, 11008]"
    # t178 = prims.mul(t176, t177)  # t178: "cuda:0 f32[1, 512, 11008]"
    # t179 = prims.convert_element_type(t178, dtypes.bfloat16)  # t179: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t183 = ltorch.linear(t179, t_transformer_h_0_mlp_proj_weight, None)  # t183: "cuda:0 bf16[1, 512, 4096]"
    # t183 = prims.linear(t179, t_transformer_h_0_mlp_proj_weight, None)  # t183: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t187 = ltorch.add(t183, t134, alpha=None)  # t187: "cuda:0 bf16[1, 512, 4096]"
    # t184 = prims.convert_element_type(t183, dtypes.float32)  # t184: "cuda:0 f32[1, 512, 4096]"
    # t185 = prims.convert_element_type(t134, dtypes.float32)  # t185: "cuda:0 f32[1, 512, 4096]"
    # t186 = prims.add(t184, t185)  # t186: "cuda:0 f32[1, 512, 4096]"
    # t187 = prims.convert_element_type(t186, dtypes.bfloat16)  # t187: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t189 = prims.convert_element_type(t187, dtypes.float32)  # t189: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t190 = ltorch.mul(t189, t189)  # t190: "cuda:0 f32[1, 512, 4096]"
    # t190 = prims.mul(t189, t189)  # t190: "cuda:0 f32[1, 512, 4096]"
  t194 = ltorch.mean(t190, -1, True, dtype=None)  # t194: "cuda:0 f32[1, 512, 1]"
    # t192 = prims.sum(t190, (2,))  # t192: "cuda:0 f32[1, 512]"
    # t193 = prims.broadcast_in_dim(t192, [1, 512, 1], [0, 1])  # t193: "cuda:0 f32[1, 512, 1]"
    # t194 = ltorch.true_divide(t193, 4096)  # t194: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t194 = prims.div(t193, 4096.0)  # t194: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t196 = ltorch.add(t194, 1e-05, alpha=None)  # t196: "cuda:0 f32[1, 512, 1]"
    # t196 = prims.add(t194, 1e-05)  # t196: "cuda:0 f32[1, 512, 1]"
  t197 = ltorch.rsqrt(t196)  # t197: "cuda:0 f32[1, 512, 1]"
    # t197 = prims.rsqrt(t196)  # t197: "cuda:0 f32[1, 512, 1]"
  t199 = ltorch.mul(t189, t197)  # t199: "cuda:0 f32[1, 512, 4096]"
    # t198 = prims.broadcast_in_dim(t197, (1, 512, 4096), (0, 1, 2))  # t198: "cuda:0 f32[1, 512, 4096]"
    # t199 = prims.mul(t189, t198)  # t199: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t200 = ltorch.to(t199, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t200: "cuda:0 bf16[1, 512, 4096]"
    # t200 = prims.convert_element_type(t199, dtypes.bfloat16)  # t200: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t210 = ltorch.mul(t200, t_transformer_h_1_norm_1_weight)  # t210: "cuda:0 bf16[1, 512, 4096]"
    # t206 = prims.broadcast_in_dim(t_transformer_h_1_norm_1_weight, (1, 512, 4096), (2,))  # t206: "cuda:0 bf16[1, 512, 4096]"
    # t207 = prims.convert_element_type(t200, dtypes.float32)  # t207: "cuda:0 f32[1, 512, 4096]"
    # t208 = prims.convert_element_type(t206, dtypes.float32)  # t208: "cuda:0 f32[1, 512, 4096]"
    # t209 = prims.mul(t207, t208)  # t209: "cuda:0 f32[1, 512, 4096]"
    # t210 = prims.convert_element_type(t209, dtypes.bfloat16)  # t210: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t215 = ltorch.linear(t210, t_transformer_h_1_attn_attn_weight, None)  # t215: "cuda:0 bf16[1, 512, 12288]"
    # t215 = prims.linear(t210, t_transformer_h_1_attn_attn_weight, None)  # t215: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t216 = ltorch.view(t215, 1, 512, 32, 3, 128)  # t216: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t216 = ltorch.reshape(t215, (1, 512, 32, 3, 128))  # t216: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t216 = prims.reshape(t215, (1, 512, 32, 3, 128))  # t216: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t217 = ltorch.permute(t216, 0, 2, 3, 1, 4)  # t217: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t217 = prims.transpose(t216, (0, 2, 3, 1, 4))  # t217: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t218, t219, t220) = ltorch.split(t217, (1, 1, 1), 2)
    # t218 = prims.slice_prim(t217, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t218: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t219 = prims.slice_prim(t217, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t219: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t220 = prims.slice_prim(t217, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t220: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t221 = ltorch.reshape(t218, 1, -1, 512, 128)  # t221: "cuda:0 bf16[1, 32, 512, 128]"
    # t221 = prims.reshape(t218, (1, 32, 512, 128))  # t221: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t222 = ltorch.reshape(t219, 1, -1, 512, 128)  # t222: "cuda:0 bf16[1, 32, 512, 128]"
    # t222 = prims.reshape(t219, (1, 32, 512, 128))  # t222: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t223 = ltorch.reshape(t220, 1, -1, 512, 128)  # t223: "cuda:0 bf16[1, 32, 512, 128]"
    # t223 = prims.reshape(t220, (1, 32, 512, 128))  # t223: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t224 = ltorch.getitem(t221, (..., slice(None, 128, None)))  # t224: "cuda:0 bf16[1, 32, 512, 128]"
    # t224 = prims.slice_prim(t221, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t224: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t225 = ltorch.getitem(t224, (..., slice(None, 64, None)))  # t225: "cuda:0 bf16[1, 32, 512, 64]"
    # t225 = prims.slice_prim(t224, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t225: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t226 = ltorch.getitem(t224, (..., slice(64, None, None)))  # t226: "cuda:0 bf16[1, 32, 512, 64]"
    # t226 = prims.slice_prim(t224, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t226: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t229 = ltorch.neg(t226)  # t229: "cuda:0 bf16[1, 32, 512, 64]"
    # t227 = prims.convert_element_type(t226, dtypes.float32)  # t227: "cuda:0 f32[1, 32, 512, 64]"
    # t228 = prims.neg(t227)  # t228: "cuda:0 f32[1, 32, 512, 64]"
    # t229 = prims.convert_element_type(t228, dtypes.bfloat16)  # t229: "cuda:0 bf16[1, 32, 512, 64]"
  t230 = ltorch.cat((t229, t225), -1)  # t230: "cuda:0 bf16[1, 32, 512, 128]"
    # t230 = prims.cat((t229, t225), -1)  # t230: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t233 = ltorch.mul(t224, cos)  # t233: "cuda:0 f32[1, 32, 512, 128]"
    # t231 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t231: "cuda:0 f32[1, 32, 512, 128]"
    # t232 = prims.convert_element_type(t224, dtypes.float32)  # t232: "cuda:0 f32[1, 32, 512, 128]"
    # t233 = prims.mul(t232, t231)  # t233: "cuda:0 f32[1, 32, 512, 128]"
  t236 = ltorch.mul(t230, sin)  # t236: "cuda:0 f32[1, 32, 512, 128]"
    # t234 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t234: "cuda:0 f32[1, 32, 512, 128]"
    # t235 = prims.convert_element_type(t230, dtypes.float32)  # t235: "cuda:0 f32[1, 32, 512, 128]"
    # t236 = prims.mul(t235, t234)  # t236: "cuda:0 f32[1, 32, 512, 128]"
  t237 = ltorch.add(t233, t236, alpha=None)  # t237: "cuda:0 f32[1, 32, 512, 128]"
    # t237 = prims.add(t233, t236)  # t237: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t238 = ltorch.to(t237, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t238: "cuda:0 bf16[1, 32, 512, 128]"
    # t238 = prims.convert_element_type(t237, dtypes.bfloat16)  # t238: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t239 = ltorch.getitem(t222, (..., slice(None, 128, None)))  # t239: "cuda:0 bf16[1, 32, 512, 128]"
    # t239 = prims.slice_prim(t222, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t239: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t240 = ltorch.getitem(t239, (..., slice(None, 64, None)))  # t240: "cuda:0 bf16[1, 32, 512, 64]"
    # t240 = prims.slice_prim(t239, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t240: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t241 = ltorch.getitem(t239, (..., slice(64, None, None)))  # t241: "cuda:0 bf16[1, 32, 512, 64]"
    # t241 = prims.slice_prim(t239, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t241: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t244 = ltorch.neg(t241)  # t244: "cuda:0 bf16[1, 32, 512, 64]"
    # t242 = prims.convert_element_type(t241, dtypes.float32)  # t242: "cuda:0 f32[1, 32, 512, 64]"
    # t243 = prims.neg(t242)  # t243: "cuda:0 f32[1, 32, 512, 64]"
    # t244 = prims.convert_element_type(t243, dtypes.bfloat16)  # t244: "cuda:0 bf16[1, 32, 512, 64]"
  t245 = ltorch.cat((t244, t240), -1)  # t245: "cuda:0 bf16[1, 32, 512, 128]"
    # t245 = prims.cat((t244, t240), -1)  # t245: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t248 = ltorch.mul(t239, cos)  # t248: "cuda:0 f32[1, 32, 512, 128]"
    # t246 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t246: "cuda:0 f32[1, 32, 512, 128]"
    # t247 = prims.convert_element_type(t239, dtypes.float32)  # t247: "cuda:0 f32[1, 32, 512, 128]"
    # t248 = prims.mul(t247, t246)  # t248: "cuda:0 f32[1, 32, 512, 128]"
  t251 = ltorch.mul(t245, sin)  # t251: "cuda:0 f32[1, 32, 512, 128]"
    # t249 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t249: "cuda:0 f32[1, 32, 512, 128]"
    # t250 = prims.convert_element_type(t245, dtypes.float32)  # t250: "cuda:0 f32[1, 32, 512, 128]"
    # t251 = prims.mul(t250, t249)  # t251: "cuda:0 f32[1, 32, 512, 128]"
  t252 = ltorch.add(t248, t251, alpha=None)  # t252: "cuda:0 f32[1, 32, 512, 128]"
    # t252 = prims.add(t248, t251)  # t252: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t253 = ltorch.to(t252, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t253: "cuda:0 bf16[1, 32, 512, 128]"
    # t253 = prims.convert_element_type(t252, dtypes.bfloat16)  # t253: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t254 = ltorch.getitem(t221, (..., slice(128, None, None)))  # t254: "cuda:0 bf16[1, 32, 512, 0]"
    # t254 = prims.slice_prim(t221, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t254: "cuda:0 bf16[1, 32, 512, 0]"
  t255 = ltorch.cat((t238, t254), -1)  # t255: "cuda:0 bf16[1, 32, 512, 128]"
    # t255 = prims.cat((t238, t254), -1)  # t255: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t256 = ltorch.getitem(t222, (..., slice(128, None, None)))  # t256: "cuda:0 bf16[1, 32, 512, 0]"
    # t256 = prims.slice_prim(t222, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t256: "cuda:0 bf16[1, 32, 512, 0]"
  t257 = ltorch.cat((t253, t256), -1)  # t257: "cuda:0 bf16[1, 32, 512, 128]"
    # t257 = prims.cat((t253, t256), -1)  # t257: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t287 = ltorch.scaled_dot_product_attention(t255, t257, t223, None, 0.0, True, scale=0.08838834764831843)  # t287: "cuda:0 bf16[1, 32, 512, 128]"
    # t260 = ltorch.mul(t255, 0.29730177875068026)  # t260: "cuda:0 bf16[1, 32, 512, 128]"
      # t258 = prims.convert_element_type(t255, dtypes.float32)  # t258: "cuda:0 f32[1, 32, 512, 128]"
      # t259 = prims.mul(t258, 0.29730177875068026)  # t259: "cuda:0 f32[1, 32, 512, 128]"
      # t260 = prims.convert_element_type(t259, dtypes.bfloat16)  # t260: "cuda:0 bf16[1, 32, 512, 128]"
    # t261 = ltorch.transpose(t257, -2, -1)  # t261: "cuda:0 bf16[1, 32, 128, 512]"
      # t261 = prims.transpose(t257, (0, 1, 3, 2))  # t261: "cuda:0 bf16[1, 32, 128, 512]"
    # t264 = ltorch.mul(t261, 0.29730177875068026)  # t264: "cuda:0 bf16[1, 32, 128, 512]"
      # t262 = prims.convert_element_type(t261, dtypes.float32)  # t262: "cuda:0 f32[1, 32, 128, 512]"
      # t263 = prims.mul(t262, 0.29730177875068026)  # t263: "cuda:0 f32[1, 32, 128, 512]"
      # t264 = prims.convert_element_type(t263, dtypes.bfloat16)  # t264: "cuda:0 bf16[1, 32, 128, 512]"
    # t265 = ltorch.matmul(t260, t264)  # t265: "cuda:0 bf16[1, 32, 512, 512]"
      # t265 = prims.matmul(t260, t264)  # t265: "cuda:0 bf16[1, 32, 512, 512]"
    # t275 = ltorch.tril(t265, 0, fill_value=-float('inf'))  # t275: "cuda:0 bf16[1, 32, 512, 512]"
      # t266 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t266: "cuda:0 i64[512]"
        # t266 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t266: "cuda:0 i64[512]"
      # t267 = ltorch.unsqueeze(t266, -1)  # t267: "cuda:0 i64[512, 1]"
        # t267 = prims.broadcast_in_dim(t266, [512, 1], [0])  # t267: "cuda:0 i64[512, 1]"
      # t268 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t268: "cuda:0 i64[512]"
        # t268 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t268: "cuda:0 i64[512]"
      # t269 = ltorch.unsqueeze(t268, -2)  # t269: "cuda:0 i64[1, 512]"
        # t269 = prims.broadcast_in_dim(t268, [1, 512], [1])  # t269: "cuda:0 i64[1, 512]"
      # t270 = ltorch.add(t267, 0, alpha=None)  # t270: "cuda:0 i64[512, 1]"
        # t270 = prims.add(t267, 0)  # t270: "cuda:0 i64[512, 1]"
      # t273 = ltorch.ge(t270, t269)  # t273: "cuda:0 b8[512, 512]"
        # t271 = prims.broadcast_in_dim(t270, (512, 512), (0, 1))  # t271: "cuda:0 i64[512, 512]"
        # t272 = prims.broadcast_in_dim(t269, (512, 512), (0, 1))  # t272: "cuda:0 i64[512, 512]"
        # t273 = prims.ge(t271, t272)  # t273: "cuda:0 b8[512, 512]"
      # t275 = ltorch.where(t273, t265, -float('inf'))  # t275: "cuda:0 bf16[1, 32, 512, 512]"
        # t274 = prims.broadcast_in_dim(t273, (1, 32, 512, 512), (2, 3))  # t274: "cuda:0 b8[1, 32, 512, 512]"
        # t275 = prims.where(t274, t265, -float('inf'))  # t275: "cuda:0 bf16[1, 32, 512, 512]"
    # t286 = ltorch._softmax(t275, -1, dtype=None)  # t286: "cuda:0 bf16[1, 32, 512, 512]"
      # t276 = ltorch.to(t275, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t276: "cuda:0 f32[1, 32, 512, 512]"
        # t276 = prims.convert_element_type(t275, dtypes.float32)  # t276: "cuda:0 f32[1, 32, 512, 512]"
      # t278 = ltorch.amax(t276, -1, True)  # t278: "cuda:0 f32[1, 32, 512, 1]"
        # t277 = prims.amax(t276, (3,))  # t277: "cuda:0 f32[1, 32, 512]"
        # t278 = prims.broadcast_in_dim(t277, [1, 32, 512, 1], [0, 1, 2])  # t278: "cuda:0 f32[1, 32, 512, 1]"
      # t280 = ltorch.sub(t276, t278, alpha=None)  # t280: "cuda:0 f32[1, 32, 512, 512]"
        # t279 = prims.broadcast_in_dim(t278, (1, 32, 512, 512), (0, 1, 2, 3))  # t279: "cuda:0 f32[1, 32, 512, 512]"
        # t280 = prims.sub(t276, t279)  # t280: "cuda:0 f32[1, 32, 512, 512]"
      # t281 = ltorch.exp(t280)  # t281: "cuda:0 f32[1, 32, 512, 512]"
        # t281 = prims.exp(t280)  # t281: "cuda:0 f32[1, 32, 512, 512]"
      # t283 = ltorch.sum(t281, -1, True, dtype=None)  # t283: "cuda:0 f32[1, 32, 512, 1]"
        # t282 = prims.sum(t281, (3,))  # t282: "cuda:0 f32[1, 32, 512]"
        # t283 = prims.broadcast_in_dim(t282, [1, 32, 512, 1], [0, 1, 2])  # t283: "cuda:0 f32[1, 32, 512, 1]"
      # t285 = ltorch.true_divide(t281, t283)  # t285: "cuda:0 f32[1, 32, 512, 512]"
        # t284 = prims.broadcast_in_dim(t283, (1, 32, 512, 512), (0, 1, 2, 3))  # t284: "cuda:0 f32[1, 32, 512, 512]"
        # t285 = prims.div(t281, t284)  # t285: "cuda:0 f32[1, 32, 512, 512]"
      # t286 = ltorch.to(t285, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t286: "cuda:0 bf16[1, 32, 512, 512]"
        # t286 = prims.convert_element_type(t285, dtypes.bfloat16)  # t286: "cuda:0 bf16[1, 32, 512, 512]"
    # t287 = ltorch.matmul(t286, t223)  # t287: "cuda:0 bf16[1, 32, 512, 128]"
      # t287 = prims.matmul(t286, t223)  # t287: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t288 = ltorch.transpose(t287, 1, 2)  # t288: "cuda:0 bf16[1, 512, 32, 128]"
    # t288 = prims.transpose(t287, (0, 2, 1, 3))  # t288: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t289 = ltorch.reshape(t288, 1, 512, 4096)  # t289: "cuda:0 bf16[1, 512, 4096]"
    # t289 = prims.reshape(t288, (1, 512, 4096))  # t289: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t293 = ltorch.linear(t289, t_transformer_h_1_attn_proj_weight, None)  # t293: "cuda:0 bf16[1, 512, 4096]"
    # t293 = prims.linear(t289, t_transformer_h_1_attn_proj_weight, None)  # t293: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t297 = ltorch.add(t293, t187, alpha=None)  # t297: "cuda:0 bf16[1, 512, 4096]"
    # t294 = prims.convert_element_type(t293, dtypes.float32)  # t294: "cuda:0 f32[1, 512, 4096]"
    # t295 = prims.convert_element_type(t187, dtypes.float32)  # t295: "cuda:0 f32[1, 512, 4096]"
    # t296 = prims.add(t294, t295)  # t296: "cuda:0 f32[1, 512, 4096]"
    # t297 = prims.convert_element_type(t296, dtypes.bfloat16)  # t297: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t298 = prims.convert_element_type(t297, dtypes.float32)  # t298: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t299 = ltorch.mul(t298, t298)  # t299: "cuda:0 f32[1, 512, 4096]"
    # t299 = prims.mul(t298, t298)  # t299: "cuda:0 f32[1, 512, 4096]"
  t303 = ltorch.mean(t299, -1, True, dtype=None)  # t303: "cuda:0 f32[1, 512, 1]"
    # t301 = prims.sum(t299, (2,))  # t301: "cuda:0 f32[1, 512]"
    # t302 = prims.broadcast_in_dim(t301, [1, 512, 1], [0, 1])  # t302: "cuda:0 f32[1, 512, 1]"
    # t303 = ltorch.true_divide(t302, 4096)  # t303: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t303 = prims.div(t302, 4096.0)  # t303: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t305 = ltorch.add(t303, 1e-05, alpha=None)  # t305: "cuda:0 f32[1, 512, 1]"
    # t305 = prims.add(t303, 1e-05)  # t305: "cuda:0 f32[1, 512, 1]"
  t306 = ltorch.rsqrt(t305)  # t306: "cuda:0 f32[1, 512, 1]"
    # t306 = prims.rsqrt(t305)  # t306: "cuda:0 f32[1, 512, 1]"
  t308 = ltorch.mul(t298, t306)  # t308: "cuda:0 f32[1, 512, 4096]"
    # t307 = prims.broadcast_in_dim(t306, (1, 512, 4096), (0, 1, 2))  # t307: "cuda:0 f32[1, 512, 4096]"
    # t308 = prims.mul(t298, t307)  # t308: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t309 = ltorch.to(t308, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t309: "cuda:0 bf16[1, 512, 4096]"
    # t309 = prims.convert_element_type(t308, dtypes.bfloat16)  # t309: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t319 = ltorch.mul(t309, t_transformer_h_1_norm_2_weight)  # t319: "cuda:0 bf16[1, 512, 4096]"
    # t315 = prims.broadcast_in_dim(t_transformer_h_1_norm_2_weight, (1, 512, 4096), (2,))  # t315: "cuda:0 bf16[1, 512, 4096]"
    # t316 = prims.convert_element_type(t309, dtypes.float32)  # t316: "cuda:0 f32[1, 512, 4096]"
    # t317 = prims.convert_element_type(t315, dtypes.float32)  # t317: "cuda:0 f32[1, 512, 4096]"
    # t318 = prims.mul(t316, t317)  # t318: "cuda:0 f32[1, 512, 4096]"
    # t319 = prims.convert_element_type(t318, dtypes.bfloat16)  # t319: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t324 = ltorch.linear(t319, t_transformer_h_1_mlp_fc_1_weight, None)  # t324: "cuda:0 bf16[1, 512, 11008]"
    # t324 = prims.linear(t319, t_transformer_h_1_mlp_fc_1_weight, None)  # t324: "cuda:0 bf16[1, 512, 11008]"
  t328 = ltorch.linear(t319, t_transformer_h_1_mlp_fc_2_weight, None)  # t328: "cuda:0 bf16[1, 512, 11008]"
    # t328 = prims.linear(t319, t_transformer_h_1_mlp_fc_2_weight, None)  # t328: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t338 = ltorch.silu(t324, False)  # t338: "cuda:0 bf16[1, 512, 11008]"
    # t329 = prims.convert_element_type(t324, dtypes.float32)  # t329: "cuda:0 f32[1, 512, 11008]"
    # t330 = prims.neg(t329)  # t330: "cuda:0 f32[1, 512, 11008]"
    # t331 = prims.exp(t330)  # t331: "cuda:0 f32[1, 512, 11008]"
    # t332 = prims.add(1.0, t331)  # t332: "cuda:0 f32[1, 512, 11008]"
    # t333 = prims.reciprocal(t332)  # t333: "cuda:0 f32[1, 512, 11008]"
    # t334 = prims.convert_element_type(t333, dtypes.bfloat16)  # t334: "cuda:0 bf16[1, 512, 11008]"
    # t335 = prims.convert_element_type(t324, dtypes.float32)  # t335: "cuda:0 f32[1, 512, 11008]"
    # t336 = prims.convert_element_type(t334, dtypes.float32)  # t336: "cuda:0 f32[1, 512, 11008]"
    # t337 = prims.mul(t335, t336)  # t337: "cuda:0 f32[1, 512, 11008]"
    # t338 = prims.convert_element_type(t337, dtypes.bfloat16)  # t338: "cuda:0 bf16[1, 512, 11008]"
  t342 = ltorch.mul(t338, t328)  # t342: "cuda:0 bf16[1, 512, 11008]"
    # t339 = prims.convert_element_type(t338, dtypes.float32)  # t339: "cuda:0 f32[1, 512, 11008]"
    # t340 = prims.convert_element_type(t328, dtypes.float32)  # t340: "cuda:0 f32[1, 512, 11008]"
    # t341 = prims.mul(t339, t340)  # t341: "cuda:0 f32[1, 512, 11008]"
    # t342 = prims.convert_element_type(t341, dtypes.bfloat16)  # t342: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t346 = ltorch.linear(t342, t_transformer_h_1_mlp_proj_weight, None)  # t346: "cuda:0 bf16[1, 512, 4096]"
    # t346 = prims.linear(t342, t_transformer_h_1_mlp_proj_weight, None)  # t346: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t350 = ltorch.add(t346, t297, alpha=None)  # t350: "cuda:0 bf16[1, 512, 4096]"
    # t347 = prims.convert_element_type(t346, dtypes.float32)  # t347: "cuda:0 f32[1, 512, 4096]"
    # t348 = prims.convert_element_type(t297, dtypes.float32)  # t348: "cuda:0 f32[1, 512, 4096]"
    # t349 = prims.add(t347, t348)  # t349: "cuda:0 f32[1, 512, 4096]"
    # t350 = prims.convert_element_type(t349, dtypes.bfloat16)  # t350: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t352 = prims.convert_element_type(t350, dtypes.float32)  # t352: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t353 = ltorch.mul(t352, t352)  # t353: "cuda:0 f32[1, 512, 4096]"
    # t353 = prims.mul(t352, t352)  # t353: "cuda:0 f32[1, 512, 4096]"
  t357 = ltorch.mean(t353, -1, True, dtype=None)  # t357: "cuda:0 f32[1, 512, 1]"
    # t355 = prims.sum(t353, (2,))  # t355: "cuda:0 f32[1, 512]"
    # t356 = prims.broadcast_in_dim(t355, [1, 512, 1], [0, 1])  # t356: "cuda:0 f32[1, 512, 1]"
    # t357 = ltorch.true_divide(t356, 4096)  # t357: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t357 = prims.div(t356, 4096.0)  # t357: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t359 = ltorch.add(t357, 1e-05, alpha=None)  # t359: "cuda:0 f32[1, 512, 1]"
    # t359 = prims.add(t357, 1e-05)  # t359: "cuda:0 f32[1, 512, 1]"
  t360 = ltorch.rsqrt(t359)  # t360: "cuda:0 f32[1, 512, 1]"
    # t360 = prims.rsqrt(t359)  # t360: "cuda:0 f32[1, 512, 1]"
  t362 = ltorch.mul(t352, t360)  # t362: "cuda:0 f32[1, 512, 4096]"
    # t361 = prims.broadcast_in_dim(t360, (1, 512, 4096), (0, 1, 2))  # t361: "cuda:0 f32[1, 512, 4096]"
    # t362 = prims.mul(t352, t361)  # t362: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t363 = ltorch.to(t362, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t363: "cuda:0 bf16[1, 512, 4096]"
    # t363 = prims.convert_element_type(t362, dtypes.bfloat16)  # t363: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t373 = ltorch.mul(t363, t_transformer_h_2_norm_1_weight)  # t373: "cuda:0 bf16[1, 512, 4096]"
    # t369 = prims.broadcast_in_dim(t_transformer_h_2_norm_1_weight, (1, 512, 4096), (2,))  # t369: "cuda:0 bf16[1, 512, 4096]"
    # t370 = prims.convert_element_type(t363, dtypes.float32)  # t370: "cuda:0 f32[1, 512, 4096]"
    # t371 = prims.convert_element_type(t369, dtypes.float32)  # t371: "cuda:0 f32[1, 512, 4096]"
    # t372 = prims.mul(t370, t371)  # t372: "cuda:0 f32[1, 512, 4096]"
    # t373 = prims.convert_element_type(t372, dtypes.bfloat16)  # t373: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t378 = ltorch.linear(t373, t_transformer_h_2_attn_attn_weight, None)  # t378: "cuda:0 bf16[1, 512, 12288]"
    # t378 = prims.linear(t373, t_transformer_h_2_attn_attn_weight, None)  # t378: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t379 = ltorch.view(t378, 1, 512, 32, 3, 128)  # t379: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t379 = ltorch.reshape(t378, (1, 512, 32, 3, 128))  # t379: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t379 = prims.reshape(t378, (1, 512, 32, 3, 128))  # t379: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t380 = ltorch.permute(t379, 0, 2, 3, 1, 4)  # t380: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t380 = prims.transpose(t379, (0, 2, 3, 1, 4))  # t380: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t381, t382, t383) = ltorch.split(t380, (1, 1, 1), 2)
    # t381 = prims.slice_prim(t380, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t381: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t382 = prims.slice_prim(t380, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t382: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t383 = prims.slice_prim(t380, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t383: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t384 = ltorch.reshape(t381, 1, -1, 512, 128)  # t384: "cuda:0 bf16[1, 32, 512, 128]"
    # t384 = prims.reshape(t381, (1, 32, 512, 128))  # t384: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t385 = ltorch.reshape(t382, 1, -1, 512, 128)  # t385: "cuda:0 bf16[1, 32, 512, 128]"
    # t385 = prims.reshape(t382, (1, 32, 512, 128))  # t385: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t386 = ltorch.reshape(t383, 1, -1, 512, 128)  # t386: "cuda:0 bf16[1, 32, 512, 128]"
    # t386 = prims.reshape(t383, (1, 32, 512, 128))  # t386: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t387 = ltorch.getitem(t384, (..., slice(None, 128, None)))  # t387: "cuda:0 bf16[1, 32, 512, 128]"
    # t387 = prims.slice_prim(t384, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t387: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t388 = ltorch.getitem(t387, (..., slice(None, 64, None)))  # t388: "cuda:0 bf16[1, 32, 512, 64]"
    # t388 = prims.slice_prim(t387, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t388: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t389 = ltorch.getitem(t387, (..., slice(64, None, None)))  # t389: "cuda:0 bf16[1, 32, 512, 64]"
    # t389 = prims.slice_prim(t387, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t389: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t392 = ltorch.neg(t389)  # t392: "cuda:0 bf16[1, 32, 512, 64]"
    # t390 = prims.convert_element_type(t389, dtypes.float32)  # t390: "cuda:0 f32[1, 32, 512, 64]"
    # t391 = prims.neg(t390)  # t391: "cuda:0 f32[1, 32, 512, 64]"
    # t392 = prims.convert_element_type(t391, dtypes.bfloat16)  # t392: "cuda:0 bf16[1, 32, 512, 64]"
  t393 = ltorch.cat((t392, t388), -1)  # t393: "cuda:0 bf16[1, 32, 512, 128]"
    # t393 = prims.cat((t392, t388), -1)  # t393: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t396 = ltorch.mul(t387, cos)  # t396: "cuda:0 f32[1, 32, 512, 128]"
    # t394 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t394: "cuda:0 f32[1, 32, 512, 128]"
    # t395 = prims.convert_element_type(t387, dtypes.float32)  # t395: "cuda:0 f32[1, 32, 512, 128]"
    # t396 = prims.mul(t395, t394)  # t396: "cuda:0 f32[1, 32, 512, 128]"
  t399 = ltorch.mul(t393, sin)  # t399: "cuda:0 f32[1, 32, 512, 128]"
    # t397 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t397: "cuda:0 f32[1, 32, 512, 128]"
    # t398 = prims.convert_element_type(t393, dtypes.float32)  # t398: "cuda:0 f32[1, 32, 512, 128]"
    # t399 = prims.mul(t398, t397)  # t399: "cuda:0 f32[1, 32, 512, 128]"
  t400 = ltorch.add(t396, t399, alpha=None)  # t400: "cuda:0 f32[1, 32, 512, 128]"
    # t400 = prims.add(t396, t399)  # t400: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t401 = ltorch.to(t400, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t401: "cuda:0 bf16[1, 32, 512, 128]"
    # t401 = prims.convert_element_type(t400, dtypes.bfloat16)  # t401: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t402 = ltorch.getitem(t385, (..., slice(None, 128, None)))  # t402: "cuda:0 bf16[1, 32, 512, 128]"
    # t402 = prims.slice_prim(t385, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t402: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t403 = ltorch.getitem(t402, (..., slice(None, 64, None)))  # t403: "cuda:0 bf16[1, 32, 512, 64]"
    # t403 = prims.slice_prim(t402, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t403: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t404 = ltorch.getitem(t402, (..., slice(64, None, None)))  # t404: "cuda:0 bf16[1, 32, 512, 64]"
    # t404 = prims.slice_prim(t402, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t404: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t407 = ltorch.neg(t404)  # t407: "cuda:0 bf16[1, 32, 512, 64]"
    # t405 = prims.convert_element_type(t404, dtypes.float32)  # t405: "cuda:0 f32[1, 32, 512, 64]"
    # t406 = prims.neg(t405)  # t406: "cuda:0 f32[1, 32, 512, 64]"
    # t407 = prims.convert_element_type(t406, dtypes.bfloat16)  # t407: "cuda:0 bf16[1, 32, 512, 64]"
  t408 = ltorch.cat((t407, t403), -1)  # t408: "cuda:0 bf16[1, 32, 512, 128]"
    # t408 = prims.cat((t407, t403), -1)  # t408: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t411 = ltorch.mul(t402, cos)  # t411: "cuda:0 f32[1, 32, 512, 128]"
    # t409 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t409: "cuda:0 f32[1, 32, 512, 128]"
    # t410 = prims.convert_element_type(t402, dtypes.float32)  # t410: "cuda:0 f32[1, 32, 512, 128]"
    # t411 = prims.mul(t410, t409)  # t411: "cuda:0 f32[1, 32, 512, 128]"
  t414 = ltorch.mul(t408, sin)  # t414: "cuda:0 f32[1, 32, 512, 128]"
    # t412 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t412: "cuda:0 f32[1, 32, 512, 128]"
    # t413 = prims.convert_element_type(t408, dtypes.float32)  # t413: "cuda:0 f32[1, 32, 512, 128]"
    # t414 = prims.mul(t413, t412)  # t414: "cuda:0 f32[1, 32, 512, 128]"
  t415 = ltorch.add(t411, t414, alpha=None)  # t415: "cuda:0 f32[1, 32, 512, 128]"
    # t415 = prims.add(t411, t414)  # t415: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t416 = ltorch.to(t415, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t416: "cuda:0 bf16[1, 32, 512, 128]"
    # t416 = prims.convert_element_type(t415, dtypes.bfloat16)  # t416: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t417 = ltorch.getitem(t384, (..., slice(128, None, None)))  # t417: "cuda:0 bf16[1, 32, 512, 0]"
    # t417 = prims.slice_prim(t384, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t417: "cuda:0 bf16[1, 32, 512, 0]"
  t418 = ltorch.cat((t401, t417), -1)  # t418: "cuda:0 bf16[1, 32, 512, 128]"
    # t418 = prims.cat((t401, t417), -1)  # t418: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t419 = ltorch.getitem(t385, (..., slice(128, None, None)))  # t419: "cuda:0 bf16[1, 32, 512, 0]"
    # t419 = prims.slice_prim(t385, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t419: "cuda:0 bf16[1, 32, 512, 0]"
  t420 = ltorch.cat((t416, t419), -1)  # t420: "cuda:0 bf16[1, 32, 512, 128]"
    # t420 = prims.cat((t416, t419), -1)  # t420: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t450 = ltorch.scaled_dot_product_attention(t418, t420, t386, None, 0.0, True, scale=0.08838834764831843)  # t450: "cuda:0 bf16[1, 32, 512, 128]"
    # t423 = ltorch.mul(t418, 0.29730177875068026)  # t423: "cuda:0 bf16[1, 32, 512, 128]"
      # t421 = prims.convert_element_type(t418, dtypes.float32)  # t421: "cuda:0 f32[1, 32, 512, 128]"
      # t422 = prims.mul(t421, 0.29730177875068026)  # t422: "cuda:0 f32[1, 32, 512, 128]"
      # t423 = prims.convert_element_type(t422, dtypes.bfloat16)  # t423: "cuda:0 bf16[1, 32, 512, 128]"
    # t424 = ltorch.transpose(t420, -2, -1)  # t424: "cuda:0 bf16[1, 32, 128, 512]"
      # t424 = prims.transpose(t420, (0, 1, 3, 2))  # t424: "cuda:0 bf16[1, 32, 128, 512]"
    # t427 = ltorch.mul(t424, 0.29730177875068026)  # t427: "cuda:0 bf16[1, 32, 128, 512]"
      # t425 = prims.convert_element_type(t424, dtypes.float32)  # t425: "cuda:0 f32[1, 32, 128, 512]"
      # t426 = prims.mul(t425, 0.29730177875068026)  # t426: "cuda:0 f32[1, 32, 128, 512]"
      # t427 = prims.convert_element_type(t426, dtypes.bfloat16)  # t427: "cuda:0 bf16[1, 32, 128, 512]"
    # t428 = ltorch.matmul(t423, t427)  # t428: "cuda:0 bf16[1, 32, 512, 512]"
      # t428 = prims.matmul(t423, t427)  # t428: "cuda:0 bf16[1, 32, 512, 512]"
    # t438 = ltorch.tril(t428, 0, fill_value=-float('inf'))  # t438: "cuda:0 bf16[1, 32, 512, 512]"
      # t429 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t429: "cuda:0 i64[512]"
        # t429 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t429: "cuda:0 i64[512]"
      # t430 = ltorch.unsqueeze(t429, -1)  # t430: "cuda:0 i64[512, 1]"
        # t430 = prims.broadcast_in_dim(t429, [512, 1], [0])  # t430: "cuda:0 i64[512, 1]"
      # t431 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t431: "cuda:0 i64[512]"
        # t431 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t431: "cuda:0 i64[512]"
      # t432 = ltorch.unsqueeze(t431, -2)  # t432: "cuda:0 i64[1, 512]"
        # t432 = prims.broadcast_in_dim(t431, [1, 512], [1])  # t432: "cuda:0 i64[1, 512]"
      # t433 = ltorch.add(t430, 0, alpha=None)  # t433: "cuda:0 i64[512, 1]"
        # t433 = prims.add(t430, 0)  # t433: "cuda:0 i64[512, 1]"
      # t436 = ltorch.ge(t433, t432)  # t436: "cuda:0 b8[512, 512]"
        # t434 = prims.broadcast_in_dim(t433, (512, 512), (0, 1))  # t434: "cuda:0 i64[512, 512]"
        # t435 = prims.broadcast_in_dim(t432, (512, 512), (0, 1))  # t435: "cuda:0 i64[512, 512]"
        # t436 = prims.ge(t434, t435)  # t436: "cuda:0 b8[512, 512]"
      # t438 = ltorch.where(t436, t428, -float('inf'))  # t438: "cuda:0 bf16[1, 32, 512, 512]"
        # t437 = prims.broadcast_in_dim(t436, (1, 32, 512, 512), (2, 3))  # t437: "cuda:0 b8[1, 32, 512, 512]"
        # t438 = prims.where(t437, t428, -float('inf'))  # t438: "cuda:0 bf16[1, 32, 512, 512]"
    # t449 = ltorch._softmax(t438, -1, dtype=None)  # t449: "cuda:0 bf16[1, 32, 512, 512]"
      # t439 = ltorch.to(t438, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t439: "cuda:0 f32[1, 32, 512, 512]"
        # t439 = prims.convert_element_type(t438, dtypes.float32)  # t439: "cuda:0 f32[1, 32, 512, 512]"
      # t441 = ltorch.amax(t439, -1, True)  # t441: "cuda:0 f32[1, 32, 512, 1]"
        # t440 = prims.amax(t439, (3,))  # t440: "cuda:0 f32[1, 32, 512]"
        # t441 = prims.broadcast_in_dim(t440, [1, 32, 512, 1], [0, 1, 2])  # t441: "cuda:0 f32[1, 32, 512, 1]"
      # t443 = ltorch.sub(t439, t441, alpha=None)  # t443: "cuda:0 f32[1, 32, 512, 512]"
        # t442 = prims.broadcast_in_dim(t441, (1, 32, 512, 512), (0, 1, 2, 3))  # t442: "cuda:0 f32[1, 32, 512, 512]"
        # t443 = prims.sub(t439, t442)  # t443: "cuda:0 f32[1, 32, 512, 512]"
      # t444 = ltorch.exp(t443)  # t444: "cuda:0 f32[1, 32, 512, 512]"
        # t444 = prims.exp(t443)  # t444: "cuda:0 f32[1, 32, 512, 512]"
      # t446 = ltorch.sum(t444, -1, True, dtype=None)  # t446: "cuda:0 f32[1, 32, 512, 1]"
        # t445 = prims.sum(t444, (3,))  # t445: "cuda:0 f32[1, 32, 512]"
        # t446 = prims.broadcast_in_dim(t445, [1, 32, 512, 1], [0, 1, 2])  # t446: "cuda:0 f32[1, 32, 512, 1]"
      # t448 = ltorch.true_divide(t444, t446)  # t448: "cuda:0 f32[1, 32, 512, 512]"
        # t447 = prims.broadcast_in_dim(t446, (1, 32, 512, 512), (0, 1, 2, 3))  # t447: "cuda:0 f32[1, 32, 512, 512]"
        # t448 = prims.div(t444, t447)  # t448: "cuda:0 f32[1, 32, 512, 512]"
      # t449 = ltorch.to(t448, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t449: "cuda:0 bf16[1, 32, 512, 512]"
        # t449 = prims.convert_element_type(t448, dtypes.bfloat16)  # t449: "cuda:0 bf16[1, 32, 512, 512]"
    # t450 = ltorch.matmul(t449, t386)  # t450: "cuda:0 bf16[1, 32, 512, 128]"
      # t450 = prims.matmul(t449, t386)  # t450: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t451 = ltorch.transpose(t450, 1, 2)  # t451: "cuda:0 bf16[1, 512, 32, 128]"
    # t451 = prims.transpose(t450, (0, 2, 1, 3))  # t451: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t452 = ltorch.reshape(t451, 1, 512, 4096)  # t452: "cuda:0 bf16[1, 512, 4096]"
    # t452 = prims.reshape(t451, (1, 512, 4096))  # t452: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t456 = ltorch.linear(t452, t_transformer_h_2_attn_proj_weight, None)  # t456: "cuda:0 bf16[1, 512, 4096]"
    # t456 = prims.linear(t452, t_transformer_h_2_attn_proj_weight, None)  # t456: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t460 = ltorch.add(t456, t350, alpha=None)  # t460: "cuda:0 bf16[1, 512, 4096]"
    # t457 = prims.convert_element_type(t456, dtypes.float32)  # t457: "cuda:0 f32[1, 512, 4096]"
    # t458 = prims.convert_element_type(t350, dtypes.float32)  # t458: "cuda:0 f32[1, 512, 4096]"
    # t459 = prims.add(t457, t458)  # t459: "cuda:0 f32[1, 512, 4096]"
    # t460 = prims.convert_element_type(t459, dtypes.bfloat16)  # t460: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t461 = prims.convert_element_type(t460, dtypes.float32)  # t461: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t462 = ltorch.mul(t461, t461)  # t462: "cuda:0 f32[1, 512, 4096]"
    # t462 = prims.mul(t461, t461)  # t462: "cuda:0 f32[1, 512, 4096]"
  t466 = ltorch.mean(t462, -1, True, dtype=None)  # t466: "cuda:0 f32[1, 512, 1]"
    # t464 = prims.sum(t462, (2,))  # t464: "cuda:0 f32[1, 512]"
    # t465 = prims.broadcast_in_dim(t464, [1, 512, 1], [0, 1])  # t465: "cuda:0 f32[1, 512, 1]"
    # t466 = ltorch.true_divide(t465, 4096)  # t466: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t466 = prims.div(t465, 4096.0)  # t466: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t468 = ltorch.add(t466, 1e-05, alpha=None)  # t468: "cuda:0 f32[1, 512, 1]"
    # t468 = prims.add(t466, 1e-05)  # t468: "cuda:0 f32[1, 512, 1]"
  t469 = ltorch.rsqrt(t468)  # t469: "cuda:0 f32[1, 512, 1]"
    # t469 = prims.rsqrt(t468)  # t469: "cuda:0 f32[1, 512, 1]"
  t471 = ltorch.mul(t461, t469)  # t471: "cuda:0 f32[1, 512, 4096]"
    # t470 = prims.broadcast_in_dim(t469, (1, 512, 4096), (0, 1, 2))  # t470: "cuda:0 f32[1, 512, 4096]"
    # t471 = prims.mul(t461, t470)  # t471: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t472 = ltorch.to(t471, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t472: "cuda:0 bf16[1, 512, 4096]"
    # t472 = prims.convert_element_type(t471, dtypes.bfloat16)  # t472: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t482 = ltorch.mul(t472, t_transformer_h_2_norm_2_weight)  # t482: "cuda:0 bf16[1, 512, 4096]"
    # t478 = prims.broadcast_in_dim(t_transformer_h_2_norm_2_weight, (1, 512, 4096), (2,))  # t478: "cuda:0 bf16[1, 512, 4096]"
    # t479 = prims.convert_element_type(t472, dtypes.float32)  # t479: "cuda:0 f32[1, 512, 4096]"
    # t480 = prims.convert_element_type(t478, dtypes.float32)  # t480: "cuda:0 f32[1, 512, 4096]"
    # t481 = prims.mul(t479, t480)  # t481: "cuda:0 f32[1, 512, 4096]"
    # t482 = prims.convert_element_type(t481, dtypes.bfloat16)  # t482: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t487 = ltorch.linear(t482, t_transformer_h_2_mlp_fc_1_weight, None)  # t487: "cuda:0 bf16[1, 512, 11008]"
    # t487 = prims.linear(t482, t_transformer_h_2_mlp_fc_1_weight, None)  # t487: "cuda:0 bf16[1, 512, 11008]"
  t491 = ltorch.linear(t482, t_transformer_h_2_mlp_fc_2_weight, None)  # t491: "cuda:0 bf16[1, 512, 11008]"
    # t491 = prims.linear(t482, t_transformer_h_2_mlp_fc_2_weight, None)  # t491: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t501 = ltorch.silu(t487, False)  # t501: "cuda:0 bf16[1, 512, 11008]"
    # t492 = prims.convert_element_type(t487, dtypes.float32)  # t492: "cuda:0 f32[1, 512, 11008]"
    # t493 = prims.neg(t492)  # t493: "cuda:0 f32[1, 512, 11008]"
    # t494 = prims.exp(t493)  # t494: "cuda:0 f32[1, 512, 11008]"
    # t495 = prims.add(1.0, t494)  # t495: "cuda:0 f32[1, 512, 11008]"
    # t496 = prims.reciprocal(t495)  # t496: "cuda:0 f32[1, 512, 11008]"
    # t497 = prims.convert_element_type(t496, dtypes.bfloat16)  # t497: "cuda:0 bf16[1, 512, 11008]"
    # t498 = prims.convert_element_type(t487, dtypes.float32)  # t498: "cuda:0 f32[1, 512, 11008]"
    # t499 = prims.convert_element_type(t497, dtypes.float32)  # t499: "cuda:0 f32[1, 512, 11008]"
    # t500 = prims.mul(t498, t499)  # t500: "cuda:0 f32[1, 512, 11008]"
    # t501 = prims.convert_element_type(t500, dtypes.bfloat16)  # t501: "cuda:0 bf16[1, 512, 11008]"
  t505 = ltorch.mul(t501, t491)  # t505: "cuda:0 bf16[1, 512, 11008]"
    # t502 = prims.convert_element_type(t501, dtypes.float32)  # t502: "cuda:0 f32[1, 512, 11008]"
    # t503 = prims.convert_element_type(t491, dtypes.float32)  # t503: "cuda:0 f32[1, 512, 11008]"
    # t504 = prims.mul(t502, t503)  # t504: "cuda:0 f32[1, 512, 11008]"
    # t505 = prims.convert_element_type(t504, dtypes.bfloat16)  # t505: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t509 = ltorch.linear(t505, t_transformer_h_2_mlp_proj_weight, None)  # t509: "cuda:0 bf16[1, 512, 4096]"
    # t509 = prims.linear(t505, t_transformer_h_2_mlp_proj_weight, None)  # t509: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t513 = ltorch.add(t509, t460, alpha=None)  # t513: "cuda:0 bf16[1, 512, 4096]"
    # t510 = prims.convert_element_type(t509, dtypes.float32)  # t510: "cuda:0 f32[1, 512, 4096]"
    # t511 = prims.convert_element_type(t460, dtypes.float32)  # t511: "cuda:0 f32[1, 512, 4096]"
    # t512 = prims.add(t510, t511)  # t512: "cuda:0 f32[1, 512, 4096]"
    # t513 = prims.convert_element_type(t512, dtypes.bfloat16)  # t513: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t515 = prims.convert_element_type(t513, dtypes.float32)  # t515: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t516 = ltorch.mul(t515, t515)  # t516: "cuda:0 f32[1, 512, 4096]"
    # t516 = prims.mul(t515, t515)  # t516: "cuda:0 f32[1, 512, 4096]"
  t520 = ltorch.mean(t516, -1, True, dtype=None)  # t520: "cuda:0 f32[1, 512, 1]"
    # t518 = prims.sum(t516, (2,))  # t518: "cuda:0 f32[1, 512]"
    # t519 = prims.broadcast_in_dim(t518, [1, 512, 1], [0, 1])  # t519: "cuda:0 f32[1, 512, 1]"
    # t520 = ltorch.true_divide(t519, 4096)  # t520: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t520 = prims.div(t519, 4096.0)  # t520: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t522 = ltorch.add(t520, 1e-05, alpha=None)  # t522: "cuda:0 f32[1, 512, 1]"
    # t522 = prims.add(t520, 1e-05)  # t522: "cuda:0 f32[1, 512, 1]"
  t523 = ltorch.rsqrt(t522)  # t523: "cuda:0 f32[1, 512, 1]"
    # t523 = prims.rsqrt(t522)  # t523: "cuda:0 f32[1, 512, 1]"
  t525 = ltorch.mul(t515, t523)  # t525: "cuda:0 f32[1, 512, 4096]"
    # t524 = prims.broadcast_in_dim(t523, (1, 512, 4096), (0, 1, 2))  # t524: "cuda:0 f32[1, 512, 4096]"
    # t525 = prims.mul(t515, t524)  # t525: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t526 = ltorch.to(t525, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t526: "cuda:0 bf16[1, 512, 4096]"
    # t526 = prims.convert_element_type(t525, dtypes.bfloat16)  # t526: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t536 = ltorch.mul(t526, t_transformer_h_3_norm_1_weight)  # t536: "cuda:0 bf16[1, 512, 4096]"
    # t532 = prims.broadcast_in_dim(t_transformer_h_3_norm_1_weight, (1, 512, 4096), (2,))  # t532: "cuda:0 bf16[1, 512, 4096]"
    # t533 = prims.convert_element_type(t526, dtypes.float32)  # t533: "cuda:0 f32[1, 512, 4096]"
    # t534 = prims.convert_element_type(t532, dtypes.float32)  # t534: "cuda:0 f32[1, 512, 4096]"
    # t535 = prims.mul(t533, t534)  # t535: "cuda:0 f32[1, 512, 4096]"
    # t536 = prims.convert_element_type(t535, dtypes.bfloat16)  # t536: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t541 = ltorch.linear(t536, t_transformer_h_3_attn_attn_weight, None)  # t541: "cuda:0 bf16[1, 512, 12288]"
    # t541 = prims.linear(t536, t_transformer_h_3_attn_attn_weight, None)  # t541: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t542 = ltorch.view(t541, 1, 512, 32, 3, 128)  # t542: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t542 = ltorch.reshape(t541, (1, 512, 32, 3, 128))  # t542: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t542 = prims.reshape(t541, (1, 512, 32, 3, 128))  # t542: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t543 = ltorch.permute(t542, 0, 2, 3, 1, 4)  # t543: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t543 = prims.transpose(t542, (0, 2, 3, 1, 4))  # t543: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t544, t545, t546) = ltorch.split(t543, (1, 1, 1), 2)
    # t544 = prims.slice_prim(t543, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t544: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t545 = prims.slice_prim(t543, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t545: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t546 = prims.slice_prim(t543, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t546: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t547 = ltorch.reshape(t544, 1, -1, 512, 128)  # t547: "cuda:0 bf16[1, 32, 512, 128]"
    # t547 = prims.reshape(t544, (1, 32, 512, 128))  # t547: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t548 = ltorch.reshape(t545, 1, -1, 512, 128)  # t548: "cuda:0 bf16[1, 32, 512, 128]"
    # t548 = prims.reshape(t545, (1, 32, 512, 128))  # t548: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t549 = ltorch.reshape(t546, 1, -1, 512, 128)  # t549: "cuda:0 bf16[1, 32, 512, 128]"
    # t549 = prims.reshape(t546, (1, 32, 512, 128))  # t549: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t550 = ltorch.getitem(t547, (..., slice(None, 128, None)))  # t550: "cuda:0 bf16[1, 32, 512, 128]"
    # t550 = prims.slice_prim(t547, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t550: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t551 = ltorch.getitem(t550, (..., slice(None, 64, None)))  # t551: "cuda:0 bf16[1, 32, 512, 64]"
    # t551 = prims.slice_prim(t550, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t551: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t552 = ltorch.getitem(t550, (..., slice(64, None, None)))  # t552: "cuda:0 bf16[1, 32, 512, 64]"
    # t552 = prims.slice_prim(t550, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t552: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t555 = ltorch.neg(t552)  # t555: "cuda:0 bf16[1, 32, 512, 64]"
    # t553 = prims.convert_element_type(t552, dtypes.float32)  # t553: "cuda:0 f32[1, 32, 512, 64]"
    # t554 = prims.neg(t553)  # t554: "cuda:0 f32[1, 32, 512, 64]"
    # t555 = prims.convert_element_type(t554, dtypes.bfloat16)  # t555: "cuda:0 bf16[1, 32, 512, 64]"
  t556 = ltorch.cat((t555, t551), -1)  # t556: "cuda:0 bf16[1, 32, 512, 128]"
    # t556 = prims.cat((t555, t551), -1)  # t556: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t559 = ltorch.mul(t550, cos)  # t559: "cuda:0 f32[1, 32, 512, 128]"
    # t557 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t557: "cuda:0 f32[1, 32, 512, 128]"
    # t558 = prims.convert_element_type(t550, dtypes.float32)  # t558: "cuda:0 f32[1, 32, 512, 128]"
    # t559 = prims.mul(t558, t557)  # t559: "cuda:0 f32[1, 32, 512, 128]"
  t562 = ltorch.mul(t556, sin)  # t562: "cuda:0 f32[1, 32, 512, 128]"
    # t560 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t560: "cuda:0 f32[1, 32, 512, 128]"
    # t561 = prims.convert_element_type(t556, dtypes.float32)  # t561: "cuda:0 f32[1, 32, 512, 128]"
    # t562 = prims.mul(t561, t560)  # t562: "cuda:0 f32[1, 32, 512, 128]"
  t563 = ltorch.add(t559, t562, alpha=None)  # t563: "cuda:0 f32[1, 32, 512, 128]"
    # t563 = prims.add(t559, t562)  # t563: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t564 = ltorch.to(t563, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t564: "cuda:0 bf16[1, 32, 512, 128]"
    # t564 = prims.convert_element_type(t563, dtypes.bfloat16)  # t564: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t565 = ltorch.getitem(t548, (..., slice(None, 128, None)))  # t565: "cuda:0 bf16[1, 32, 512, 128]"
    # t565 = prims.slice_prim(t548, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t565: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t566 = ltorch.getitem(t565, (..., slice(None, 64, None)))  # t566: "cuda:0 bf16[1, 32, 512, 64]"
    # t566 = prims.slice_prim(t565, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t566: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t567 = ltorch.getitem(t565, (..., slice(64, None, None)))  # t567: "cuda:0 bf16[1, 32, 512, 64]"
    # t567 = prims.slice_prim(t565, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t567: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t570 = ltorch.neg(t567)  # t570: "cuda:0 bf16[1, 32, 512, 64]"
    # t568 = prims.convert_element_type(t567, dtypes.float32)  # t568: "cuda:0 f32[1, 32, 512, 64]"
    # t569 = prims.neg(t568)  # t569: "cuda:0 f32[1, 32, 512, 64]"
    # t570 = prims.convert_element_type(t569, dtypes.bfloat16)  # t570: "cuda:0 bf16[1, 32, 512, 64]"
  t571 = ltorch.cat((t570, t566), -1)  # t571: "cuda:0 bf16[1, 32, 512, 128]"
    # t571 = prims.cat((t570, t566), -1)  # t571: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t574 = ltorch.mul(t565, cos)  # t574: "cuda:0 f32[1, 32, 512, 128]"
    # t572 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t572: "cuda:0 f32[1, 32, 512, 128]"
    # t573 = prims.convert_element_type(t565, dtypes.float32)  # t573: "cuda:0 f32[1, 32, 512, 128]"
    # t574 = prims.mul(t573, t572)  # t574: "cuda:0 f32[1, 32, 512, 128]"
  t577 = ltorch.mul(t571, sin)  # t577: "cuda:0 f32[1, 32, 512, 128]"
    # t575 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t575: "cuda:0 f32[1, 32, 512, 128]"
    # t576 = prims.convert_element_type(t571, dtypes.float32)  # t576: "cuda:0 f32[1, 32, 512, 128]"
    # t577 = prims.mul(t576, t575)  # t577: "cuda:0 f32[1, 32, 512, 128]"
  t578 = ltorch.add(t574, t577, alpha=None)  # t578: "cuda:0 f32[1, 32, 512, 128]"
    # t578 = prims.add(t574, t577)  # t578: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t579 = ltorch.to(t578, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t579: "cuda:0 bf16[1, 32, 512, 128]"
    # t579 = prims.convert_element_type(t578, dtypes.bfloat16)  # t579: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t580 = ltorch.getitem(t547, (..., slice(128, None, None)))  # t580: "cuda:0 bf16[1, 32, 512, 0]"
    # t580 = prims.slice_prim(t547, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t580: "cuda:0 bf16[1, 32, 512, 0]"
  t581 = ltorch.cat((t564, t580), -1)  # t581: "cuda:0 bf16[1, 32, 512, 128]"
    # t581 = prims.cat((t564, t580), -1)  # t581: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t582 = ltorch.getitem(t548, (..., slice(128, None, None)))  # t582: "cuda:0 bf16[1, 32, 512, 0]"
    # t582 = prims.slice_prim(t548, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t582: "cuda:0 bf16[1, 32, 512, 0]"
  t583 = ltorch.cat((t579, t582), -1)  # t583: "cuda:0 bf16[1, 32, 512, 128]"
    # t583 = prims.cat((t579, t582), -1)  # t583: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t613 = ltorch.scaled_dot_product_attention(t581, t583, t549, None, 0.0, True, scale=0.08838834764831843)  # t613: "cuda:0 bf16[1, 32, 512, 128]"
    # t586 = ltorch.mul(t581, 0.29730177875068026)  # t586: "cuda:0 bf16[1, 32, 512, 128]"
      # t584 = prims.convert_element_type(t581, dtypes.float32)  # t584: "cuda:0 f32[1, 32, 512, 128]"
      # t585 = prims.mul(t584, 0.29730177875068026)  # t585: "cuda:0 f32[1, 32, 512, 128]"
      # t586 = prims.convert_element_type(t585, dtypes.bfloat16)  # t586: "cuda:0 bf16[1, 32, 512, 128]"
    # t587 = ltorch.transpose(t583, -2, -1)  # t587: "cuda:0 bf16[1, 32, 128, 512]"
      # t587 = prims.transpose(t583, (0, 1, 3, 2))  # t587: "cuda:0 bf16[1, 32, 128, 512]"
    # t590 = ltorch.mul(t587, 0.29730177875068026)  # t590: "cuda:0 bf16[1, 32, 128, 512]"
      # t588 = prims.convert_element_type(t587, dtypes.float32)  # t588: "cuda:0 f32[1, 32, 128, 512]"
      # t589 = prims.mul(t588, 0.29730177875068026)  # t589: "cuda:0 f32[1, 32, 128, 512]"
      # t590 = prims.convert_element_type(t589, dtypes.bfloat16)  # t590: "cuda:0 bf16[1, 32, 128, 512]"
    # t591 = ltorch.matmul(t586, t590)  # t591: "cuda:0 bf16[1, 32, 512, 512]"
      # t591 = prims.matmul(t586, t590)  # t591: "cuda:0 bf16[1, 32, 512, 512]"
    # t601 = ltorch.tril(t591, 0, fill_value=-float('inf'))  # t601: "cuda:0 bf16[1, 32, 512, 512]"
      # t592 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t592: "cuda:0 i64[512]"
        # t592 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t592: "cuda:0 i64[512]"
      # t593 = ltorch.unsqueeze(t592, -1)  # t593: "cuda:0 i64[512, 1]"
        # t593 = prims.broadcast_in_dim(t592, [512, 1], [0])  # t593: "cuda:0 i64[512, 1]"
      # t594 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t594: "cuda:0 i64[512]"
        # t594 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t594: "cuda:0 i64[512]"
      # t595 = ltorch.unsqueeze(t594, -2)  # t595: "cuda:0 i64[1, 512]"
        # t595 = prims.broadcast_in_dim(t594, [1, 512], [1])  # t595: "cuda:0 i64[1, 512]"
      # t596 = ltorch.add(t593, 0, alpha=None)  # t596: "cuda:0 i64[512, 1]"
        # t596 = prims.add(t593, 0)  # t596: "cuda:0 i64[512, 1]"
      # t599 = ltorch.ge(t596, t595)  # t599: "cuda:0 b8[512, 512]"
        # t597 = prims.broadcast_in_dim(t596, (512, 512), (0, 1))  # t597: "cuda:0 i64[512, 512]"
        # t598 = prims.broadcast_in_dim(t595, (512, 512), (0, 1))  # t598: "cuda:0 i64[512, 512]"
        # t599 = prims.ge(t597, t598)  # t599: "cuda:0 b8[512, 512]"
      # t601 = ltorch.where(t599, t591, -float('inf'))  # t601: "cuda:0 bf16[1, 32, 512, 512]"
        # t600 = prims.broadcast_in_dim(t599, (1, 32, 512, 512), (2, 3))  # t600: "cuda:0 b8[1, 32, 512, 512]"
        # t601 = prims.where(t600, t591, -float('inf'))  # t601: "cuda:0 bf16[1, 32, 512, 512]"
    # t612 = ltorch._softmax(t601, -1, dtype=None)  # t612: "cuda:0 bf16[1, 32, 512, 512]"
      # t602 = ltorch.to(t601, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t602: "cuda:0 f32[1, 32, 512, 512]"
        # t602 = prims.convert_element_type(t601, dtypes.float32)  # t602: "cuda:0 f32[1, 32, 512, 512]"
      # t604 = ltorch.amax(t602, -1, True)  # t604: "cuda:0 f32[1, 32, 512, 1]"
        # t603 = prims.amax(t602, (3,))  # t603: "cuda:0 f32[1, 32, 512]"
        # t604 = prims.broadcast_in_dim(t603, [1, 32, 512, 1], [0, 1, 2])  # t604: "cuda:0 f32[1, 32, 512, 1]"
      # t606 = ltorch.sub(t602, t604, alpha=None)  # t606: "cuda:0 f32[1, 32, 512, 512]"
        # t605 = prims.broadcast_in_dim(t604, (1, 32, 512, 512), (0, 1, 2, 3))  # t605: "cuda:0 f32[1, 32, 512, 512]"
        # t606 = prims.sub(t602, t605)  # t606: "cuda:0 f32[1, 32, 512, 512]"
      # t607 = ltorch.exp(t606)  # t607: "cuda:0 f32[1, 32, 512, 512]"
        # t607 = prims.exp(t606)  # t607: "cuda:0 f32[1, 32, 512, 512]"
      # t609 = ltorch.sum(t607, -1, True, dtype=None)  # t609: "cuda:0 f32[1, 32, 512, 1]"
        # t608 = prims.sum(t607, (3,))  # t608: "cuda:0 f32[1, 32, 512]"
        # t609 = prims.broadcast_in_dim(t608, [1, 32, 512, 1], [0, 1, 2])  # t609: "cuda:0 f32[1, 32, 512, 1]"
      # t611 = ltorch.true_divide(t607, t609)  # t611: "cuda:0 f32[1, 32, 512, 512]"
        # t610 = prims.broadcast_in_dim(t609, (1, 32, 512, 512), (0, 1, 2, 3))  # t610: "cuda:0 f32[1, 32, 512, 512]"
        # t611 = prims.div(t607, t610)  # t611: "cuda:0 f32[1, 32, 512, 512]"
      # t612 = ltorch.to(t611, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t612: "cuda:0 bf16[1, 32, 512, 512]"
        # t612 = prims.convert_element_type(t611, dtypes.bfloat16)  # t612: "cuda:0 bf16[1, 32, 512, 512]"
    # t613 = ltorch.matmul(t612, t549)  # t613: "cuda:0 bf16[1, 32, 512, 128]"
      # t613 = prims.matmul(t612, t549)  # t613: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t614 = ltorch.transpose(t613, 1, 2)  # t614: "cuda:0 bf16[1, 512, 32, 128]"
    # t614 = prims.transpose(t613, (0, 2, 1, 3))  # t614: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t615 = ltorch.reshape(t614, 1, 512, 4096)  # t615: "cuda:0 bf16[1, 512, 4096]"
    # t615 = prims.reshape(t614, (1, 512, 4096))  # t615: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t619 = ltorch.linear(t615, t_transformer_h_3_attn_proj_weight, None)  # t619: "cuda:0 bf16[1, 512, 4096]"
    # t619 = prims.linear(t615, t_transformer_h_3_attn_proj_weight, None)  # t619: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t623 = ltorch.add(t619, t513, alpha=None)  # t623: "cuda:0 bf16[1, 512, 4096]"
    # t620 = prims.convert_element_type(t619, dtypes.float32)  # t620: "cuda:0 f32[1, 512, 4096]"
    # t621 = prims.convert_element_type(t513, dtypes.float32)  # t621: "cuda:0 f32[1, 512, 4096]"
    # t622 = prims.add(t620, t621)  # t622: "cuda:0 f32[1, 512, 4096]"
    # t623 = prims.convert_element_type(t622, dtypes.bfloat16)  # t623: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t624 = prims.convert_element_type(t623, dtypes.float32)  # t624: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t625 = ltorch.mul(t624, t624)  # t625: "cuda:0 f32[1, 512, 4096]"
    # t625 = prims.mul(t624, t624)  # t625: "cuda:0 f32[1, 512, 4096]"
  t629 = ltorch.mean(t625, -1, True, dtype=None)  # t629: "cuda:0 f32[1, 512, 1]"
    # t627 = prims.sum(t625, (2,))  # t627: "cuda:0 f32[1, 512]"
    # t628 = prims.broadcast_in_dim(t627, [1, 512, 1], [0, 1])  # t628: "cuda:0 f32[1, 512, 1]"
    # t629 = ltorch.true_divide(t628, 4096)  # t629: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t629 = prims.div(t628, 4096.0)  # t629: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t631 = ltorch.add(t629, 1e-05, alpha=None)  # t631: "cuda:0 f32[1, 512, 1]"
    # t631 = prims.add(t629, 1e-05)  # t631: "cuda:0 f32[1, 512, 1]"
  t632 = ltorch.rsqrt(t631)  # t632: "cuda:0 f32[1, 512, 1]"
    # t632 = prims.rsqrt(t631)  # t632: "cuda:0 f32[1, 512, 1]"
  t634 = ltorch.mul(t624, t632)  # t634: "cuda:0 f32[1, 512, 4096]"
    # t633 = prims.broadcast_in_dim(t632, (1, 512, 4096), (0, 1, 2))  # t633: "cuda:0 f32[1, 512, 4096]"
    # t634 = prims.mul(t624, t633)  # t634: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t635 = ltorch.to(t634, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t635: "cuda:0 bf16[1, 512, 4096]"
    # t635 = prims.convert_element_type(t634, dtypes.bfloat16)  # t635: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t645 = ltorch.mul(t635, t_transformer_h_3_norm_2_weight)  # t645: "cuda:0 bf16[1, 512, 4096]"
    # t641 = prims.broadcast_in_dim(t_transformer_h_3_norm_2_weight, (1, 512, 4096), (2,))  # t641: "cuda:0 bf16[1, 512, 4096]"
    # t642 = prims.convert_element_type(t635, dtypes.float32)  # t642: "cuda:0 f32[1, 512, 4096]"
    # t643 = prims.convert_element_type(t641, dtypes.float32)  # t643: "cuda:0 f32[1, 512, 4096]"
    # t644 = prims.mul(t642, t643)  # t644: "cuda:0 f32[1, 512, 4096]"
    # t645 = prims.convert_element_type(t644, dtypes.bfloat16)  # t645: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t650 = ltorch.linear(t645, t_transformer_h_3_mlp_fc_1_weight, None)  # t650: "cuda:0 bf16[1, 512, 11008]"
    # t650 = prims.linear(t645, t_transformer_h_3_mlp_fc_1_weight, None)  # t650: "cuda:0 bf16[1, 512, 11008]"
  t654 = ltorch.linear(t645, t_transformer_h_3_mlp_fc_2_weight, None)  # t654: "cuda:0 bf16[1, 512, 11008]"
    # t654 = prims.linear(t645, t_transformer_h_3_mlp_fc_2_weight, None)  # t654: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t664 = ltorch.silu(t650, False)  # t664: "cuda:0 bf16[1, 512, 11008]"
    # t655 = prims.convert_element_type(t650, dtypes.float32)  # t655: "cuda:0 f32[1, 512, 11008]"
    # t656 = prims.neg(t655)  # t656: "cuda:0 f32[1, 512, 11008]"
    # t657 = prims.exp(t656)  # t657: "cuda:0 f32[1, 512, 11008]"
    # t658 = prims.add(1.0, t657)  # t658: "cuda:0 f32[1, 512, 11008]"
    # t659 = prims.reciprocal(t658)  # t659: "cuda:0 f32[1, 512, 11008]"
    # t660 = prims.convert_element_type(t659, dtypes.bfloat16)  # t660: "cuda:0 bf16[1, 512, 11008]"
    # t661 = prims.convert_element_type(t650, dtypes.float32)  # t661: "cuda:0 f32[1, 512, 11008]"
    # t662 = prims.convert_element_type(t660, dtypes.float32)  # t662: "cuda:0 f32[1, 512, 11008]"
    # t663 = prims.mul(t661, t662)  # t663: "cuda:0 f32[1, 512, 11008]"
    # t664 = prims.convert_element_type(t663, dtypes.bfloat16)  # t664: "cuda:0 bf16[1, 512, 11008]"
  t668 = ltorch.mul(t664, t654)  # t668: "cuda:0 bf16[1, 512, 11008]"
    # t665 = prims.convert_element_type(t664, dtypes.float32)  # t665: "cuda:0 f32[1, 512, 11008]"
    # t666 = prims.convert_element_type(t654, dtypes.float32)  # t666: "cuda:0 f32[1, 512, 11008]"
    # t667 = prims.mul(t665, t666)  # t667: "cuda:0 f32[1, 512, 11008]"
    # t668 = prims.convert_element_type(t667, dtypes.bfloat16)  # t668: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t672 = ltorch.linear(t668, t_transformer_h_3_mlp_proj_weight, None)  # t672: "cuda:0 bf16[1, 512, 4096]"
    # t672 = prims.linear(t668, t_transformer_h_3_mlp_proj_weight, None)  # t672: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t676 = ltorch.add(t672, t623, alpha=None)  # t676: "cuda:0 bf16[1, 512, 4096]"
    # t673 = prims.convert_element_type(t672, dtypes.float32)  # t673: "cuda:0 f32[1, 512, 4096]"
    # t674 = prims.convert_element_type(t623, dtypes.float32)  # t674: "cuda:0 f32[1, 512, 4096]"
    # t675 = prims.add(t673, t674)  # t675: "cuda:0 f32[1, 512, 4096]"
    # t676 = prims.convert_element_type(t675, dtypes.bfloat16)  # t676: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t678 = prims.convert_element_type(t676, dtypes.float32)  # t678: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t679 = ltorch.mul(t678, t678)  # t679: "cuda:0 f32[1, 512, 4096]"
    # t679 = prims.mul(t678, t678)  # t679: "cuda:0 f32[1, 512, 4096]"
  t683 = ltorch.mean(t679, -1, True, dtype=None)  # t683: "cuda:0 f32[1, 512, 1]"
    # t681 = prims.sum(t679, (2,))  # t681: "cuda:0 f32[1, 512]"
    # t682 = prims.broadcast_in_dim(t681, [1, 512, 1], [0, 1])  # t682: "cuda:0 f32[1, 512, 1]"
    # t683 = ltorch.true_divide(t682, 4096)  # t683: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t683 = prims.div(t682, 4096.0)  # t683: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t685 = ltorch.add(t683, 1e-05, alpha=None)  # t685: "cuda:0 f32[1, 512, 1]"
    # t685 = prims.add(t683, 1e-05)  # t685: "cuda:0 f32[1, 512, 1]"
  t686 = ltorch.rsqrt(t685)  # t686: "cuda:0 f32[1, 512, 1]"
    # t686 = prims.rsqrt(t685)  # t686: "cuda:0 f32[1, 512, 1]"
  t688 = ltorch.mul(t678, t686)  # t688: "cuda:0 f32[1, 512, 4096]"
    # t687 = prims.broadcast_in_dim(t686, (1, 512, 4096), (0, 1, 2))  # t687: "cuda:0 f32[1, 512, 4096]"
    # t688 = prims.mul(t678, t687)  # t688: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t689 = ltorch.to(t688, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t689: "cuda:0 bf16[1, 512, 4096]"
    # t689 = prims.convert_element_type(t688, dtypes.bfloat16)  # t689: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t699 = ltorch.mul(t689, t_transformer_h_4_norm_1_weight)  # t699: "cuda:0 bf16[1, 512, 4096]"
    # t695 = prims.broadcast_in_dim(t_transformer_h_4_norm_1_weight, (1, 512, 4096), (2,))  # t695: "cuda:0 bf16[1, 512, 4096]"
    # t696 = prims.convert_element_type(t689, dtypes.float32)  # t696: "cuda:0 f32[1, 512, 4096]"
    # t697 = prims.convert_element_type(t695, dtypes.float32)  # t697: "cuda:0 f32[1, 512, 4096]"
    # t698 = prims.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 4096]"
    # t699 = prims.convert_element_type(t698, dtypes.bfloat16)  # t699: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t704 = ltorch.linear(t699, t_transformer_h_4_attn_attn_weight, None)  # t704: "cuda:0 bf16[1, 512, 12288]"
    # t704 = prims.linear(t699, t_transformer_h_4_attn_attn_weight, None)  # t704: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t705 = ltorch.view(t704, 1, 512, 32, 3, 128)  # t705: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t705 = ltorch.reshape(t704, (1, 512, 32, 3, 128))  # t705: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t705 = prims.reshape(t704, (1, 512, 32, 3, 128))  # t705: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t706 = ltorch.permute(t705, 0, 2, 3, 1, 4)  # t706: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t706 = prims.transpose(t705, (0, 2, 3, 1, 4))  # t706: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t707, t708, t709) = ltorch.split(t706, (1, 1, 1), 2)
    # t707 = prims.slice_prim(t706, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t707: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t708 = prims.slice_prim(t706, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t708: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t709 = prims.slice_prim(t706, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t709: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t710 = ltorch.reshape(t707, 1, -1, 512, 128)  # t710: "cuda:0 bf16[1, 32, 512, 128]"
    # t710 = prims.reshape(t707, (1, 32, 512, 128))  # t710: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t711 = ltorch.reshape(t708, 1, -1, 512, 128)  # t711: "cuda:0 bf16[1, 32, 512, 128]"
    # t711 = prims.reshape(t708, (1, 32, 512, 128))  # t711: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t712 = ltorch.reshape(t709, 1, -1, 512, 128)  # t712: "cuda:0 bf16[1, 32, 512, 128]"
    # t712 = prims.reshape(t709, (1, 32, 512, 128))  # t712: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t713 = ltorch.getitem(t710, (..., slice(None, 128, None)))  # t713: "cuda:0 bf16[1, 32, 512, 128]"
    # t713 = prims.slice_prim(t710, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t713: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t714 = ltorch.getitem(t713, (..., slice(None, 64, None)))  # t714: "cuda:0 bf16[1, 32, 512, 64]"
    # t714 = prims.slice_prim(t713, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t714: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t715 = ltorch.getitem(t713, (..., slice(64, None, None)))  # t715: "cuda:0 bf16[1, 32, 512, 64]"
    # t715 = prims.slice_prim(t713, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t715: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t718 = ltorch.neg(t715)  # t718: "cuda:0 bf16[1, 32, 512, 64]"
    # t716 = prims.convert_element_type(t715, dtypes.float32)  # t716: "cuda:0 f32[1, 32, 512, 64]"
    # t717 = prims.neg(t716)  # t717: "cuda:0 f32[1, 32, 512, 64]"
    # t718 = prims.convert_element_type(t717, dtypes.bfloat16)  # t718: "cuda:0 bf16[1, 32, 512, 64]"
  t719 = ltorch.cat((t718, t714), -1)  # t719: "cuda:0 bf16[1, 32, 512, 128]"
    # t719 = prims.cat((t718, t714), -1)  # t719: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t722 = ltorch.mul(t713, cos)  # t722: "cuda:0 f32[1, 32, 512, 128]"
    # t720 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t720: "cuda:0 f32[1, 32, 512, 128]"
    # t721 = prims.convert_element_type(t713, dtypes.float32)  # t721: "cuda:0 f32[1, 32, 512, 128]"
    # t722 = prims.mul(t721, t720)  # t722: "cuda:0 f32[1, 32, 512, 128]"
  t725 = ltorch.mul(t719, sin)  # t725: "cuda:0 f32[1, 32, 512, 128]"
    # t723 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t723: "cuda:0 f32[1, 32, 512, 128]"
    # t724 = prims.convert_element_type(t719, dtypes.float32)  # t724: "cuda:0 f32[1, 32, 512, 128]"
    # t725 = prims.mul(t724, t723)  # t725: "cuda:0 f32[1, 32, 512, 128]"
  t726 = ltorch.add(t722, t725, alpha=None)  # t726: "cuda:0 f32[1, 32, 512, 128]"
    # t726 = prims.add(t722, t725)  # t726: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t727 = ltorch.to(t726, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t727: "cuda:0 bf16[1, 32, 512, 128]"
    # t727 = prims.convert_element_type(t726, dtypes.bfloat16)  # t727: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t728 = ltorch.getitem(t711, (..., slice(None, 128, None)))  # t728: "cuda:0 bf16[1, 32, 512, 128]"
    # t728 = prims.slice_prim(t711, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t728: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t729 = ltorch.getitem(t728, (..., slice(None, 64, None)))  # t729: "cuda:0 bf16[1, 32, 512, 64]"
    # t729 = prims.slice_prim(t728, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t729: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t730 = ltorch.getitem(t728, (..., slice(64, None, None)))  # t730: "cuda:0 bf16[1, 32, 512, 64]"
    # t730 = prims.slice_prim(t728, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t730: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t733 = ltorch.neg(t730)  # t733: "cuda:0 bf16[1, 32, 512, 64]"
    # t731 = prims.convert_element_type(t730, dtypes.float32)  # t731: "cuda:0 f32[1, 32, 512, 64]"
    # t732 = prims.neg(t731)  # t732: "cuda:0 f32[1, 32, 512, 64]"
    # t733 = prims.convert_element_type(t732, dtypes.bfloat16)  # t733: "cuda:0 bf16[1, 32, 512, 64]"
  t734 = ltorch.cat((t733, t729), -1)  # t734: "cuda:0 bf16[1, 32, 512, 128]"
    # t734 = prims.cat((t733, t729), -1)  # t734: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t737 = ltorch.mul(t728, cos)  # t737: "cuda:0 f32[1, 32, 512, 128]"
    # t735 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t735: "cuda:0 f32[1, 32, 512, 128]"
    # t736 = prims.convert_element_type(t728, dtypes.float32)  # t736: "cuda:0 f32[1, 32, 512, 128]"
    # t737 = prims.mul(t736, t735)  # t737: "cuda:0 f32[1, 32, 512, 128]"
  t740 = ltorch.mul(t734, sin)  # t740: "cuda:0 f32[1, 32, 512, 128]"
    # t738 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t738: "cuda:0 f32[1, 32, 512, 128]"
    # t739 = prims.convert_element_type(t734, dtypes.float32)  # t739: "cuda:0 f32[1, 32, 512, 128]"
    # t740 = prims.mul(t739, t738)  # t740: "cuda:0 f32[1, 32, 512, 128]"
  t741 = ltorch.add(t737, t740, alpha=None)  # t741: "cuda:0 f32[1, 32, 512, 128]"
    # t741 = prims.add(t737, t740)  # t741: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t742 = ltorch.to(t741, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t742: "cuda:0 bf16[1, 32, 512, 128]"
    # t742 = prims.convert_element_type(t741, dtypes.bfloat16)  # t742: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t743 = ltorch.getitem(t710, (..., slice(128, None, None)))  # t743: "cuda:0 bf16[1, 32, 512, 0]"
    # t743 = prims.slice_prim(t710, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t743: "cuda:0 bf16[1, 32, 512, 0]"
  t744 = ltorch.cat((t727, t743), -1)  # t744: "cuda:0 bf16[1, 32, 512, 128]"
    # t744 = prims.cat((t727, t743), -1)  # t744: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t745 = ltorch.getitem(t711, (..., slice(128, None, None)))  # t745: "cuda:0 bf16[1, 32, 512, 0]"
    # t745 = prims.slice_prim(t711, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t745: "cuda:0 bf16[1, 32, 512, 0]"
  t746 = ltorch.cat((t742, t745), -1)  # t746: "cuda:0 bf16[1, 32, 512, 128]"
    # t746 = prims.cat((t742, t745), -1)  # t746: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t776 = ltorch.scaled_dot_product_attention(t744, t746, t712, None, 0.0, True, scale=0.08838834764831843)  # t776: "cuda:0 bf16[1, 32, 512, 128]"
    # t749 = ltorch.mul(t744, 0.29730177875068026)  # t749: "cuda:0 bf16[1, 32, 512, 128]"
      # t747 = prims.convert_element_type(t744, dtypes.float32)  # t747: "cuda:0 f32[1, 32, 512, 128]"
      # t748 = prims.mul(t747, 0.29730177875068026)  # t748: "cuda:0 f32[1, 32, 512, 128]"
      # t749 = prims.convert_element_type(t748, dtypes.bfloat16)  # t749: "cuda:0 bf16[1, 32, 512, 128]"
    # t750 = ltorch.transpose(t746, -2, -1)  # t750: "cuda:0 bf16[1, 32, 128, 512]"
      # t750 = prims.transpose(t746, (0, 1, 3, 2))  # t750: "cuda:0 bf16[1, 32, 128, 512]"
    # t753 = ltorch.mul(t750, 0.29730177875068026)  # t753: "cuda:0 bf16[1, 32, 128, 512]"
      # t751 = prims.convert_element_type(t750, dtypes.float32)  # t751: "cuda:0 f32[1, 32, 128, 512]"
      # t752 = prims.mul(t751, 0.29730177875068026)  # t752: "cuda:0 f32[1, 32, 128, 512]"
      # t753 = prims.convert_element_type(t752, dtypes.bfloat16)  # t753: "cuda:0 bf16[1, 32, 128, 512]"
    # t754 = ltorch.matmul(t749, t753)  # t754: "cuda:0 bf16[1, 32, 512, 512]"
      # t754 = prims.matmul(t749, t753)  # t754: "cuda:0 bf16[1, 32, 512, 512]"
    # t764 = ltorch.tril(t754, 0, fill_value=-float('inf'))  # t764: "cuda:0 bf16[1, 32, 512, 512]"
      # t755 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t755: "cuda:0 i64[512]"
        # t755 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t755: "cuda:0 i64[512]"
      # t756 = ltorch.unsqueeze(t755, -1)  # t756: "cuda:0 i64[512, 1]"
        # t756 = prims.broadcast_in_dim(t755, [512, 1], [0])  # t756: "cuda:0 i64[512, 1]"
      # t757 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t757: "cuda:0 i64[512]"
        # t757 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t757: "cuda:0 i64[512]"
      # t758 = ltorch.unsqueeze(t757, -2)  # t758: "cuda:0 i64[1, 512]"
        # t758 = prims.broadcast_in_dim(t757, [1, 512], [1])  # t758: "cuda:0 i64[1, 512]"
      # t759 = ltorch.add(t756, 0, alpha=None)  # t759: "cuda:0 i64[512, 1]"
        # t759 = prims.add(t756, 0)  # t759: "cuda:0 i64[512, 1]"
      # t762 = ltorch.ge(t759, t758)  # t762: "cuda:0 b8[512, 512]"
        # t760 = prims.broadcast_in_dim(t759, (512, 512), (0, 1))  # t760: "cuda:0 i64[512, 512]"
        # t761 = prims.broadcast_in_dim(t758, (512, 512), (0, 1))  # t761: "cuda:0 i64[512, 512]"
        # t762 = prims.ge(t760, t761)  # t762: "cuda:0 b8[512, 512]"
      # t764 = ltorch.where(t762, t754, -float('inf'))  # t764: "cuda:0 bf16[1, 32, 512, 512]"
        # t763 = prims.broadcast_in_dim(t762, (1, 32, 512, 512), (2, 3))  # t763: "cuda:0 b8[1, 32, 512, 512]"
        # t764 = prims.where(t763, t754, -float('inf'))  # t764: "cuda:0 bf16[1, 32, 512, 512]"
    # t775 = ltorch._softmax(t764, -1, dtype=None)  # t775: "cuda:0 bf16[1, 32, 512, 512]"
      # t765 = ltorch.to(t764, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t765: "cuda:0 f32[1, 32, 512, 512]"
        # t765 = prims.convert_element_type(t764, dtypes.float32)  # t765: "cuda:0 f32[1, 32, 512, 512]"
      # t767 = ltorch.amax(t765, -1, True)  # t767: "cuda:0 f32[1, 32, 512, 1]"
        # t766 = prims.amax(t765, (3,))  # t766: "cuda:0 f32[1, 32, 512]"
        # t767 = prims.broadcast_in_dim(t766, [1, 32, 512, 1], [0, 1, 2])  # t767: "cuda:0 f32[1, 32, 512, 1]"
      # t769 = ltorch.sub(t765, t767, alpha=None)  # t769: "cuda:0 f32[1, 32, 512, 512]"
        # t768 = prims.broadcast_in_dim(t767, (1, 32, 512, 512), (0, 1, 2, 3))  # t768: "cuda:0 f32[1, 32, 512, 512]"
        # t769 = prims.sub(t765, t768)  # t769: "cuda:0 f32[1, 32, 512, 512]"
      # t770 = ltorch.exp(t769)  # t770: "cuda:0 f32[1, 32, 512, 512]"
        # t770 = prims.exp(t769)  # t770: "cuda:0 f32[1, 32, 512, 512]"
      # t772 = ltorch.sum(t770, -1, True, dtype=None)  # t772: "cuda:0 f32[1, 32, 512, 1]"
        # t771 = prims.sum(t770, (3,))  # t771: "cuda:0 f32[1, 32, 512]"
        # t772 = prims.broadcast_in_dim(t771, [1, 32, 512, 1], [0, 1, 2])  # t772: "cuda:0 f32[1, 32, 512, 1]"
      # t774 = ltorch.true_divide(t770, t772)  # t774: "cuda:0 f32[1, 32, 512, 512]"
        # t773 = prims.broadcast_in_dim(t772, (1, 32, 512, 512), (0, 1, 2, 3))  # t773: "cuda:0 f32[1, 32, 512, 512]"
        # t774 = prims.div(t770, t773)  # t774: "cuda:0 f32[1, 32, 512, 512]"
      # t775 = ltorch.to(t774, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t775: "cuda:0 bf16[1, 32, 512, 512]"
        # t775 = prims.convert_element_type(t774, dtypes.bfloat16)  # t775: "cuda:0 bf16[1, 32, 512, 512]"
    # t776 = ltorch.matmul(t775, t712)  # t776: "cuda:0 bf16[1, 32, 512, 128]"
      # t776 = prims.matmul(t775, t712)  # t776: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t777 = ltorch.transpose(t776, 1, 2)  # t777: "cuda:0 bf16[1, 512, 32, 128]"
    # t777 = prims.transpose(t776, (0, 2, 1, 3))  # t777: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t778 = ltorch.reshape(t777, 1, 512, 4096)  # t778: "cuda:0 bf16[1, 512, 4096]"
    # t778 = prims.reshape(t777, (1, 512, 4096))  # t778: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t782 = ltorch.linear(t778, t_transformer_h_4_attn_proj_weight, None)  # t782: "cuda:0 bf16[1, 512, 4096]"
    # t782 = prims.linear(t778, t_transformer_h_4_attn_proj_weight, None)  # t782: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t786 = ltorch.add(t782, t676, alpha=None)  # t786: "cuda:0 bf16[1, 512, 4096]"
    # t783 = prims.convert_element_type(t782, dtypes.float32)  # t783: "cuda:0 f32[1, 512, 4096]"
    # t784 = prims.convert_element_type(t676, dtypes.float32)  # t784: "cuda:0 f32[1, 512, 4096]"
    # t785 = prims.add(t783, t784)  # t785: "cuda:0 f32[1, 512, 4096]"
    # t786 = prims.convert_element_type(t785, dtypes.bfloat16)  # t786: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t787 = prims.convert_element_type(t786, dtypes.float32)  # t787: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t788 = ltorch.mul(t787, t787)  # t788: "cuda:0 f32[1, 512, 4096]"
    # t788 = prims.mul(t787, t787)  # t788: "cuda:0 f32[1, 512, 4096]"
  t792 = ltorch.mean(t788, -1, True, dtype=None)  # t792: "cuda:0 f32[1, 512, 1]"
    # t790 = prims.sum(t788, (2,))  # t790: "cuda:0 f32[1, 512]"
    # t791 = prims.broadcast_in_dim(t790, [1, 512, 1], [0, 1])  # t791: "cuda:0 f32[1, 512, 1]"
    # t792 = ltorch.true_divide(t791, 4096)  # t792: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t792 = prims.div(t791, 4096.0)  # t792: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t794 = ltorch.add(t792, 1e-05, alpha=None)  # t794: "cuda:0 f32[1, 512, 1]"
    # t794 = prims.add(t792, 1e-05)  # t794: "cuda:0 f32[1, 512, 1]"
  t795 = ltorch.rsqrt(t794)  # t795: "cuda:0 f32[1, 512, 1]"
    # t795 = prims.rsqrt(t794)  # t795: "cuda:0 f32[1, 512, 1]"
  t797 = ltorch.mul(t787, t795)  # t797: "cuda:0 f32[1, 512, 4096]"
    # t796 = prims.broadcast_in_dim(t795, (1, 512, 4096), (0, 1, 2))  # t796: "cuda:0 f32[1, 512, 4096]"
    # t797 = prims.mul(t787, t796)  # t797: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t798 = ltorch.to(t797, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t798: "cuda:0 bf16[1, 512, 4096]"
    # t798 = prims.convert_element_type(t797, dtypes.bfloat16)  # t798: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t808 = ltorch.mul(t798, t_transformer_h_4_norm_2_weight)  # t808: "cuda:0 bf16[1, 512, 4096]"
    # t804 = prims.broadcast_in_dim(t_transformer_h_4_norm_2_weight, (1, 512, 4096), (2,))  # t804: "cuda:0 bf16[1, 512, 4096]"
    # t805 = prims.convert_element_type(t798, dtypes.float32)  # t805: "cuda:0 f32[1, 512, 4096]"
    # t806 = prims.convert_element_type(t804, dtypes.float32)  # t806: "cuda:0 f32[1, 512, 4096]"
    # t807 = prims.mul(t805, t806)  # t807: "cuda:0 f32[1, 512, 4096]"
    # t808 = prims.convert_element_type(t807, dtypes.bfloat16)  # t808: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t813 = ltorch.linear(t808, t_transformer_h_4_mlp_fc_1_weight, None)  # t813: "cuda:0 bf16[1, 512, 11008]"
    # t813 = prims.linear(t808, t_transformer_h_4_mlp_fc_1_weight, None)  # t813: "cuda:0 bf16[1, 512, 11008]"
  t817 = ltorch.linear(t808, t_transformer_h_4_mlp_fc_2_weight, None)  # t817: "cuda:0 bf16[1, 512, 11008]"
    # t817 = prims.linear(t808, t_transformer_h_4_mlp_fc_2_weight, None)  # t817: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t827 = ltorch.silu(t813, False)  # t827: "cuda:0 bf16[1, 512, 11008]"
    # t818 = prims.convert_element_type(t813, dtypes.float32)  # t818: "cuda:0 f32[1, 512, 11008]"
    # t819 = prims.neg(t818)  # t819: "cuda:0 f32[1, 512, 11008]"
    # t820 = prims.exp(t819)  # t820: "cuda:0 f32[1, 512, 11008]"
    # t821 = prims.add(1.0, t820)  # t821: "cuda:0 f32[1, 512, 11008]"
    # t822 = prims.reciprocal(t821)  # t822: "cuda:0 f32[1, 512, 11008]"
    # t823 = prims.convert_element_type(t822, dtypes.bfloat16)  # t823: "cuda:0 bf16[1, 512, 11008]"
    # t824 = prims.convert_element_type(t813, dtypes.float32)  # t824: "cuda:0 f32[1, 512, 11008]"
    # t825 = prims.convert_element_type(t823, dtypes.float32)  # t825: "cuda:0 f32[1, 512, 11008]"
    # t826 = prims.mul(t824, t825)  # t826: "cuda:0 f32[1, 512, 11008]"
    # t827 = prims.convert_element_type(t826, dtypes.bfloat16)  # t827: "cuda:0 bf16[1, 512, 11008]"
  t831 = ltorch.mul(t827, t817)  # t831: "cuda:0 bf16[1, 512, 11008]"
    # t828 = prims.convert_element_type(t827, dtypes.float32)  # t828: "cuda:0 f32[1, 512, 11008]"
    # t829 = prims.convert_element_type(t817, dtypes.float32)  # t829: "cuda:0 f32[1, 512, 11008]"
    # t830 = prims.mul(t828, t829)  # t830: "cuda:0 f32[1, 512, 11008]"
    # t831 = prims.convert_element_type(t830, dtypes.bfloat16)  # t831: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t835 = ltorch.linear(t831, t_transformer_h_4_mlp_proj_weight, None)  # t835: "cuda:0 bf16[1, 512, 4096]"
    # t835 = prims.linear(t831, t_transformer_h_4_mlp_proj_weight, None)  # t835: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t839 = ltorch.add(t835, t786, alpha=None)  # t839: "cuda:0 bf16[1, 512, 4096]"
    # t836 = prims.convert_element_type(t835, dtypes.float32)  # t836: "cuda:0 f32[1, 512, 4096]"
    # t837 = prims.convert_element_type(t786, dtypes.float32)  # t837: "cuda:0 f32[1, 512, 4096]"
    # t838 = prims.add(t836, t837)  # t838: "cuda:0 f32[1, 512, 4096]"
    # t839 = prims.convert_element_type(t838, dtypes.bfloat16)  # t839: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t841 = prims.convert_element_type(t839, dtypes.float32)  # t841: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t842 = ltorch.mul(t841, t841)  # t842: "cuda:0 f32[1, 512, 4096]"
    # t842 = prims.mul(t841, t841)  # t842: "cuda:0 f32[1, 512, 4096]"
  t846 = ltorch.mean(t842, -1, True, dtype=None)  # t846: "cuda:0 f32[1, 512, 1]"
    # t844 = prims.sum(t842, (2,))  # t844: "cuda:0 f32[1, 512]"
    # t845 = prims.broadcast_in_dim(t844, [1, 512, 1], [0, 1])  # t845: "cuda:0 f32[1, 512, 1]"
    # t846 = ltorch.true_divide(t845, 4096)  # t846: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t846 = prims.div(t845, 4096.0)  # t846: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t848 = ltorch.add(t846, 1e-05, alpha=None)  # t848: "cuda:0 f32[1, 512, 1]"
    # t848 = prims.add(t846, 1e-05)  # t848: "cuda:0 f32[1, 512, 1]"
  t849 = ltorch.rsqrt(t848)  # t849: "cuda:0 f32[1, 512, 1]"
    # t849 = prims.rsqrt(t848)  # t849: "cuda:0 f32[1, 512, 1]"
  t851 = ltorch.mul(t841, t849)  # t851: "cuda:0 f32[1, 512, 4096]"
    # t850 = prims.broadcast_in_dim(t849, (1, 512, 4096), (0, 1, 2))  # t850: "cuda:0 f32[1, 512, 4096]"
    # t851 = prims.mul(t841, t850)  # t851: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t852 = ltorch.to(t851, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t852: "cuda:0 bf16[1, 512, 4096]"
    # t852 = prims.convert_element_type(t851, dtypes.bfloat16)  # t852: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t862 = ltorch.mul(t852, t_transformer_h_5_norm_1_weight)  # t862: "cuda:0 bf16[1, 512, 4096]"
    # t858 = prims.broadcast_in_dim(t_transformer_h_5_norm_1_weight, (1, 512, 4096), (2,))  # t858: "cuda:0 bf16[1, 512, 4096]"
    # t859 = prims.convert_element_type(t852, dtypes.float32)  # t859: "cuda:0 f32[1, 512, 4096]"
    # t860 = prims.convert_element_type(t858, dtypes.float32)  # t860: "cuda:0 f32[1, 512, 4096]"
    # t861 = prims.mul(t859, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
    # t862 = prims.convert_element_type(t861, dtypes.bfloat16)  # t862: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t867 = ltorch.linear(t862, t_transformer_h_5_attn_attn_weight, None)  # t867: "cuda:0 bf16[1, 512, 12288]"
    # t867 = prims.linear(t862, t_transformer_h_5_attn_attn_weight, None)  # t867: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t868 = ltorch.view(t867, 1, 512, 32, 3, 128)  # t868: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t868 = ltorch.reshape(t867, (1, 512, 32, 3, 128))  # t868: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t868 = prims.reshape(t867, (1, 512, 32, 3, 128))  # t868: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t869 = ltorch.permute(t868, 0, 2, 3, 1, 4)  # t869: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t869 = prims.transpose(t868, (0, 2, 3, 1, 4))  # t869: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t870, t871, t872) = ltorch.split(t869, (1, 1, 1), 2)
    # t870 = prims.slice_prim(t869, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t870: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t871 = prims.slice_prim(t869, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t871: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t872 = prims.slice_prim(t869, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t872: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t873 = ltorch.reshape(t870, 1, -1, 512, 128)  # t873: "cuda:0 bf16[1, 32, 512, 128]"
    # t873 = prims.reshape(t870, (1, 32, 512, 128))  # t873: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t874 = ltorch.reshape(t871, 1, -1, 512, 128)  # t874: "cuda:0 bf16[1, 32, 512, 128]"
    # t874 = prims.reshape(t871, (1, 32, 512, 128))  # t874: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t875 = ltorch.reshape(t872, 1, -1, 512, 128)  # t875: "cuda:0 bf16[1, 32, 512, 128]"
    # t875 = prims.reshape(t872, (1, 32, 512, 128))  # t875: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t876 = ltorch.getitem(t873, (..., slice(None, 128, None)))  # t876: "cuda:0 bf16[1, 32, 512, 128]"
    # t876 = prims.slice_prim(t873, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t876: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t877 = ltorch.getitem(t876, (..., slice(None, 64, None)))  # t877: "cuda:0 bf16[1, 32, 512, 64]"
    # t877 = prims.slice_prim(t876, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t877: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t878 = ltorch.getitem(t876, (..., slice(64, None, None)))  # t878: "cuda:0 bf16[1, 32, 512, 64]"
    # t878 = prims.slice_prim(t876, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t878: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t881 = ltorch.neg(t878)  # t881: "cuda:0 bf16[1, 32, 512, 64]"
    # t879 = prims.convert_element_type(t878, dtypes.float32)  # t879: "cuda:0 f32[1, 32, 512, 64]"
    # t880 = prims.neg(t879)  # t880: "cuda:0 f32[1, 32, 512, 64]"
    # t881 = prims.convert_element_type(t880, dtypes.bfloat16)  # t881: "cuda:0 bf16[1, 32, 512, 64]"
  t882 = ltorch.cat((t881, t877), -1)  # t882: "cuda:0 bf16[1, 32, 512, 128]"
    # t882 = prims.cat((t881, t877), -1)  # t882: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t885 = ltorch.mul(t876, cos)  # t885: "cuda:0 f32[1, 32, 512, 128]"
    # t883 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t883: "cuda:0 f32[1, 32, 512, 128]"
    # t884 = prims.convert_element_type(t876, dtypes.float32)  # t884: "cuda:0 f32[1, 32, 512, 128]"
    # t885 = prims.mul(t884, t883)  # t885: "cuda:0 f32[1, 32, 512, 128]"
  t888 = ltorch.mul(t882, sin)  # t888: "cuda:0 f32[1, 32, 512, 128]"
    # t886 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t886: "cuda:0 f32[1, 32, 512, 128]"
    # t887 = prims.convert_element_type(t882, dtypes.float32)  # t887: "cuda:0 f32[1, 32, 512, 128]"
    # t888 = prims.mul(t887, t886)  # t888: "cuda:0 f32[1, 32, 512, 128]"
  t889 = ltorch.add(t885, t888, alpha=None)  # t889: "cuda:0 f32[1, 32, 512, 128]"
    # t889 = prims.add(t885, t888)  # t889: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t890 = ltorch.to(t889, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t890: "cuda:0 bf16[1, 32, 512, 128]"
    # t890 = prims.convert_element_type(t889, dtypes.bfloat16)  # t890: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t891 = ltorch.getitem(t874, (..., slice(None, 128, None)))  # t891: "cuda:0 bf16[1, 32, 512, 128]"
    # t891 = prims.slice_prim(t874, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t891: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t892 = ltorch.getitem(t891, (..., slice(None, 64, None)))  # t892: "cuda:0 bf16[1, 32, 512, 64]"
    # t892 = prims.slice_prim(t891, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t892: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t893 = ltorch.getitem(t891, (..., slice(64, None, None)))  # t893: "cuda:0 bf16[1, 32, 512, 64]"
    # t893 = prims.slice_prim(t891, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t893: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t896 = ltorch.neg(t893)  # t896: "cuda:0 bf16[1, 32, 512, 64]"
    # t894 = prims.convert_element_type(t893, dtypes.float32)  # t894: "cuda:0 f32[1, 32, 512, 64]"
    # t895 = prims.neg(t894)  # t895: "cuda:0 f32[1, 32, 512, 64]"
    # t896 = prims.convert_element_type(t895, dtypes.bfloat16)  # t896: "cuda:0 bf16[1, 32, 512, 64]"
  t897 = ltorch.cat((t896, t892), -1)  # t897: "cuda:0 bf16[1, 32, 512, 128]"
    # t897 = prims.cat((t896, t892), -1)  # t897: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t900 = ltorch.mul(t891, cos)  # t900: "cuda:0 f32[1, 32, 512, 128]"
    # t898 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t898: "cuda:0 f32[1, 32, 512, 128]"
    # t899 = prims.convert_element_type(t891, dtypes.float32)  # t899: "cuda:0 f32[1, 32, 512, 128]"
    # t900 = prims.mul(t899, t898)  # t900: "cuda:0 f32[1, 32, 512, 128]"
  t903 = ltorch.mul(t897, sin)  # t903: "cuda:0 f32[1, 32, 512, 128]"
    # t901 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t901: "cuda:0 f32[1, 32, 512, 128]"
    # t902 = prims.convert_element_type(t897, dtypes.float32)  # t902: "cuda:0 f32[1, 32, 512, 128]"
    # t903 = prims.mul(t902, t901)  # t903: "cuda:0 f32[1, 32, 512, 128]"
  t904 = ltorch.add(t900, t903, alpha=None)  # t904: "cuda:0 f32[1, 32, 512, 128]"
    # t904 = prims.add(t900, t903)  # t904: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t905 = ltorch.to(t904, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t905: "cuda:0 bf16[1, 32, 512, 128]"
    # t905 = prims.convert_element_type(t904, dtypes.bfloat16)  # t905: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t906 = ltorch.getitem(t873, (..., slice(128, None, None)))  # t906: "cuda:0 bf16[1, 32, 512, 0]"
    # t906 = prims.slice_prim(t873, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t906: "cuda:0 bf16[1, 32, 512, 0]"
  t907 = ltorch.cat((t890, t906), -1)  # t907: "cuda:0 bf16[1, 32, 512, 128]"
    # t907 = prims.cat((t890, t906), -1)  # t907: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t908 = ltorch.getitem(t874, (..., slice(128, None, None)))  # t908: "cuda:0 bf16[1, 32, 512, 0]"
    # t908 = prims.slice_prim(t874, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t908: "cuda:0 bf16[1, 32, 512, 0]"
  t909 = ltorch.cat((t905, t908), -1)  # t909: "cuda:0 bf16[1, 32, 512, 128]"
    # t909 = prims.cat((t905, t908), -1)  # t909: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t939 = ltorch.scaled_dot_product_attention(t907, t909, t875, None, 0.0, True, scale=0.08838834764831843)  # t939: "cuda:0 bf16[1, 32, 512, 128]"
    # t912 = ltorch.mul(t907, 0.29730177875068026)  # t912: "cuda:0 bf16[1, 32, 512, 128]"
      # t910 = prims.convert_element_type(t907, dtypes.float32)  # t910: "cuda:0 f32[1, 32, 512, 128]"
      # t911 = prims.mul(t910, 0.29730177875068026)  # t911: "cuda:0 f32[1, 32, 512, 128]"
      # t912 = prims.convert_element_type(t911, dtypes.bfloat16)  # t912: "cuda:0 bf16[1, 32, 512, 128]"
    # t913 = ltorch.transpose(t909, -2, -1)  # t913: "cuda:0 bf16[1, 32, 128, 512]"
      # t913 = prims.transpose(t909, (0, 1, 3, 2))  # t913: "cuda:0 bf16[1, 32, 128, 512]"
    # t916 = ltorch.mul(t913, 0.29730177875068026)  # t916: "cuda:0 bf16[1, 32, 128, 512]"
      # t914 = prims.convert_element_type(t913, dtypes.float32)  # t914: "cuda:0 f32[1, 32, 128, 512]"
      # t915 = prims.mul(t914, 0.29730177875068026)  # t915: "cuda:0 f32[1, 32, 128, 512]"
      # t916 = prims.convert_element_type(t915, dtypes.bfloat16)  # t916: "cuda:0 bf16[1, 32, 128, 512]"
    # t917 = ltorch.matmul(t912, t916)  # t917: "cuda:0 bf16[1, 32, 512, 512]"
      # t917 = prims.matmul(t912, t916)  # t917: "cuda:0 bf16[1, 32, 512, 512]"
    # t927 = ltorch.tril(t917, 0, fill_value=-float('inf'))  # t927: "cuda:0 bf16[1, 32, 512, 512]"
      # t918 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t918: "cuda:0 i64[512]"
        # t918 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t918: "cuda:0 i64[512]"
      # t919 = ltorch.unsqueeze(t918, -1)  # t919: "cuda:0 i64[512, 1]"
        # t919 = prims.broadcast_in_dim(t918, [512, 1], [0])  # t919: "cuda:0 i64[512, 1]"
      # t920 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t920: "cuda:0 i64[512]"
        # t920 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t920: "cuda:0 i64[512]"
      # t921 = ltorch.unsqueeze(t920, -2)  # t921: "cuda:0 i64[1, 512]"
        # t921 = prims.broadcast_in_dim(t920, [1, 512], [1])  # t921: "cuda:0 i64[1, 512]"
      # t922 = ltorch.add(t919, 0, alpha=None)  # t922: "cuda:0 i64[512, 1]"
        # t922 = prims.add(t919, 0)  # t922: "cuda:0 i64[512, 1]"
      # t925 = ltorch.ge(t922, t921)  # t925: "cuda:0 b8[512, 512]"
        # t923 = prims.broadcast_in_dim(t922, (512, 512), (0, 1))  # t923: "cuda:0 i64[512, 512]"
        # t924 = prims.broadcast_in_dim(t921, (512, 512), (0, 1))  # t924: "cuda:0 i64[512, 512]"
        # t925 = prims.ge(t923, t924)  # t925: "cuda:0 b8[512, 512]"
      # t927 = ltorch.where(t925, t917, -float('inf'))  # t927: "cuda:0 bf16[1, 32, 512, 512]"
        # t926 = prims.broadcast_in_dim(t925, (1, 32, 512, 512), (2, 3))  # t926: "cuda:0 b8[1, 32, 512, 512]"
        # t927 = prims.where(t926, t917, -float('inf'))  # t927: "cuda:0 bf16[1, 32, 512, 512]"
    # t938 = ltorch._softmax(t927, -1, dtype=None)  # t938: "cuda:0 bf16[1, 32, 512, 512]"
      # t928 = ltorch.to(t927, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t928: "cuda:0 f32[1, 32, 512, 512]"
        # t928 = prims.convert_element_type(t927, dtypes.float32)  # t928: "cuda:0 f32[1, 32, 512, 512]"
      # t930 = ltorch.amax(t928, -1, True)  # t930: "cuda:0 f32[1, 32, 512, 1]"
        # t929 = prims.amax(t928, (3,))  # t929: "cuda:0 f32[1, 32, 512]"
        # t930 = prims.broadcast_in_dim(t929, [1, 32, 512, 1], [0, 1, 2])  # t930: "cuda:0 f32[1, 32, 512, 1]"
      # t932 = ltorch.sub(t928, t930, alpha=None)  # t932: "cuda:0 f32[1, 32, 512, 512]"
        # t931 = prims.broadcast_in_dim(t930, (1, 32, 512, 512), (0, 1, 2, 3))  # t931: "cuda:0 f32[1, 32, 512, 512]"
        # t932 = prims.sub(t928, t931)  # t932: "cuda:0 f32[1, 32, 512, 512]"
      # t933 = ltorch.exp(t932)  # t933: "cuda:0 f32[1, 32, 512, 512]"
        # t933 = prims.exp(t932)  # t933: "cuda:0 f32[1, 32, 512, 512]"
      # t935 = ltorch.sum(t933, -1, True, dtype=None)  # t935: "cuda:0 f32[1, 32, 512, 1]"
        # t934 = prims.sum(t933, (3,))  # t934: "cuda:0 f32[1, 32, 512]"
        # t935 = prims.broadcast_in_dim(t934, [1, 32, 512, 1], [0, 1, 2])  # t935: "cuda:0 f32[1, 32, 512, 1]"
      # t937 = ltorch.true_divide(t933, t935)  # t937: "cuda:0 f32[1, 32, 512, 512]"
        # t936 = prims.broadcast_in_dim(t935, (1, 32, 512, 512), (0, 1, 2, 3))  # t936: "cuda:0 f32[1, 32, 512, 512]"
        # t937 = prims.div(t933, t936)  # t937: "cuda:0 f32[1, 32, 512, 512]"
      # t938 = ltorch.to(t937, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t938: "cuda:0 bf16[1, 32, 512, 512]"
        # t938 = prims.convert_element_type(t937, dtypes.bfloat16)  # t938: "cuda:0 bf16[1, 32, 512, 512]"
    # t939 = ltorch.matmul(t938, t875)  # t939: "cuda:0 bf16[1, 32, 512, 128]"
      # t939 = prims.matmul(t938, t875)  # t939: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t940 = ltorch.transpose(t939, 1, 2)  # t940: "cuda:0 bf16[1, 512, 32, 128]"
    # t940 = prims.transpose(t939, (0, 2, 1, 3))  # t940: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t941 = ltorch.reshape(t940, 1, 512, 4096)  # t941: "cuda:0 bf16[1, 512, 4096]"
    # t941 = prims.reshape(t940, (1, 512, 4096))  # t941: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t945 = ltorch.linear(t941, t_transformer_h_5_attn_proj_weight, None)  # t945: "cuda:0 bf16[1, 512, 4096]"
    # t945 = prims.linear(t941, t_transformer_h_5_attn_proj_weight, None)  # t945: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t949 = ltorch.add(t945, t839, alpha=None)  # t949: "cuda:0 bf16[1, 512, 4096]"
    # t946 = prims.convert_element_type(t945, dtypes.float32)  # t946: "cuda:0 f32[1, 512, 4096]"
    # t947 = prims.convert_element_type(t839, dtypes.float32)  # t947: "cuda:0 f32[1, 512, 4096]"
    # t948 = prims.add(t946, t947)  # t948: "cuda:0 f32[1, 512, 4096]"
    # t949 = prims.convert_element_type(t948, dtypes.bfloat16)  # t949: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t950 = prims.convert_element_type(t949, dtypes.float32)  # t950: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t951 = ltorch.mul(t950, t950)  # t951: "cuda:0 f32[1, 512, 4096]"
    # t951 = prims.mul(t950, t950)  # t951: "cuda:0 f32[1, 512, 4096]"
  t955 = ltorch.mean(t951, -1, True, dtype=None)  # t955: "cuda:0 f32[1, 512, 1]"
    # t953 = prims.sum(t951, (2,))  # t953: "cuda:0 f32[1, 512]"
    # t954 = prims.broadcast_in_dim(t953, [1, 512, 1], [0, 1])  # t954: "cuda:0 f32[1, 512, 1]"
    # t955 = ltorch.true_divide(t954, 4096)  # t955: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t955 = prims.div(t954, 4096.0)  # t955: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t957 = ltorch.add(t955, 1e-05, alpha=None)  # t957: "cuda:0 f32[1, 512, 1]"
    # t957 = prims.add(t955, 1e-05)  # t957: "cuda:0 f32[1, 512, 1]"
  t958 = ltorch.rsqrt(t957)  # t958: "cuda:0 f32[1, 512, 1]"
    # t958 = prims.rsqrt(t957)  # t958: "cuda:0 f32[1, 512, 1]"
  t960 = ltorch.mul(t950, t958)  # t960: "cuda:0 f32[1, 512, 4096]"
    # t959 = prims.broadcast_in_dim(t958, (1, 512, 4096), (0, 1, 2))  # t959: "cuda:0 f32[1, 512, 4096]"
    # t960 = prims.mul(t950, t959)  # t960: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t961 = ltorch.to(t960, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t961: "cuda:0 bf16[1, 512, 4096]"
    # t961 = prims.convert_element_type(t960, dtypes.bfloat16)  # t961: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t971 = ltorch.mul(t961, t_transformer_h_5_norm_2_weight)  # t971: "cuda:0 bf16[1, 512, 4096]"
    # t967 = prims.broadcast_in_dim(t_transformer_h_5_norm_2_weight, (1, 512, 4096), (2,))  # t967: "cuda:0 bf16[1, 512, 4096]"
    # t968 = prims.convert_element_type(t961, dtypes.float32)  # t968: "cuda:0 f32[1, 512, 4096]"
    # t969 = prims.convert_element_type(t967, dtypes.float32)  # t969: "cuda:0 f32[1, 512, 4096]"
    # t970 = prims.mul(t968, t969)  # t970: "cuda:0 f32[1, 512, 4096]"
    # t971 = prims.convert_element_type(t970, dtypes.bfloat16)  # t971: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t976 = ltorch.linear(t971, t_transformer_h_5_mlp_fc_1_weight, None)  # t976: "cuda:0 bf16[1, 512, 11008]"
    # t976 = prims.linear(t971, t_transformer_h_5_mlp_fc_1_weight, None)  # t976: "cuda:0 bf16[1, 512, 11008]"
  t980 = ltorch.linear(t971, t_transformer_h_5_mlp_fc_2_weight, None)  # t980: "cuda:0 bf16[1, 512, 11008]"
    # t980 = prims.linear(t971, t_transformer_h_5_mlp_fc_2_weight, None)  # t980: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t990 = ltorch.silu(t976, False)  # t990: "cuda:0 bf16[1, 512, 11008]"
    # t981 = prims.convert_element_type(t976, dtypes.float32)  # t981: "cuda:0 f32[1, 512, 11008]"
    # t982 = prims.neg(t981)  # t982: "cuda:0 f32[1, 512, 11008]"
    # t983 = prims.exp(t982)  # t983: "cuda:0 f32[1, 512, 11008]"
    # t984 = prims.add(1.0, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
    # t985 = prims.reciprocal(t984)  # t985: "cuda:0 f32[1, 512, 11008]"
    # t986 = prims.convert_element_type(t985, dtypes.bfloat16)  # t986: "cuda:0 bf16[1, 512, 11008]"
    # t987 = prims.convert_element_type(t976, dtypes.float32)  # t987: "cuda:0 f32[1, 512, 11008]"
    # t988 = prims.convert_element_type(t986, dtypes.float32)  # t988: "cuda:0 f32[1, 512, 11008]"
    # t989 = prims.mul(t987, t988)  # t989: "cuda:0 f32[1, 512, 11008]"
    # t990 = prims.convert_element_type(t989, dtypes.bfloat16)  # t990: "cuda:0 bf16[1, 512, 11008]"
  t994 = ltorch.mul(t990, t980)  # t994: "cuda:0 bf16[1, 512, 11008]"
    # t991 = prims.convert_element_type(t990, dtypes.float32)  # t991: "cuda:0 f32[1, 512, 11008]"
    # t992 = prims.convert_element_type(t980, dtypes.float32)  # t992: "cuda:0 f32[1, 512, 11008]"
    # t993 = prims.mul(t991, t992)  # t993: "cuda:0 f32[1, 512, 11008]"
    # t994 = prims.convert_element_type(t993, dtypes.bfloat16)  # t994: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t998 = ltorch.linear(t994, t_transformer_h_5_mlp_proj_weight, None)  # t998: "cuda:0 bf16[1, 512, 4096]"
    # t998 = prims.linear(t994, t_transformer_h_5_mlp_proj_weight, None)  # t998: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1002 = ltorch.add(t998, t949, alpha=None)  # t1002: "cuda:0 bf16[1, 512, 4096]"
    # t999 = prims.convert_element_type(t998, dtypes.float32)  # t999: "cuda:0 f32[1, 512, 4096]"
    # t1000 = prims.convert_element_type(t949, dtypes.float32)  # t1000: "cuda:0 f32[1, 512, 4096]"
    # t1001 = prims.add(t999, t1000)  # t1001: "cuda:0 f32[1, 512, 4096]"
    # t1002 = prims.convert_element_type(t1001, dtypes.bfloat16)  # t1002: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1004 = prims.convert_element_type(t1002, dtypes.float32)  # t1004: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1005 = ltorch.mul(t1004, t1004)  # t1005: "cuda:0 f32[1, 512, 4096]"
    # t1005 = prims.mul(t1004, t1004)  # t1005: "cuda:0 f32[1, 512, 4096]"
  t1009 = ltorch.mean(t1005, -1, True, dtype=None)  # t1009: "cuda:0 f32[1, 512, 1]"
    # t1007 = prims.sum(t1005, (2,))  # t1007: "cuda:0 f32[1, 512]"
    # t1008 = prims.broadcast_in_dim(t1007, [1, 512, 1], [0, 1])  # t1008: "cuda:0 f32[1, 512, 1]"
    # t1009 = ltorch.true_divide(t1008, 4096)  # t1009: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1009 = prims.div(t1008, 4096.0)  # t1009: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1011 = ltorch.add(t1009, 1e-05, alpha=None)  # t1011: "cuda:0 f32[1, 512, 1]"
    # t1011 = prims.add(t1009, 1e-05)  # t1011: "cuda:0 f32[1, 512, 1]"
  t1012 = ltorch.rsqrt(t1011)  # t1012: "cuda:0 f32[1, 512, 1]"
    # t1012 = prims.rsqrt(t1011)  # t1012: "cuda:0 f32[1, 512, 1]"
  t1014 = ltorch.mul(t1004, t1012)  # t1014: "cuda:0 f32[1, 512, 4096]"
    # t1013 = prims.broadcast_in_dim(t1012, (1, 512, 4096), (0, 1, 2))  # t1013: "cuda:0 f32[1, 512, 4096]"
    # t1014 = prims.mul(t1004, t1013)  # t1014: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1015 = ltorch.to(t1014, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1015: "cuda:0 bf16[1, 512, 4096]"
    # t1015 = prims.convert_element_type(t1014, dtypes.bfloat16)  # t1015: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1025 = ltorch.mul(t1015, t_transformer_h_6_norm_1_weight)  # t1025: "cuda:0 bf16[1, 512, 4096]"
    # t1021 = prims.broadcast_in_dim(t_transformer_h_6_norm_1_weight, (1, 512, 4096), (2,))  # t1021: "cuda:0 bf16[1, 512, 4096]"
    # t1022 = prims.convert_element_type(t1015, dtypes.float32)  # t1022: "cuda:0 f32[1, 512, 4096]"
    # t1023 = prims.convert_element_type(t1021, dtypes.float32)  # t1023: "cuda:0 f32[1, 512, 4096]"
    # t1024 = prims.mul(t1022, t1023)  # t1024: "cuda:0 f32[1, 512, 4096]"
    # t1025 = prims.convert_element_type(t1024, dtypes.bfloat16)  # t1025: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1030 = ltorch.linear(t1025, t_transformer_h_6_attn_attn_weight, None)  # t1030: "cuda:0 bf16[1, 512, 12288]"
    # t1030 = prims.linear(t1025, t_transformer_h_6_attn_attn_weight, None)  # t1030: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1031 = ltorch.view(t1030, 1, 512, 32, 3, 128)  # t1031: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1031 = ltorch.reshape(t1030, (1, 512, 32, 3, 128))  # t1031: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1031 = prims.reshape(t1030, (1, 512, 32, 3, 128))  # t1031: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1032 = ltorch.permute(t1031, 0, 2, 3, 1, 4)  # t1032: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1032 = prims.transpose(t1031, (0, 2, 3, 1, 4))  # t1032: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1033, t1034, t1035) = ltorch.split(t1032, (1, 1, 1), 2)
    # t1033 = prims.slice_prim(t1032, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1033: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1034 = prims.slice_prim(t1032, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1034: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1035 = prims.slice_prim(t1032, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1035: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1036 = ltorch.reshape(t1033, 1, -1, 512, 128)  # t1036: "cuda:0 bf16[1, 32, 512, 128]"
    # t1036 = prims.reshape(t1033, (1, 32, 512, 128))  # t1036: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1037 = ltorch.reshape(t1034, 1, -1, 512, 128)  # t1037: "cuda:0 bf16[1, 32, 512, 128]"
    # t1037 = prims.reshape(t1034, (1, 32, 512, 128))  # t1037: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1038 = ltorch.reshape(t1035, 1, -1, 512, 128)  # t1038: "cuda:0 bf16[1, 32, 512, 128]"
    # t1038 = prims.reshape(t1035, (1, 32, 512, 128))  # t1038: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1039 = ltorch.getitem(t1036, (..., slice(None, 128, None)))  # t1039: "cuda:0 bf16[1, 32, 512, 128]"
    # t1039 = prims.slice_prim(t1036, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1039: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1040 = ltorch.getitem(t1039, (..., slice(None, 64, None)))  # t1040: "cuda:0 bf16[1, 32, 512, 64]"
    # t1040 = prims.slice_prim(t1039, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1040: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1041 = ltorch.getitem(t1039, (..., slice(64, None, None)))  # t1041: "cuda:0 bf16[1, 32, 512, 64]"
    # t1041 = prims.slice_prim(t1039, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1041: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1044 = ltorch.neg(t1041)  # t1044: "cuda:0 bf16[1, 32, 512, 64]"
    # t1042 = prims.convert_element_type(t1041, dtypes.float32)  # t1042: "cuda:0 f32[1, 32, 512, 64]"
    # t1043 = prims.neg(t1042)  # t1043: "cuda:0 f32[1, 32, 512, 64]"
    # t1044 = prims.convert_element_type(t1043, dtypes.bfloat16)  # t1044: "cuda:0 bf16[1, 32, 512, 64]"
  t1045 = ltorch.cat((t1044, t1040), -1)  # t1045: "cuda:0 bf16[1, 32, 512, 128]"
    # t1045 = prims.cat((t1044, t1040), -1)  # t1045: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1048 = ltorch.mul(t1039, cos)  # t1048: "cuda:0 f32[1, 32, 512, 128]"
    # t1046 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1046: "cuda:0 f32[1, 32, 512, 128]"
    # t1047 = prims.convert_element_type(t1039, dtypes.float32)  # t1047: "cuda:0 f32[1, 32, 512, 128]"
    # t1048 = prims.mul(t1047, t1046)  # t1048: "cuda:0 f32[1, 32, 512, 128]"
  t1051 = ltorch.mul(t1045, sin)  # t1051: "cuda:0 f32[1, 32, 512, 128]"
    # t1049 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1049: "cuda:0 f32[1, 32, 512, 128]"
    # t1050 = prims.convert_element_type(t1045, dtypes.float32)  # t1050: "cuda:0 f32[1, 32, 512, 128]"
    # t1051 = prims.mul(t1050, t1049)  # t1051: "cuda:0 f32[1, 32, 512, 128]"
  t1052 = ltorch.add(t1048, t1051, alpha=None)  # t1052: "cuda:0 f32[1, 32, 512, 128]"
    # t1052 = prims.add(t1048, t1051)  # t1052: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1053 = ltorch.to(t1052, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1053: "cuda:0 bf16[1, 32, 512, 128]"
    # t1053 = prims.convert_element_type(t1052, dtypes.bfloat16)  # t1053: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1054 = ltorch.getitem(t1037, (..., slice(None, 128, None)))  # t1054: "cuda:0 bf16[1, 32, 512, 128]"
    # t1054 = prims.slice_prim(t1037, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1054: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1055 = ltorch.getitem(t1054, (..., slice(None, 64, None)))  # t1055: "cuda:0 bf16[1, 32, 512, 64]"
    # t1055 = prims.slice_prim(t1054, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1055: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1056 = ltorch.getitem(t1054, (..., slice(64, None, None)))  # t1056: "cuda:0 bf16[1, 32, 512, 64]"
    # t1056 = prims.slice_prim(t1054, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1056: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1059 = ltorch.neg(t1056)  # t1059: "cuda:0 bf16[1, 32, 512, 64]"
    # t1057 = prims.convert_element_type(t1056, dtypes.float32)  # t1057: "cuda:0 f32[1, 32, 512, 64]"
    # t1058 = prims.neg(t1057)  # t1058: "cuda:0 f32[1, 32, 512, 64]"
    # t1059 = prims.convert_element_type(t1058, dtypes.bfloat16)  # t1059: "cuda:0 bf16[1, 32, 512, 64]"
  t1060 = ltorch.cat((t1059, t1055), -1)  # t1060: "cuda:0 bf16[1, 32, 512, 128]"
    # t1060 = prims.cat((t1059, t1055), -1)  # t1060: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1063 = ltorch.mul(t1054, cos)  # t1063: "cuda:0 f32[1, 32, 512, 128]"
    # t1061 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1061: "cuda:0 f32[1, 32, 512, 128]"
    # t1062 = prims.convert_element_type(t1054, dtypes.float32)  # t1062: "cuda:0 f32[1, 32, 512, 128]"
    # t1063 = prims.mul(t1062, t1061)  # t1063: "cuda:0 f32[1, 32, 512, 128]"
  t1066 = ltorch.mul(t1060, sin)  # t1066: "cuda:0 f32[1, 32, 512, 128]"
    # t1064 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1064: "cuda:0 f32[1, 32, 512, 128]"
    # t1065 = prims.convert_element_type(t1060, dtypes.float32)  # t1065: "cuda:0 f32[1, 32, 512, 128]"
    # t1066 = prims.mul(t1065, t1064)  # t1066: "cuda:0 f32[1, 32, 512, 128]"
  t1067 = ltorch.add(t1063, t1066, alpha=None)  # t1067: "cuda:0 f32[1, 32, 512, 128]"
    # t1067 = prims.add(t1063, t1066)  # t1067: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1068 = ltorch.to(t1067, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1068: "cuda:0 bf16[1, 32, 512, 128]"
    # t1068 = prims.convert_element_type(t1067, dtypes.bfloat16)  # t1068: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1069 = ltorch.getitem(t1036, (..., slice(128, None, None)))  # t1069: "cuda:0 bf16[1, 32, 512, 0]"
    # t1069 = prims.slice_prim(t1036, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1069: "cuda:0 bf16[1, 32, 512, 0]"
  t1070 = ltorch.cat((t1053, t1069), -1)  # t1070: "cuda:0 bf16[1, 32, 512, 128]"
    # t1070 = prims.cat((t1053, t1069), -1)  # t1070: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1071 = ltorch.getitem(t1037, (..., slice(128, None, None)))  # t1071: "cuda:0 bf16[1, 32, 512, 0]"
    # t1071 = prims.slice_prim(t1037, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1071: "cuda:0 bf16[1, 32, 512, 0]"
  t1072 = ltorch.cat((t1068, t1071), -1)  # t1072: "cuda:0 bf16[1, 32, 512, 128]"
    # t1072 = prims.cat((t1068, t1071), -1)  # t1072: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1102 = ltorch.scaled_dot_product_attention(t1070, t1072, t1038, None, 0.0, True, scale=0.08838834764831843)  # t1102: "cuda:0 bf16[1, 32, 512, 128]"
    # t1075 = ltorch.mul(t1070, 0.29730177875068026)  # t1075: "cuda:0 bf16[1, 32, 512, 128]"
      # t1073 = prims.convert_element_type(t1070, dtypes.float32)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
      # t1074 = prims.mul(t1073, 0.29730177875068026)  # t1074: "cuda:0 f32[1, 32, 512, 128]"
      # t1075 = prims.convert_element_type(t1074, dtypes.bfloat16)  # t1075: "cuda:0 bf16[1, 32, 512, 128]"
    # t1076 = ltorch.transpose(t1072, -2, -1)  # t1076: "cuda:0 bf16[1, 32, 128, 512]"
      # t1076 = prims.transpose(t1072, (0, 1, 3, 2))  # t1076: "cuda:0 bf16[1, 32, 128, 512]"
    # t1079 = ltorch.mul(t1076, 0.29730177875068026)  # t1079: "cuda:0 bf16[1, 32, 128, 512]"
      # t1077 = prims.convert_element_type(t1076, dtypes.float32)  # t1077: "cuda:0 f32[1, 32, 128, 512]"
      # t1078 = prims.mul(t1077, 0.29730177875068026)  # t1078: "cuda:0 f32[1, 32, 128, 512]"
      # t1079 = prims.convert_element_type(t1078, dtypes.bfloat16)  # t1079: "cuda:0 bf16[1, 32, 128, 512]"
    # t1080 = ltorch.matmul(t1075, t1079)  # t1080: "cuda:0 bf16[1, 32, 512, 512]"
      # t1080 = prims.matmul(t1075, t1079)  # t1080: "cuda:0 bf16[1, 32, 512, 512]"
    # t1090 = ltorch.tril(t1080, 0, fill_value=-float('inf'))  # t1090: "cuda:0 bf16[1, 32, 512, 512]"
      # t1081 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1081: "cuda:0 i64[512]"
        # t1081 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1081: "cuda:0 i64[512]"
      # t1082 = ltorch.unsqueeze(t1081, -1)  # t1082: "cuda:0 i64[512, 1]"
        # t1082 = prims.broadcast_in_dim(t1081, [512, 1], [0])  # t1082: "cuda:0 i64[512, 1]"
      # t1083 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1083: "cuda:0 i64[512]"
        # t1083 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1083: "cuda:0 i64[512]"
      # t1084 = ltorch.unsqueeze(t1083, -2)  # t1084: "cuda:0 i64[1, 512]"
        # t1084 = prims.broadcast_in_dim(t1083, [1, 512], [1])  # t1084: "cuda:0 i64[1, 512]"
      # t1085 = ltorch.add(t1082, 0, alpha=None)  # t1085: "cuda:0 i64[512, 1]"
        # t1085 = prims.add(t1082, 0)  # t1085: "cuda:0 i64[512, 1]"
      # t1088 = ltorch.ge(t1085, t1084)  # t1088: "cuda:0 b8[512, 512]"
        # t1086 = prims.broadcast_in_dim(t1085, (512, 512), (0, 1))  # t1086: "cuda:0 i64[512, 512]"
        # t1087 = prims.broadcast_in_dim(t1084, (512, 512), (0, 1))  # t1087: "cuda:0 i64[512, 512]"
        # t1088 = prims.ge(t1086, t1087)  # t1088: "cuda:0 b8[512, 512]"
      # t1090 = ltorch.where(t1088, t1080, -float('inf'))  # t1090: "cuda:0 bf16[1, 32, 512, 512]"
        # t1089 = prims.broadcast_in_dim(t1088, (1, 32, 512, 512), (2, 3))  # t1089: "cuda:0 b8[1, 32, 512, 512]"
        # t1090 = prims.where(t1089, t1080, -float('inf'))  # t1090: "cuda:0 bf16[1, 32, 512, 512]"
    # t1101 = ltorch._softmax(t1090, -1, dtype=None)  # t1101: "cuda:0 bf16[1, 32, 512, 512]"
      # t1091 = ltorch.to(t1090, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1091: "cuda:0 f32[1, 32, 512, 512]"
        # t1091 = prims.convert_element_type(t1090, dtypes.float32)  # t1091: "cuda:0 f32[1, 32, 512, 512]"
      # t1093 = ltorch.amax(t1091, -1, True)  # t1093: "cuda:0 f32[1, 32, 512, 1]"
        # t1092 = prims.amax(t1091, (3,))  # t1092: "cuda:0 f32[1, 32, 512]"
        # t1093 = prims.broadcast_in_dim(t1092, [1, 32, 512, 1], [0, 1, 2])  # t1093: "cuda:0 f32[1, 32, 512, 1]"
      # t1095 = ltorch.sub(t1091, t1093, alpha=None)  # t1095: "cuda:0 f32[1, 32, 512, 512]"
        # t1094 = prims.broadcast_in_dim(t1093, (1, 32, 512, 512), (0, 1, 2, 3))  # t1094: "cuda:0 f32[1, 32, 512, 512]"
        # t1095 = prims.sub(t1091, t1094)  # t1095: "cuda:0 f32[1, 32, 512, 512]"
      # t1096 = ltorch.exp(t1095)  # t1096: "cuda:0 f32[1, 32, 512, 512]"
        # t1096 = prims.exp(t1095)  # t1096: "cuda:0 f32[1, 32, 512, 512]"
      # t1098 = ltorch.sum(t1096, -1, True, dtype=None)  # t1098: "cuda:0 f32[1, 32, 512, 1]"
        # t1097 = prims.sum(t1096, (3,))  # t1097: "cuda:0 f32[1, 32, 512]"
        # t1098 = prims.broadcast_in_dim(t1097, [1, 32, 512, 1], [0, 1, 2])  # t1098: "cuda:0 f32[1, 32, 512, 1]"
      # t1100 = ltorch.true_divide(t1096, t1098)  # t1100: "cuda:0 f32[1, 32, 512, 512]"
        # t1099 = prims.broadcast_in_dim(t1098, (1, 32, 512, 512), (0, 1, 2, 3))  # t1099: "cuda:0 f32[1, 32, 512, 512]"
        # t1100 = prims.div(t1096, t1099)  # t1100: "cuda:0 f32[1, 32, 512, 512]"
      # t1101 = ltorch.to(t1100, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1101: "cuda:0 bf16[1, 32, 512, 512]"
        # t1101 = prims.convert_element_type(t1100, dtypes.bfloat16)  # t1101: "cuda:0 bf16[1, 32, 512, 512]"
    # t1102 = ltorch.matmul(t1101, t1038)  # t1102: "cuda:0 bf16[1, 32, 512, 128]"
      # t1102 = prims.matmul(t1101, t1038)  # t1102: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1103 = ltorch.transpose(t1102, 1, 2)  # t1103: "cuda:0 bf16[1, 512, 32, 128]"
    # t1103 = prims.transpose(t1102, (0, 2, 1, 3))  # t1103: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1104 = ltorch.reshape(t1103, 1, 512, 4096)  # t1104: "cuda:0 bf16[1, 512, 4096]"
    # t1104 = prims.reshape(t1103, (1, 512, 4096))  # t1104: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1108 = ltorch.linear(t1104, t_transformer_h_6_attn_proj_weight, None)  # t1108: "cuda:0 bf16[1, 512, 4096]"
    # t1108 = prims.linear(t1104, t_transformer_h_6_attn_proj_weight, None)  # t1108: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1112 = ltorch.add(t1108, t1002, alpha=None)  # t1112: "cuda:0 bf16[1, 512, 4096]"
    # t1109 = prims.convert_element_type(t1108, dtypes.float32)  # t1109: "cuda:0 f32[1, 512, 4096]"
    # t1110 = prims.convert_element_type(t1002, dtypes.float32)  # t1110: "cuda:0 f32[1, 512, 4096]"
    # t1111 = prims.add(t1109, t1110)  # t1111: "cuda:0 f32[1, 512, 4096]"
    # t1112 = prims.convert_element_type(t1111, dtypes.bfloat16)  # t1112: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1113 = prims.convert_element_type(t1112, dtypes.float32)  # t1113: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1114 = ltorch.mul(t1113, t1113)  # t1114: "cuda:0 f32[1, 512, 4096]"
    # t1114 = prims.mul(t1113, t1113)  # t1114: "cuda:0 f32[1, 512, 4096]"
  t1118 = ltorch.mean(t1114, -1, True, dtype=None)  # t1118: "cuda:0 f32[1, 512, 1]"
    # t1116 = prims.sum(t1114, (2,))  # t1116: "cuda:0 f32[1, 512]"
    # t1117 = prims.broadcast_in_dim(t1116, [1, 512, 1], [0, 1])  # t1117: "cuda:0 f32[1, 512, 1]"
    # t1118 = ltorch.true_divide(t1117, 4096)  # t1118: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1118 = prims.div(t1117, 4096.0)  # t1118: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1120 = ltorch.add(t1118, 1e-05, alpha=None)  # t1120: "cuda:0 f32[1, 512, 1]"
    # t1120 = prims.add(t1118, 1e-05)  # t1120: "cuda:0 f32[1, 512, 1]"
  t1121 = ltorch.rsqrt(t1120)  # t1121: "cuda:0 f32[1, 512, 1]"
    # t1121 = prims.rsqrt(t1120)  # t1121: "cuda:0 f32[1, 512, 1]"
  t1123 = ltorch.mul(t1113, t1121)  # t1123: "cuda:0 f32[1, 512, 4096]"
    # t1122 = prims.broadcast_in_dim(t1121, (1, 512, 4096), (0, 1, 2))  # t1122: "cuda:0 f32[1, 512, 4096]"
    # t1123 = prims.mul(t1113, t1122)  # t1123: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1124 = ltorch.to(t1123, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1124: "cuda:0 bf16[1, 512, 4096]"
    # t1124 = prims.convert_element_type(t1123, dtypes.bfloat16)  # t1124: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1134 = ltorch.mul(t1124, t_transformer_h_6_norm_2_weight)  # t1134: "cuda:0 bf16[1, 512, 4096]"
    # t1130 = prims.broadcast_in_dim(t_transformer_h_6_norm_2_weight, (1, 512, 4096), (2,))  # t1130: "cuda:0 bf16[1, 512, 4096]"
    # t1131 = prims.convert_element_type(t1124, dtypes.float32)  # t1131: "cuda:0 f32[1, 512, 4096]"
    # t1132 = prims.convert_element_type(t1130, dtypes.float32)  # t1132: "cuda:0 f32[1, 512, 4096]"
    # t1133 = prims.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 4096]"
    # t1134 = prims.convert_element_type(t1133, dtypes.bfloat16)  # t1134: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1139 = ltorch.linear(t1134, t_transformer_h_6_mlp_fc_1_weight, None)  # t1139: "cuda:0 bf16[1, 512, 11008]"
    # t1139 = prims.linear(t1134, t_transformer_h_6_mlp_fc_1_weight, None)  # t1139: "cuda:0 bf16[1, 512, 11008]"
  t1143 = ltorch.linear(t1134, t_transformer_h_6_mlp_fc_2_weight, None)  # t1143: "cuda:0 bf16[1, 512, 11008]"
    # t1143 = prims.linear(t1134, t_transformer_h_6_mlp_fc_2_weight, None)  # t1143: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1153 = ltorch.silu(t1139, False)  # t1153: "cuda:0 bf16[1, 512, 11008]"
    # t1144 = prims.convert_element_type(t1139, dtypes.float32)  # t1144: "cuda:0 f32[1, 512, 11008]"
    # t1145 = prims.neg(t1144)  # t1145: "cuda:0 f32[1, 512, 11008]"
    # t1146 = prims.exp(t1145)  # t1146: "cuda:0 f32[1, 512, 11008]"
    # t1147 = prims.add(1.0, t1146)  # t1147: "cuda:0 f32[1, 512, 11008]"
    # t1148 = prims.reciprocal(t1147)  # t1148: "cuda:0 f32[1, 512, 11008]"
    # t1149 = prims.convert_element_type(t1148, dtypes.bfloat16)  # t1149: "cuda:0 bf16[1, 512, 11008]"
    # t1150 = prims.convert_element_type(t1139, dtypes.float32)  # t1150: "cuda:0 f32[1, 512, 11008]"
    # t1151 = prims.convert_element_type(t1149, dtypes.float32)  # t1151: "cuda:0 f32[1, 512, 11008]"
    # t1152 = prims.mul(t1150, t1151)  # t1152: "cuda:0 f32[1, 512, 11008]"
    # t1153 = prims.convert_element_type(t1152, dtypes.bfloat16)  # t1153: "cuda:0 bf16[1, 512, 11008]"
  t1157 = ltorch.mul(t1153, t1143)  # t1157: "cuda:0 bf16[1, 512, 11008]"
    # t1154 = prims.convert_element_type(t1153, dtypes.float32)  # t1154: "cuda:0 f32[1, 512, 11008]"
    # t1155 = prims.convert_element_type(t1143, dtypes.float32)  # t1155: "cuda:0 f32[1, 512, 11008]"
    # t1156 = prims.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 11008]"
    # t1157 = prims.convert_element_type(t1156, dtypes.bfloat16)  # t1157: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1161 = ltorch.linear(t1157, t_transformer_h_6_mlp_proj_weight, None)  # t1161: "cuda:0 bf16[1, 512, 4096]"
    # t1161 = prims.linear(t1157, t_transformer_h_6_mlp_proj_weight, None)  # t1161: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1165 = ltorch.add(t1161, t1112, alpha=None)  # t1165: "cuda:0 bf16[1, 512, 4096]"
    # t1162 = prims.convert_element_type(t1161, dtypes.float32)  # t1162: "cuda:0 f32[1, 512, 4096]"
    # t1163 = prims.convert_element_type(t1112, dtypes.float32)  # t1163: "cuda:0 f32[1, 512, 4096]"
    # t1164 = prims.add(t1162, t1163)  # t1164: "cuda:0 f32[1, 512, 4096]"
    # t1165 = prims.convert_element_type(t1164, dtypes.bfloat16)  # t1165: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1167 = prims.convert_element_type(t1165, dtypes.float32)  # t1167: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1168 = ltorch.mul(t1167, t1167)  # t1168: "cuda:0 f32[1, 512, 4096]"
    # t1168 = prims.mul(t1167, t1167)  # t1168: "cuda:0 f32[1, 512, 4096]"
  t1172 = ltorch.mean(t1168, -1, True, dtype=None)  # t1172: "cuda:0 f32[1, 512, 1]"
    # t1170 = prims.sum(t1168, (2,))  # t1170: "cuda:0 f32[1, 512]"
    # t1171 = prims.broadcast_in_dim(t1170, [1, 512, 1], [0, 1])  # t1171: "cuda:0 f32[1, 512, 1]"
    # t1172 = ltorch.true_divide(t1171, 4096)  # t1172: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1172 = prims.div(t1171, 4096.0)  # t1172: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1174 = ltorch.add(t1172, 1e-05, alpha=None)  # t1174: "cuda:0 f32[1, 512, 1]"
    # t1174 = prims.add(t1172, 1e-05)  # t1174: "cuda:0 f32[1, 512, 1]"
  t1175 = ltorch.rsqrt(t1174)  # t1175: "cuda:0 f32[1, 512, 1]"
    # t1175 = prims.rsqrt(t1174)  # t1175: "cuda:0 f32[1, 512, 1]"
  t1177 = ltorch.mul(t1167, t1175)  # t1177: "cuda:0 f32[1, 512, 4096]"
    # t1176 = prims.broadcast_in_dim(t1175, (1, 512, 4096), (0, 1, 2))  # t1176: "cuda:0 f32[1, 512, 4096]"
    # t1177 = prims.mul(t1167, t1176)  # t1177: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1178 = ltorch.to(t1177, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1178: "cuda:0 bf16[1, 512, 4096]"
    # t1178 = prims.convert_element_type(t1177, dtypes.bfloat16)  # t1178: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1188 = ltorch.mul(t1178, t_transformer_h_7_norm_1_weight)  # t1188: "cuda:0 bf16[1, 512, 4096]"
    # t1184 = prims.broadcast_in_dim(t_transformer_h_7_norm_1_weight, (1, 512, 4096), (2,))  # t1184: "cuda:0 bf16[1, 512, 4096]"
    # t1185 = prims.convert_element_type(t1178, dtypes.float32)  # t1185: "cuda:0 f32[1, 512, 4096]"
    # t1186 = prims.convert_element_type(t1184, dtypes.float32)  # t1186: "cuda:0 f32[1, 512, 4096]"
    # t1187 = prims.mul(t1185, t1186)  # t1187: "cuda:0 f32[1, 512, 4096]"
    # t1188 = prims.convert_element_type(t1187, dtypes.bfloat16)  # t1188: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1193 = ltorch.linear(t1188, t_transformer_h_7_attn_attn_weight, None)  # t1193: "cuda:0 bf16[1, 512, 12288]"
    # t1193 = prims.linear(t1188, t_transformer_h_7_attn_attn_weight, None)  # t1193: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1194 = ltorch.view(t1193, 1, 512, 32, 3, 128)  # t1194: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1194 = ltorch.reshape(t1193, (1, 512, 32, 3, 128))  # t1194: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1194 = prims.reshape(t1193, (1, 512, 32, 3, 128))  # t1194: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1195 = ltorch.permute(t1194, 0, 2, 3, 1, 4)  # t1195: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1195 = prims.transpose(t1194, (0, 2, 3, 1, 4))  # t1195: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1196, t1197, t1198) = ltorch.split(t1195, (1, 1, 1), 2)
    # t1196 = prims.slice_prim(t1195, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1196: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1197 = prims.slice_prim(t1195, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1197: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1198 = prims.slice_prim(t1195, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1198: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1199 = ltorch.reshape(t1196, 1, -1, 512, 128)  # t1199: "cuda:0 bf16[1, 32, 512, 128]"
    # t1199 = prims.reshape(t1196, (1, 32, 512, 128))  # t1199: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1200 = ltorch.reshape(t1197, 1, -1, 512, 128)  # t1200: "cuda:0 bf16[1, 32, 512, 128]"
    # t1200 = prims.reshape(t1197, (1, 32, 512, 128))  # t1200: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1201 = ltorch.reshape(t1198, 1, -1, 512, 128)  # t1201: "cuda:0 bf16[1, 32, 512, 128]"
    # t1201 = prims.reshape(t1198, (1, 32, 512, 128))  # t1201: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1202 = ltorch.getitem(t1199, (..., slice(None, 128, None)))  # t1202: "cuda:0 bf16[1, 32, 512, 128]"
    # t1202 = prims.slice_prim(t1199, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1202: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1203 = ltorch.getitem(t1202, (..., slice(None, 64, None)))  # t1203: "cuda:0 bf16[1, 32, 512, 64]"
    # t1203 = prims.slice_prim(t1202, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1203: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1204 = ltorch.getitem(t1202, (..., slice(64, None, None)))  # t1204: "cuda:0 bf16[1, 32, 512, 64]"
    # t1204 = prims.slice_prim(t1202, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1204: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1207 = ltorch.neg(t1204)  # t1207: "cuda:0 bf16[1, 32, 512, 64]"
    # t1205 = prims.convert_element_type(t1204, dtypes.float32)  # t1205: "cuda:0 f32[1, 32, 512, 64]"
    # t1206 = prims.neg(t1205)  # t1206: "cuda:0 f32[1, 32, 512, 64]"
    # t1207 = prims.convert_element_type(t1206, dtypes.bfloat16)  # t1207: "cuda:0 bf16[1, 32, 512, 64]"
  t1208 = ltorch.cat((t1207, t1203), -1)  # t1208: "cuda:0 bf16[1, 32, 512, 128]"
    # t1208 = prims.cat((t1207, t1203), -1)  # t1208: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1211 = ltorch.mul(t1202, cos)  # t1211: "cuda:0 f32[1, 32, 512, 128]"
    # t1209 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1209: "cuda:0 f32[1, 32, 512, 128]"
    # t1210 = prims.convert_element_type(t1202, dtypes.float32)  # t1210: "cuda:0 f32[1, 32, 512, 128]"
    # t1211 = prims.mul(t1210, t1209)  # t1211: "cuda:0 f32[1, 32, 512, 128]"
  t1214 = ltorch.mul(t1208, sin)  # t1214: "cuda:0 f32[1, 32, 512, 128]"
    # t1212 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1212: "cuda:0 f32[1, 32, 512, 128]"
    # t1213 = prims.convert_element_type(t1208, dtypes.float32)  # t1213: "cuda:0 f32[1, 32, 512, 128]"
    # t1214 = prims.mul(t1213, t1212)  # t1214: "cuda:0 f32[1, 32, 512, 128]"
  t1215 = ltorch.add(t1211, t1214, alpha=None)  # t1215: "cuda:0 f32[1, 32, 512, 128]"
    # t1215 = prims.add(t1211, t1214)  # t1215: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1216 = ltorch.to(t1215, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1216: "cuda:0 bf16[1, 32, 512, 128]"
    # t1216 = prims.convert_element_type(t1215, dtypes.bfloat16)  # t1216: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1217 = ltorch.getitem(t1200, (..., slice(None, 128, None)))  # t1217: "cuda:0 bf16[1, 32, 512, 128]"
    # t1217 = prims.slice_prim(t1200, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1217: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1218 = ltorch.getitem(t1217, (..., slice(None, 64, None)))  # t1218: "cuda:0 bf16[1, 32, 512, 64]"
    # t1218 = prims.slice_prim(t1217, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1218: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1219 = ltorch.getitem(t1217, (..., slice(64, None, None)))  # t1219: "cuda:0 bf16[1, 32, 512, 64]"
    # t1219 = prims.slice_prim(t1217, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1219: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1222 = ltorch.neg(t1219)  # t1222: "cuda:0 bf16[1, 32, 512, 64]"
    # t1220 = prims.convert_element_type(t1219, dtypes.float32)  # t1220: "cuda:0 f32[1, 32, 512, 64]"
    # t1221 = prims.neg(t1220)  # t1221: "cuda:0 f32[1, 32, 512, 64]"
    # t1222 = prims.convert_element_type(t1221, dtypes.bfloat16)  # t1222: "cuda:0 bf16[1, 32, 512, 64]"
  t1223 = ltorch.cat((t1222, t1218), -1)  # t1223: "cuda:0 bf16[1, 32, 512, 128]"
    # t1223 = prims.cat((t1222, t1218), -1)  # t1223: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1226 = ltorch.mul(t1217, cos)  # t1226: "cuda:0 f32[1, 32, 512, 128]"
    # t1224 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1224: "cuda:0 f32[1, 32, 512, 128]"
    # t1225 = prims.convert_element_type(t1217, dtypes.float32)  # t1225: "cuda:0 f32[1, 32, 512, 128]"
    # t1226 = prims.mul(t1225, t1224)  # t1226: "cuda:0 f32[1, 32, 512, 128]"
  t1229 = ltorch.mul(t1223, sin)  # t1229: "cuda:0 f32[1, 32, 512, 128]"
    # t1227 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1227: "cuda:0 f32[1, 32, 512, 128]"
    # t1228 = prims.convert_element_type(t1223, dtypes.float32)  # t1228: "cuda:0 f32[1, 32, 512, 128]"
    # t1229 = prims.mul(t1228, t1227)  # t1229: "cuda:0 f32[1, 32, 512, 128]"
  t1230 = ltorch.add(t1226, t1229, alpha=None)  # t1230: "cuda:0 f32[1, 32, 512, 128]"
    # t1230 = prims.add(t1226, t1229)  # t1230: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1231 = ltorch.to(t1230, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1231: "cuda:0 bf16[1, 32, 512, 128]"
    # t1231 = prims.convert_element_type(t1230, dtypes.bfloat16)  # t1231: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1232 = ltorch.getitem(t1199, (..., slice(128, None, None)))  # t1232: "cuda:0 bf16[1, 32, 512, 0]"
    # t1232 = prims.slice_prim(t1199, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1232: "cuda:0 bf16[1, 32, 512, 0]"
  t1233 = ltorch.cat((t1216, t1232), -1)  # t1233: "cuda:0 bf16[1, 32, 512, 128]"
    # t1233 = prims.cat((t1216, t1232), -1)  # t1233: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1234 = ltorch.getitem(t1200, (..., slice(128, None, None)))  # t1234: "cuda:0 bf16[1, 32, 512, 0]"
    # t1234 = prims.slice_prim(t1200, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1234: "cuda:0 bf16[1, 32, 512, 0]"
  t1235 = ltorch.cat((t1231, t1234), -1)  # t1235: "cuda:0 bf16[1, 32, 512, 128]"
    # t1235 = prims.cat((t1231, t1234), -1)  # t1235: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1265 = ltorch.scaled_dot_product_attention(t1233, t1235, t1201, None, 0.0, True, scale=0.08838834764831843)  # t1265: "cuda:0 bf16[1, 32, 512, 128]"
    # t1238 = ltorch.mul(t1233, 0.29730177875068026)  # t1238: "cuda:0 bf16[1, 32, 512, 128]"
      # t1236 = prims.convert_element_type(t1233, dtypes.float32)  # t1236: "cuda:0 f32[1, 32, 512, 128]"
      # t1237 = prims.mul(t1236, 0.29730177875068026)  # t1237: "cuda:0 f32[1, 32, 512, 128]"
      # t1238 = prims.convert_element_type(t1237, dtypes.bfloat16)  # t1238: "cuda:0 bf16[1, 32, 512, 128]"
    # t1239 = ltorch.transpose(t1235, -2, -1)  # t1239: "cuda:0 bf16[1, 32, 128, 512]"
      # t1239 = prims.transpose(t1235, (0, 1, 3, 2))  # t1239: "cuda:0 bf16[1, 32, 128, 512]"
    # t1242 = ltorch.mul(t1239, 0.29730177875068026)  # t1242: "cuda:0 bf16[1, 32, 128, 512]"
      # t1240 = prims.convert_element_type(t1239, dtypes.float32)  # t1240: "cuda:0 f32[1, 32, 128, 512]"
      # t1241 = prims.mul(t1240, 0.29730177875068026)  # t1241: "cuda:0 f32[1, 32, 128, 512]"
      # t1242 = prims.convert_element_type(t1241, dtypes.bfloat16)  # t1242: "cuda:0 bf16[1, 32, 128, 512]"
    # t1243 = ltorch.matmul(t1238, t1242)  # t1243: "cuda:0 bf16[1, 32, 512, 512]"
      # t1243 = prims.matmul(t1238, t1242)  # t1243: "cuda:0 bf16[1, 32, 512, 512]"
    # t1253 = ltorch.tril(t1243, 0, fill_value=-float('inf'))  # t1253: "cuda:0 bf16[1, 32, 512, 512]"
      # t1244 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1244: "cuda:0 i64[512]"
        # t1244 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1244: "cuda:0 i64[512]"
      # t1245 = ltorch.unsqueeze(t1244, -1)  # t1245: "cuda:0 i64[512, 1]"
        # t1245 = prims.broadcast_in_dim(t1244, [512, 1], [0])  # t1245: "cuda:0 i64[512, 1]"
      # t1246 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1246: "cuda:0 i64[512]"
        # t1246 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1246: "cuda:0 i64[512]"
      # t1247 = ltorch.unsqueeze(t1246, -2)  # t1247: "cuda:0 i64[1, 512]"
        # t1247 = prims.broadcast_in_dim(t1246, [1, 512], [1])  # t1247: "cuda:0 i64[1, 512]"
      # t1248 = ltorch.add(t1245, 0, alpha=None)  # t1248: "cuda:0 i64[512, 1]"
        # t1248 = prims.add(t1245, 0)  # t1248: "cuda:0 i64[512, 1]"
      # t1251 = ltorch.ge(t1248, t1247)  # t1251: "cuda:0 b8[512, 512]"
        # t1249 = prims.broadcast_in_dim(t1248, (512, 512), (0, 1))  # t1249: "cuda:0 i64[512, 512]"
        # t1250 = prims.broadcast_in_dim(t1247, (512, 512), (0, 1))  # t1250: "cuda:0 i64[512, 512]"
        # t1251 = prims.ge(t1249, t1250)  # t1251: "cuda:0 b8[512, 512]"
      # t1253 = ltorch.where(t1251, t1243, -float('inf'))  # t1253: "cuda:0 bf16[1, 32, 512, 512]"
        # t1252 = prims.broadcast_in_dim(t1251, (1, 32, 512, 512), (2, 3))  # t1252: "cuda:0 b8[1, 32, 512, 512]"
        # t1253 = prims.where(t1252, t1243, -float('inf'))  # t1253: "cuda:0 bf16[1, 32, 512, 512]"
    # t1264 = ltorch._softmax(t1253, -1, dtype=None)  # t1264: "cuda:0 bf16[1, 32, 512, 512]"
      # t1254 = ltorch.to(t1253, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1254: "cuda:0 f32[1, 32, 512, 512]"
        # t1254 = prims.convert_element_type(t1253, dtypes.float32)  # t1254: "cuda:0 f32[1, 32, 512, 512]"
      # t1256 = ltorch.amax(t1254, -1, True)  # t1256: "cuda:0 f32[1, 32, 512, 1]"
        # t1255 = prims.amax(t1254, (3,))  # t1255: "cuda:0 f32[1, 32, 512]"
        # t1256 = prims.broadcast_in_dim(t1255, [1, 32, 512, 1], [0, 1, 2])  # t1256: "cuda:0 f32[1, 32, 512, 1]"
      # t1258 = ltorch.sub(t1254, t1256, alpha=None)  # t1258: "cuda:0 f32[1, 32, 512, 512]"
        # t1257 = prims.broadcast_in_dim(t1256, (1, 32, 512, 512), (0, 1, 2, 3))  # t1257: "cuda:0 f32[1, 32, 512, 512]"
        # t1258 = prims.sub(t1254, t1257)  # t1258: "cuda:0 f32[1, 32, 512, 512]"
      # t1259 = ltorch.exp(t1258)  # t1259: "cuda:0 f32[1, 32, 512, 512]"
        # t1259 = prims.exp(t1258)  # t1259: "cuda:0 f32[1, 32, 512, 512]"
      # t1261 = ltorch.sum(t1259, -1, True, dtype=None)  # t1261: "cuda:0 f32[1, 32, 512, 1]"
        # t1260 = prims.sum(t1259, (3,))  # t1260: "cuda:0 f32[1, 32, 512]"
        # t1261 = prims.broadcast_in_dim(t1260, [1, 32, 512, 1], [0, 1, 2])  # t1261: "cuda:0 f32[1, 32, 512, 1]"
      # t1263 = ltorch.true_divide(t1259, t1261)  # t1263: "cuda:0 f32[1, 32, 512, 512]"
        # t1262 = prims.broadcast_in_dim(t1261, (1, 32, 512, 512), (0, 1, 2, 3))  # t1262: "cuda:0 f32[1, 32, 512, 512]"
        # t1263 = prims.div(t1259, t1262)  # t1263: "cuda:0 f32[1, 32, 512, 512]"
      # t1264 = ltorch.to(t1263, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1264: "cuda:0 bf16[1, 32, 512, 512]"
        # t1264 = prims.convert_element_type(t1263, dtypes.bfloat16)  # t1264: "cuda:0 bf16[1, 32, 512, 512]"
    # t1265 = ltorch.matmul(t1264, t1201)  # t1265: "cuda:0 bf16[1, 32, 512, 128]"
      # t1265 = prims.matmul(t1264, t1201)  # t1265: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1266 = ltorch.transpose(t1265, 1, 2)  # t1266: "cuda:0 bf16[1, 512, 32, 128]"
    # t1266 = prims.transpose(t1265, (0, 2, 1, 3))  # t1266: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1267 = ltorch.reshape(t1266, 1, 512, 4096)  # t1267: "cuda:0 bf16[1, 512, 4096]"
    # t1267 = prims.reshape(t1266, (1, 512, 4096))  # t1267: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1271 = ltorch.linear(t1267, t_transformer_h_7_attn_proj_weight, None)  # t1271: "cuda:0 bf16[1, 512, 4096]"
    # t1271 = prims.linear(t1267, t_transformer_h_7_attn_proj_weight, None)  # t1271: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1275 = ltorch.add(t1271, t1165, alpha=None)  # t1275: "cuda:0 bf16[1, 512, 4096]"
    # t1272 = prims.convert_element_type(t1271, dtypes.float32)  # t1272: "cuda:0 f32[1, 512, 4096]"
    # t1273 = prims.convert_element_type(t1165, dtypes.float32)  # t1273: "cuda:0 f32[1, 512, 4096]"
    # t1274 = prims.add(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 4096]"
    # t1275 = prims.convert_element_type(t1274, dtypes.bfloat16)  # t1275: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1276 = prims.convert_element_type(t1275, dtypes.float32)  # t1276: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1277 = ltorch.mul(t1276, t1276)  # t1277: "cuda:0 f32[1, 512, 4096]"
    # t1277 = prims.mul(t1276, t1276)  # t1277: "cuda:0 f32[1, 512, 4096]"
  t1281 = ltorch.mean(t1277, -1, True, dtype=None)  # t1281: "cuda:0 f32[1, 512, 1]"
    # t1279 = prims.sum(t1277, (2,))  # t1279: "cuda:0 f32[1, 512]"
    # t1280 = prims.broadcast_in_dim(t1279, [1, 512, 1], [0, 1])  # t1280: "cuda:0 f32[1, 512, 1]"
    # t1281 = ltorch.true_divide(t1280, 4096)  # t1281: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1281 = prims.div(t1280, 4096.0)  # t1281: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1283 = ltorch.add(t1281, 1e-05, alpha=None)  # t1283: "cuda:0 f32[1, 512, 1]"
    # t1283 = prims.add(t1281, 1e-05)  # t1283: "cuda:0 f32[1, 512, 1]"
  t1284 = ltorch.rsqrt(t1283)  # t1284: "cuda:0 f32[1, 512, 1]"
    # t1284 = prims.rsqrt(t1283)  # t1284: "cuda:0 f32[1, 512, 1]"
  t1286 = ltorch.mul(t1276, t1284)  # t1286: "cuda:0 f32[1, 512, 4096]"
    # t1285 = prims.broadcast_in_dim(t1284, (1, 512, 4096), (0, 1, 2))  # t1285: "cuda:0 f32[1, 512, 4096]"
    # t1286 = prims.mul(t1276, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1287 = ltorch.to(t1286, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1287: "cuda:0 bf16[1, 512, 4096]"
    # t1287 = prims.convert_element_type(t1286, dtypes.bfloat16)  # t1287: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1297 = ltorch.mul(t1287, t_transformer_h_7_norm_2_weight)  # t1297: "cuda:0 bf16[1, 512, 4096]"
    # t1293 = prims.broadcast_in_dim(t_transformer_h_7_norm_2_weight, (1, 512, 4096), (2,))  # t1293: "cuda:0 bf16[1, 512, 4096]"
    # t1294 = prims.convert_element_type(t1287, dtypes.float32)  # t1294: "cuda:0 f32[1, 512, 4096]"
    # t1295 = prims.convert_element_type(t1293, dtypes.float32)  # t1295: "cuda:0 f32[1, 512, 4096]"
    # t1296 = prims.mul(t1294, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
    # t1297 = prims.convert_element_type(t1296, dtypes.bfloat16)  # t1297: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1302 = ltorch.linear(t1297, t_transformer_h_7_mlp_fc_1_weight, None)  # t1302: "cuda:0 bf16[1, 512, 11008]"
    # t1302 = prims.linear(t1297, t_transformer_h_7_mlp_fc_1_weight, None)  # t1302: "cuda:0 bf16[1, 512, 11008]"
  t1306 = ltorch.linear(t1297, t_transformer_h_7_mlp_fc_2_weight, None)  # t1306: "cuda:0 bf16[1, 512, 11008]"
    # t1306 = prims.linear(t1297, t_transformer_h_7_mlp_fc_2_weight, None)  # t1306: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1316 = ltorch.silu(t1302, False)  # t1316: "cuda:0 bf16[1, 512, 11008]"
    # t1307 = prims.convert_element_type(t1302, dtypes.float32)  # t1307: "cuda:0 f32[1, 512, 11008]"
    # t1308 = prims.neg(t1307)  # t1308: "cuda:0 f32[1, 512, 11008]"
    # t1309 = prims.exp(t1308)  # t1309: "cuda:0 f32[1, 512, 11008]"
    # t1310 = prims.add(1.0, t1309)  # t1310: "cuda:0 f32[1, 512, 11008]"
    # t1311 = prims.reciprocal(t1310)  # t1311: "cuda:0 f32[1, 512, 11008]"
    # t1312 = prims.convert_element_type(t1311, dtypes.bfloat16)  # t1312: "cuda:0 bf16[1, 512, 11008]"
    # t1313 = prims.convert_element_type(t1302, dtypes.float32)  # t1313: "cuda:0 f32[1, 512, 11008]"
    # t1314 = prims.convert_element_type(t1312, dtypes.float32)  # t1314: "cuda:0 f32[1, 512, 11008]"
    # t1315 = prims.mul(t1313, t1314)  # t1315: "cuda:0 f32[1, 512, 11008]"
    # t1316 = prims.convert_element_type(t1315, dtypes.bfloat16)  # t1316: "cuda:0 bf16[1, 512, 11008]"
  t1320 = ltorch.mul(t1316, t1306)  # t1320: "cuda:0 bf16[1, 512, 11008]"
    # t1317 = prims.convert_element_type(t1316, dtypes.float32)  # t1317: "cuda:0 f32[1, 512, 11008]"
    # t1318 = prims.convert_element_type(t1306, dtypes.float32)  # t1318: "cuda:0 f32[1, 512, 11008]"
    # t1319 = prims.mul(t1317, t1318)  # t1319: "cuda:0 f32[1, 512, 11008]"
    # t1320 = prims.convert_element_type(t1319, dtypes.bfloat16)  # t1320: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1324 = ltorch.linear(t1320, t_transformer_h_7_mlp_proj_weight, None)  # t1324: "cuda:0 bf16[1, 512, 4096]"
    # t1324 = prims.linear(t1320, t_transformer_h_7_mlp_proj_weight, None)  # t1324: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1328 = ltorch.add(t1324, t1275, alpha=None)  # t1328: "cuda:0 bf16[1, 512, 4096]"
    # t1325 = prims.convert_element_type(t1324, dtypes.float32)  # t1325: "cuda:0 f32[1, 512, 4096]"
    # t1326 = prims.convert_element_type(t1275, dtypes.float32)  # t1326: "cuda:0 f32[1, 512, 4096]"
    # t1327 = prims.add(t1325, t1326)  # t1327: "cuda:0 f32[1, 512, 4096]"
    # t1328 = prims.convert_element_type(t1327, dtypes.bfloat16)  # t1328: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1330 = prims.convert_element_type(t1328, dtypes.float32)  # t1330: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1331 = ltorch.mul(t1330, t1330)  # t1331: "cuda:0 f32[1, 512, 4096]"
    # t1331 = prims.mul(t1330, t1330)  # t1331: "cuda:0 f32[1, 512, 4096]"
  t1335 = ltorch.mean(t1331, -1, True, dtype=None)  # t1335: "cuda:0 f32[1, 512, 1]"
    # t1333 = prims.sum(t1331, (2,))  # t1333: "cuda:0 f32[1, 512]"
    # t1334 = prims.broadcast_in_dim(t1333, [1, 512, 1], [0, 1])  # t1334: "cuda:0 f32[1, 512, 1]"
    # t1335 = ltorch.true_divide(t1334, 4096)  # t1335: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1335 = prims.div(t1334, 4096.0)  # t1335: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1337 = ltorch.add(t1335, 1e-05, alpha=None)  # t1337: "cuda:0 f32[1, 512, 1]"
    # t1337 = prims.add(t1335, 1e-05)  # t1337: "cuda:0 f32[1, 512, 1]"
  t1338 = ltorch.rsqrt(t1337)  # t1338: "cuda:0 f32[1, 512, 1]"
    # t1338 = prims.rsqrt(t1337)  # t1338: "cuda:0 f32[1, 512, 1]"
  t1340 = ltorch.mul(t1330, t1338)  # t1340: "cuda:0 f32[1, 512, 4096]"
    # t1339 = prims.broadcast_in_dim(t1338, (1, 512, 4096), (0, 1, 2))  # t1339: "cuda:0 f32[1, 512, 4096]"
    # t1340 = prims.mul(t1330, t1339)  # t1340: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1341 = ltorch.to(t1340, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1341: "cuda:0 bf16[1, 512, 4096]"
    # t1341 = prims.convert_element_type(t1340, dtypes.bfloat16)  # t1341: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1351 = ltorch.mul(t1341, t_transformer_h_8_norm_1_weight)  # t1351: "cuda:0 bf16[1, 512, 4096]"
    # t1347 = prims.broadcast_in_dim(t_transformer_h_8_norm_1_weight, (1, 512, 4096), (2,))  # t1347: "cuda:0 bf16[1, 512, 4096]"
    # t1348 = prims.convert_element_type(t1341, dtypes.float32)  # t1348: "cuda:0 f32[1, 512, 4096]"
    # t1349 = prims.convert_element_type(t1347, dtypes.float32)  # t1349: "cuda:0 f32[1, 512, 4096]"
    # t1350 = prims.mul(t1348, t1349)  # t1350: "cuda:0 f32[1, 512, 4096]"
    # t1351 = prims.convert_element_type(t1350, dtypes.bfloat16)  # t1351: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1356 = ltorch.linear(t1351, t_transformer_h_8_attn_attn_weight, None)  # t1356: "cuda:0 bf16[1, 512, 12288]"
    # t1356 = prims.linear(t1351, t_transformer_h_8_attn_attn_weight, None)  # t1356: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1357 = ltorch.view(t1356, 1, 512, 32, 3, 128)  # t1357: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1357 = ltorch.reshape(t1356, (1, 512, 32, 3, 128))  # t1357: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1357 = prims.reshape(t1356, (1, 512, 32, 3, 128))  # t1357: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1358 = ltorch.permute(t1357, 0, 2, 3, 1, 4)  # t1358: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1358 = prims.transpose(t1357, (0, 2, 3, 1, 4))  # t1358: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1359, t1360, t1361) = ltorch.split(t1358, (1, 1, 1), 2)
    # t1359 = prims.slice_prim(t1358, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1359: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1360 = prims.slice_prim(t1358, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1360: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1361 = prims.slice_prim(t1358, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1361: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1362 = ltorch.reshape(t1359, 1, -1, 512, 128)  # t1362: "cuda:0 bf16[1, 32, 512, 128]"
    # t1362 = prims.reshape(t1359, (1, 32, 512, 128))  # t1362: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1363 = ltorch.reshape(t1360, 1, -1, 512, 128)  # t1363: "cuda:0 bf16[1, 32, 512, 128]"
    # t1363 = prims.reshape(t1360, (1, 32, 512, 128))  # t1363: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1364 = ltorch.reshape(t1361, 1, -1, 512, 128)  # t1364: "cuda:0 bf16[1, 32, 512, 128]"
    # t1364 = prims.reshape(t1361, (1, 32, 512, 128))  # t1364: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1365 = ltorch.getitem(t1362, (..., slice(None, 128, None)))  # t1365: "cuda:0 bf16[1, 32, 512, 128]"
    # t1365 = prims.slice_prim(t1362, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1365: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1366 = ltorch.getitem(t1365, (..., slice(None, 64, None)))  # t1366: "cuda:0 bf16[1, 32, 512, 64]"
    # t1366 = prims.slice_prim(t1365, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1366: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1367 = ltorch.getitem(t1365, (..., slice(64, None, None)))  # t1367: "cuda:0 bf16[1, 32, 512, 64]"
    # t1367 = prims.slice_prim(t1365, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1367: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1370 = ltorch.neg(t1367)  # t1370: "cuda:0 bf16[1, 32, 512, 64]"
    # t1368 = prims.convert_element_type(t1367, dtypes.float32)  # t1368: "cuda:0 f32[1, 32, 512, 64]"
    # t1369 = prims.neg(t1368)  # t1369: "cuda:0 f32[1, 32, 512, 64]"
    # t1370 = prims.convert_element_type(t1369, dtypes.bfloat16)  # t1370: "cuda:0 bf16[1, 32, 512, 64]"
  t1371 = ltorch.cat((t1370, t1366), -1)  # t1371: "cuda:0 bf16[1, 32, 512, 128]"
    # t1371 = prims.cat((t1370, t1366), -1)  # t1371: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1374 = ltorch.mul(t1365, cos)  # t1374: "cuda:0 f32[1, 32, 512, 128]"
    # t1372 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1372: "cuda:0 f32[1, 32, 512, 128]"
    # t1373 = prims.convert_element_type(t1365, dtypes.float32)  # t1373: "cuda:0 f32[1, 32, 512, 128]"
    # t1374 = prims.mul(t1373, t1372)  # t1374: "cuda:0 f32[1, 32, 512, 128]"
  t1377 = ltorch.mul(t1371, sin)  # t1377: "cuda:0 f32[1, 32, 512, 128]"
    # t1375 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1375: "cuda:0 f32[1, 32, 512, 128]"
    # t1376 = prims.convert_element_type(t1371, dtypes.float32)  # t1376: "cuda:0 f32[1, 32, 512, 128]"
    # t1377 = prims.mul(t1376, t1375)  # t1377: "cuda:0 f32[1, 32, 512, 128]"
  t1378 = ltorch.add(t1374, t1377, alpha=None)  # t1378: "cuda:0 f32[1, 32, 512, 128]"
    # t1378 = prims.add(t1374, t1377)  # t1378: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1379 = ltorch.to(t1378, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1379: "cuda:0 bf16[1, 32, 512, 128]"
    # t1379 = prims.convert_element_type(t1378, dtypes.bfloat16)  # t1379: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1380 = ltorch.getitem(t1363, (..., slice(None, 128, None)))  # t1380: "cuda:0 bf16[1, 32, 512, 128]"
    # t1380 = prims.slice_prim(t1363, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1380: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1381 = ltorch.getitem(t1380, (..., slice(None, 64, None)))  # t1381: "cuda:0 bf16[1, 32, 512, 64]"
    # t1381 = prims.slice_prim(t1380, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1381: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1382 = ltorch.getitem(t1380, (..., slice(64, None, None)))  # t1382: "cuda:0 bf16[1, 32, 512, 64]"
    # t1382 = prims.slice_prim(t1380, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1382: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1385 = ltorch.neg(t1382)  # t1385: "cuda:0 bf16[1, 32, 512, 64]"
    # t1383 = prims.convert_element_type(t1382, dtypes.float32)  # t1383: "cuda:0 f32[1, 32, 512, 64]"
    # t1384 = prims.neg(t1383)  # t1384: "cuda:0 f32[1, 32, 512, 64]"
    # t1385 = prims.convert_element_type(t1384, dtypes.bfloat16)  # t1385: "cuda:0 bf16[1, 32, 512, 64]"
  t1386 = ltorch.cat((t1385, t1381), -1)  # t1386: "cuda:0 bf16[1, 32, 512, 128]"
    # t1386 = prims.cat((t1385, t1381), -1)  # t1386: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1389 = ltorch.mul(t1380, cos)  # t1389: "cuda:0 f32[1, 32, 512, 128]"
    # t1387 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1387: "cuda:0 f32[1, 32, 512, 128]"
    # t1388 = prims.convert_element_type(t1380, dtypes.float32)  # t1388: "cuda:0 f32[1, 32, 512, 128]"
    # t1389 = prims.mul(t1388, t1387)  # t1389: "cuda:0 f32[1, 32, 512, 128]"
  t1392 = ltorch.mul(t1386, sin)  # t1392: "cuda:0 f32[1, 32, 512, 128]"
    # t1390 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1390: "cuda:0 f32[1, 32, 512, 128]"
    # t1391 = prims.convert_element_type(t1386, dtypes.float32)  # t1391: "cuda:0 f32[1, 32, 512, 128]"
    # t1392 = prims.mul(t1391, t1390)  # t1392: "cuda:0 f32[1, 32, 512, 128]"
  t1393 = ltorch.add(t1389, t1392, alpha=None)  # t1393: "cuda:0 f32[1, 32, 512, 128]"
    # t1393 = prims.add(t1389, t1392)  # t1393: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1394 = ltorch.to(t1393, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1394: "cuda:0 bf16[1, 32, 512, 128]"
    # t1394 = prims.convert_element_type(t1393, dtypes.bfloat16)  # t1394: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1395 = ltorch.getitem(t1362, (..., slice(128, None, None)))  # t1395: "cuda:0 bf16[1, 32, 512, 0]"
    # t1395 = prims.slice_prim(t1362, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1395: "cuda:0 bf16[1, 32, 512, 0]"
  t1396 = ltorch.cat((t1379, t1395), -1)  # t1396: "cuda:0 bf16[1, 32, 512, 128]"
    # t1396 = prims.cat((t1379, t1395), -1)  # t1396: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1397 = ltorch.getitem(t1363, (..., slice(128, None, None)))  # t1397: "cuda:0 bf16[1, 32, 512, 0]"
    # t1397 = prims.slice_prim(t1363, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1397: "cuda:0 bf16[1, 32, 512, 0]"
  t1398 = ltorch.cat((t1394, t1397), -1)  # t1398: "cuda:0 bf16[1, 32, 512, 128]"
    # t1398 = prims.cat((t1394, t1397), -1)  # t1398: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1428 = ltorch.scaled_dot_product_attention(t1396, t1398, t1364, None, 0.0, True, scale=0.08838834764831843)  # t1428: "cuda:0 bf16[1, 32, 512, 128]"
    # t1401 = ltorch.mul(t1396, 0.29730177875068026)  # t1401: "cuda:0 bf16[1, 32, 512, 128]"
      # t1399 = prims.convert_element_type(t1396, dtypes.float32)  # t1399: "cuda:0 f32[1, 32, 512, 128]"
      # t1400 = prims.mul(t1399, 0.29730177875068026)  # t1400: "cuda:0 f32[1, 32, 512, 128]"
      # t1401 = prims.convert_element_type(t1400, dtypes.bfloat16)  # t1401: "cuda:0 bf16[1, 32, 512, 128]"
    # t1402 = ltorch.transpose(t1398, -2, -1)  # t1402: "cuda:0 bf16[1, 32, 128, 512]"
      # t1402 = prims.transpose(t1398, (0, 1, 3, 2))  # t1402: "cuda:0 bf16[1, 32, 128, 512]"
    # t1405 = ltorch.mul(t1402, 0.29730177875068026)  # t1405: "cuda:0 bf16[1, 32, 128, 512]"
      # t1403 = prims.convert_element_type(t1402, dtypes.float32)  # t1403: "cuda:0 f32[1, 32, 128, 512]"
      # t1404 = prims.mul(t1403, 0.29730177875068026)  # t1404: "cuda:0 f32[1, 32, 128, 512]"
      # t1405 = prims.convert_element_type(t1404, dtypes.bfloat16)  # t1405: "cuda:0 bf16[1, 32, 128, 512]"
    # t1406 = ltorch.matmul(t1401, t1405)  # t1406: "cuda:0 bf16[1, 32, 512, 512]"
      # t1406 = prims.matmul(t1401, t1405)  # t1406: "cuda:0 bf16[1, 32, 512, 512]"
    # t1416 = ltorch.tril(t1406, 0, fill_value=-float('inf'))  # t1416: "cuda:0 bf16[1, 32, 512, 512]"
      # t1407 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1407: "cuda:0 i64[512]"
        # t1407 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1407: "cuda:0 i64[512]"
      # t1408 = ltorch.unsqueeze(t1407, -1)  # t1408: "cuda:0 i64[512, 1]"
        # t1408 = prims.broadcast_in_dim(t1407, [512, 1], [0])  # t1408: "cuda:0 i64[512, 1]"
      # t1409 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1409: "cuda:0 i64[512]"
        # t1409 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1409: "cuda:0 i64[512]"
      # t1410 = ltorch.unsqueeze(t1409, -2)  # t1410: "cuda:0 i64[1, 512]"
        # t1410 = prims.broadcast_in_dim(t1409, [1, 512], [1])  # t1410: "cuda:0 i64[1, 512]"
      # t1411 = ltorch.add(t1408, 0, alpha=None)  # t1411: "cuda:0 i64[512, 1]"
        # t1411 = prims.add(t1408, 0)  # t1411: "cuda:0 i64[512, 1]"
      # t1414 = ltorch.ge(t1411, t1410)  # t1414: "cuda:0 b8[512, 512]"
        # t1412 = prims.broadcast_in_dim(t1411, (512, 512), (0, 1))  # t1412: "cuda:0 i64[512, 512]"
        # t1413 = prims.broadcast_in_dim(t1410, (512, 512), (0, 1))  # t1413: "cuda:0 i64[512, 512]"
        # t1414 = prims.ge(t1412, t1413)  # t1414: "cuda:0 b8[512, 512]"
      # t1416 = ltorch.where(t1414, t1406, -float('inf'))  # t1416: "cuda:0 bf16[1, 32, 512, 512]"
        # t1415 = prims.broadcast_in_dim(t1414, (1, 32, 512, 512), (2, 3))  # t1415: "cuda:0 b8[1, 32, 512, 512]"
        # t1416 = prims.where(t1415, t1406, -float('inf'))  # t1416: "cuda:0 bf16[1, 32, 512, 512]"
    # t1427 = ltorch._softmax(t1416, -1, dtype=None)  # t1427: "cuda:0 bf16[1, 32, 512, 512]"
      # t1417 = ltorch.to(t1416, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1417: "cuda:0 f32[1, 32, 512, 512]"
        # t1417 = prims.convert_element_type(t1416, dtypes.float32)  # t1417: "cuda:0 f32[1, 32, 512, 512]"
      # t1419 = ltorch.amax(t1417, -1, True)  # t1419: "cuda:0 f32[1, 32, 512, 1]"
        # t1418 = prims.amax(t1417, (3,))  # t1418: "cuda:0 f32[1, 32, 512]"
        # t1419 = prims.broadcast_in_dim(t1418, [1, 32, 512, 1], [0, 1, 2])  # t1419: "cuda:0 f32[1, 32, 512, 1]"
      # t1421 = ltorch.sub(t1417, t1419, alpha=None)  # t1421: "cuda:0 f32[1, 32, 512, 512]"
        # t1420 = prims.broadcast_in_dim(t1419, (1, 32, 512, 512), (0, 1, 2, 3))  # t1420: "cuda:0 f32[1, 32, 512, 512]"
        # t1421 = prims.sub(t1417, t1420)  # t1421: "cuda:0 f32[1, 32, 512, 512]"
      # t1422 = ltorch.exp(t1421)  # t1422: "cuda:0 f32[1, 32, 512, 512]"
        # t1422 = prims.exp(t1421)  # t1422: "cuda:0 f32[1, 32, 512, 512]"
      # t1424 = ltorch.sum(t1422, -1, True, dtype=None)  # t1424: "cuda:0 f32[1, 32, 512, 1]"
        # t1423 = prims.sum(t1422, (3,))  # t1423: "cuda:0 f32[1, 32, 512]"
        # t1424 = prims.broadcast_in_dim(t1423, [1, 32, 512, 1], [0, 1, 2])  # t1424: "cuda:0 f32[1, 32, 512, 1]"
      # t1426 = ltorch.true_divide(t1422, t1424)  # t1426: "cuda:0 f32[1, 32, 512, 512]"
        # t1425 = prims.broadcast_in_dim(t1424, (1, 32, 512, 512), (0, 1, 2, 3))  # t1425: "cuda:0 f32[1, 32, 512, 512]"
        # t1426 = prims.div(t1422, t1425)  # t1426: "cuda:0 f32[1, 32, 512, 512]"
      # t1427 = ltorch.to(t1426, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1427: "cuda:0 bf16[1, 32, 512, 512]"
        # t1427 = prims.convert_element_type(t1426, dtypes.bfloat16)  # t1427: "cuda:0 bf16[1, 32, 512, 512]"
    # t1428 = ltorch.matmul(t1427, t1364)  # t1428: "cuda:0 bf16[1, 32, 512, 128]"
      # t1428 = prims.matmul(t1427, t1364)  # t1428: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1429 = ltorch.transpose(t1428, 1, 2)  # t1429: "cuda:0 bf16[1, 512, 32, 128]"
    # t1429 = prims.transpose(t1428, (0, 2, 1, 3))  # t1429: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1430 = ltorch.reshape(t1429, 1, 512, 4096)  # t1430: "cuda:0 bf16[1, 512, 4096]"
    # t1430 = prims.reshape(t1429, (1, 512, 4096))  # t1430: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1434 = ltorch.linear(t1430, t_transformer_h_8_attn_proj_weight, None)  # t1434: "cuda:0 bf16[1, 512, 4096]"
    # t1434 = prims.linear(t1430, t_transformer_h_8_attn_proj_weight, None)  # t1434: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1438 = ltorch.add(t1434, t1328, alpha=None)  # t1438: "cuda:0 bf16[1, 512, 4096]"
    # t1435 = prims.convert_element_type(t1434, dtypes.float32)  # t1435: "cuda:0 f32[1, 512, 4096]"
    # t1436 = prims.convert_element_type(t1328, dtypes.float32)  # t1436: "cuda:0 f32[1, 512, 4096]"
    # t1437 = prims.add(t1435, t1436)  # t1437: "cuda:0 f32[1, 512, 4096]"
    # t1438 = prims.convert_element_type(t1437, dtypes.bfloat16)  # t1438: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1439 = prims.convert_element_type(t1438, dtypes.float32)  # t1439: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1440 = ltorch.mul(t1439, t1439)  # t1440: "cuda:0 f32[1, 512, 4096]"
    # t1440 = prims.mul(t1439, t1439)  # t1440: "cuda:0 f32[1, 512, 4096]"
  t1444 = ltorch.mean(t1440, -1, True, dtype=None)  # t1444: "cuda:0 f32[1, 512, 1]"
    # t1442 = prims.sum(t1440, (2,))  # t1442: "cuda:0 f32[1, 512]"
    # t1443 = prims.broadcast_in_dim(t1442, [1, 512, 1], [0, 1])  # t1443: "cuda:0 f32[1, 512, 1]"
    # t1444 = ltorch.true_divide(t1443, 4096)  # t1444: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1444 = prims.div(t1443, 4096.0)  # t1444: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1446 = ltorch.add(t1444, 1e-05, alpha=None)  # t1446: "cuda:0 f32[1, 512, 1]"
    # t1446 = prims.add(t1444, 1e-05)  # t1446: "cuda:0 f32[1, 512, 1]"
  t1447 = ltorch.rsqrt(t1446)  # t1447: "cuda:0 f32[1, 512, 1]"
    # t1447 = prims.rsqrt(t1446)  # t1447: "cuda:0 f32[1, 512, 1]"
  t1449 = ltorch.mul(t1439, t1447)  # t1449: "cuda:0 f32[1, 512, 4096]"
    # t1448 = prims.broadcast_in_dim(t1447, (1, 512, 4096), (0, 1, 2))  # t1448: "cuda:0 f32[1, 512, 4096]"
    # t1449 = prims.mul(t1439, t1448)  # t1449: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1450 = ltorch.to(t1449, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1450: "cuda:0 bf16[1, 512, 4096]"
    # t1450 = prims.convert_element_type(t1449, dtypes.bfloat16)  # t1450: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1460 = ltorch.mul(t1450, t_transformer_h_8_norm_2_weight)  # t1460: "cuda:0 bf16[1, 512, 4096]"
    # t1456 = prims.broadcast_in_dim(t_transformer_h_8_norm_2_weight, (1, 512, 4096), (2,))  # t1456: "cuda:0 bf16[1, 512, 4096]"
    # t1457 = prims.convert_element_type(t1450, dtypes.float32)  # t1457: "cuda:0 f32[1, 512, 4096]"
    # t1458 = prims.convert_element_type(t1456, dtypes.float32)  # t1458: "cuda:0 f32[1, 512, 4096]"
    # t1459 = prims.mul(t1457, t1458)  # t1459: "cuda:0 f32[1, 512, 4096]"
    # t1460 = prims.convert_element_type(t1459, dtypes.bfloat16)  # t1460: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1465 = ltorch.linear(t1460, t_transformer_h_8_mlp_fc_1_weight, None)  # t1465: "cuda:0 bf16[1, 512, 11008]"
    # t1465 = prims.linear(t1460, t_transformer_h_8_mlp_fc_1_weight, None)  # t1465: "cuda:0 bf16[1, 512, 11008]"
  t1469 = ltorch.linear(t1460, t_transformer_h_8_mlp_fc_2_weight, None)  # t1469: "cuda:0 bf16[1, 512, 11008]"
    # t1469 = prims.linear(t1460, t_transformer_h_8_mlp_fc_2_weight, None)  # t1469: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1479 = ltorch.silu(t1465, False)  # t1479: "cuda:0 bf16[1, 512, 11008]"
    # t1470 = prims.convert_element_type(t1465, dtypes.float32)  # t1470: "cuda:0 f32[1, 512, 11008]"
    # t1471 = prims.neg(t1470)  # t1471: "cuda:0 f32[1, 512, 11008]"
    # t1472 = prims.exp(t1471)  # t1472: "cuda:0 f32[1, 512, 11008]"
    # t1473 = prims.add(1.0, t1472)  # t1473: "cuda:0 f32[1, 512, 11008]"
    # t1474 = prims.reciprocal(t1473)  # t1474: "cuda:0 f32[1, 512, 11008]"
    # t1475 = prims.convert_element_type(t1474, dtypes.bfloat16)  # t1475: "cuda:0 bf16[1, 512, 11008]"
    # t1476 = prims.convert_element_type(t1465, dtypes.float32)  # t1476: "cuda:0 f32[1, 512, 11008]"
    # t1477 = prims.convert_element_type(t1475, dtypes.float32)  # t1477: "cuda:0 f32[1, 512, 11008]"
    # t1478 = prims.mul(t1476, t1477)  # t1478: "cuda:0 f32[1, 512, 11008]"
    # t1479 = prims.convert_element_type(t1478, dtypes.bfloat16)  # t1479: "cuda:0 bf16[1, 512, 11008]"
  t1483 = ltorch.mul(t1479, t1469)  # t1483: "cuda:0 bf16[1, 512, 11008]"
    # t1480 = prims.convert_element_type(t1479, dtypes.float32)  # t1480: "cuda:0 f32[1, 512, 11008]"
    # t1481 = prims.convert_element_type(t1469, dtypes.float32)  # t1481: "cuda:0 f32[1, 512, 11008]"
    # t1482 = prims.mul(t1480, t1481)  # t1482: "cuda:0 f32[1, 512, 11008]"
    # t1483 = prims.convert_element_type(t1482, dtypes.bfloat16)  # t1483: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1487 = ltorch.linear(t1483, t_transformer_h_8_mlp_proj_weight, None)  # t1487: "cuda:0 bf16[1, 512, 4096]"
    # t1487 = prims.linear(t1483, t_transformer_h_8_mlp_proj_weight, None)  # t1487: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1491 = ltorch.add(t1487, t1438, alpha=None)  # t1491: "cuda:0 bf16[1, 512, 4096]"
    # t1488 = prims.convert_element_type(t1487, dtypes.float32)  # t1488: "cuda:0 f32[1, 512, 4096]"
    # t1489 = prims.convert_element_type(t1438, dtypes.float32)  # t1489: "cuda:0 f32[1, 512, 4096]"
    # t1490 = prims.add(t1488, t1489)  # t1490: "cuda:0 f32[1, 512, 4096]"
    # t1491 = prims.convert_element_type(t1490, dtypes.bfloat16)  # t1491: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1493 = prims.convert_element_type(t1491, dtypes.float32)  # t1493: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1494 = ltorch.mul(t1493, t1493)  # t1494: "cuda:0 f32[1, 512, 4096]"
    # t1494 = prims.mul(t1493, t1493)  # t1494: "cuda:0 f32[1, 512, 4096]"
  t1498 = ltorch.mean(t1494, -1, True, dtype=None)  # t1498: "cuda:0 f32[1, 512, 1]"
    # t1496 = prims.sum(t1494, (2,))  # t1496: "cuda:0 f32[1, 512]"
    # t1497 = prims.broadcast_in_dim(t1496, [1, 512, 1], [0, 1])  # t1497: "cuda:0 f32[1, 512, 1]"
    # t1498 = ltorch.true_divide(t1497, 4096)  # t1498: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1498 = prims.div(t1497, 4096.0)  # t1498: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1500 = ltorch.add(t1498, 1e-05, alpha=None)  # t1500: "cuda:0 f32[1, 512, 1]"
    # t1500 = prims.add(t1498, 1e-05)  # t1500: "cuda:0 f32[1, 512, 1]"
  t1501 = ltorch.rsqrt(t1500)  # t1501: "cuda:0 f32[1, 512, 1]"
    # t1501 = prims.rsqrt(t1500)  # t1501: "cuda:0 f32[1, 512, 1]"
  t1503 = ltorch.mul(t1493, t1501)  # t1503: "cuda:0 f32[1, 512, 4096]"
    # t1502 = prims.broadcast_in_dim(t1501, (1, 512, 4096), (0, 1, 2))  # t1502: "cuda:0 f32[1, 512, 4096]"
    # t1503 = prims.mul(t1493, t1502)  # t1503: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1504 = ltorch.to(t1503, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1504: "cuda:0 bf16[1, 512, 4096]"
    # t1504 = prims.convert_element_type(t1503, dtypes.bfloat16)  # t1504: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1514 = ltorch.mul(t1504, t_transformer_h_9_norm_1_weight)  # t1514: "cuda:0 bf16[1, 512, 4096]"
    # t1510 = prims.broadcast_in_dim(t_transformer_h_9_norm_1_weight, (1, 512, 4096), (2,))  # t1510: "cuda:0 bf16[1, 512, 4096]"
    # t1511 = prims.convert_element_type(t1504, dtypes.float32)  # t1511: "cuda:0 f32[1, 512, 4096]"
    # t1512 = prims.convert_element_type(t1510, dtypes.float32)  # t1512: "cuda:0 f32[1, 512, 4096]"
    # t1513 = prims.mul(t1511, t1512)  # t1513: "cuda:0 f32[1, 512, 4096]"
    # t1514 = prims.convert_element_type(t1513, dtypes.bfloat16)  # t1514: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1519 = ltorch.linear(t1514, t_transformer_h_9_attn_attn_weight, None)  # t1519: "cuda:0 bf16[1, 512, 12288]"
    # t1519 = prims.linear(t1514, t_transformer_h_9_attn_attn_weight, None)  # t1519: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1520 = ltorch.view(t1519, 1, 512, 32, 3, 128)  # t1520: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1520 = ltorch.reshape(t1519, (1, 512, 32, 3, 128))  # t1520: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1520 = prims.reshape(t1519, (1, 512, 32, 3, 128))  # t1520: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1521 = ltorch.permute(t1520, 0, 2, 3, 1, 4)  # t1521: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1521 = prims.transpose(t1520, (0, 2, 3, 1, 4))  # t1521: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1522, t1523, t1524) = ltorch.split(t1521, (1, 1, 1), 2)
    # t1522 = prims.slice_prim(t1521, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1522: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1523 = prims.slice_prim(t1521, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1523: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1524 = prims.slice_prim(t1521, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1524: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1525 = ltorch.reshape(t1522, 1, -1, 512, 128)  # t1525: "cuda:0 bf16[1, 32, 512, 128]"
    # t1525 = prims.reshape(t1522, (1, 32, 512, 128))  # t1525: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1526 = ltorch.reshape(t1523, 1, -1, 512, 128)  # t1526: "cuda:0 bf16[1, 32, 512, 128]"
    # t1526 = prims.reshape(t1523, (1, 32, 512, 128))  # t1526: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1527 = ltorch.reshape(t1524, 1, -1, 512, 128)  # t1527: "cuda:0 bf16[1, 32, 512, 128]"
    # t1527 = prims.reshape(t1524, (1, 32, 512, 128))  # t1527: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1528 = ltorch.getitem(t1525, (..., slice(None, 128, None)))  # t1528: "cuda:0 bf16[1, 32, 512, 128]"
    # t1528 = prims.slice_prim(t1525, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1528: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1529 = ltorch.getitem(t1528, (..., slice(None, 64, None)))  # t1529: "cuda:0 bf16[1, 32, 512, 64]"
    # t1529 = prims.slice_prim(t1528, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1529: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1530 = ltorch.getitem(t1528, (..., slice(64, None, None)))  # t1530: "cuda:0 bf16[1, 32, 512, 64]"
    # t1530 = prims.slice_prim(t1528, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1530: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1533 = ltorch.neg(t1530)  # t1533: "cuda:0 bf16[1, 32, 512, 64]"
    # t1531 = prims.convert_element_type(t1530, dtypes.float32)  # t1531: "cuda:0 f32[1, 32, 512, 64]"
    # t1532 = prims.neg(t1531)  # t1532: "cuda:0 f32[1, 32, 512, 64]"
    # t1533 = prims.convert_element_type(t1532, dtypes.bfloat16)  # t1533: "cuda:0 bf16[1, 32, 512, 64]"
  t1534 = ltorch.cat((t1533, t1529), -1)  # t1534: "cuda:0 bf16[1, 32, 512, 128]"
    # t1534 = prims.cat((t1533, t1529), -1)  # t1534: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1537 = ltorch.mul(t1528, cos)  # t1537: "cuda:0 f32[1, 32, 512, 128]"
    # t1535 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1535: "cuda:0 f32[1, 32, 512, 128]"
    # t1536 = prims.convert_element_type(t1528, dtypes.float32)  # t1536: "cuda:0 f32[1, 32, 512, 128]"
    # t1537 = prims.mul(t1536, t1535)  # t1537: "cuda:0 f32[1, 32, 512, 128]"
  t1540 = ltorch.mul(t1534, sin)  # t1540: "cuda:0 f32[1, 32, 512, 128]"
    # t1538 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1538: "cuda:0 f32[1, 32, 512, 128]"
    # t1539 = prims.convert_element_type(t1534, dtypes.float32)  # t1539: "cuda:0 f32[1, 32, 512, 128]"
    # t1540 = prims.mul(t1539, t1538)  # t1540: "cuda:0 f32[1, 32, 512, 128]"
  t1541 = ltorch.add(t1537, t1540, alpha=None)  # t1541: "cuda:0 f32[1, 32, 512, 128]"
    # t1541 = prims.add(t1537, t1540)  # t1541: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1542 = ltorch.to(t1541, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1542: "cuda:0 bf16[1, 32, 512, 128]"
    # t1542 = prims.convert_element_type(t1541, dtypes.bfloat16)  # t1542: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1543 = ltorch.getitem(t1526, (..., slice(None, 128, None)))  # t1543: "cuda:0 bf16[1, 32, 512, 128]"
    # t1543 = prims.slice_prim(t1526, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1543: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1544 = ltorch.getitem(t1543, (..., slice(None, 64, None)))  # t1544: "cuda:0 bf16[1, 32, 512, 64]"
    # t1544 = prims.slice_prim(t1543, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1544: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1545 = ltorch.getitem(t1543, (..., slice(64, None, None)))  # t1545: "cuda:0 bf16[1, 32, 512, 64]"
    # t1545 = prims.slice_prim(t1543, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1545: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1548 = ltorch.neg(t1545)  # t1548: "cuda:0 bf16[1, 32, 512, 64]"
    # t1546 = prims.convert_element_type(t1545, dtypes.float32)  # t1546: "cuda:0 f32[1, 32, 512, 64]"
    # t1547 = prims.neg(t1546)  # t1547: "cuda:0 f32[1, 32, 512, 64]"
    # t1548 = prims.convert_element_type(t1547, dtypes.bfloat16)  # t1548: "cuda:0 bf16[1, 32, 512, 64]"
  t1549 = ltorch.cat((t1548, t1544), -1)  # t1549: "cuda:0 bf16[1, 32, 512, 128]"
    # t1549 = prims.cat((t1548, t1544), -1)  # t1549: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1552 = ltorch.mul(t1543, cos)  # t1552: "cuda:0 f32[1, 32, 512, 128]"
    # t1550 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1550: "cuda:0 f32[1, 32, 512, 128]"
    # t1551 = prims.convert_element_type(t1543, dtypes.float32)  # t1551: "cuda:0 f32[1, 32, 512, 128]"
    # t1552 = prims.mul(t1551, t1550)  # t1552: "cuda:0 f32[1, 32, 512, 128]"
  t1555 = ltorch.mul(t1549, sin)  # t1555: "cuda:0 f32[1, 32, 512, 128]"
    # t1553 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1553: "cuda:0 f32[1, 32, 512, 128]"
    # t1554 = prims.convert_element_type(t1549, dtypes.float32)  # t1554: "cuda:0 f32[1, 32, 512, 128]"
    # t1555 = prims.mul(t1554, t1553)  # t1555: "cuda:0 f32[1, 32, 512, 128]"
  t1556 = ltorch.add(t1552, t1555, alpha=None)  # t1556: "cuda:0 f32[1, 32, 512, 128]"
    # t1556 = prims.add(t1552, t1555)  # t1556: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1557 = ltorch.to(t1556, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1557: "cuda:0 bf16[1, 32, 512, 128]"
    # t1557 = prims.convert_element_type(t1556, dtypes.bfloat16)  # t1557: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1558 = ltorch.getitem(t1525, (..., slice(128, None, None)))  # t1558: "cuda:0 bf16[1, 32, 512, 0]"
    # t1558 = prims.slice_prim(t1525, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1558: "cuda:0 bf16[1, 32, 512, 0]"
  t1559 = ltorch.cat((t1542, t1558), -1)  # t1559: "cuda:0 bf16[1, 32, 512, 128]"
    # t1559 = prims.cat((t1542, t1558), -1)  # t1559: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1560 = ltorch.getitem(t1526, (..., slice(128, None, None)))  # t1560: "cuda:0 bf16[1, 32, 512, 0]"
    # t1560 = prims.slice_prim(t1526, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1560: "cuda:0 bf16[1, 32, 512, 0]"
  t1561 = ltorch.cat((t1557, t1560), -1)  # t1561: "cuda:0 bf16[1, 32, 512, 128]"
    # t1561 = prims.cat((t1557, t1560), -1)  # t1561: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1591 = ltorch.scaled_dot_product_attention(t1559, t1561, t1527, None, 0.0, True, scale=0.08838834764831843)  # t1591: "cuda:0 bf16[1, 32, 512, 128]"
    # t1564 = ltorch.mul(t1559, 0.29730177875068026)  # t1564: "cuda:0 bf16[1, 32, 512, 128]"
      # t1562 = prims.convert_element_type(t1559, dtypes.float32)  # t1562: "cuda:0 f32[1, 32, 512, 128]"
      # t1563 = prims.mul(t1562, 0.29730177875068026)  # t1563: "cuda:0 f32[1, 32, 512, 128]"
      # t1564 = prims.convert_element_type(t1563, dtypes.bfloat16)  # t1564: "cuda:0 bf16[1, 32, 512, 128]"
    # t1565 = ltorch.transpose(t1561, -2, -1)  # t1565: "cuda:0 bf16[1, 32, 128, 512]"
      # t1565 = prims.transpose(t1561, (0, 1, 3, 2))  # t1565: "cuda:0 bf16[1, 32, 128, 512]"
    # t1568 = ltorch.mul(t1565, 0.29730177875068026)  # t1568: "cuda:0 bf16[1, 32, 128, 512]"
      # t1566 = prims.convert_element_type(t1565, dtypes.float32)  # t1566: "cuda:0 f32[1, 32, 128, 512]"
      # t1567 = prims.mul(t1566, 0.29730177875068026)  # t1567: "cuda:0 f32[1, 32, 128, 512]"
      # t1568 = prims.convert_element_type(t1567, dtypes.bfloat16)  # t1568: "cuda:0 bf16[1, 32, 128, 512]"
    # t1569 = ltorch.matmul(t1564, t1568)  # t1569: "cuda:0 bf16[1, 32, 512, 512]"
      # t1569 = prims.matmul(t1564, t1568)  # t1569: "cuda:0 bf16[1, 32, 512, 512]"
    # t1579 = ltorch.tril(t1569, 0, fill_value=-float('inf'))  # t1579: "cuda:0 bf16[1, 32, 512, 512]"
      # t1570 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1570: "cuda:0 i64[512]"
        # t1570 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1570: "cuda:0 i64[512]"
      # t1571 = ltorch.unsqueeze(t1570, -1)  # t1571: "cuda:0 i64[512, 1]"
        # t1571 = prims.broadcast_in_dim(t1570, [512, 1], [0])  # t1571: "cuda:0 i64[512, 1]"
      # t1572 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1572: "cuda:0 i64[512]"
        # t1572 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1572: "cuda:0 i64[512]"
      # t1573 = ltorch.unsqueeze(t1572, -2)  # t1573: "cuda:0 i64[1, 512]"
        # t1573 = prims.broadcast_in_dim(t1572, [1, 512], [1])  # t1573: "cuda:0 i64[1, 512]"
      # t1574 = ltorch.add(t1571, 0, alpha=None)  # t1574: "cuda:0 i64[512, 1]"
        # t1574 = prims.add(t1571, 0)  # t1574: "cuda:0 i64[512, 1]"
      # t1577 = ltorch.ge(t1574, t1573)  # t1577: "cuda:0 b8[512, 512]"
        # t1575 = prims.broadcast_in_dim(t1574, (512, 512), (0, 1))  # t1575: "cuda:0 i64[512, 512]"
        # t1576 = prims.broadcast_in_dim(t1573, (512, 512), (0, 1))  # t1576: "cuda:0 i64[512, 512]"
        # t1577 = prims.ge(t1575, t1576)  # t1577: "cuda:0 b8[512, 512]"
      # t1579 = ltorch.where(t1577, t1569, -float('inf'))  # t1579: "cuda:0 bf16[1, 32, 512, 512]"
        # t1578 = prims.broadcast_in_dim(t1577, (1, 32, 512, 512), (2, 3))  # t1578: "cuda:0 b8[1, 32, 512, 512]"
        # t1579 = prims.where(t1578, t1569, -float('inf'))  # t1579: "cuda:0 bf16[1, 32, 512, 512]"
    # t1590 = ltorch._softmax(t1579, -1, dtype=None)  # t1590: "cuda:0 bf16[1, 32, 512, 512]"
      # t1580 = ltorch.to(t1579, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1580: "cuda:0 f32[1, 32, 512, 512]"
        # t1580 = prims.convert_element_type(t1579, dtypes.float32)  # t1580: "cuda:0 f32[1, 32, 512, 512]"
      # t1582 = ltorch.amax(t1580, -1, True)  # t1582: "cuda:0 f32[1, 32, 512, 1]"
        # t1581 = prims.amax(t1580, (3,))  # t1581: "cuda:0 f32[1, 32, 512]"
        # t1582 = prims.broadcast_in_dim(t1581, [1, 32, 512, 1], [0, 1, 2])  # t1582: "cuda:0 f32[1, 32, 512, 1]"
      # t1584 = ltorch.sub(t1580, t1582, alpha=None)  # t1584: "cuda:0 f32[1, 32, 512, 512]"
        # t1583 = prims.broadcast_in_dim(t1582, (1, 32, 512, 512), (0, 1, 2, 3))  # t1583: "cuda:0 f32[1, 32, 512, 512]"
        # t1584 = prims.sub(t1580, t1583)  # t1584: "cuda:0 f32[1, 32, 512, 512]"
      # t1585 = ltorch.exp(t1584)  # t1585: "cuda:0 f32[1, 32, 512, 512]"
        # t1585 = prims.exp(t1584)  # t1585: "cuda:0 f32[1, 32, 512, 512]"
      # t1587 = ltorch.sum(t1585, -1, True, dtype=None)  # t1587: "cuda:0 f32[1, 32, 512, 1]"
        # t1586 = prims.sum(t1585, (3,))  # t1586: "cuda:0 f32[1, 32, 512]"
        # t1587 = prims.broadcast_in_dim(t1586, [1, 32, 512, 1], [0, 1, 2])  # t1587: "cuda:0 f32[1, 32, 512, 1]"
      # t1589 = ltorch.true_divide(t1585, t1587)  # t1589: "cuda:0 f32[1, 32, 512, 512]"
        # t1588 = prims.broadcast_in_dim(t1587, (1, 32, 512, 512), (0, 1, 2, 3))  # t1588: "cuda:0 f32[1, 32, 512, 512]"
        # t1589 = prims.div(t1585, t1588)  # t1589: "cuda:0 f32[1, 32, 512, 512]"
      # t1590 = ltorch.to(t1589, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1590: "cuda:0 bf16[1, 32, 512, 512]"
        # t1590 = prims.convert_element_type(t1589, dtypes.bfloat16)  # t1590: "cuda:0 bf16[1, 32, 512, 512]"
    # t1591 = ltorch.matmul(t1590, t1527)  # t1591: "cuda:0 bf16[1, 32, 512, 128]"
      # t1591 = prims.matmul(t1590, t1527)  # t1591: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1592 = ltorch.transpose(t1591, 1, 2)  # t1592: "cuda:0 bf16[1, 512, 32, 128]"
    # t1592 = prims.transpose(t1591, (0, 2, 1, 3))  # t1592: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1593 = ltorch.reshape(t1592, 1, 512, 4096)  # t1593: "cuda:0 bf16[1, 512, 4096]"
    # t1593 = prims.reshape(t1592, (1, 512, 4096))  # t1593: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1597 = ltorch.linear(t1593, t_transformer_h_9_attn_proj_weight, None)  # t1597: "cuda:0 bf16[1, 512, 4096]"
    # t1597 = prims.linear(t1593, t_transformer_h_9_attn_proj_weight, None)  # t1597: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1601 = ltorch.add(t1597, t1491, alpha=None)  # t1601: "cuda:0 bf16[1, 512, 4096]"
    # t1598 = prims.convert_element_type(t1597, dtypes.float32)  # t1598: "cuda:0 f32[1, 512, 4096]"
    # t1599 = prims.convert_element_type(t1491, dtypes.float32)  # t1599: "cuda:0 f32[1, 512, 4096]"
    # t1600 = prims.add(t1598, t1599)  # t1600: "cuda:0 f32[1, 512, 4096]"
    # t1601 = prims.convert_element_type(t1600, dtypes.bfloat16)  # t1601: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1602 = prims.convert_element_type(t1601, dtypes.float32)  # t1602: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1603 = ltorch.mul(t1602, t1602)  # t1603: "cuda:0 f32[1, 512, 4096]"
    # t1603 = prims.mul(t1602, t1602)  # t1603: "cuda:0 f32[1, 512, 4096]"
  t1607 = ltorch.mean(t1603, -1, True, dtype=None)  # t1607: "cuda:0 f32[1, 512, 1]"
    # t1605 = prims.sum(t1603, (2,))  # t1605: "cuda:0 f32[1, 512]"
    # t1606 = prims.broadcast_in_dim(t1605, [1, 512, 1], [0, 1])  # t1606: "cuda:0 f32[1, 512, 1]"
    # t1607 = ltorch.true_divide(t1606, 4096)  # t1607: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1607 = prims.div(t1606, 4096.0)  # t1607: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1609 = ltorch.add(t1607, 1e-05, alpha=None)  # t1609: "cuda:0 f32[1, 512, 1]"
    # t1609 = prims.add(t1607, 1e-05)  # t1609: "cuda:0 f32[1, 512, 1]"
  t1610 = ltorch.rsqrt(t1609)  # t1610: "cuda:0 f32[1, 512, 1]"
    # t1610 = prims.rsqrt(t1609)  # t1610: "cuda:0 f32[1, 512, 1]"
  t1612 = ltorch.mul(t1602, t1610)  # t1612: "cuda:0 f32[1, 512, 4096]"
    # t1611 = prims.broadcast_in_dim(t1610, (1, 512, 4096), (0, 1, 2))  # t1611: "cuda:0 f32[1, 512, 4096]"
    # t1612 = prims.mul(t1602, t1611)  # t1612: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1613 = ltorch.to(t1612, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1613: "cuda:0 bf16[1, 512, 4096]"
    # t1613 = prims.convert_element_type(t1612, dtypes.bfloat16)  # t1613: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1623 = ltorch.mul(t1613, t_transformer_h_9_norm_2_weight)  # t1623: "cuda:0 bf16[1, 512, 4096]"
    # t1619 = prims.broadcast_in_dim(t_transformer_h_9_norm_2_weight, (1, 512, 4096), (2,))  # t1619: "cuda:0 bf16[1, 512, 4096]"
    # t1620 = prims.convert_element_type(t1613, dtypes.float32)  # t1620: "cuda:0 f32[1, 512, 4096]"
    # t1621 = prims.convert_element_type(t1619, dtypes.float32)  # t1621: "cuda:0 f32[1, 512, 4096]"
    # t1622 = prims.mul(t1620, t1621)  # t1622: "cuda:0 f32[1, 512, 4096]"
    # t1623 = prims.convert_element_type(t1622, dtypes.bfloat16)  # t1623: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1628 = ltorch.linear(t1623, t_transformer_h_9_mlp_fc_1_weight, None)  # t1628: "cuda:0 bf16[1, 512, 11008]"
    # t1628 = prims.linear(t1623, t_transformer_h_9_mlp_fc_1_weight, None)  # t1628: "cuda:0 bf16[1, 512, 11008]"
  t1632 = ltorch.linear(t1623, t_transformer_h_9_mlp_fc_2_weight, None)  # t1632: "cuda:0 bf16[1, 512, 11008]"
    # t1632 = prims.linear(t1623, t_transformer_h_9_mlp_fc_2_weight, None)  # t1632: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1642 = ltorch.silu(t1628, False)  # t1642: "cuda:0 bf16[1, 512, 11008]"
    # t1633 = prims.convert_element_type(t1628, dtypes.float32)  # t1633: "cuda:0 f32[1, 512, 11008]"
    # t1634 = prims.neg(t1633)  # t1634: "cuda:0 f32[1, 512, 11008]"
    # t1635 = prims.exp(t1634)  # t1635: "cuda:0 f32[1, 512, 11008]"
    # t1636 = prims.add(1.0, t1635)  # t1636: "cuda:0 f32[1, 512, 11008]"
    # t1637 = prims.reciprocal(t1636)  # t1637: "cuda:0 f32[1, 512, 11008]"
    # t1638 = prims.convert_element_type(t1637, dtypes.bfloat16)  # t1638: "cuda:0 bf16[1, 512, 11008]"
    # t1639 = prims.convert_element_type(t1628, dtypes.float32)  # t1639: "cuda:0 f32[1, 512, 11008]"
    # t1640 = prims.convert_element_type(t1638, dtypes.float32)  # t1640: "cuda:0 f32[1, 512, 11008]"
    # t1641 = prims.mul(t1639, t1640)  # t1641: "cuda:0 f32[1, 512, 11008]"
    # t1642 = prims.convert_element_type(t1641, dtypes.bfloat16)  # t1642: "cuda:0 bf16[1, 512, 11008]"
  t1646 = ltorch.mul(t1642, t1632)  # t1646: "cuda:0 bf16[1, 512, 11008]"
    # t1643 = prims.convert_element_type(t1642, dtypes.float32)  # t1643: "cuda:0 f32[1, 512, 11008]"
    # t1644 = prims.convert_element_type(t1632, dtypes.float32)  # t1644: "cuda:0 f32[1, 512, 11008]"
    # t1645 = prims.mul(t1643, t1644)  # t1645: "cuda:0 f32[1, 512, 11008]"
    # t1646 = prims.convert_element_type(t1645, dtypes.bfloat16)  # t1646: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1650 = ltorch.linear(t1646, t_transformer_h_9_mlp_proj_weight, None)  # t1650: "cuda:0 bf16[1, 512, 4096]"
    # t1650 = prims.linear(t1646, t_transformer_h_9_mlp_proj_weight, None)  # t1650: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1654 = ltorch.add(t1650, t1601, alpha=None)  # t1654: "cuda:0 bf16[1, 512, 4096]"
    # t1651 = prims.convert_element_type(t1650, dtypes.float32)  # t1651: "cuda:0 f32[1, 512, 4096]"
    # t1652 = prims.convert_element_type(t1601, dtypes.float32)  # t1652: "cuda:0 f32[1, 512, 4096]"
    # t1653 = prims.add(t1651, t1652)  # t1653: "cuda:0 f32[1, 512, 4096]"
    # t1654 = prims.convert_element_type(t1653, dtypes.bfloat16)  # t1654: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1656 = prims.convert_element_type(t1654, dtypes.float32)  # t1656: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1657 = ltorch.mul(t1656, t1656)  # t1657: "cuda:0 f32[1, 512, 4096]"
    # t1657 = prims.mul(t1656, t1656)  # t1657: "cuda:0 f32[1, 512, 4096]"
  t1661 = ltorch.mean(t1657, -1, True, dtype=None)  # t1661: "cuda:0 f32[1, 512, 1]"
    # t1659 = prims.sum(t1657, (2,))  # t1659: "cuda:0 f32[1, 512]"
    # t1660 = prims.broadcast_in_dim(t1659, [1, 512, 1], [0, 1])  # t1660: "cuda:0 f32[1, 512, 1]"
    # t1661 = ltorch.true_divide(t1660, 4096)  # t1661: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1661 = prims.div(t1660, 4096.0)  # t1661: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1663 = ltorch.add(t1661, 1e-05, alpha=None)  # t1663: "cuda:0 f32[1, 512, 1]"
    # t1663 = prims.add(t1661, 1e-05)  # t1663: "cuda:0 f32[1, 512, 1]"
  t1664 = ltorch.rsqrt(t1663)  # t1664: "cuda:0 f32[1, 512, 1]"
    # t1664 = prims.rsqrt(t1663)  # t1664: "cuda:0 f32[1, 512, 1]"
  t1666 = ltorch.mul(t1656, t1664)  # t1666: "cuda:0 f32[1, 512, 4096]"
    # t1665 = prims.broadcast_in_dim(t1664, (1, 512, 4096), (0, 1, 2))  # t1665: "cuda:0 f32[1, 512, 4096]"
    # t1666 = prims.mul(t1656, t1665)  # t1666: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1667 = ltorch.to(t1666, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1667: "cuda:0 bf16[1, 512, 4096]"
    # t1667 = prims.convert_element_type(t1666, dtypes.bfloat16)  # t1667: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1677 = ltorch.mul(t1667, t_transformer_h_10_norm_1_weight)  # t1677: "cuda:0 bf16[1, 512, 4096]"
    # t1673 = prims.broadcast_in_dim(t_transformer_h_10_norm_1_weight, (1, 512, 4096), (2,))  # t1673: "cuda:0 bf16[1, 512, 4096]"
    # t1674 = prims.convert_element_type(t1667, dtypes.float32)  # t1674: "cuda:0 f32[1, 512, 4096]"
    # t1675 = prims.convert_element_type(t1673, dtypes.float32)  # t1675: "cuda:0 f32[1, 512, 4096]"
    # t1676 = prims.mul(t1674, t1675)  # t1676: "cuda:0 f32[1, 512, 4096]"
    # t1677 = prims.convert_element_type(t1676, dtypes.bfloat16)  # t1677: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1682 = ltorch.linear(t1677, t_transformer_h_10_attn_attn_weight, None)  # t1682: "cuda:0 bf16[1, 512, 12288]"
    # t1682 = prims.linear(t1677, t_transformer_h_10_attn_attn_weight, None)  # t1682: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1683 = ltorch.view(t1682, 1, 512, 32, 3, 128)  # t1683: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1683 = ltorch.reshape(t1682, (1, 512, 32, 3, 128))  # t1683: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1683 = prims.reshape(t1682, (1, 512, 32, 3, 128))  # t1683: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1684 = ltorch.permute(t1683, 0, 2, 3, 1, 4)  # t1684: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1684 = prims.transpose(t1683, (0, 2, 3, 1, 4))  # t1684: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1685, t1686, t1687) = ltorch.split(t1684, (1, 1, 1), 2)
    # t1685 = prims.slice_prim(t1684, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1685: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1686 = prims.slice_prim(t1684, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1686: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1687 = prims.slice_prim(t1684, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1687: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1688 = ltorch.reshape(t1685, 1, -1, 512, 128)  # t1688: "cuda:0 bf16[1, 32, 512, 128]"
    # t1688 = prims.reshape(t1685, (1, 32, 512, 128))  # t1688: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1689 = ltorch.reshape(t1686, 1, -1, 512, 128)  # t1689: "cuda:0 bf16[1, 32, 512, 128]"
    # t1689 = prims.reshape(t1686, (1, 32, 512, 128))  # t1689: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1690 = ltorch.reshape(t1687, 1, -1, 512, 128)  # t1690: "cuda:0 bf16[1, 32, 512, 128]"
    # t1690 = prims.reshape(t1687, (1, 32, 512, 128))  # t1690: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1691 = ltorch.getitem(t1688, (..., slice(None, 128, None)))  # t1691: "cuda:0 bf16[1, 32, 512, 128]"
    # t1691 = prims.slice_prim(t1688, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1691: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1692 = ltorch.getitem(t1691, (..., slice(None, 64, None)))  # t1692: "cuda:0 bf16[1, 32, 512, 64]"
    # t1692 = prims.slice_prim(t1691, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1692: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1693 = ltorch.getitem(t1691, (..., slice(64, None, None)))  # t1693: "cuda:0 bf16[1, 32, 512, 64]"
    # t1693 = prims.slice_prim(t1691, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1693: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1696 = ltorch.neg(t1693)  # t1696: "cuda:0 bf16[1, 32, 512, 64]"
    # t1694 = prims.convert_element_type(t1693, dtypes.float32)  # t1694: "cuda:0 f32[1, 32, 512, 64]"
    # t1695 = prims.neg(t1694)  # t1695: "cuda:0 f32[1, 32, 512, 64]"
    # t1696 = prims.convert_element_type(t1695, dtypes.bfloat16)  # t1696: "cuda:0 bf16[1, 32, 512, 64]"
  t1697 = ltorch.cat((t1696, t1692), -1)  # t1697: "cuda:0 bf16[1, 32, 512, 128]"
    # t1697 = prims.cat((t1696, t1692), -1)  # t1697: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1700 = ltorch.mul(t1691, cos)  # t1700: "cuda:0 f32[1, 32, 512, 128]"
    # t1698 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1698: "cuda:0 f32[1, 32, 512, 128]"
    # t1699 = prims.convert_element_type(t1691, dtypes.float32)  # t1699: "cuda:0 f32[1, 32, 512, 128]"
    # t1700 = prims.mul(t1699, t1698)  # t1700: "cuda:0 f32[1, 32, 512, 128]"
  t1703 = ltorch.mul(t1697, sin)  # t1703: "cuda:0 f32[1, 32, 512, 128]"
    # t1701 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1701: "cuda:0 f32[1, 32, 512, 128]"
    # t1702 = prims.convert_element_type(t1697, dtypes.float32)  # t1702: "cuda:0 f32[1, 32, 512, 128]"
    # t1703 = prims.mul(t1702, t1701)  # t1703: "cuda:0 f32[1, 32, 512, 128]"
  t1704 = ltorch.add(t1700, t1703, alpha=None)  # t1704: "cuda:0 f32[1, 32, 512, 128]"
    # t1704 = prims.add(t1700, t1703)  # t1704: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1705 = ltorch.to(t1704, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1705: "cuda:0 bf16[1, 32, 512, 128]"
    # t1705 = prims.convert_element_type(t1704, dtypes.bfloat16)  # t1705: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1706 = ltorch.getitem(t1689, (..., slice(None, 128, None)))  # t1706: "cuda:0 bf16[1, 32, 512, 128]"
    # t1706 = prims.slice_prim(t1689, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1706: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1707 = ltorch.getitem(t1706, (..., slice(None, 64, None)))  # t1707: "cuda:0 bf16[1, 32, 512, 64]"
    # t1707 = prims.slice_prim(t1706, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1707: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1708 = ltorch.getitem(t1706, (..., slice(64, None, None)))  # t1708: "cuda:0 bf16[1, 32, 512, 64]"
    # t1708 = prims.slice_prim(t1706, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1708: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1711 = ltorch.neg(t1708)  # t1711: "cuda:0 bf16[1, 32, 512, 64]"
    # t1709 = prims.convert_element_type(t1708, dtypes.float32)  # t1709: "cuda:0 f32[1, 32, 512, 64]"
    # t1710 = prims.neg(t1709)  # t1710: "cuda:0 f32[1, 32, 512, 64]"
    # t1711 = prims.convert_element_type(t1710, dtypes.bfloat16)  # t1711: "cuda:0 bf16[1, 32, 512, 64]"
  t1712 = ltorch.cat((t1711, t1707), -1)  # t1712: "cuda:0 bf16[1, 32, 512, 128]"
    # t1712 = prims.cat((t1711, t1707), -1)  # t1712: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1715 = ltorch.mul(t1706, cos)  # t1715: "cuda:0 f32[1, 32, 512, 128]"
    # t1713 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1713: "cuda:0 f32[1, 32, 512, 128]"
    # t1714 = prims.convert_element_type(t1706, dtypes.float32)  # t1714: "cuda:0 f32[1, 32, 512, 128]"
    # t1715 = prims.mul(t1714, t1713)  # t1715: "cuda:0 f32[1, 32, 512, 128]"
  t1718 = ltorch.mul(t1712, sin)  # t1718: "cuda:0 f32[1, 32, 512, 128]"
    # t1716 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1716: "cuda:0 f32[1, 32, 512, 128]"
    # t1717 = prims.convert_element_type(t1712, dtypes.float32)  # t1717: "cuda:0 f32[1, 32, 512, 128]"
    # t1718 = prims.mul(t1717, t1716)  # t1718: "cuda:0 f32[1, 32, 512, 128]"
  t1719 = ltorch.add(t1715, t1718, alpha=None)  # t1719: "cuda:0 f32[1, 32, 512, 128]"
    # t1719 = prims.add(t1715, t1718)  # t1719: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1720 = ltorch.to(t1719, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1720: "cuda:0 bf16[1, 32, 512, 128]"
    # t1720 = prims.convert_element_type(t1719, dtypes.bfloat16)  # t1720: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1721 = ltorch.getitem(t1688, (..., slice(128, None, None)))  # t1721: "cuda:0 bf16[1, 32, 512, 0]"
    # t1721 = prims.slice_prim(t1688, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1721: "cuda:0 bf16[1, 32, 512, 0]"
  t1722 = ltorch.cat((t1705, t1721), -1)  # t1722: "cuda:0 bf16[1, 32, 512, 128]"
    # t1722 = prims.cat((t1705, t1721), -1)  # t1722: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1723 = ltorch.getitem(t1689, (..., slice(128, None, None)))  # t1723: "cuda:0 bf16[1, 32, 512, 0]"
    # t1723 = prims.slice_prim(t1689, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1723: "cuda:0 bf16[1, 32, 512, 0]"
  t1724 = ltorch.cat((t1720, t1723), -1)  # t1724: "cuda:0 bf16[1, 32, 512, 128]"
    # t1724 = prims.cat((t1720, t1723), -1)  # t1724: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1754 = ltorch.scaled_dot_product_attention(t1722, t1724, t1690, None, 0.0, True, scale=0.08838834764831843)  # t1754: "cuda:0 bf16[1, 32, 512, 128]"
    # t1727 = ltorch.mul(t1722, 0.29730177875068026)  # t1727: "cuda:0 bf16[1, 32, 512, 128]"
      # t1725 = prims.convert_element_type(t1722, dtypes.float32)  # t1725: "cuda:0 f32[1, 32, 512, 128]"
      # t1726 = prims.mul(t1725, 0.29730177875068026)  # t1726: "cuda:0 f32[1, 32, 512, 128]"
      # t1727 = prims.convert_element_type(t1726, dtypes.bfloat16)  # t1727: "cuda:0 bf16[1, 32, 512, 128]"
    # t1728 = ltorch.transpose(t1724, -2, -1)  # t1728: "cuda:0 bf16[1, 32, 128, 512]"
      # t1728 = prims.transpose(t1724, (0, 1, 3, 2))  # t1728: "cuda:0 bf16[1, 32, 128, 512]"
    # t1731 = ltorch.mul(t1728, 0.29730177875068026)  # t1731: "cuda:0 bf16[1, 32, 128, 512]"
      # t1729 = prims.convert_element_type(t1728, dtypes.float32)  # t1729: "cuda:0 f32[1, 32, 128, 512]"
      # t1730 = prims.mul(t1729, 0.29730177875068026)  # t1730: "cuda:0 f32[1, 32, 128, 512]"
      # t1731 = prims.convert_element_type(t1730, dtypes.bfloat16)  # t1731: "cuda:0 bf16[1, 32, 128, 512]"
    # t1732 = ltorch.matmul(t1727, t1731)  # t1732: "cuda:0 bf16[1, 32, 512, 512]"
      # t1732 = prims.matmul(t1727, t1731)  # t1732: "cuda:0 bf16[1, 32, 512, 512]"
    # t1742 = ltorch.tril(t1732, 0, fill_value=-float('inf'))  # t1742: "cuda:0 bf16[1, 32, 512, 512]"
      # t1733 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1733: "cuda:0 i64[512]"
        # t1733 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1733: "cuda:0 i64[512]"
      # t1734 = ltorch.unsqueeze(t1733, -1)  # t1734: "cuda:0 i64[512, 1]"
        # t1734 = prims.broadcast_in_dim(t1733, [512, 1], [0])  # t1734: "cuda:0 i64[512, 1]"
      # t1735 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1735: "cuda:0 i64[512]"
        # t1735 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1735: "cuda:0 i64[512]"
      # t1736 = ltorch.unsqueeze(t1735, -2)  # t1736: "cuda:0 i64[1, 512]"
        # t1736 = prims.broadcast_in_dim(t1735, [1, 512], [1])  # t1736: "cuda:0 i64[1, 512]"
      # t1737 = ltorch.add(t1734, 0, alpha=None)  # t1737: "cuda:0 i64[512, 1]"
        # t1737 = prims.add(t1734, 0)  # t1737: "cuda:0 i64[512, 1]"
      # t1740 = ltorch.ge(t1737, t1736)  # t1740: "cuda:0 b8[512, 512]"
        # t1738 = prims.broadcast_in_dim(t1737, (512, 512), (0, 1))  # t1738: "cuda:0 i64[512, 512]"
        # t1739 = prims.broadcast_in_dim(t1736, (512, 512), (0, 1))  # t1739: "cuda:0 i64[512, 512]"
        # t1740 = prims.ge(t1738, t1739)  # t1740: "cuda:0 b8[512, 512]"
      # t1742 = ltorch.where(t1740, t1732, -float('inf'))  # t1742: "cuda:0 bf16[1, 32, 512, 512]"
        # t1741 = prims.broadcast_in_dim(t1740, (1, 32, 512, 512), (2, 3))  # t1741: "cuda:0 b8[1, 32, 512, 512]"
        # t1742 = prims.where(t1741, t1732, -float('inf'))  # t1742: "cuda:0 bf16[1, 32, 512, 512]"
    # t1753 = ltorch._softmax(t1742, -1, dtype=None)  # t1753: "cuda:0 bf16[1, 32, 512, 512]"
      # t1743 = ltorch.to(t1742, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1743: "cuda:0 f32[1, 32, 512, 512]"
        # t1743 = prims.convert_element_type(t1742, dtypes.float32)  # t1743: "cuda:0 f32[1, 32, 512, 512]"
      # t1745 = ltorch.amax(t1743, -1, True)  # t1745: "cuda:0 f32[1, 32, 512, 1]"
        # t1744 = prims.amax(t1743, (3,))  # t1744: "cuda:0 f32[1, 32, 512]"
        # t1745 = prims.broadcast_in_dim(t1744, [1, 32, 512, 1], [0, 1, 2])  # t1745: "cuda:0 f32[1, 32, 512, 1]"
      # t1747 = ltorch.sub(t1743, t1745, alpha=None)  # t1747: "cuda:0 f32[1, 32, 512, 512]"
        # t1746 = prims.broadcast_in_dim(t1745, (1, 32, 512, 512), (0, 1, 2, 3))  # t1746: "cuda:0 f32[1, 32, 512, 512]"
        # t1747 = prims.sub(t1743, t1746)  # t1747: "cuda:0 f32[1, 32, 512, 512]"
      # t1748 = ltorch.exp(t1747)  # t1748: "cuda:0 f32[1, 32, 512, 512]"
        # t1748 = prims.exp(t1747)  # t1748: "cuda:0 f32[1, 32, 512, 512]"
      # t1750 = ltorch.sum(t1748, -1, True, dtype=None)  # t1750: "cuda:0 f32[1, 32, 512, 1]"
        # t1749 = prims.sum(t1748, (3,))  # t1749: "cuda:0 f32[1, 32, 512]"
        # t1750 = prims.broadcast_in_dim(t1749, [1, 32, 512, 1], [0, 1, 2])  # t1750: "cuda:0 f32[1, 32, 512, 1]"
      # t1752 = ltorch.true_divide(t1748, t1750)  # t1752: "cuda:0 f32[1, 32, 512, 512]"
        # t1751 = prims.broadcast_in_dim(t1750, (1, 32, 512, 512), (0, 1, 2, 3))  # t1751: "cuda:0 f32[1, 32, 512, 512]"
        # t1752 = prims.div(t1748, t1751)  # t1752: "cuda:0 f32[1, 32, 512, 512]"
      # t1753 = ltorch.to(t1752, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1753: "cuda:0 bf16[1, 32, 512, 512]"
        # t1753 = prims.convert_element_type(t1752, dtypes.bfloat16)  # t1753: "cuda:0 bf16[1, 32, 512, 512]"
    # t1754 = ltorch.matmul(t1753, t1690)  # t1754: "cuda:0 bf16[1, 32, 512, 128]"
      # t1754 = prims.matmul(t1753, t1690)  # t1754: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1755 = ltorch.transpose(t1754, 1, 2)  # t1755: "cuda:0 bf16[1, 512, 32, 128]"
    # t1755 = prims.transpose(t1754, (0, 2, 1, 3))  # t1755: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1756 = ltorch.reshape(t1755, 1, 512, 4096)  # t1756: "cuda:0 bf16[1, 512, 4096]"
    # t1756 = prims.reshape(t1755, (1, 512, 4096))  # t1756: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1760 = ltorch.linear(t1756, t_transformer_h_10_attn_proj_weight, None)  # t1760: "cuda:0 bf16[1, 512, 4096]"
    # t1760 = prims.linear(t1756, t_transformer_h_10_attn_proj_weight, None)  # t1760: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1764 = ltorch.add(t1760, t1654, alpha=None)  # t1764: "cuda:0 bf16[1, 512, 4096]"
    # t1761 = prims.convert_element_type(t1760, dtypes.float32)  # t1761: "cuda:0 f32[1, 512, 4096]"
    # t1762 = prims.convert_element_type(t1654, dtypes.float32)  # t1762: "cuda:0 f32[1, 512, 4096]"
    # t1763 = prims.add(t1761, t1762)  # t1763: "cuda:0 f32[1, 512, 4096]"
    # t1764 = prims.convert_element_type(t1763, dtypes.bfloat16)  # t1764: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1765 = prims.convert_element_type(t1764, dtypes.float32)  # t1765: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1766 = ltorch.mul(t1765, t1765)  # t1766: "cuda:0 f32[1, 512, 4096]"
    # t1766 = prims.mul(t1765, t1765)  # t1766: "cuda:0 f32[1, 512, 4096]"
  t1770 = ltorch.mean(t1766, -1, True, dtype=None)  # t1770: "cuda:0 f32[1, 512, 1]"
    # t1768 = prims.sum(t1766, (2,))  # t1768: "cuda:0 f32[1, 512]"
    # t1769 = prims.broadcast_in_dim(t1768, [1, 512, 1], [0, 1])  # t1769: "cuda:0 f32[1, 512, 1]"
    # t1770 = ltorch.true_divide(t1769, 4096)  # t1770: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1770 = prims.div(t1769, 4096.0)  # t1770: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1772 = ltorch.add(t1770, 1e-05, alpha=None)  # t1772: "cuda:0 f32[1, 512, 1]"
    # t1772 = prims.add(t1770, 1e-05)  # t1772: "cuda:0 f32[1, 512, 1]"
  t1773 = ltorch.rsqrt(t1772)  # t1773: "cuda:0 f32[1, 512, 1]"
    # t1773 = prims.rsqrt(t1772)  # t1773: "cuda:0 f32[1, 512, 1]"
  t1775 = ltorch.mul(t1765, t1773)  # t1775: "cuda:0 f32[1, 512, 4096]"
    # t1774 = prims.broadcast_in_dim(t1773, (1, 512, 4096), (0, 1, 2))  # t1774: "cuda:0 f32[1, 512, 4096]"
    # t1775 = prims.mul(t1765, t1774)  # t1775: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1776 = ltorch.to(t1775, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1776: "cuda:0 bf16[1, 512, 4096]"
    # t1776 = prims.convert_element_type(t1775, dtypes.bfloat16)  # t1776: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1786 = ltorch.mul(t1776, t_transformer_h_10_norm_2_weight)  # t1786: "cuda:0 bf16[1, 512, 4096]"
    # t1782 = prims.broadcast_in_dim(t_transformer_h_10_norm_2_weight, (1, 512, 4096), (2,))  # t1782: "cuda:0 bf16[1, 512, 4096]"
    # t1783 = prims.convert_element_type(t1776, dtypes.float32)  # t1783: "cuda:0 f32[1, 512, 4096]"
    # t1784 = prims.convert_element_type(t1782, dtypes.float32)  # t1784: "cuda:0 f32[1, 512, 4096]"
    # t1785 = prims.mul(t1783, t1784)  # t1785: "cuda:0 f32[1, 512, 4096]"
    # t1786 = prims.convert_element_type(t1785, dtypes.bfloat16)  # t1786: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1791 = ltorch.linear(t1786, t_transformer_h_10_mlp_fc_1_weight, None)  # t1791: "cuda:0 bf16[1, 512, 11008]"
    # t1791 = prims.linear(t1786, t_transformer_h_10_mlp_fc_1_weight, None)  # t1791: "cuda:0 bf16[1, 512, 11008]"
  t1795 = ltorch.linear(t1786, t_transformer_h_10_mlp_fc_2_weight, None)  # t1795: "cuda:0 bf16[1, 512, 11008]"
    # t1795 = prims.linear(t1786, t_transformer_h_10_mlp_fc_2_weight, None)  # t1795: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1805 = ltorch.silu(t1791, False)  # t1805: "cuda:0 bf16[1, 512, 11008]"
    # t1796 = prims.convert_element_type(t1791, dtypes.float32)  # t1796: "cuda:0 f32[1, 512, 11008]"
    # t1797 = prims.neg(t1796)  # t1797: "cuda:0 f32[1, 512, 11008]"
    # t1798 = prims.exp(t1797)  # t1798: "cuda:0 f32[1, 512, 11008]"
    # t1799 = prims.add(1.0, t1798)  # t1799: "cuda:0 f32[1, 512, 11008]"
    # t1800 = prims.reciprocal(t1799)  # t1800: "cuda:0 f32[1, 512, 11008]"
    # t1801 = prims.convert_element_type(t1800, dtypes.bfloat16)  # t1801: "cuda:0 bf16[1, 512, 11008]"
    # t1802 = prims.convert_element_type(t1791, dtypes.float32)  # t1802: "cuda:0 f32[1, 512, 11008]"
    # t1803 = prims.convert_element_type(t1801, dtypes.float32)  # t1803: "cuda:0 f32[1, 512, 11008]"
    # t1804 = prims.mul(t1802, t1803)  # t1804: "cuda:0 f32[1, 512, 11008]"
    # t1805 = prims.convert_element_type(t1804, dtypes.bfloat16)  # t1805: "cuda:0 bf16[1, 512, 11008]"
  t1809 = ltorch.mul(t1805, t1795)  # t1809: "cuda:0 bf16[1, 512, 11008]"
    # t1806 = prims.convert_element_type(t1805, dtypes.float32)  # t1806: "cuda:0 f32[1, 512, 11008]"
    # t1807 = prims.convert_element_type(t1795, dtypes.float32)  # t1807: "cuda:0 f32[1, 512, 11008]"
    # t1808 = prims.mul(t1806, t1807)  # t1808: "cuda:0 f32[1, 512, 11008]"
    # t1809 = prims.convert_element_type(t1808, dtypes.bfloat16)  # t1809: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1813 = ltorch.linear(t1809, t_transformer_h_10_mlp_proj_weight, None)  # t1813: "cuda:0 bf16[1, 512, 4096]"
    # t1813 = prims.linear(t1809, t_transformer_h_10_mlp_proj_weight, None)  # t1813: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1817 = ltorch.add(t1813, t1764, alpha=None)  # t1817: "cuda:0 bf16[1, 512, 4096]"
    # t1814 = prims.convert_element_type(t1813, dtypes.float32)  # t1814: "cuda:0 f32[1, 512, 4096]"
    # t1815 = prims.convert_element_type(t1764, dtypes.float32)  # t1815: "cuda:0 f32[1, 512, 4096]"
    # t1816 = prims.add(t1814, t1815)  # t1816: "cuda:0 f32[1, 512, 4096]"
    # t1817 = prims.convert_element_type(t1816, dtypes.bfloat16)  # t1817: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1819 = prims.convert_element_type(t1817, dtypes.float32)  # t1819: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1820 = ltorch.mul(t1819, t1819)  # t1820: "cuda:0 f32[1, 512, 4096]"
    # t1820 = prims.mul(t1819, t1819)  # t1820: "cuda:0 f32[1, 512, 4096]"
  t1824 = ltorch.mean(t1820, -1, True, dtype=None)  # t1824: "cuda:0 f32[1, 512, 1]"
    # t1822 = prims.sum(t1820, (2,))  # t1822: "cuda:0 f32[1, 512]"
    # t1823 = prims.broadcast_in_dim(t1822, [1, 512, 1], [0, 1])  # t1823: "cuda:0 f32[1, 512, 1]"
    # t1824 = ltorch.true_divide(t1823, 4096)  # t1824: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1824 = prims.div(t1823, 4096.0)  # t1824: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1826 = ltorch.add(t1824, 1e-05, alpha=None)  # t1826: "cuda:0 f32[1, 512, 1]"
    # t1826 = prims.add(t1824, 1e-05)  # t1826: "cuda:0 f32[1, 512, 1]"
  t1827 = ltorch.rsqrt(t1826)  # t1827: "cuda:0 f32[1, 512, 1]"
    # t1827 = prims.rsqrt(t1826)  # t1827: "cuda:0 f32[1, 512, 1]"
  t1829 = ltorch.mul(t1819, t1827)  # t1829: "cuda:0 f32[1, 512, 4096]"
    # t1828 = prims.broadcast_in_dim(t1827, (1, 512, 4096), (0, 1, 2))  # t1828: "cuda:0 f32[1, 512, 4096]"
    # t1829 = prims.mul(t1819, t1828)  # t1829: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1830 = ltorch.to(t1829, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1830: "cuda:0 bf16[1, 512, 4096]"
    # t1830 = prims.convert_element_type(t1829, dtypes.bfloat16)  # t1830: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1840 = ltorch.mul(t1830, t_transformer_h_11_norm_1_weight)  # t1840: "cuda:0 bf16[1, 512, 4096]"
    # t1836 = prims.broadcast_in_dim(t_transformer_h_11_norm_1_weight, (1, 512, 4096), (2,))  # t1836: "cuda:0 bf16[1, 512, 4096]"
    # t1837 = prims.convert_element_type(t1830, dtypes.float32)  # t1837: "cuda:0 f32[1, 512, 4096]"
    # t1838 = prims.convert_element_type(t1836, dtypes.float32)  # t1838: "cuda:0 f32[1, 512, 4096]"
    # t1839 = prims.mul(t1837, t1838)  # t1839: "cuda:0 f32[1, 512, 4096]"
    # t1840 = prims.convert_element_type(t1839, dtypes.bfloat16)  # t1840: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1845 = ltorch.linear(t1840, t_transformer_h_11_attn_attn_weight, None)  # t1845: "cuda:0 bf16[1, 512, 12288]"
    # t1845 = prims.linear(t1840, t_transformer_h_11_attn_attn_weight, None)  # t1845: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1846 = ltorch.view(t1845, 1, 512, 32, 3, 128)  # t1846: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1846 = ltorch.reshape(t1845, (1, 512, 32, 3, 128))  # t1846: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1846 = prims.reshape(t1845, (1, 512, 32, 3, 128))  # t1846: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1847 = ltorch.permute(t1846, 0, 2, 3, 1, 4)  # t1847: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1847 = prims.transpose(t1846, (0, 2, 3, 1, 4))  # t1847: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1848, t1849, t1850) = ltorch.split(t1847, (1, 1, 1), 2)
    # t1848 = prims.slice_prim(t1847, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1848: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1849 = prims.slice_prim(t1847, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1849: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1850 = prims.slice_prim(t1847, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1850: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1851 = ltorch.reshape(t1848, 1, -1, 512, 128)  # t1851: "cuda:0 bf16[1, 32, 512, 128]"
    # t1851 = prims.reshape(t1848, (1, 32, 512, 128))  # t1851: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1852 = ltorch.reshape(t1849, 1, -1, 512, 128)  # t1852: "cuda:0 bf16[1, 32, 512, 128]"
    # t1852 = prims.reshape(t1849, (1, 32, 512, 128))  # t1852: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1853 = ltorch.reshape(t1850, 1, -1, 512, 128)  # t1853: "cuda:0 bf16[1, 32, 512, 128]"
    # t1853 = prims.reshape(t1850, (1, 32, 512, 128))  # t1853: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1854 = ltorch.getitem(t1851, (..., slice(None, 128, None)))  # t1854: "cuda:0 bf16[1, 32, 512, 128]"
    # t1854 = prims.slice_prim(t1851, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1854: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1855 = ltorch.getitem(t1854, (..., slice(None, 64, None)))  # t1855: "cuda:0 bf16[1, 32, 512, 64]"
    # t1855 = prims.slice_prim(t1854, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1855: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1856 = ltorch.getitem(t1854, (..., slice(64, None, None)))  # t1856: "cuda:0 bf16[1, 32, 512, 64]"
    # t1856 = prims.slice_prim(t1854, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1856: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1859 = ltorch.neg(t1856)  # t1859: "cuda:0 bf16[1, 32, 512, 64]"
    # t1857 = prims.convert_element_type(t1856, dtypes.float32)  # t1857: "cuda:0 f32[1, 32, 512, 64]"
    # t1858 = prims.neg(t1857)  # t1858: "cuda:0 f32[1, 32, 512, 64]"
    # t1859 = prims.convert_element_type(t1858, dtypes.bfloat16)  # t1859: "cuda:0 bf16[1, 32, 512, 64]"
  t1860 = ltorch.cat((t1859, t1855), -1)  # t1860: "cuda:0 bf16[1, 32, 512, 128]"
    # t1860 = prims.cat((t1859, t1855), -1)  # t1860: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1863 = ltorch.mul(t1854, cos)  # t1863: "cuda:0 f32[1, 32, 512, 128]"
    # t1861 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1861: "cuda:0 f32[1, 32, 512, 128]"
    # t1862 = prims.convert_element_type(t1854, dtypes.float32)  # t1862: "cuda:0 f32[1, 32, 512, 128]"
    # t1863 = prims.mul(t1862, t1861)  # t1863: "cuda:0 f32[1, 32, 512, 128]"
  t1866 = ltorch.mul(t1860, sin)  # t1866: "cuda:0 f32[1, 32, 512, 128]"
    # t1864 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1864: "cuda:0 f32[1, 32, 512, 128]"
    # t1865 = prims.convert_element_type(t1860, dtypes.float32)  # t1865: "cuda:0 f32[1, 32, 512, 128]"
    # t1866 = prims.mul(t1865, t1864)  # t1866: "cuda:0 f32[1, 32, 512, 128]"
  t1867 = ltorch.add(t1863, t1866, alpha=None)  # t1867: "cuda:0 f32[1, 32, 512, 128]"
    # t1867 = prims.add(t1863, t1866)  # t1867: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1868 = ltorch.to(t1867, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1868: "cuda:0 bf16[1, 32, 512, 128]"
    # t1868 = prims.convert_element_type(t1867, dtypes.bfloat16)  # t1868: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1869 = ltorch.getitem(t1852, (..., slice(None, 128, None)))  # t1869: "cuda:0 bf16[1, 32, 512, 128]"
    # t1869 = prims.slice_prim(t1852, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1869: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1870 = ltorch.getitem(t1869, (..., slice(None, 64, None)))  # t1870: "cuda:0 bf16[1, 32, 512, 64]"
    # t1870 = prims.slice_prim(t1869, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1870: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1871 = ltorch.getitem(t1869, (..., slice(64, None, None)))  # t1871: "cuda:0 bf16[1, 32, 512, 64]"
    # t1871 = prims.slice_prim(t1869, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1871: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1874 = ltorch.neg(t1871)  # t1874: "cuda:0 bf16[1, 32, 512, 64]"
    # t1872 = prims.convert_element_type(t1871, dtypes.float32)  # t1872: "cuda:0 f32[1, 32, 512, 64]"
    # t1873 = prims.neg(t1872)  # t1873: "cuda:0 f32[1, 32, 512, 64]"
    # t1874 = prims.convert_element_type(t1873, dtypes.bfloat16)  # t1874: "cuda:0 bf16[1, 32, 512, 64]"
  t1875 = ltorch.cat((t1874, t1870), -1)  # t1875: "cuda:0 bf16[1, 32, 512, 128]"
    # t1875 = prims.cat((t1874, t1870), -1)  # t1875: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1878 = ltorch.mul(t1869, cos)  # t1878: "cuda:0 f32[1, 32, 512, 128]"
    # t1876 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1876: "cuda:0 f32[1, 32, 512, 128]"
    # t1877 = prims.convert_element_type(t1869, dtypes.float32)  # t1877: "cuda:0 f32[1, 32, 512, 128]"
    # t1878 = prims.mul(t1877, t1876)  # t1878: "cuda:0 f32[1, 32, 512, 128]"
  t1881 = ltorch.mul(t1875, sin)  # t1881: "cuda:0 f32[1, 32, 512, 128]"
    # t1879 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1879: "cuda:0 f32[1, 32, 512, 128]"
    # t1880 = prims.convert_element_type(t1875, dtypes.float32)  # t1880: "cuda:0 f32[1, 32, 512, 128]"
    # t1881 = prims.mul(t1880, t1879)  # t1881: "cuda:0 f32[1, 32, 512, 128]"
  t1882 = ltorch.add(t1878, t1881, alpha=None)  # t1882: "cuda:0 f32[1, 32, 512, 128]"
    # t1882 = prims.add(t1878, t1881)  # t1882: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1883 = ltorch.to(t1882, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1883: "cuda:0 bf16[1, 32, 512, 128]"
    # t1883 = prims.convert_element_type(t1882, dtypes.bfloat16)  # t1883: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1884 = ltorch.getitem(t1851, (..., slice(128, None, None)))  # t1884: "cuda:0 bf16[1, 32, 512, 0]"
    # t1884 = prims.slice_prim(t1851, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1884: "cuda:0 bf16[1, 32, 512, 0]"
  t1885 = ltorch.cat((t1868, t1884), -1)  # t1885: "cuda:0 bf16[1, 32, 512, 128]"
    # t1885 = prims.cat((t1868, t1884), -1)  # t1885: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1886 = ltorch.getitem(t1852, (..., slice(128, None, None)))  # t1886: "cuda:0 bf16[1, 32, 512, 0]"
    # t1886 = prims.slice_prim(t1852, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1886: "cuda:0 bf16[1, 32, 512, 0]"
  t1887 = ltorch.cat((t1883, t1886), -1)  # t1887: "cuda:0 bf16[1, 32, 512, 128]"
    # t1887 = prims.cat((t1883, t1886), -1)  # t1887: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1917 = ltorch.scaled_dot_product_attention(t1885, t1887, t1853, None, 0.0, True, scale=0.08838834764831843)  # t1917: "cuda:0 bf16[1, 32, 512, 128]"
    # t1890 = ltorch.mul(t1885, 0.29730177875068026)  # t1890: "cuda:0 bf16[1, 32, 512, 128]"
      # t1888 = prims.convert_element_type(t1885, dtypes.float32)  # t1888: "cuda:0 f32[1, 32, 512, 128]"
      # t1889 = prims.mul(t1888, 0.29730177875068026)  # t1889: "cuda:0 f32[1, 32, 512, 128]"
      # t1890 = prims.convert_element_type(t1889, dtypes.bfloat16)  # t1890: "cuda:0 bf16[1, 32, 512, 128]"
    # t1891 = ltorch.transpose(t1887, -2, -1)  # t1891: "cuda:0 bf16[1, 32, 128, 512]"
      # t1891 = prims.transpose(t1887, (0, 1, 3, 2))  # t1891: "cuda:0 bf16[1, 32, 128, 512]"
    # t1894 = ltorch.mul(t1891, 0.29730177875068026)  # t1894: "cuda:0 bf16[1, 32, 128, 512]"
      # t1892 = prims.convert_element_type(t1891, dtypes.float32)  # t1892: "cuda:0 f32[1, 32, 128, 512]"
      # t1893 = prims.mul(t1892, 0.29730177875068026)  # t1893: "cuda:0 f32[1, 32, 128, 512]"
      # t1894 = prims.convert_element_type(t1893, dtypes.bfloat16)  # t1894: "cuda:0 bf16[1, 32, 128, 512]"
    # t1895 = ltorch.matmul(t1890, t1894)  # t1895: "cuda:0 bf16[1, 32, 512, 512]"
      # t1895 = prims.matmul(t1890, t1894)  # t1895: "cuda:0 bf16[1, 32, 512, 512]"
    # t1905 = ltorch.tril(t1895, 0, fill_value=-float('inf'))  # t1905: "cuda:0 bf16[1, 32, 512, 512]"
      # t1896 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1896: "cuda:0 i64[512]"
        # t1896 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1896: "cuda:0 i64[512]"
      # t1897 = ltorch.unsqueeze(t1896, -1)  # t1897: "cuda:0 i64[512, 1]"
        # t1897 = prims.broadcast_in_dim(t1896, [512, 1], [0])  # t1897: "cuda:0 i64[512, 1]"
      # t1898 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1898: "cuda:0 i64[512]"
        # t1898 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1898: "cuda:0 i64[512]"
      # t1899 = ltorch.unsqueeze(t1898, -2)  # t1899: "cuda:0 i64[1, 512]"
        # t1899 = prims.broadcast_in_dim(t1898, [1, 512], [1])  # t1899: "cuda:0 i64[1, 512]"
      # t1900 = ltorch.add(t1897, 0, alpha=None)  # t1900: "cuda:0 i64[512, 1]"
        # t1900 = prims.add(t1897, 0)  # t1900: "cuda:0 i64[512, 1]"
      # t1903 = ltorch.ge(t1900, t1899)  # t1903: "cuda:0 b8[512, 512]"
        # t1901 = prims.broadcast_in_dim(t1900, (512, 512), (0, 1))  # t1901: "cuda:0 i64[512, 512]"
        # t1902 = prims.broadcast_in_dim(t1899, (512, 512), (0, 1))  # t1902: "cuda:0 i64[512, 512]"
        # t1903 = prims.ge(t1901, t1902)  # t1903: "cuda:0 b8[512, 512]"
      # t1905 = ltorch.where(t1903, t1895, -float('inf'))  # t1905: "cuda:0 bf16[1, 32, 512, 512]"
        # t1904 = prims.broadcast_in_dim(t1903, (1, 32, 512, 512), (2, 3))  # t1904: "cuda:0 b8[1, 32, 512, 512]"
        # t1905 = prims.where(t1904, t1895, -float('inf'))  # t1905: "cuda:0 bf16[1, 32, 512, 512]"
    # t1916 = ltorch._softmax(t1905, -1, dtype=None)  # t1916: "cuda:0 bf16[1, 32, 512, 512]"
      # t1906 = ltorch.to(t1905, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1906: "cuda:0 f32[1, 32, 512, 512]"
        # t1906 = prims.convert_element_type(t1905, dtypes.float32)  # t1906: "cuda:0 f32[1, 32, 512, 512]"
      # t1908 = ltorch.amax(t1906, -1, True)  # t1908: "cuda:0 f32[1, 32, 512, 1]"
        # t1907 = prims.amax(t1906, (3,))  # t1907: "cuda:0 f32[1, 32, 512]"
        # t1908 = prims.broadcast_in_dim(t1907, [1, 32, 512, 1], [0, 1, 2])  # t1908: "cuda:0 f32[1, 32, 512, 1]"
      # t1910 = ltorch.sub(t1906, t1908, alpha=None)  # t1910: "cuda:0 f32[1, 32, 512, 512]"
        # t1909 = prims.broadcast_in_dim(t1908, (1, 32, 512, 512), (0, 1, 2, 3))  # t1909: "cuda:0 f32[1, 32, 512, 512]"
        # t1910 = prims.sub(t1906, t1909)  # t1910: "cuda:0 f32[1, 32, 512, 512]"
      # t1911 = ltorch.exp(t1910)  # t1911: "cuda:0 f32[1, 32, 512, 512]"
        # t1911 = prims.exp(t1910)  # t1911: "cuda:0 f32[1, 32, 512, 512]"
      # t1913 = ltorch.sum(t1911, -1, True, dtype=None)  # t1913: "cuda:0 f32[1, 32, 512, 1]"
        # t1912 = prims.sum(t1911, (3,))  # t1912: "cuda:0 f32[1, 32, 512]"
        # t1913 = prims.broadcast_in_dim(t1912, [1, 32, 512, 1], [0, 1, 2])  # t1913: "cuda:0 f32[1, 32, 512, 1]"
      # t1915 = ltorch.true_divide(t1911, t1913)  # t1915: "cuda:0 f32[1, 32, 512, 512]"
        # t1914 = prims.broadcast_in_dim(t1913, (1, 32, 512, 512), (0, 1, 2, 3))  # t1914: "cuda:0 f32[1, 32, 512, 512]"
        # t1915 = prims.div(t1911, t1914)  # t1915: "cuda:0 f32[1, 32, 512, 512]"
      # t1916 = ltorch.to(t1915, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1916: "cuda:0 bf16[1, 32, 512, 512]"
        # t1916 = prims.convert_element_type(t1915, dtypes.bfloat16)  # t1916: "cuda:0 bf16[1, 32, 512, 512]"
    # t1917 = ltorch.matmul(t1916, t1853)  # t1917: "cuda:0 bf16[1, 32, 512, 128]"
      # t1917 = prims.matmul(t1916, t1853)  # t1917: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1918 = ltorch.transpose(t1917, 1, 2)  # t1918: "cuda:0 bf16[1, 512, 32, 128]"
    # t1918 = prims.transpose(t1917, (0, 2, 1, 3))  # t1918: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1919 = ltorch.reshape(t1918, 1, 512, 4096)  # t1919: "cuda:0 bf16[1, 512, 4096]"
    # t1919 = prims.reshape(t1918, (1, 512, 4096))  # t1919: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1923 = ltorch.linear(t1919, t_transformer_h_11_attn_proj_weight, None)  # t1923: "cuda:0 bf16[1, 512, 4096]"
    # t1923 = prims.linear(t1919, t_transformer_h_11_attn_proj_weight, None)  # t1923: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1927 = ltorch.add(t1923, t1817, alpha=None)  # t1927: "cuda:0 bf16[1, 512, 4096]"
    # t1924 = prims.convert_element_type(t1923, dtypes.float32)  # t1924: "cuda:0 f32[1, 512, 4096]"
    # t1925 = prims.convert_element_type(t1817, dtypes.float32)  # t1925: "cuda:0 f32[1, 512, 4096]"
    # t1926 = prims.add(t1924, t1925)  # t1926: "cuda:0 f32[1, 512, 4096]"
    # t1927 = prims.convert_element_type(t1926, dtypes.bfloat16)  # t1927: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1928 = prims.convert_element_type(t1927, dtypes.float32)  # t1928: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1929 = ltorch.mul(t1928, t1928)  # t1929: "cuda:0 f32[1, 512, 4096]"
    # t1929 = prims.mul(t1928, t1928)  # t1929: "cuda:0 f32[1, 512, 4096]"
  t1933 = ltorch.mean(t1929, -1, True, dtype=None)  # t1933: "cuda:0 f32[1, 512, 1]"
    # t1931 = prims.sum(t1929, (2,))  # t1931: "cuda:0 f32[1, 512]"
    # t1932 = prims.broadcast_in_dim(t1931, [1, 512, 1], [0, 1])  # t1932: "cuda:0 f32[1, 512, 1]"
    # t1933 = ltorch.true_divide(t1932, 4096)  # t1933: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1933 = prims.div(t1932, 4096.0)  # t1933: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1935 = ltorch.add(t1933, 1e-05, alpha=None)  # t1935: "cuda:0 f32[1, 512, 1]"
    # t1935 = prims.add(t1933, 1e-05)  # t1935: "cuda:0 f32[1, 512, 1]"
  t1936 = ltorch.rsqrt(t1935)  # t1936: "cuda:0 f32[1, 512, 1]"
    # t1936 = prims.rsqrt(t1935)  # t1936: "cuda:0 f32[1, 512, 1]"
  t1938 = ltorch.mul(t1928, t1936)  # t1938: "cuda:0 f32[1, 512, 4096]"
    # t1937 = prims.broadcast_in_dim(t1936, (1, 512, 4096), (0, 1, 2))  # t1937: "cuda:0 f32[1, 512, 4096]"
    # t1938 = prims.mul(t1928, t1937)  # t1938: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1939 = ltorch.to(t1938, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1939: "cuda:0 bf16[1, 512, 4096]"
    # t1939 = prims.convert_element_type(t1938, dtypes.bfloat16)  # t1939: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1949 = ltorch.mul(t1939, t_transformer_h_11_norm_2_weight)  # t1949: "cuda:0 bf16[1, 512, 4096]"
    # t1945 = prims.broadcast_in_dim(t_transformer_h_11_norm_2_weight, (1, 512, 4096), (2,))  # t1945: "cuda:0 bf16[1, 512, 4096]"
    # t1946 = prims.convert_element_type(t1939, dtypes.float32)  # t1946: "cuda:0 f32[1, 512, 4096]"
    # t1947 = prims.convert_element_type(t1945, dtypes.float32)  # t1947: "cuda:0 f32[1, 512, 4096]"
    # t1948 = prims.mul(t1946, t1947)  # t1948: "cuda:0 f32[1, 512, 4096]"
    # t1949 = prims.convert_element_type(t1948, dtypes.bfloat16)  # t1949: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1954 = ltorch.linear(t1949, t_transformer_h_11_mlp_fc_1_weight, None)  # t1954: "cuda:0 bf16[1, 512, 11008]"
    # t1954 = prims.linear(t1949, t_transformer_h_11_mlp_fc_1_weight, None)  # t1954: "cuda:0 bf16[1, 512, 11008]"
  t1958 = ltorch.linear(t1949, t_transformer_h_11_mlp_fc_2_weight, None)  # t1958: "cuda:0 bf16[1, 512, 11008]"
    # t1958 = prims.linear(t1949, t_transformer_h_11_mlp_fc_2_weight, None)  # t1958: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1968 = ltorch.silu(t1954, False)  # t1968: "cuda:0 bf16[1, 512, 11008]"
    # t1959 = prims.convert_element_type(t1954, dtypes.float32)  # t1959: "cuda:0 f32[1, 512, 11008]"
    # t1960 = prims.neg(t1959)  # t1960: "cuda:0 f32[1, 512, 11008]"
    # t1961 = prims.exp(t1960)  # t1961: "cuda:0 f32[1, 512, 11008]"
    # t1962 = prims.add(1.0, t1961)  # t1962: "cuda:0 f32[1, 512, 11008]"
    # t1963 = prims.reciprocal(t1962)  # t1963: "cuda:0 f32[1, 512, 11008]"
    # t1964 = prims.convert_element_type(t1963, dtypes.bfloat16)  # t1964: "cuda:0 bf16[1, 512, 11008]"
    # t1965 = prims.convert_element_type(t1954, dtypes.float32)  # t1965: "cuda:0 f32[1, 512, 11008]"
    # t1966 = prims.convert_element_type(t1964, dtypes.float32)  # t1966: "cuda:0 f32[1, 512, 11008]"
    # t1967 = prims.mul(t1965, t1966)  # t1967: "cuda:0 f32[1, 512, 11008]"
    # t1968 = prims.convert_element_type(t1967, dtypes.bfloat16)  # t1968: "cuda:0 bf16[1, 512, 11008]"
  t1972 = ltorch.mul(t1968, t1958)  # t1972: "cuda:0 bf16[1, 512, 11008]"
    # t1969 = prims.convert_element_type(t1968, dtypes.float32)  # t1969: "cuda:0 f32[1, 512, 11008]"
    # t1970 = prims.convert_element_type(t1958, dtypes.float32)  # t1970: "cuda:0 f32[1, 512, 11008]"
    # t1971 = prims.mul(t1969, t1970)  # t1971: "cuda:0 f32[1, 512, 11008]"
    # t1972 = prims.convert_element_type(t1971, dtypes.bfloat16)  # t1972: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1976 = ltorch.linear(t1972, t_transformer_h_11_mlp_proj_weight, None)  # t1976: "cuda:0 bf16[1, 512, 4096]"
    # t1976 = prims.linear(t1972, t_transformer_h_11_mlp_proj_weight, None)  # t1976: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1980 = ltorch.add(t1976, t1927, alpha=None)  # t1980: "cuda:0 bf16[1, 512, 4096]"
    # t1977 = prims.convert_element_type(t1976, dtypes.float32)  # t1977: "cuda:0 f32[1, 512, 4096]"
    # t1978 = prims.convert_element_type(t1927, dtypes.float32)  # t1978: "cuda:0 f32[1, 512, 4096]"
    # t1979 = prims.add(t1977, t1978)  # t1979: "cuda:0 f32[1, 512, 4096]"
    # t1980 = prims.convert_element_type(t1979, dtypes.bfloat16)  # t1980: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1982 = prims.convert_element_type(t1980, dtypes.float32)  # t1982: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1983 = ltorch.mul(t1982, t1982)  # t1983: "cuda:0 f32[1, 512, 4096]"
    # t1983 = prims.mul(t1982, t1982)  # t1983: "cuda:0 f32[1, 512, 4096]"
  t1987 = ltorch.mean(t1983, -1, True, dtype=None)  # t1987: "cuda:0 f32[1, 512, 1]"
    # t1985 = prims.sum(t1983, (2,))  # t1985: "cuda:0 f32[1, 512]"
    # t1986 = prims.broadcast_in_dim(t1985, [1, 512, 1], [0, 1])  # t1986: "cuda:0 f32[1, 512, 1]"
    # t1987 = ltorch.true_divide(t1986, 4096)  # t1987: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1987 = prims.div(t1986, 4096.0)  # t1987: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1989 = ltorch.add(t1987, 1e-05, alpha=None)  # t1989: "cuda:0 f32[1, 512, 1]"
    # t1989 = prims.add(t1987, 1e-05)  # t1989: "cuda:0 f32[1, 512, 1]"
  t1990 = ltorch.rsqrt(t1989)  # t1990: "cuda:0 f32[1, 512, 1]"
    # t1990 = prims.rsqrt(t1989)  # t1990: "cuda:0 f32[1, 512, 1]"
  t1992 = ltorch.mul(t1982, t1990)  # t1992: "cuda:0 f32[1, 512, 4096]"
    # t1991 = prims.broadcast_in_dim(t1990, (1, 512, 4096), (0, 1, 2))  # t1991: "cuda:0 f32[1, 512, 4096]"
    # t1992 = prims.mul(t1982, t1991)  # t1992: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1993 = ltorch.to(t1992, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1993: "cuda:0 bf16[1, 512, 4096]"
    # t1993 = prims.convert_element_type(t1992, dtypes.bfloat16)  # t1993: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2003 = ltorch.mul(t1993, t_transformer_h_12_norm_1_weight)  # t2003: "cuda:0 bf16[1, 512, 4096]"
    # t1999 = prims.broadcast_in_dim(t_transformer_h_12_norm_1_weight, (1, 512, 4096), (2,))  # t1999: "cuda:0 bf16[1, 512, 4096]"
    # t2000 = prims.convert_element_type(t1993, dtypes.float32)  # t2000: "cuda:0 f32[1, 512, 4096]"
    # t2001 = prims.convert_element_type(t1999, dtypes.float32)  # t2001: "cuda:0 f32[1, 512, 4096]"
    # t2002 = prims.mul(t2000, t2001)  # t2002: "cuda:0 f32[1, 512, 4096]"
    # t2003 = prims.convert_element_type(t2002, dtypes.bfloat16)  # t2003: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2008 = ltorch.linear(t2003, t_transformer_h_12_attn_attn_weight, None)  # t2008: "cuda:0 bf16[1, 512, 12288]"
    # t2008 = prims.linear(t2003, t_transformer_h_12_attn_attn_weight, None)  # t2008: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t2009 = ltorch.view(t2008, 1, 512, 32, 3, 128)  # t2009: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t2009 = ltorch.reshape(t2008, (1, 512, 32, 3, 128))  # t2009: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t2009 = prims.reshape(t2008, (1, 512, 32, 3, 128))  # t2009: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t2010 = ltorch.permute(t2009, 0, 2, 3, 1, 4)  # t2010: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t2010 = prims.transpose(t2009, (0, 2, 3, 1, 4))  # t2010: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t2011, t2012, t2013) = ltorch.split(t2010, (1, 1, 1), 2)
    # t2011 = prims.slice_prim(t2010, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2011: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2012 = prims.slice_prim(t2010, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2012: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2013 = prims.slice_prim(t2010, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2013: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t2014 = ltorch.reshape(t2011, 1, -1, 512, 128)  # t2014: "cuda:0 bf16[1, 32, 512, 128]"
    # t2014 = prims.reshape(t2011, (1, 32, 512, 128))  # t2014: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t2015 = ltorch.reshape(t2012, 1, -1, 512, 128)  # t2015: "cuda:0 bf16[1, 32, 512, 128]"
    # t2015 = prims.reshape(t2012, (1, 32, 512, 128))  # t2015: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t2016 = ltorch.reshape(t2013, 1, -1, 512, 128)  # t2016: "cuda:0 bf16[1, 32, 512, 128]"
    # t2016 = prims.reshape(t2013, (1, 32, 512, 128))  # t2016: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t2017 = ltorch.getitem(t2014, (..., slice(None, 128, None)))  # t2017: "cuda:0 bf16[1, 32, 512, 128]"
    # t2017 = prims.slice_prim(t2014, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2017: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2018 = ltorch.getitem(t2017, (..., slice(None, 64, None)))  # t2018: "cuda:0 bf16[1, 32, 512, 64]"
    # t2018 = prims.slice_prim(t2017, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2018: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2019 = ltorch.getitem(t2017, (..., slice(64, None, None)))  # t2019: "cuda:0 bf16[1, 32, 512, 64]"
    # t2019 = prims.slice_prim(t2017, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2019: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2022 = ltorch.neg(t2019)  # t2022: "cuda:0 bf16[1, 32, 512, 64]"
    # t2020 = prims.convert_element_type(t2019, dtypes.float32)  # t2020: "cuda:0 f32[1, 32, 512, 64]"
    # t2021 = prims.neg(t2020)  # t2021: "cuda:0 f32[1, 32, 512, 64]"
    # t2022 = prims.convert_element_type(t2021, dtypes.bfloat16)  # t2022: "cuda:0 bf16[1, 32, 512, 64]"
  t2023 = ltorch.cat((t2022, t2018), -1)  # t2023: "cuda:0 bf16[1, 32, 512, 128]"
    # t2023 = prims.cat((t2022, t2018), -1)  # t2023: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2026 = ltorch.mul(t2017, cos)  # t2026: "cuda:0 f32[1, 32, 512, 128]"
    # t2024 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2024: "cuda:0 f32[1, 32, 512, 128]"
    # t2025 = prims.convert_element_type(t2017, dtypes.float32)  # t2025: "cuda:0 f32[1, 32, 512, 128]"
    # t2026 = prims.mul(t2025, t2024)  # t2026: "cuda:0 f32[1, 32, 512, 128]"
  t2029 = ltorch.mul(t2023, sin)  # t2029: "cuda:0 f32[1, 32, 512, 128]"
    # t2027 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2027: "cuda:0 f32[1, 32, 512, 128]"
    # t2028 = prims.convert_element_type(t2023, dtypes.float32)  # t2028: "cuda:0 f32[1, 32, 512, 128]"
    # t2029 = prims.mul(t2028, t2027)  # t2029: "cuda:0 f32[1, 32, 512, 128]"
  t2030 = ltorch.add(t2026, t2029, alpha=None)  # t2030: "cuda:0 f32[1, 32, 512, 128]"
    # t2030 = prims.add(t2026, t2029)  # t2030: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2031 = ltorch.to(t2030, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2031: "cuda:0 bf16[1, 32, 512, 128]"
    # t2031 = prims.convert_element_type(t2030, dtypes.bfloat16)  # t2031: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t2032 = ltorch.getitem(t2015, (..., slice(None, 128, None)))  # t2032: "cuda:0 bf16[1, 32, 512, 128]"
    # t2032 = prims.slice_prim(t2015, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2032: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2033 = ltorch.getitem(t2032, (..., slice(None, 64, None)))  # t2033: "cuda:0 bf16[1, 32, 512, 64]"
    # t2033 = prims.slice_prim(t2032, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2033: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2034 = ltorch.getitem(t2032, (..., slice(64, None, None)))  # t2034: "cuda:0 bf16[1, 32, 512, 64]"
    # t2034 = prims.slice_prim(t2032, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2034: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2037 = ltorch.neg(t2034)  # t2037: "cuda:0 bf16[1, 32, 512, 64]"
    # t2035 = prims.convert_element_type(t2034, dtypes.float32)  # t2035: "cuda:0 f32[1, 32, 512, 64]"
    # t2036 = prims.neg(t2035)  # t2036: "cuda:0 f32[1, 32, 512, 64]"
    # t2037 = prims.convert_element_type(t2036, dtypes.bfloat16)  # t2037: "cuda:0 bf16[1, 32, 512, 64]"
  t2038 = ltorch.cat((t2037, t2033), -1)  # t2038: "cuda:0 bf16[1, 32, 512, 128]"
    # t2038 = prims.cat((t2037, t2033), -1)  # t2038: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2041 = ltorch.mul(t2032, cos)  # t2041: "cuda:0 f32[1, 32, 512, 128]"
    # t2039 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2039: "cuda:0 f32[1, 32, 512, 128]"
    # t2040 = prims.convert_element_type(t2032, dtypes.float32)  # t2040: "cuda:0 f32[1, 32, 512, 128]"
    # t2041 = prims.mul(t2040, t2039)  # t2041: "cuda:0 f32[1, 32, 512, 128]"
  t2044 = ltorch.mul(t2038, sin)  # t2044: "cuda:0 f32[1, 32, 512, 128]"
    # t2042 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2042: "cuda:0 f32[1, 32, 512, 128]"
    # t2043 = prims.convert_element_type(t2038, dtypes.float32)  # t2043: "cuda:0 f32[1, 32, 512, 128]"
    # t2044 = prims.mul(t2043, t2042)  # t2044: "cuda:0 f32[1, 32, 512, 128]"
  t2045 = ltorch.add(t2041, t2044, alpha=None)  # t2045: "cuda:0 f32[1, 32, 512, 128]"
    # t2045 = prims.add(t2041, t2044)  # t2045: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2046 = ltorch.to(t2045, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2046: "cuda:0 bf16[1, 32, 512, 128]"
    # t2046 = prims.convert_element_type(t2045, dtypes.bfloat16)  # t2046: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t2047 = ltorch.getitem(t2014, (..., slice(128, None, None)))  # t2047: "cuda:0 bf16[1, 32, 512, 0]"
    # t2047 = prims.slice_prim(t2014, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2047: "cuda:0 bf16[1, 32, 512, 0]"
  t2048 = ltorch.cat((t2031, t2047), -1)  # t2048: "cuda:0 bf16[1, 32, 512, 128]"
    # t2048 = prims.cat((t2031, t2047), -1)  # t2048: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t2049 = ltorch.getitem(t2015, (..., slice(128, None, None)))  # t2049: "cuda:0 bf16[1, 32, 512, 0]"
    # t2049 = prims.slice_prim(t2015, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2049: "cuda:0 bf16[1, 32, 512, 0]"
  t2050 = ltorch.cat((t2046, t2049), -1)  # t2050: "cuda:0 bf16[1, 32, 512, 128]"
    # t2050 = prims.cat((t2046, t2049), -1)  # t2050: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t2080 = ltorch.scaled_dot_product_attention(t2048, t2050, t2016, None, 0.0, True, scale=0.08838834764831843)  # t2080: "cuda:0 bf16[1, 32, 512, 128]"
    # t2053 = ltorch.mul(t2048, 0.29730177875068026)  # t2053: "cuda:0 bf16[1, 32, 512, 128]"
      # t2051 = prims.convert_element_type(t2048, dtypes.float32)  # t2051: "cuda:0 f32[1, 32, 512, 128]"
      # t2052 = prims.mul(t2051, 0.29730177875068026)  # t2052: "cuda:0 f32[1, 32, 512, 128]"
      # t2053 = prims.convert_element_type(t2052, dtypes.bfloat16)  # t2053: "cuda:0 bf16[1, 32, 512, 128]"
    # t2054 = ltorch.transpose(t2050, -2, -1)  # t2054: "cuda:0 bf16[1, 32, 128, 512]"
      # t2054 = prims.transpose(t2050, (0, 1, 3, 2))  # t2054: "cuda:0 bf16[1, 32, 128, 512]"
    # t2057 = ltorch.mul(t2054, 0.29730177875068026)  # t2057: "cuda:0 bf16[1, 32, 128, 512]"
      # t2055 = prims.convert_element_type(t2054, dtypes.float32)  # t2055: "cuda:0 f32[1, 32, 128, 512]"
      # t2056 = prims.mul(t2055, 0.29730177875068026)  # t2056: "cuda:0 f32[1, 32, 128, 512]"
      # t2057 = prims.convert_element_type(t2056, dtypes.bfloat16)  # t2057: "cuda:0 bf16[1, 32, 128, 512]"
    # t2058 = ltorch.matmul(t2053, t2057)  # t2058: "cuda:0 bf16[1, 32, 512, 512]"
      # t2058 = prims.matmul(t2053, t2057)  # t2058: "cuda:0 bf16[1, 32, 512, 512]"
    # t2068 = ltorch.tril(t2058, 0, fill_value=-float('inf'))  # t2068: "cuda:0 bf16[1, 32, 512, 512]"
      # t2059 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2059: "cuda:0 i64[512]"
        # t2059 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2059: "cuda:0 i64[512]"
      # t2060 = ltorch.unsqueeze(t2059, -1)  # t2060: "cuda:0 i64[512, 1]"
        # t2060 = prims.broadcast_in_dim(t2059, [512, 1], [0])  # t2060: "cuda:0 i64[512, 1]"
      # t2061 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2061: "cuda:0 i64[512]"
        # t2061 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2061: "cuda:0 i64[512]"
      # t2062 = ltorch.unsqueeze(t2061, -2)  # t2062: "cuda:0 i64[1, 512]"
        # t2062 = prims.broadcast_in_dim(t2061, [1, 512], [1])  # t2062: "cuda:0 i64[1, 512]"
      # t2063 = ltorch.add(t2060, 0, alpha=None)  # t2063: "cuda:0 i64[512, 1]"
        # t2063 = prims.add(t2060, 0)  # t2063: "cuda:0 i64[512, 1]"
      # t2066 = ltorch.ge(t2063, t2062)  # t2066: "cuda:0 b8[512, 512]"
        # t2064 = prims.broadcast_in_dim(t2063, (512, 512), (0, 1))  # t2064: "cuda:0 i64[512, 512]"
        # t2065 = prims.broadcast_in_dim(t2062, (512, 512), (0, 1))  # t2065: "cuda:0 i64[512, 512]"
        # t2066 = prims.ge(t2064, t2065)  # t2066: "cuda:0 b8[512, 512]"
      # t2068 = ltorch.where(t2066, t2058, -float('inf'))  # t2068: "cuda:0 bf16[1, 32, 512, 512]"
        # t2067 = prims.broadcast_in_dim(t2066, (1, 32, 512, 512), (2, 3))  # t2067: "cuda:0 b8[1, 32, 512, 512]"
        # t2068 = prims.where(t2067, t2058, -float('inf'))  # t2068: "cuda:0 bf16[1, 32, 512, 512]"
    # t2079 = ltorch._softmax(t2068, -1, dtype=None)  # t2079: "cuda:0 bf16[1, 32, 512, 512]"
      # t2069 = ltorch.to(t2068, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t2069: "cuda:0 f32[1, 32, 512, 512]"
        # t2069 = prims.convert_element_type(t2068, dtypes.float32)  # t2069: "cuda:0 f32[1, 32, 512, 512]"
      # t2071 = ltorch.amax(t2069, -1, True)  # t2071: "cuda:0 f32[1, 32, 512, 1]"
        # t2070 = prims.amax(t2069, (3,))  # t2070: "cuda:0 f32[1, 32, 512]"
        # t2071 = prims.broadcast_in_dim(t2070, [1, 32, 512, 1], [0, 1, 2])  # t2071: "cuda:0 f32[1, 32, 512, 1]"
      # t2073 = ltorch.sub(t2069, t2071, alpha=None)  # t2073: "cuda:0 f32[1, 32, 512, 512]"
        # t2072 = prims.broadcast_in_dim(t2071, (1, 32, 512, 512), (0, 1, 2, 3))  # t2072: "cuda:0 f32[1, 32, 512, 512]"
        # t2073 = prims.sub(t2069, t2072)  # t2073: "cuda:0 f32[1, 32, 512, 512]"
      # t2074 = ltorch.exp(t2073)  # t2074: "cuda:0 f32[1, 32, 512, 512]"
        # t2074 = prims.exp(t2073)  # t2074: "cuda:0 f32[1, 32, 512, 512]"
      # t2076 = ltorch.sum(t2074, -1, True, dtype=None)  # t2076: "cuda:0 f32[1, 32, 512, 1]"
        # t2075 = prims.sum(t2074, (3,))  # t2075: "cuda:0 f32[1, 32, 512]"
        # t2076 = prims.broadcast_in_dim(t2075, [1, 32, 512, 1], [0, 1, 2])  # t2076: "cuda:0 f32[1, 32, 512, 1]"
      # t2078 = ltorch.true_divide(t2074, t2076)  # t2078: "cuda:0 f32[1, 32, 512, 512]"
        # t2077 = prims.broadcast_in_dim(t2076, (1, 32, 512, 512), (0, 1, 2, 3))  # t2077: "cuda:0 f32[1, 32, 512, 512]"
        # t2078 = prims.div(t2074, t2077)  # t2078: "cuda:0 f32[1, 32, 512, 512]"
      # t2079 = ltorch.to(t2078, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t2079: "cuda:0 bf16[1, 32, 512, 512]"
        # t2079 = prims.convert_element_type(t2078, dtypes.bfloat16)  # t2079: "cuda:0 bf16[1, 32, 512, 512]"
    # t2080 = ltorch.matmul(t2079, t2016)  # t2080: "cuda:0 bf16[1, 32, 512, 128]"
      # t2080 = prims.matmul(t2079, t2016)  # t2080: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t2081 = ltorch.transpose(t2080, 1, 2)  # t2081: "cuda:0 bf16[1, 512, 32, 128]"
    # t2081 = prims.transpose(t2080, (0, 2, 1, 3))  # t2081: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t2082 = ltorch.reshape(t2081, 1, 512, 4096)  # t2082: "cuda:0 bf16[1, 512, 4096]"
    # t2082 = prims.reshape(t2081, (1, 512, 4096))  # t2082: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2086 = ltorch.linear(t2082, t_transformer_h_12_attn_proj_weight, None)  # t2086: "cuda:0 bf16[1, 512, 4096]"
    # t2086 = prims.linear(t2082, t_transformer_h_12_attn_proj_weight, None)  # t2086: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t2090 = ltorch.add(t2086, t1980, alpha=None)  # t2090: "cuda:0 bf16[1, 512, 4096]"
    # t2087 = prims.convert_element_type(t2086, dtypes.float32)  # t2087: "cuda:0 f32[1, 512, 4096]"
    # t2088 = prims.convert_element_type(t1980, dtypes.float32)  # t2088: "cuda:0 f32[1, 512, 4096]"
    # t2089 = prims.add(t2087, t2088)  # t2089: "cuda:0 f32[1, 512, 4096]"
    # t2090 = prims.convert_element_type(t2089, dtypes.bfloat16)  # t2090: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2091 = prims.convert_element_type(t2090, dtypes.float32)  # t2091: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2092 = ltorch.mul(t2091, t2091)  # t2092: "cuda:0 f32[1, 512, 4096]"
    # t2092 = prims.mul(t2091, t2091)  # t2092: "cuda:0 f32[1, 512, 4096]"
  t2096 = ltorch.mean(t2092, -1, True, dtype=None)  # t2096: "cuda:0 f32[1, 512, 1]"
    # t2094 = prims.sum(t2092, (2,))  # t2094: "cuda:0 f32[1, 512]"
    # t2095 = prims.broadcast_in_dim(t2094, [1, 512, 1], [0, 1])  # t2095: "cuda:0 f32[1, 512, 1]"
    # t2096 = ltorch.true_divide(t2095, 4096)  # t2096: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2096 = prims.div(t2095, 4096.0)  # t2096: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2098 = ltorch.add(t2096, 1e-05, alpha=None)  # t2098: "cuda:0 f32[1, 512, 1]"
    # t2098 = prims.add(t2096, 1e-05)  # t2098: "cuda:0 f32[1, 512, 1]"
  t2099 = ltorch.rsqrt(t2098)  # t2099: "cuda:0 f32[1, 512, 1]"
    # t2099 = prims.rsqrt(t2098)  # t2099: "cuda:0 f32[1, 512, 1]"
  t2101 = ltorch.mul(t2091, t2099)  # t2101: "cuda:0 f32[1, 512, 4096]"
    # t2100 = prims.broadcast_in_dim(t2099, (1, 512, 4096), (0, 1, 2))  # t2100: "cuda:0 f32[1, 512, 4096]"
    # t2101 = prims.mul(t2091, t2100)  # t2101: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2102 = ltorch.to(t2101, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2102: "cuda:0 bf16[1, 512, 4096]"
    # t2102 = prims.convert_element_type(t2101, dtypes.bfloat16)  # t2102: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2112 = ltorch.mul(t2102, t_transformer_h_12_norm_2_weight)  # t2112: "cuda:0 bf16[1, 512, 4096]"
    # t2108 = prims.broadcast_in_dim(t_transformer_h_12_norm_2_weight, (1, 512, 4096), (2,))  # t2108: "cuda:0 bf16[1, 512, 4096]"
    # t2109 = prims.convert_element_type(t2102, dtypes.float32)  # t2109: "cuda:0 f32[1, 512, 4096]"
    # t2110 = prims.convert_element_type(t2108, dtypes.float32)  # t2110: "cuda:0 f32[1, 512, 4096]"
    # t2111 = prims.mul(t2109, t2110)  # t2111: "cuda:0 f32[1, 512, 4096]"
    # t2112 = prims.convert_element_type(t2111, dtypes.bfloat16)  # t2112: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2117 = ltorch.linear(t2112, t_transformer_h_12_mlp_fc_1_weight, None)  # t2117: "cuda:0 bf16[1, 512, 11008]"
    # t2117 = prims.linear(t2112, t_transformer_h_12_mlp_fc_1_weight, None)  # t2117: "cuda:0 bf16[1, 512, 11008]"
  t2121 = ltorch.linear(t2112, t_transformer_h_12_mlp_fc_2_weight, None)  # t2121: "cuda:0 bf16[1, 512, 11008]"
    # t2121 = prims.linear(t2112, t_transformer_h_12_mlp_fc_2_weight, None)  # t2121: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t2131 = ltorch.silu(t2117, False)  # t2131: "cuda:0 bf16[1, 512, 11008]"
    # t2122 = prims.convert_element_type(t2117, dtypes.float32)  # t2122: "cuda:0 f32[1, 512, 11008]"
    # t2123 = prims.neg(t2122)  # t2123: "cuda:0 f32[1, 512, 11008]"
    # t2124 = prims.exp(t2123)  # t2124: "cuda:0 f32[1, 512, 11008]"
    # t2125 = prims.add(1.0, t2124)  # t2125: "cuda:0 f32[1, 512, 11008]"
    # t2126 = prims.reciprocal(t2125)  # t2126: "cuda:0 f32[1, 512, 11008]"
    # t2127 = prims.convert_element_type(t2126, dtypes.bfloat16)  # t2127: "cuda:0 bf16[1, 512, 11008]"
    # t2128 = prims.convert_element_type(t2117, dtypes.float32)  # t2128: "cuda:0 f32[1, 512, 11008]"
    # t2129 = prims.convert_element_type(t2127, dtypes.float32)  # t2129: "cuda:0 f32[1, 512, 11008]"
    # t2130 = prims.mul(t2128, t2129)  # t2130: "cuda:0 f32[1, 512, 11008]"
    # t2131 = prims.convert_element_type(t2130, dtypes.bfloat16)  # t2131: "cuda:0 bf16[1, 512, 11008]"
  t2135 = ltorch.mul(t2131, t2121)  # t2135: "cuda:0 bf16[1, 512, 11008]"
    # t2132 = prims.convert_element_type(t2131, dtypes.float32)  # t2132: "cuda:0 f32[1, 512, 11008]"
    # t2133 = prims.convert_element_type(t2121, dtypes.float32)  # t2133: "cuda:0 f32[1, 512, 11008]"
    # t2134 = prims.mul(t2132, t2133)  # t2134: "cuda:0 f32[1, 512, 11008]"
    # t2135 = prims.convert_element_type(t2134, dtypes.bfloat16)  # t2135: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2139 = ltorch.linear(t2135, t_transformer_h_12_mlp_proj_weight, None)  # t2139: "cuda:0 bf16[1, 512, 4096]"
    # t2139 = prims.linear(t2135, t_transformer_h_12_mlp_proj_weight, None)  # t2139: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t2143 = ltorch.add(t2139, t2090, alpha=None)  # t2143: "cuda:0 bf16[1, 512, 4096]"
    # t2140 = prims.convert_element_type(t2139, dtypes.float32)  # t2140: "cuda:0 f32[1, 512, 4096]"
    # t2141 = prims.convert_element_type(t2090, dtypes.float32)  # t2141: "cuda:0 f32[1, 512, 4096]"
    # t2142 = prims.add(t2140, t2141)  # t2142: "cuda:0 f32[1, 512, 4096]"
    # t2143 = prims.convert_element_type(t2142, dtypes.bfloat16)  # t2143: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2145 = prims.convert_element_type(t2143, dtypes.float32)  # t2145: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2146 = ltorch.mul(t2145, t2145)  # t2146: "cuda:0 f32[1, 512, 4096]"
    # t2146 = prims.mul(t2145, t2145)  # t2146: "cuda:0 f32[1, 512, 4096]"
  t2150 = ltorch.mean(t2146, -1, True, dtype=None)  # t2150: "cuda:0 f32[1, 512, 1]"
    # t2148 = prims.sum(t2146, (2,))  # t2148: "cuda:0 f32[1, 512]"
    # t2149 = prims.broadcast_in_dim(t2148, [1, 512, 1], [0, 1])  # t2149: "cuda:0 f32[1, 512, 1]"
    # t2150 = ltorch.true_divide(t2149, 4096)  # t2150: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2150 = prims.div(t2149, 4096.0)  # t2150: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2152 = ltorch.add(t2150, 1e-05, alpha=None)  # t2152: "cuda:0 f32[1, 512, 1]"
    # t2152 = prims.add(t2150, 1e-05)  # t2152: "cuda:0 f32[1, 512, 1]"
  t2153 = ltorch.rsqrt(t2152)  # t2153: "cuda:0 f32[1, 512, 1]"
    # t2153 = prims.rsqrt(t2152)  # t2153: "cuda:0 f32[1, 512, 1]"
  t2155 = ltorch.mul(t2145, t2153)  # t2155: "cuda:0 f32[1, 512, 4096]"
    # t2154 = prims.broadcast_in_dim(t2153, (1, 512, 4096), (0, 1, 2))  # t2154: "cuda:0 f32[1, 512, 4096]"
    # t2155 = prims.mul(t2145, t2154)  # t2155: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2156 = ltorch.to(t2155, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2156: "cuda:0 bf16[1, 512, 4096]"
    # t2156 = prims.convert_element_type(t2155, dtypes.bfloat16)  # t2156: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2166 = ltorch.mul(t2156, t_transformer_h_13_norm_1_weight)  # t2166: "cuda:0 bf16[1, 512, 4096]"
    # t2162 = prims.broadcast_in_dim(t_transformer_h_13_norm_1_weight, (1, 512, 4096), (2,))  # t2162: "cuda:0 bf16[1, 512, 4096]"
    # t2163 = prims.convert_element_type(t2156, dtypes.float32)  # t2163: "cuda:0 f32[1, 512, 4096]"
    # t2164 = prims.convert_element_type(t2162, dtypes.float32)  # t2164: "cuda:0 f32[1, 512, 4096]"
    # t2165 = prims.mul(t2163, t2164)  # t2165: "cuda:0 f32[1, 512, 4096]"
    # t2166 = prims.convert_element_type(t2165, dtypes.bfloat16)  # t2166: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2171 = ltorch.linear(t2166, t_transformer_h_13_attn_attn_weight, None)  # t2171: "cuda:0 bf16[1, 512, 12288]"
    # t2171 = prims.linear(t2166, t_transformer_h_13_attn_attn_weight, None)  # t2171: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t2172 = ltorch.view(t2171, 1, 512, 32, 3, 128)  # t2172: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t2172 = ltorch.reshape(t2171, (1, 512, 32, 3, 128))  # t2172: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t2172 = prims.reshape(t2171, (1, 512, 32, 3, 128))  # t2172: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t2173 = ltorch.permute(t2172, 0, 2, 3, 1, 4)  # t2173: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t2173 = prims.transpose(t2172, (0, 2, 3, 1, 4))  # t2173: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t2174, t2175, t2176) = ltorch.split(t2173, (1, 1, 1), 2)
    # t2174 = prims.slice_prim(t2173, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2174: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2175 = prims.slice_prim(t2173, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2175: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2176 = prims.slice_prim(t2173, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2176: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t2177 = ltorch.reshape(t2174, 1, -1, 512, 128)  # t2177: "cuda:0 bf16[1, 32, 512, 128]"
    # t2177 = prims.reshape(t2174, (1, 32, 512, 128))  # t2177: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t2178 = ltorch.reshape(t2175, 1, -1, 512, 128)  # t2178: "cuda:0 bf16[1, 32, 512, 128]"
    # t2178 = prims.reshape(t2175, (1, 32, 512, 128))  # t2178: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t2179 = ltorch.reshape(t2176, 1, -1, 512, 128)  # t2179: "cuda:0 bf16[1, 32, 512, 128]"
    # t2179 = prims.reshape(t2176, (1, 32, 512, 128))  # t2179: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t2180 = ltorch.getitem(t2177, (..., slice(None, 128, None)))  # t2180: "cuda:0 bf16[1, 32, 512, 128]"
    # t2180 = prims.slice_prim(t2177, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2180: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2181 = ltorch.getitem(t2180, (..., slice(None, 64, None)))  # t2181: "cuda:0 bf16[1, 32, 512, 64]"
    # t2181 = prims.slice_prim(t2180, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2181: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2182 = ltorch.getitem(t2180, (..., slice(64, None, None)))  # t2182: "cuda:0 bf16[1, 32, 512, 64]"
    # t2182 = prims.slice_prim(t2180, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2182: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2185 = ltorch.neg(t2182)  # t2185: "cuda:0 bf16[1, 32, 512, 64]"
    # t2183 = prims.convert_element_type(t2182, dtypes.float32)  # t2183: "cuda:0 f32[1, 32, 512, 64]"
    # t2184 = prims.neg(t2183)  # t2184: "cuda:0 f32[1, 32, 512, 64]"
    # t2185 = prims.convert_element_type(t2184, dtypes.bfloat16)  # t2185: "cuda:0 bf16[1, 32, 512, 64]"
  t2186 = ltorch.cat((t2185, t2181), -1)  # t2186: "cuda:0 bf16[1, 32, 512, 128]"
    # t2186 = prims.cat((t2185, t2181), -1)  # t2186: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2189 = ltorch.mul(t2180, cos)  # t2189: "cuda:0 f32[1, 32, 512, 128]"
    # t2187 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2187: "cuda:0 f32[1, 32, 512, 128]"
    # t2188 = prims.convert_element_type(t2180, dtypes.float32)  # t2188: "cuda:0 f32[1, 32, 512, 128]"
    # t2189 = prims.mul(t2188, t2187)  # t2189: "cuda:0 f32[1, 32, 512, 128]"
  t2192 = ltorch.mul(t2186, sin)  # t2192: "cuda:0 f32[1, 32, 512, 128]"
    # t2190 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2190: "cuda:0 f32[1, 32, 512, 128]"
    # t2191 = prims.convert_element_type(t2186, dtypes.float32)  # t2191: "cuda:0 f32[1, 32, 512, 128]"
    # t2192 = prims.mul(t2191, t2190)  # t2192: "cuda:0 f32[1, 32, 512, 128]"
  t2193 = ltorch.add(t2189, t2192, alpha=None)  # t2193: "cuda:0 f32[1, 32, 512, 128]"
    # t2193 = prims.add(t2189, t2192)  # t2193: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2194 = ltorch.to(t2193, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2194: "cuda:0 bf16[1, 32, 512, 128]"
    # t2194 = prims.convert_element_type(t2193, dtypes.bfloat16)  # t2194: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t2195 = ltorch.getitem(t2178, (..., slice(None, 128, None)))  # t2195: "cuda:0 bf16[1, 32, 512, 128]"
    # t2195 = prims.slice_prim(t2178, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2195: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2196 = ltorch.getitem(t2195, (..., slice(None, 64, None)))  # t2196: "cuda:0 bf16[1, 32, 512, 64]"
    # t2196 = prims.slice_prim(t2195, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2196: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2197 = ltorch.getitem(t2195, (..., slice(64, None, None)))  # t2197: "cuda:0 bf16[1, 32, 512, 64]"
    # t2197 = prims.slice_prim(t2195, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2197: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2200 = ltorch.neg(t2197)  # t2200: "cuda:0 bf16[1, 32, 512, 64]"
    # t2198 = prims.convert_element_type(t2197, dtypes.float32)  # t2198: "cuda:0 f32[1, 32, 512, 64]"
    # t2199 = prims.neg(t2198)  # t2199: "cuda:0 f32[1, 32, 512, 64]"
    # t2200 = prims.convert_element_type(t2199, dtypes.bfloat16)  # t2200: "cuda:0 bf16[1, 32, 512, 64]"
  t2201 = ltorch.cat((t2200, t2196), -1)  # t2201: "cuda:0 bf16[1, 32, 512, 128]"
    # t2201 = prims.cat((t2200, t2196), -1)  # t2201: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2204 = ltorch.mul(t2195, cos)  # t2204: "cuda:0 f32[1, 32, 512, 128]"
    # t2202 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2202: "cuda:0 f32[1, 32, 512, 128]"
    # t2203 = prims.convert_element_type(t2195, dtypes.float32)  # t2203: "cuda:0 f32[1, 32, 512, 128]"
    # t2204 = prims.mul(t2203, t2202)  # t2204: "cuda:0 f32[1, 32, 512, 128]"
  t2207 = ltorch.mul(t2201, sin)  # t2207: "cuda:0 f32[1, 32, 512, 128]"
    # t2205 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2205: "cuda:0 f32[1, 32, 512, 128]"
    # t2206 = prims.convert_element_type(t2201, dtypes.float32)  # t2206: "cuda:0 f32[1, 32, 512, 128]"
    # t2207 = prims.mul(t2206, t2205)  # t2207: "cuda:0 f32[1, 32, 512, 128]"
  t2208 = ltorch.add(t2204, t2207, alpha=None)  # t2208: "cuda:0 f32[1, 32, 512, 128]"
    # t2208 = prims.add(t2204, t2207)  # t2208: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2209 = ltorch.to(t2208, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2209: "cuda:0 bf16[1, 32, 512, 128]"
    # t2209 = prims.convert_element_type(t2208, dtypes.bfloat16)  # t2209: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t2210 = ltorch.getitem(t2177, (..., slice(128, None, None)))  # t2210: "cuda:0 bf16[1, 32, 512, 0]"
    # t2210 = prims.slice_prim(t2177, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2210: "cuda:0 bf16[1, 32, 512, 0]"
  t2211 = ltorch.cat((t2194, t2210), -1)  # t2211: "cuda:0 bf16[1, 32, 512, 128]"
    # t2211 = prims.cat((t2194, t2210), -1)  # t2211: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t2212 = ltorch.getitem(t2178, (..., slice(128, None, None)))  # t2212: "cuda:0 bf16[1, 32, 512, 0]"
    # t2212 = prims.slice_prim(t2178, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2212: "cuda:0 bf16[1, 32, 512, 0]"
  t2213 = ltorch.cat((t2209, t2212), -1)  # t2213: "cuda:0 bf16[1, 32, 512, 128]"
    # t2213 = prims.cat((t2209, t2212), -1)  # t2213: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t2243 = ltorch.scaled_dot_product_attention(t2211, t2213, t2179, None, 0.0, True, scale=0.08838834764831843)  # t2243: "cuda:0 bf16[1, 32, 512, 128]"
    # t2216 = ltorch.mul(t2211, 0.29730177875068026)  # t2216: "cuda:0 bf16[1, 32, 512, 128]"
      # t2214 = prims.convert_element_type(t2211, dtypes.float32)  # t2214: "cuda:0 f32[1, 32, 512, 128]"
      # t2215 = prims.mul(t2214, 0.29730177875068026)  # t2215: "cuda:0 f32[1, 32, 512, 128]"
      # t2216 = prims.convert_element_type(t2215, dtypes.bfloat16)  # t2216: "cuda:0 bf16[1, 32, 512, 128]"
    # t2217 = ltorch.transpose(t2213, -2, -1)  # t2217: "cuda:0 bf16[1, 32, 128, 512]"
      # t2217 = prims.transpose(t2213, (0, 1, 3, 2))  # t2217: "cuda:0 bf16[1, 32, 128, 512]"
    # t2220 = ltorch.mul(t2217, 0.29730177875068026)  # t2220: "cuda:0 bf16[1, 32, 128, 512]"
      # t2218 = prims.convert_element_type(t2217, dtypes.float32)  # t2218: "cuda:0 f32[1, 32, 128, 512]"
      # t2219 = prims.mul(t2218, 0.29730177875068026)  # t2219: "cuda:0 f32[1, 32, 128, 512]"
      # t2220 = prims.convert_element_type(t2219, dtypes.bfloat16)  # t2220: "cuda:0 bf16[1, 32, 128, 512]"
    # t2221 = ltorch.matmul(t2216, t2220)  # t2221: "cuda:0 bf16[1, 32, 512, 512]"
      # t2221 = prims.matmul(t2216, t2220)  # t2221: "cuda:0 bf16[1, 32, 512, 512]"
    # t2231 = ltorch.tril(t2221, 0, fill_value=-float('inf'))  # t2231: "cuda:0 bf16[1, 32, 512, 512]"
      # t2222 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2222: "cuda:0 i64[512]"
        # t2222 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2222: "cuda:0 i64[512]"
      # t2223 = ltorch.unsqueeze(t2222, -1)  # t2223: "cuda:0 i64[512, 1]"
        # t2223 = prims.broadcast_in_dim(t2222, [512, 1], [0])  # t2223: "cuda:0 i64[512, 1]"
      # t2224 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2224: "cuda:0 i64[512]"
        # t2224 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2224: "cuda:0 i64[512]"
      # t2225 = ltorch.unsqueeze(t2224, -2)  # t2225: "cuda:0 i64[1, 512]"
        # t2225 = prims.broadcast_in_dim(t2224, [1, 512], [1])  # t2225: "cuda:0 i64[1, 512]"
      # t2226 = ltorch.add(t2223, 0, alpha=None)  # t2226: "cuda:0 i64[512, 1]"
        # t2226 = prims.add(t2223, 0)  # t2226: "cuda:0 i64[512, 1]"
      # t2229 = ltorch.ge(t2226, t2225)  # t2229: "cuda:0 b8[512, 512]"
        # t2227 = prims.broadcast_in_dim(t2226, (512, 512), (0, 1))  # t2227: "cuda:0 i64[512, 512]"
        # t2228 = prims.broadcast_in_dim(t2225, (512, 512), (0, 1))  # t2228: "cuda:0 i64[512, 512]"
        # t2229 = prims.ge(t2227, t2228)  # t2229: "cuda:0 b8[512, 512]"
      # t2231 = ltorch.where(t2229, t2221, -float('inf'))  # t2231: "cuda:0 bf16[1, 32, 512, 512]"
        # t2230 = prims.broadcast_in_dim(t2229, (1, 32, 512, 512), (2, 3))  # t2230: "cuda:0 b8[1, 32, 512, 512]"
        # t2231 = prims.where(t2230, t2221, -float('inf'))  # t2231: "cuda:0 bf16[1, 32, 512, 512]"
    # t2242 = ltorch._softmax(t2231, -1, dtype=None)  # t2242: "cuda:0 bf16[1, 32, 512, 512]"
      # t2232 = ltorch.to(t2231, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t2232: "cuda:0 f32[1, 32, 512, 512]"
        # t2232 = prims.convert_element_type(t2231, dtypes.float32)  # t2232: "cuda:0 f32[1, 32, 512, 512]"
      # t2234 = ltorch.amax(t2232, -1, True)  # t2234: "cuda:0 f32[1, 32, 512, 1]"
        # t2233 = prims.amax(t2232, (3,))  # t2233: "cuda:0 f32[1, 32, 512]"
        # t2234 = prims.broadcast_in_dim(t2233, [1, 32, 512, 1], [0, 1, 2])  # t2234: "cuda:0 f32[1, 32, 512, 1]"
      # t2236 = ltorch.sub(t2232, t2234, alpha=None)  # t2236: "cuda:0 f32[1, 32, 512, 512]"
        # t2235 = prims.broadcast_in_dim(t2234, (1, 32, 512, 512), (0, 1, 2, 3))  # t2235: "cuda:0 f32[1, 32, 512, 512]"
        # t2236 = prims.sub(t2232, t2235)  # t2236: "cuda:0 f32[1, 32, 512, 512]"
      # t2237 = ltorch.exp(t2236)  # t2237: "cuda:0 f32[1, 32, 512, 512]"
        # t2237 = prims.exp(t2236)  # t2237: "cuda:0 f32[1, 32, 512, 512]"
      # t2239 = ltorch.sum(t2237, -1, True, dtype=None)  # t2239: "cuda:0 f32[1, 32, 512, 1]"
        # t2238 = prims.sum(t2237, (3,))  # t2238: "cuda:0 f32[1, 32, 512]"
        # t2239 = prims.broadcast_in_dim(t2238, [1, 32, 512, 1], [0, 1, 2])  # t2239: "cuda:0 f32[1, 32, 512, 1]"
      # t2241 = ltorch.true_divide(t2237, t2239)  # t2241: "cuda:0 f32[1, 32, 512, 512]"
        # t2240 = prims.broadcast_in_dim(t2239, (1, 32, 512, 512), (0, 1, 2, 3))  # t2240: "cuda:0 f32[1, 32, 512, 512]"
        # t2241 = prims.div(t2237, t2240)  # t2241: "cuda:0 f32[1, 32, 512, 512]"
      # t2242 = ltorch.to(t2241, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t2242: "cuda:0 bf16[1, 32, 512, 512]"
        # t2242 = prims.convert_element_type(t2241, dtypes.bfloat16)  # t2242: "cuda:0 bf16[1, 32, 512, 512]"
    # t2243 = ltorch.matmul(t2242, t2179)  # t2243: "cuda:0 bf16[1, 32, 512, 128]"
      # t2243 = prims.matmul(t2242, t2179)  # t2243: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t2244 = ltorch.transpose(t2243, 1, 2)  # t2244: "cuda:0 bf16[1, 512, 32, 128]"
    # t2244 = prims.transpose(t2243, (0, 2, 1, 3))  # t2244: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t2245 = ltorch.reshape(t2244, 1, 512, 4096)  # t2245: "cuda:0 bf16[1, 512, 4096]"
    # t2245 = prims.reshape(t2244, (1, 512, 4096))  # t2245: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2249 = ltorch.linear(t2245, t_transformer_h_13_attn_proj_weight, None)  # t2249: "cuda:0 bf16[1, 512, 4096]"
    # t2249 = prims.linear(t2245, t_transformer_h_13_attn_proj_weight, None)  # t2249: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t2253 = ltorch.add(t2249, t2143, alpha=None)  # t2253: "cuda:0 bf16[1, 512, 4096]"
    # t2250 = prims.convert_element_type(t2249, dtypes.float32)  # t2250: "cuda:0 f32[1, 512, 4096]"
    # t2251 = prims.convert_element_type(t2143, dtypes.float32)  # t2251: "cuda:0 f32[1, 512, 4096]"
    # t2252 = prims.add(t2250, t2251)  # t2252: "cuda:0 f32[1, 512, 4096]"
    # t2253 = prims.convert_element_type(t2252, dtypes.bfloat16)  # t2253: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2254 = prims.convert_element_type(t2253, dtypes.float32)  # t2254: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2255 = ltorch.mul(t2254, t2254)  # t2255: "cuda:0 f32[1, 512, 4096]"
    # t2255 = prims.mul(t2254, t2254)  # t2255: "cuda:0 f32[1, 512, 4096]"
  t2259 = ltorch.mean(t2255, -1, True, dtype=None)  # t2259: "cuda:0 f32[1, 512, 1]"
    # t2257 = prims.sum(t2255, (2,))  # t2257: "cuda:0 f32[1, 512]"
    # t2258 = prims.broadcast_in_dim(t2257, [1, 512, 1], [0, 1])  # t2258: "cuda:0 f32[1, 512, 1]"
    # t2259 = ltorch.true_divide(t2258, 4096)  # t2259: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2259 = prims.div(t2258, 4096.0)  # t2259: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2261 = ltorch.add(t2259, 1e-05, alpha=None)  # t2261: "cuda:0 f32[1, 512, 1]"
    # t2261 = prims.add(t2259, 1e-05)  # t2261: "cuda:0 f32[1, 512, 1]"
  t2262 = ltorch.rsqrt(t2261)  # t2262: "cuda:0 f32[1, 512, 1]"
    # t2262 = prims.rsqrt(t2261)  # t2262: "cuda:0 f32[1, 512, 1]"
  t2264 = ltorch.mul(t2254, t2262)  # t2264: "cuda:0 f32[1, 512, 4096]"
    # t2263 = prims.broadcast_in_dim(t2262, (1, 512, 4096), (0, 1, 2))  # t2263: "cuda:0 f32[1, 512, 4096]"
    # t2264 = prims.mul(t2254, t2263)  # t2264: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2265 = ltorch.to(t2264, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2265: "cuda:0 bf16[1, 512, 4096]"
    # t2265 = prims.convert_element_type(t2264, dtypes.bfloat16)  # t2265: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2275 = ltorch.mul(t2265, t_transformer_h_13_norm_2_weight)  # t2275: "cuda:0 bf16[1, 512, 4096]"
    # t2271 = prims.broadcast_in_dim(t_transformer_h_13_norm_2_weight, (1, 512, 4096), (2,))  # t2271: "cuda:0 bf16[1, 512, 4096]"
    # t2272 = prims.convert_element_type(t2265, dtypes.float32)  # t2272: "cuda:0 f32[1, 512, 4096]"
    # t2273 = prims.convert_element_type(t2271, dtypes.float32)  # t2273: "cuda:0 f32[1, 512, 4096]"
    # t2274 = prims.mul(t2272, t2273)  # t2274: "cuda:0 f32[1, 512, 4096]"
    # t2275 = prims.convert_element_type(t2274, dtypes.bfloat16)  # t2275: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2280 = ltorch.linear(t2275, t_transformer_h_13_mlp_fc_1_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
    # t2280 = prims.linear(t2275, t_transformer_h_13_mlp_fc_1_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
  t2284 = ltorch.linear(t2275, t_transformer_h_13_mlp_fc_2_weight, None)  # t2284: "cuda:0 bf16[1, 512, 11008]"
    # t2284 = prims.linear(t2275, t_transformer_h_13_mlp_fc_2_weight, None)  # t2284: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t2294 = ltorch.silu(t2280, False)  # t2294: "cuda:0 bf16[1, 512, 11008]"
    # t2285 = prims.convert_element_type(t2280, dtypes.float32)  # t2285: "cuda:0 f32[1, 512, 11008]"
    # t2286 = prims.neg(t2285)  # t2286: "cuda:0 f32[1, 512, 11008]"
    # t2287 = prims.exp(t2286)  # t2287: "cuda:0 f32[1, 512, 11008]"
    # t2288 = prims.add(1.0, t2287)  # t2288: "cuda:0 f32[1, 512, 11008]"
    # t2289 = prims.reciprocal(t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
    # t2290 = prims.convert_element_type(t2289, dtypes.bfloat16)  # t2290: "cuda:0 bf16[1, 512, 11008]"
    # t2291 = prims.convert_element_type(t2280, dtypes.float32)  # t2291: "cuda:0 f32[1, 512, 11008]"
    # t2292 = prims.convert_element_type(t2290, dtypes.float32)  # t2292: "cuda:0 f32[1, 512, 11008]"
    # t2293 = prims.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
    # t2294 = prims.convert_element_type(t2293, dtypes.bfloat16)  # t2294: "cuda:0 bf16[1, 512, 11008]"
  t2298 = ltorch.mul(t2294, t2284)  # t2298: "cuda:0 bf16[1, 512, 11008]"
    # t2295 = prims.convert_element_type(t2294, dtypes.float32)  # t2295: "cuda:0 f32[1, 512, 11008]"
    # t2296 = prims.convert_element_type(t2284, dtypes.float32)  # t2296: "cuda:0 f32[1, 512, 11008]"
    # t2297 = prims.mul(t2295, t2296)  # t2297: "cuda:0 f32[1, 512, 11008]"
    # t2298 = prims.convert_element_type(t2297, dtypes.bfloat16)  # t2298: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2302 = ltorch.linear(t2298, t_transformer_h_13_mlp_proj_weight, None)  # t2302: "cuda:0 bf16[1, 512, 4096]"
    # t2302 = prims.linear(t2298, t_transformer_h_13_mlp_proj_weight, None)  # t2302: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t2306 = ltorch.add(t2302, t2253, alpha=None)  # t2306: "cuda:0 bf16[1, 512, 4096]"
    # t2303 = prims.convert_element_type(t2302, dtypes.float32)  # t2303: "cuda:0 f32[1, 512, 4096]"
    # t2304 = prims.convert_element_type(t2253, dtypes.float32)  # t2304: "cuda:0 f32[1, 512, 4096]"
    # t2305 = prims.add(t2303, t2304)  # t2305: "cuda:0 f32[1, 512, 4096]"
    # t2306 = prims.convert_element_type(t2305, dtypes.bfloat16)  # t2306: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2308 = prims.convert_element_type(t2306, dtypes.float32)  # t2308: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2309 = ltorch.mul(t2308, t2308)  # t2309: "cuda:0 f32[1, 512, 4096]"
    # t2309 = prims.mul(t2308, t2308)  # t2309: "cuda:0 f32[1, 512, 4096]"
  t2313 = ltorch.mean(t2309, -1, True, dtype=None)  # t2313: "cuda:0 f32[1, 512, 1]"
    # t2311 = prims.sum(t2309, (2,))  # t2311: "cuda:0 f32[1, 512]"
    # t2312 = prims.broadcast_in_dim(t2311, [1, 512, 1], [0, 1])  # t2312: "cuda:0 f32[1, 512, 1]"
    # t2313 = ltorch.true_divide(t2312, 4096)  # t2313: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2313 = prims.div(t2312, 4096.0)  # t2313: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2315 = ltorch.add(t2313, 1e-05, alpha=None)  # t2315: "cuda:0 f32[1, 512, 1]"
    # t2315 = prims.add(t2313, 1e-05)  # t2315: "cuda:0 f32[1, 512, 1]"
  t2316 = ltorch.rsqrt(t2315)  # t2316: "cuda:0 f32[1, 512, 1]"
    # t2316 = prims.rsqrt(t2315)  # t2316: "cuda:0 f32[1, 512, 1]"
  t2318 = ltorch.mul(t2308, t2316)  # t2318: "cuda:0 f32[1, 512, 4096]"
    # t2317 = prims.broadcast_in_dim(t2316, (1, 512, 4096), (0, 1, 2))  # t2317: "cuda:0 f32[1, 512, 4096]"
    # t2318 = prims.mul(t2308, t2317)  # t2318: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2319 = ltorch.to(t2318, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2319: "cuda:0 bf16[1, 512, 4096]"
    # t2319 = prims.convert_element_type(t2318, dtypes.bfloat16)  # t2319: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2329 = ltorch.mul(t2319, t_transformer_h_14_norm_1_weight)  # t2329: "cuda:0 bf16[1, 512, 4096]"
    # t2325 = prims.broadcast_in_dim(t_transformer_h_14_norm_1_weight, (1, 512, 4096), (2,))  # t2325: "cuda:0 bf16[1, 512, 4096]"
    # t2326 = prims.convert_element_type(t2319, dtypes.float32)  # t2326: "cuda:0 f32[1, 512, 4096]"
    # t2327 = prims.convert_element_type(t2325, dtypes.float32)  # t2327: "cuda:0 f32[1, 512, 4096]"
    # t2328 = prims.mul(t2326, t2327)  # t2328: "cuda:0 f32[1, 512, 4096]"
    # t2329 = prims.convert_element_type(t2328, dtypes.bfloat16)  # t2329: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2334 = ltorch.linear(t2329, t_transformer_h_14_attn_attn_weight, None)  # t2334: "cuda:0 bf16[1, 512, 12288]"
    # t2334 = prims.linear(t2329, t_transformer_h_14_attn_attn_weight, None)  # t2334: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t2335 = ltorch.view(t2334, 1, 512, 32, 3, 128)  # t2335: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t2335 = ltorch.reshape(t2334, (1, 512, 32, 3, 128))  # t2335: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t2335 = prims.reshape(t2334, (1, 512, 32, 3, 128))  # t2335: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t2336 = ltorch.permute(t2335, 0, 2, 3, 1, 4)  # t2336: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t2336 = prims.transpose(t2335, (0, 2, 3, 1, 4))  # t2336: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t2337, t2338, t2339) = ltorch.split(t2336, (1, 1, 1), 2)
    # t2337 = prims.slice_prim(t2336, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2337: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2338 = prims.slice_prim(t2336, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2338: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2339 = prims.slice_prim(t2336, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2339: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t2340 = ltorch.reshape(t2337, 1, -1, 512, 128)  # t2340: "cuda:0 bf16[1, 32, 512, 128]"
    # t2340 = prims.reshape(t2337, (1, 32, 512, 128))  # t2340: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t2341 = ltorch.reshape(t2338, 1, -1, 512, 128)  # t2341: "cuda:0 bf16[1, 32, 512, 128]"
    # t2341 = prims.reshape(t2338, (1, 32, 512, 128))  # t2341: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t2342 = ltorch.reshape(t2339, 1, -1, 512, 128)  # t2342: "cuda:0 bf16[1, 32, 512, 128]"
    # t2342 = prims.reshape(t2339, (1, 32, 512, 128))  # t2342: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t2343 = ltorch.getitem(t2340, (..., slice(None, 128, None)))  # t2343: "cuda:0 bf16[1, 32, 512, 128]"
    # t2343 = prims.slice_prim(t2340, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2343: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2344 = ltorch.getitem(t2343, (..., slice(None, 64, None)))  # t2344: "cuda:0 bf16[1, 32, 512, 64]"
    # t2344 = prims.slice_prim(t2343, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2344: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2345 = ltorch.getitem(t2343, (..., slice(64, None, None)))  # t2345: "cuda:0 bf16[1, 32, 512, 64]"
    # t2345 = prims.slice_prim(t2343, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2345: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2348 = ltorch.neg(t2345)  # t2348: "cuda:0 bf16[1, 32, 512, 64]"
    # t2346 = prims.convert_element_type(t2345, dtypes.float32)  # t2346: "cuda:0 f32[1, 32, 512, 64]"
    # t2347 = prims.neg(t2346)  # t2347: "cuda:0 f32[1, 32, 512, 64]"
    # t2348 = prims.convert_element_type(t2347, dtypes.bfloat16)  # t2348: "cuda:0 bf16[1, 32, 512, 64]"
  t2349 = ltorch.cat((t2348, t2344), -1)  # t2349: "cuda:0 bf16[1, 32, 512, 128]"
    # t2349 = prims.cat((t2348, t2344), -1)  # t2349: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2352 = ltorch.mul(t2343, cos)  # t2352: "cuda:0 f32[1, 32, 512, 128]"
    # t2350 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2350: "cuda:0 f32[1, 32, 512, 128]"
    # t2351 = prims.convert_element_type(t2343, dtypes.float32)  # t2351: "cuda:0 f32[1, 32, 512, 128]"
    # t2352 = prims.mul(t2351, t2350)  # t2352: "cuda:0 f32[1, 32, 512, 128]"
  t2355 = ltorch.mul(t2349, sin)  # t2355: "cuda:0 f32[1, 32, 512, 128]"
    # t2353 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2353: "cuda:0 f32[1, 32, 512, 128]"
    # t2354 = prims.convert_element_type(t2349, dtypes.float32)  # t2354: "cuda:0 f32[1, 32, 512, 128]"
    # t2355 = prims.mul(t2354, t2353)  # t2355: "cuda:0 f32[1, 32, 512, 128]"
  t2356 = ltorch.add(t2352, t2355, alpha=None)  # t2356: "cuda:0 f32[1, 32, 512, 128]"
    # t2356 = prims.add(t2352, t2355)  # t2356: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2357 = ltorch.to(t2356, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2357: "cuda:0 bf16[1, 32, 512, 128]"
    # t2357 = prims.convert_element_type(t2356, dtypes.bfloat16)  # t2357: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t2358 = ltorch.getitem(t2341, (..., slice(None, 128, None)))  # t2358: "cuda:0 bf16[1, 32, 512, 128]"
    # t2358 = prims.slice_prim(t2341, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2358: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2359 = ltorch.getitem(t2358, (..., slice(None, 64, None)))  # t2359: "cuda:0 bf16[1, 32, 512, 64]"
    # t2359 = prims.slice_prim(t2358, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2359: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2360 = ltorch.getitem(t2358, (..., slice(64, None, None)))  # t2360: "cuda:0 bf16[1, 32, 512, 64]"
    # t2360 = prims.slice_prim(t2358, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2360: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2363 = ltorch.neg(t2360)  # t2363: "cuda:0 bf16[1, 32, 512, 64]"
    # t2361 = prims.convert_element_type(t2360, dtypes.float32)  # t2361: "cuda:0 f32[1, 32, 512, 64]"
    # t2362 = prims.neg(t2361)  # t2362: "cuda:0 f32[1, 32, 512, 64]"
    # t2363 = prims.convert_element_type(t2362, dtypes.bfloat16)  # t2363: "cuda:0 bf16[1, 32, 512, 64]"
  t2364 = ltorch.cat((t2363, t2359), -1)  # t2364: "cuda:0 bf16[1, 32, 512, 128]"
    # t2364 = prims.cat((t2363, t2359), -1)  # t2364: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2367 = ltorch.mul(t2358, cos)  # t2367: "cuda:0 f32[1, 32, 512, 128]"
    # t2365 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2365: "cuda:0 f32[1, 32, 512, 128]"
    # t2366 = prims.convert_element_type(t2358, dtypes.float32)  # t2366: "cuda:0 f32[1, 32, 512, 128]"
    # t2367 = prims.mul(t2366, t2365)  # t2367: "cuda:0 f32[1, 32, 512, 128]"
  t2370 = ltorch.mul(t2364, sin)  # t2370: "cuda:0 f32[1, 32, 512, 128]"
    # t2368 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2368: "cuda:0 f32[1, 32, 512, 128]"
    # t2369 = prims.convert_element_type(t2364, dtypes.float32)  # t2369: "cuda:0 f32[1, 32, 512, 128]"
    # t2370 = prims.mul(t2369, t2368)  # t2370: "cuda:0 f32[1, 32, 512, 128]"
  t2371 = ltorch.add(t2367, t2370, alpha=None)  # t2371: "cuda:0 f32[1, 32, 512, 128]"
    # t2371 = prims.add(t2367, t2370)  # t2371: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2372 = ltorch.to(t2371, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2372: "cuda:0 bf16[1, 32, 512, 128]"
    # t2372 = prims.convert_element_type(t2371, dtypes.bfloat16)  # t2372: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t2373 = ltorch.getitem(t2340, (..., slice(128, None, None)))  # t2373: "cuda:0 bf16[1, 32, 512, 0]"
    # t2373 = prims.slice_prim(t2340, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2373: "cuda:0 bf16[1, 32, 512, 0]"
  t2374 = ltorch.cat((t2357, t2373), -1)  # t2374: "cuda:0 bf16[1, 32, 512, 128]"
    # t2374 = prims.cat((t2357, t2373), -1)  # t2374: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t2375 = ltorch.getitem(t2341, (..., slice(128, None, None)))  # t2375: "cuda:0 bf16[1, 32, 512, 0]"
    # t2375 = prims.slice_prim(t2341, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2375: "cuda:0 bf16[1, 32, 512, 0]"
  t2376 = ltorch.cat((t2372, t2375), -1)  # t2376: "cuda:0 bf16[1, 32, 512, 128]"
    # t2376 = prims.cat((t2372, t2375), -1)  # t2376: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t2406 = ltorch.scaled_dot_product_attention(t2374, t2376, t2342, None, 0.0, True, scale=0.08838834764831843)  # t2406: "cuda:0 bf16[1, 32, 512, 128]"
    # t2379 = ltorch.mul(t2374, 0.29730177875068026)  # t2379: "cuda:0 bf16[1, 32, 512, 128]"
      # t2377 = prims.convert_element_type(t2374, dtypes.float32)  # t2377: "cuda:0 f32[1, 32, 512, 128]"
      # t2378 = prims.mul(t2377, 0.29730177875068026)  # t2378: "cuda:0 f32[1, 32, 512, 128]"
      # t2379 = prims.convert_element_type(t2378, dtypes.bfloat16)  # t2379: "cuda:0 bf16[1, 32, 512, 128]"
    # t2380 = ltorch.transpose(t2376, -2, -1)  # t2380: "cuda:0 bf16[1, 32, 128, 512]"
      # t2380 = prims.transpose(t2376, (0, 1, 3, 2))  # t2380: "cuda:0 bf16[1, 32, 128, 512]"
    # t2383 = ltorch.mul(t2380, 0.29730177875068026)  # t2383: "cuda:0 bf16[1, 32, 128, 512]"
      # t2381 = prims.convert_element_type(t2380, dtypes.float32)  # t2381: "cuda:0 f32[1, 32, 128, 512]"
      # t2382 = prims.mul(t2381, 0.29730177875068026)  # t2382: "cuda:0 f32[1, 32, 128, 512]"
      # t2383 = prims.convert_element_type(t2382, dtypes.bfloat16)  # t2383: "cuda:0 bf16[1, 32, 128, 512]"
    # t2384 = ltorch.matmul(t2379, t2383)  # t2384: "cuda:0 bf16[1, 32, 512, 512]"
      # t2384 = prims.matmul(t2379, t2383)  # t2384: "cuda:0 bf16[1, 32, 512, 512]"
    # t2394 = ltorch.tril(t2384, 0, fill_value=-float('inf'))  # t2394: "cuda:0 bf16[1, 32, 512, 512]"
      # t2385 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2385: "cuda:0 i64[512]"
        # t2385 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2385: "cuda:0 i64[512]"
      # t2386 = ltorch.unsqueeze(t2385, -1)  # t2386: "cuda:0 i64[512, 1]"
        # t2386 = prims.broadcast_in_dim(t2385, [512, 1], [0])  # t2386: "cuda:0 i64[512, 1]"
      # t2387 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2387: "cuda:0 i64[512]"
        # t2387 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2387: "cuda:0 i64[512]"
      # t2388 = ltorch.unsqueeze(t2387, -2)  # t2388: "cuda:0 i64[1, 512]"
        # t2388 = prims.broadcast_in_dim(t2387, [1, 512], [1])  # t2388: "cuda:0 i64[1, 512]"
      # t2389 = ltorch.add(t2386, 0, alpha=None)  # t2389: "cuda:0 i64[512, 1]"
        # t2389 = prims.add(t2386, 0)  # t2389: "cuda:0 i64[512, 1]"
      # t2392 = ltorch.ge(t2389, t2388)  # t2392: "cuda:0 b8[512, 512]"
        # t2390 = prims.broadcast_in_dim(t2389, (512, 512), (0, 1))  # t2390: "cuda:0 i64[512, 512]"
        # t2391 = prims.broadcast_in_dim(t2388, (512, 512), (0, 1))  # t2391: "cuda:0 i64[512, 512]"
        # t2392 = prims.ge(t2390, t2391)  # t2392: "cuda:0 b8[512, 512]"
      # t2394 = ltorch.where(t2392, t2384, -float('inf'))  # t2394: "cuda:0 bf16[1, 32, 512, 512]"
        # t2393 = prims.broadcast_in_dim(t2392, (1, 32, 512, 512), (2, 3))  # t2393: "cuda:0 b8[1, 32, 512, 512]"
        # t2394 = prims.where(t2393, t2384, -float('inf'))  # t2394: "cuda:0 bf16[1, 32, 512, 512]"
    # t2405 = ltorch._softmax(t2394, -1, dtype=None)  # t2405: "cuda:0 bf16[1, 32, 512, 512]"
      # t2395 = ltorch.to(t2394, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t2395: "cuda:0 f32[1, 32, 512, 512]"
        # t2395 = prims.convert_element_type(t2394, dtypes.float32)  # t2395: "cuda:0 f32[1, 32, 512, 512]"
      # t2397 = ltorch.amax(t2395, -1, True)  # t2397: "cuda:0 f32[1, 32, 512, 1]"
        # t2396 = prims.amax(t2395, (3,))  # t2396: "cuda:0 f32[1, 32, 512]"
        # t2397 = prims.broadcast_in_dim(t2396, [1, 32, 512, 1], [0, 1, 2])  # t2397: "cuda:0 f32[1, 32, 512, 1]"
      # t2399 = ltorch.sub(t2395, t2397, alpha=None)  # t2399: "cuda:0 f32[1, 32, 512, 512]"
        # t2398 = prims.broadcast_in_dim(t2397, (1, 32, 512, 512), (0, 1, 2, 3))  # t2398: "cuda:0 f32[1, 32, 512, 512]"
        # t2399 = prims.sub(t2395, t2398)  # t2399: "cuda:0 f32[1, 32, 512, 512]"
      # t2400 = ltorch.exp(t2399)  # t2400: "cuda:0 f32[1, 32, 512, 512]"
        # t2400 = prims.exp(t2399)  # t2400: "cuda:0 f32[1, 32, 512, 512]"
      # t2402 = ltorch.sum(t2400, -1, True, dtype=None)  # t2402: "cuda:0 f32[1, 32, 512, 1]"
        # t2401 = prims.sum(t2400, (3,))  # t2401: "cuda:0 f32[1, 32, 512]"
        # t2402 = prims.broadcast_in_dim(t2401, [1, 32, 512, 1], [0, 1, 2])  # t2402: "cuda:0 f32[1, 32, 512, 1]"
      # t2404 = ltorch.true_divide(t2400, t2402)  # t2404: "cuda:0 f32[1, 32, 512, 512]"
        # t2403 = prims.broadcast_in_dim(t2402, (1, 32, 512, 512), (0, 1, 2, 3))  # t2403: "cuda:0 f32[1, 32, 512, 512]"
        # t2404 = prims.div(t2400, t2403)  # t2404: "cuda:0 f32[1, 32, 512, 512]"
      # t2405 = ltorch.to(t2404, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t2405: "cuda:0 bf16[1, 32, 512, 512]"
        # t2405 = prims.convert_element_type(t2404, dtypes.bfloat16)  # t2405: "cuda:0 bf16[1, 32, 512, 512]"
    # t2406 = ltorch.matmul(t2405, t2342)  # t2406: "cuda:0 bf16[1, 32, 512, 128]"
      # t2406 = prims.matmul(t2405, t2342)  # t2406: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t2407 = ltorch.transpose(t2406, 1, 2)  # t2407: "cuda:0 bf16[1, 512, 32, 128]"
    # t2407 = prims.transpose(t2406, (0, 2, 1, 3))  # t2407: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t2408 = ltorch.reshape(t2407, 1, 512, 4096)  # t2408: "cuda:0 bf16[1, 512, 4096]"
    # t2408 = prims.reshape(t2407, (1, 512, 4096))  # t2408: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2412 = ltorch.linear(t2408, t_transformer_h_14_attn_proj_weight, None)  # t2412: "cuda:0 bf16[1, 512, 4096]"
    # t2412 = prims.linear(t2408, t_transformer_h_14_attn_proj_weight, None)  # t2412: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t2416 = ltorch.add(t2412, t2306, alpha=None)  # t2416: "cuda:0 bf16[1, 512, 4096]"
    # t2413 = prims.convert_element_type(t2412, dtypes.float32)  # t2413: "cuda:0 f32[1, 512, 4096]"
    # t2414 = prims.convert_element_type(t2306, dtypes.float32)  # t2414: "cuda:0 f32[1, 512, 4096]"
    # t2415 = prims.add(t2413, t2414)  # t2415: "cuda:0 f32[1, 512, 4096]"
    # t2416 = prims.convert_element_type(t2415, dtypes.bfloat16)  # t2416: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2417 = prims.convert_element_type(t2416, dtypes.float32)  # t2417: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2418 = ltorch.mul(t2417, t2417)  # t2418: "cuda:0 f32[1, 512, 4096]"
    # t2418 = prims.mul(t2417, t2417)  # t2418: "cuda:0 f32[1, 512, 4096]"
  t2422 = ltorch.mean(t2418, -1, True, dtype=None)  # t2422: "cuda:0 f32[1, 512, 1]"
    # t2420 = prims.sum(t2418, (2,))  # t2420: "cuda:0 f32[1, 512]"
    # t2421 = prims.broadcast_in_dim(t2420, [1, 512, 1], [0, 1])  # t2421: "cuda:0 f32[1, 512, 1]"
    # t2422 = ltorch.true_divide(t2421, 4096)  # t2422: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2422 = prims.div(t2421, 4096.0)  # t2422: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2424 = ltorch.add(t2422, 1e-05, alpha=None)  # t2424: "cuda:0 f32[1, 512, 1]"
    # t2424 = prims.add(t2422, 1e-05)  # t2424: "cuda:0 f32[1, 512, 1]"
  t2425 = ltorch.rsqrt(t2424)  # t2425: "cuda:0 f32[1, 512, 1]"
    # t2425 = prims.rsqrt(t2424)  # t2425: "cuda:0 f32[1, 512, 1]"
  t2427 = ltorch.mul(t2417, t2425)  # t2427: "cuda:0 f32[1, 512, 4096]"
    # t2426 = prims.broadcast_in_dim(t2425, (1, 512, 4096), (0, 1, 2))  # t2426: "cuda:0 f32[1, 512, 4096]"
    # t2427 = prims.mul(t2417, t2426)  # t2427: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2428 = ltorch.to(t2427, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2428: "cuda:0 bf16[1, 512, 4096]"
    # t2428 = prims.convert_element_type(t2427, dtypes.bfloat16)  # t2428: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2438 = ltorch.mul(t2428, t_transformer_h_14_norm_2_weight)  # t2438: "cuda:0 bf16[1, 512, 4096]"
    # t2434 = prims.broadcast_in_dim(t_transformer_h_14_norm_2_weight, (1, 512, 4096), (2,))  # t2434: "cuda:0 bf16[1, 512, 4096]"
    # t2435 = prims.convert_element_type(t2428, dtypes.float32)  # t2435: "cuda:0 f32[1, 512, 4096]"
    # t2436 = prims.convert_element_type(t2434, dtypes.float32)  # t2436: "cuda:0 f32[1, 512, 4096]"
    # t2437 = prims.mul(t2435, t2436)  # t2437: "cuda:0 f32[1, 512, 4096]"
    # t2438 = prims.convert_element_type(t2437, dtypes.bfloat16)  # t2438: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2443 = ltorch.linear(t2438, t_transformer_h_14_mlp_fc_1_weight, None)  # t2443: "cuda:0 bf16[1, 512, 11008]"
    # t2443 = prims.linear(t2438, t_transformer_h_14_mlp_fc_1_weight, None)  # t2443: "cuda:0 bf16[1, 512, 11008]"
  t2447 = ltorch.linear(t2438, t_transformer_h_14_mlp_fc_2_weight, None)  # t2447: "cuda:0 bf16[1, 512, 11008]"
    # t2447 = prims.linear(t2438, t_transformer_h_14_mlp_fc_2_weight, None)  # t2447: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t2457 = ltorch.silu(t2443, False)  # t2457: "cuda:0 bf16[1, 512, 11008]"
    # t2448 = prims.convert_element_type(t2443, dtypes.float32)  # t2448: "cuda:0 f32[1, 512, 11008]"
    # t2449 = prims.neg(t2448)  # t2449: "cuda:0 f32[1, 512, 11008]"
    # t2450 = prims.exp(t2449)  # t2450: "cuda:0 f32[1, 512, 11008]"
    # t2451 = prims.add(1.0, t2450)  # t2451: "cuda:0 f32[1, 512, 11008]"
    # t2452 = prims.reciprocal(t2451)  # t2452: "cuda:0 f32[1, 512, 11008]"
    # t2453 = prims.convert_element_type(t2452, dtypes.bfloat16)  # t2453: "cuda:0 bf16[1, 512, 11008]"
    # t2454 = prims.convert_element_type(t2443, dtypes.float32)  # t2454: "cuda:0 f32[1, 512, 11008]"
    # t2455 = prims.convert_element_type(t2453, dtypes.float32)  # t2455: "cuda:0 f32[1, 512, 11008]"
    # t2456 = prims.mul(t2454, t2455)  # t2456: "cuda:0 f32[1, 512, 11008]"
    # t2457 = prims.convert_element_type(t2456, dtypes.bfloat16)  # t2457: "cuda:0 bf16[1, 512, 11008]"
  t2461 = ltorch.mul(t2457, t2447)  # t2461: "cuda:0 bf16[1, 512, 11008]"
    # t2458 = prims.convert_element_type(t2457, dtypes.float32)  # t2458: "cuda:0 f32[1, 512, 11008]"
    # t2459 = prims.convert_element_type(t2447, dtypes.float32)  # t2459: "cuda:0 f32[1, 512, 11008]"
    # t2460 = prims.mul(t2458, t2459)  # t2460: "cuda:0 f32[1, 512, 11008]"
    # t2461 = prims.convert_element_type(t2460, dtypes.bfloat16)  # t2461: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2465 = ltorch.linear(t2461, t_transformer_h_14_mlp_proj_weight, None)  # t2465: "cuda:0 bf16[1, 512, 4096]"
    # t2465 = prims.linear(t2461, t_transformer_h_14_mlp_proj_weight, None)  # t2465: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t2469 = ltorch.add(t2465, t2416, alpha=None)  # t2469: "cuda:0 bf16[1, 512, 4096]"
    # t2466 = prims.convert_element_type(t2465, dtypes.float32)  # t2466: "cuda:0 f32[1, 512, 4096]"
    # t2467 = prims.convert_element_type(t2416, dtypes.float32)  # t2467: "cuda:0 f32[1, 512, 4096]"
    # t2468 = prims.add(t2466, t2467)  # t2468: "cuda:0 f32[1, 512, 4096]"
    # t2469 = prims.convert_element_type(t2468, dtypes.bfloat16)  # t2469: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2471 = prims.convert_element_type(t2469, dtypes.float32)  # t2471: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2472 = ltorch.mul(t2471, t2471)  # t2472: "cuda:0 f32[1, 512, 4096]"
    # t2472 = prims.mul(t2471, t2471)  # t2472: "cuda:0 f32[1, 512, 4096]"
  t2476 = ltorch.mean(t2472, -1, True, dtype=None)  # t2476: "cuda:0 f32[1, 512, 1]"
    # t2474 = prims.sum(t2472, (2,))  # t2474: "cuda:0 f32[1, 512]"
    # t2475 = prims.broadcast_in_dim(t2474, [1, 512, 1], [0, 1])  # t2475: "cuda:0 f32[1, 512, 1]"
    # t2476 = ltorch.true_divide(t2475, 4096)  # t2476: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2476 = prims.div(t2475, 4096.0)  # t2476: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2478 = ltorch.add(t2476, 1e-05, alpha=None)  # t2478: "cuda:0 f32[1, 512, 1]"
    # t2478 = prims.add(t2476, 1e-05)  # t2478: "cuda:0 f32[1, 512, 1]"
  t2479 = ltorch.rsqrt(t2478)  # t2479: "cuda:0 f32[1, 512, 1]"
    # t2479 = prims.rsqrt(t2478)  # t2479: "cuda:0 f32[1, 512, 1]"
  t2481 = ltorch.mul(t2471, t2479)  # t2481: "cuda:0 f32[1, 512, 4096]"
    # t2480 = prims.broadcast_in_dim(t2479, (1, 512, 4096), (0, 1, 2))  # t2480: "cuda:0 f32[1, 512, 4096]"
    # t2481 = prims.mul(t2471, t2480)  # t2481: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2482 = ltorch.to(t2481, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2482: "cuda:0 bf16[1, 512, 4096]"
    # t2482 = prims.convert_element_type(t2481, dtypes.bfloat16)  # t2482: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2492 = ltorch.mul(t2482, t_transformer_h_15_norm_1_weight)  # t2492: "cuda:0 bf16[1, 512, 4096]"
    # t2488 = prims.broadcast_in_dim(t_transformer_h_15_norm_1_weight, (1, 512, 4096), (2,))  # t2488: "cuda:0 bf16[1, 512, 4096]"
    # t2489 = prims.convert_element_type(t2482, dtypes.float32)  # t2489: "cuda:0 f32[1, 512, 4096]"
    # t2490 = prims.convert_element_type(t2488, dtypes.float32)  # t2490: "cuda:0 f32[1, 512, 4096]"
    # t2491 = prims.mul(t2489, t2490)  # t2491: "cuda:0 f32[1, 512, 4096]"
    # t2492 = prims.convert_element_type(t2491, dtypes.bfloat16)  # t2492: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2497 = ltorch.linear(t2492, t_transformer_h_15_attn_attn_weight, None)  # t2497: "cuda:0 bf16[1, 512, 12288]"
    # t2497 = prims.linear(t2492, t_transformer_h_15_attn_attn_weight, None)  # t2497: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t2498 = ltorch.view(t2497, 1, 512, 32, 3, 128)  # t2498: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t2498 = ltorch.reshape(t2497, (1, 512, 32, 3, 128))  # t2498: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t2498 = prims.reshape(t2497, (1, 512, 32, 3, 128))  # t2498: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t2499 = ltorch.permute(t2498, 0, 2, 3, 1, 4)  # t2499: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t2499 = prims.transpose(t2498, (0, 2, 3, 1, 4))  # t2499: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t2500, t2501, t2502) = ltorch.split(t2499, (1, 1, 1), 2)
    # t2500 = prims.slice_prim(t2499, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2500: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2501 = prims.slice_prim(t2499, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2501: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2502 = prims.slice_prim(t2499, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2502: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t2503 = ltorch.reshape(t2500, 1, -1, 512, 128)  # t2503: "cuda:0 bf16[1, 32, 512, 128]"
    # t2503 = prims.reshape(t2500, (1, 32, 512, 128))  # t2503: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t2504 = ltorch.reshape(t2501, 1, -1, 512, 128)  # t2504: "cuda:0 bf16[1, 32, 512, 128]"
    # t2504 = prims.reshape(t2501, (1, 32, 512, 128))  # t2504: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t2505 = ltorch.reshape(t2502, 1, -1, 512, 128)  # t2505: "cuda:0 bf16[1, 32, 512, 128]"
    # t2505 = prims.reshape(t2502, (1, 32, 512, 128))  # t2505: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t2506 = ltorch.getitem(t2503, (..., slice(None, 128, None)))  # t2506: "cuda:0 bf16[1, 32, 512, 128]"
    # t2506 = prims.slice_prim(t2503, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2506: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2507 = ltorch.getitem(t2506, (..., slice(None, 64, None)))  # t2507: "cuda:0 bf16[1, 32, 512, 64]"
    # t2507 = prims.slice_prim(t2506, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2507: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2508 = ltorch.getitem(t2506, (..., slice(64, None, None)))  # t2508: "cuda:0 bf16[1, 32, 512, 64]"
    # t2508 = prims.slice_prim(t2506, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2508: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2511 = ltorch.neg(t2508)  # t2511: "cuda:0 bf16[1, 32, 512, 64]"
    # t2509 = prims.convert_element_type(t2508, dtypes.float32)  # t2509: "cuda:0 f32[1, 32, 512, 64]"
    # t2510 = prims.neg(t2509)  # t2510: "cuda:0 f32[1, 32, 512, 64]"
    # t2511 = prims.convert_element_type(t2510, dtypes.bfloat16)  # t2511: "cuda:0 bf16[1, 32, 512, 64]"
  t2512 = ltorch.cat((t2511, t2507), -1)  # t2512: "cuda:0 bf16[1, 32, 512, 128]"
    # t2512 = prims.cat((t2511, t2507), -1)  # t2512: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2515 = ltorch.mul(t2506, cos)  # t2515: "cuda:0 f32[1, 32, 512, 128]"
    # t2513 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2513: "cuda:0 f32[1, 32, 512, 128]"
    # t2514 = prims.convert_element_type(t2506, dtypes.float32)  # t2514: "cuda:0 f32[1, 32, 512, 128]"
    # t2515 = prims.mul(t2514, t2513)  # t2515: "cuda:0 f32[1, 32, 512, 128]"
  t2518 = ltorch.mul(t2512, sin)  # t2518: "cuda:0 f32[1, 32, 512, 128]"
    # t2516 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2516: "cuda:0 f32[1, 32, 512, 128]"
    # t2517 = prims.convert_element_type(t2512, dtypes.float32)  # t2517: "cuda:0 f32[1, 32, 512, 128]"
    # t2518 = prims.mul(t2517, t2516)  # t2518: "cuda:0 f32[1, 32, 512, 128]"
  t2519 = ltorch.add(t2515, t2518, alpha=None)  # t2519: "cuda:0 f32[1, 32, 512, 128]"
    # t2519 = prims.add(t2515, t2518)  # t2519: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2520 = ltorch.to(t2519, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2520: "cuda:0 bf16[1, 32, 512, 128]"
    # t2520 = prims.convert_element_type(t2519, dtypes.bfloat16)  # t2520: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t2521 = ltorch.getitem(t2504, (..., slice(None, 128, None)))  # t2521: "cuda:0 bf16[1, 32, 512, 128]"
    # t2521 = prims.slice_prim(t2504, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2521: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2522 = ltorch.getitem(t2521, (..., slice(None, 64, None)))  # t2522: "cuda:0 bf16[1, 32, 512, 64]"
    # t2522 = prims.slice_prim(t2521, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2522: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2523 = ltorch.getitem(t2521, (..., slice(64, None, None)))  # t2523: "cuda:0 bf16[1, 32, 512, 64]"
    # t2523 = prims.slice_prim(t2521, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2523: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2526 = ltorch.neg(t2523)  # t2526: "cuda:0 bf16[1, 32, 512, 64]"
    # t2524 = prims.convert_element_type(t2523, dtypes.float32)  # t2524: "cuda:0 f32[1, 32, 512, 64]"
    # t2525 = prims.neg(t2524)  # t2525: "cuda:0 f32[1, 32, 512, 64]"
    # t2526 = prims.convert_element_type(t2525, dtypes.bfloat16)  # t2526: "cuda:0 bf16[1, 32, 512, 64]"
  t2527 = ltorch.cat((t2526, t2522), -1)  # t2527: "cuda:0 bf16[1, 32, 512, 128]"
    # t2527 = prims.cat((t2526, t2522), -1)  # t2527: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2530 = ltorch.mul(t2521, cos)  # t2530: "cuda:0 f32[1, 32, 512, 128]"
    # t2528 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2528: "cuda:0 f32[1, 32, 512, 128]"
    # t2529 = prims.convert_element_type(t2521, dtypes.float32)  # t2529: "cuda:0 f32[1, 32, 512, 128]"
    # t2530 = prims.mul(t2529, t2528)  # t2530: "cuda:0 f32[1, 32, 512, 128]"
  t2533 = ltorch.mul(t2527, sin)  # t2533: "cuda:0 f32[1, 32, 512, 128]"
    # t2531 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2531: "cuda:0 f32[1, 32, 512, 128]"
    # t2532 = prims.convert_element_type(t2527, dtypes.float32)  # t2532: "cuda:0 f32[1, 32, 512, 128]"
    # t2533 = prims.mul(t2532, t2531)  # t2533: "cuda:0 f32[1, 32, 512, 128]"
  t2534 = ltorch.add(t2530, t2533, alpha=None)  # t2534: "cuda:0 f32[1, 32, 512, 128]"
    # t2534 = prims.add(t2530, t2533)  # t2534: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2535 = ltorch.to(t2534, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2535: "cuda:0 bf16[1, 32, 512, 128]"
    # t2535 = prims.convert_element_type(t2534, dtypes.bfloat16)  # t2535: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t2536 = ltorch.getitem(t2503, (..., slice(128, None, None)))  # t2536: "cuda:0 bf16[1, 32, 512, 0]"
    # t2536 = prims.slice_prim(t2503, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2536: "cuda:0 bf16[1, 32, 512, 0]"
  t2537 = ltorch.cat((t2520, t2536), -1)  # t2537: "cuda:0 bf16[1, 32, 512, 128]"
    # t2537 = prims.cat((t2520, t2536), -1)  # t2537: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t2538 = ltorch.getitem(t2504, (..., slice(128, None, None)))  # t2538: "cuda:0 bf16[1, 32, 512, 0]"
    # t2538 = prims.slice_prim(t2504, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2538: "cuda:0 bf16[1, 32, 512, 0]"
  t2539 = ltorch.cat((t2535, t2538), -1)  # t2539: "cuda:0 bf16[1, 32, 512, 128]"
    # t2539 = prims.cat((t2535, t2538), -1)  # t2539: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t2569 = ltorch.scaled_dot_product_attention(t2537, t2539, t2505, None, 0.0, True, scale=0.08838834764831843)  # t2569: "cuda:0 bf16[1, 32, 512, 128]"
    # t2542 = ltorch.mul(t2537, 0.29730177875068026)  # t2542: "cuda:0 bf16[1, 32, 512, 128]"
      # t2540 = prims.convert_element_type(t2537, dtypes.float32)  # t2540: "cuda:0 f32[1, 32, 512, 128]"
      # t2541 = prims.mul(t2540, 0.29730177875068026)  # t2541: "cuda:0 f32[1, 32, 512, 128]"
      # t2542 = prims.convert_element_type(t2541, dtypes.bfloat16)  # t2542: "cuda:0 bf16[1, 32, 512, 128]"
    # t2543 = ltorch.transpose(t2539, -2, -1)  # t2543: "cuda:0 bf16[1, 32, 128, 512]"
      # t2543 = prims.transpose(t2539, (0, 1, 3, 2))  # t2543: "cuda:0 bf16[1, 32, 128, 512]"
    # t2546 = ltorch.mul(t2543, 0.29730177875068026)  # t2546: "cuda:0 bf16[1, 32, 128, 512]"
      # t2544 = prims.convert_element_type(t2543, dtypes.float32)  # t2544: "cuda:0 f32[1, 32, 128, 512]"
      # t2545 = prims.mul(t2544, 0.29730177875068026)  # t2545: "cuda:0 f32[1, 32, 128, 512]"
      # t2546 = prims.convert_element_type(t2545, dtypes.bfloat16)  # t2546: "cuda:0 bf16[1, 32, 128, 512]"
    # t2547 = ltorch.matmul(t2542, t2546)  # t2547: "cuda:0 bf16[1, 32, 512, 512]"
      # t2547 = prims.matmul(t2542, t2546)  # t2547: "cuda:0 bf16[1, 32, 512, 512]"
    # t2557 = ltorch.tril(t2547, 0, fill_value=-float('inf'))  # t2557: "cuda:0 bf16[1, 32, 512, 512]"
      # t2548 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2548: "cuda:0 i64[512]"
        # t2548 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2548: "cuda:0 i64[512]"
      # t2549 = ltorch.unsqueeze(t2548, -1)  # t2549: "cuda:0 i64[512, 1]"
        # t2549 = prims.broadcast_in_dim(t2548, [512, 1], [0])  # t2549: "cuda:0 i64[512, 1]"
      # t2550 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2550: "cuda:0 i64[512]"
        # t2550 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2550: "cuda:0 i64[512]"
      # t2551 = ltorch.unsqueeze(t2550, -2)  # t2551: "cuda:0 i64[1, 512]"
        # t2551 = prims.broadcast_in_dim(t2550, [1, 512], [1])  # t2551: "cuda:0 i64[1, 512]"
      # t2552 = ltorch.add(t2549, 0, alpha=None)  # t2552: "cuda:0 i64[512, 1]"
        # t2552 = prims.add(t2549, 0)  # t2552: "cuda:0 i64[512, 1]"
      # t2555 = ltorch.ge(t2552, t2551)  # t2555: "cuda:0 b8[512, 512]"
        # t2553 = prims.broadcast_in_dim(t2552, (512, 512), (0, 1))  # t2553: "cuda:0 i64[512, 512]"
        # t2554 = prims.broadcast_in_dim(t2551, (512, 512), (0, 1))  # t2554: "cuda:0 i64[512, 512]"
        # t2555 = prims.ge(t2553, t2554)  # t2555: "cuda:0 b8[512, 512]"
      # t2557 = ltorch.where(t2555, t2547, -float('inf'))  # t2557: "cuda:0 bf16[1, 32, 512, 512]"
        # t2556 = prims.broadcast_in_dim(t2555, (1, 32, 512, 512), (2, 3))  # t2556: "cuda:0 b8[1, 32, 512, 512]"
        # t2557 = prims.where(t2556, t2547, -float('inf'))  # t2557: "cuda:0 bf16[1, 32, 512, 512]"
    # t2568 = ltorch._softmax(t2557, -1, dtype=None)  # t2568: "cuda:0 bf16[1, 32, 512, 512]"
      # t2558 = ltorch.to(t2557, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t2558: "cuda:0 f32[1, 32, 512, 512]"
        # t2558 = prims.convert_element_type(t2557, dtypes.float32)  # t2558: "cuda:0 f32[1, 32, 512, 512]"
      # t2560 = ltorch.amax(t2558, -1, True)  # t2560: "cuda:0 f32[1, 32, 512, 1]"
        # t2559 = prims.amax(t2558, (3,))  # t2559: "cuda:0 f32[1, 32, 512]"
        # t2560 = prims.broadcast_in_dim(t2559, [1, 32, 512, 1], [0, 1, 2])  # t2560: "cuda:0 f32[1, 32, 512, 1]"
      # t2562 = ltorch.sub(t2558, t2560, alpha=None)  # t2562: "cuda:0 f32[1, 32, 512, 512]"
        # t2561 = prims.broadcast_in_dim(t2560, (1, 32, 512, 512), (0, 1, 2, 3))  # t2561: "cuda:0 f32[1, 32, 512, 512]"
        # t2562 = prims.sub(t2558, t2561)  # t2562: "cuda:0 f32[1, 32, 512, 512]"
      # t2563 = ltorch.exp(t2562)  # t2563: "cuda:0 f32[1, 32, 512, 512]"
        # t2563 = prims.exp(t2562)  # t2563: "cuda:0 f32[1, 32, 512, 512]"
      # t2565 = ltorch.sum(t2563, -1, True, dtype=None)  # t2565: "cuda:0 f32[1, 32, 512, 1]"
        # t2564 = prims.sum(t2563, (3,))  # t2564: "cuda:0 f32[1, 32, 512]"
        # t2565 = prims.broadcast_in_dim(t2564, [1, 32, 512, 1], [0, 1, 2])  # t2565: "cuda:0 f32[1, 32, 512, 1]"
      # t2567 = ltorch.true_divide(t2563, t2565)  # t2567: "cuda:0 f32[1, 32, 512, 512]"
        # t2566 = prims.broadcast_in_dim(t2565, (1, 32, 512, 512), (0, 1, 2, 3))  # t2566: "cuda:0 f32[1, 32, 512, 512]"
        # t2567 = prims.div(t2563, t2566)  # t2567: "cuda:0 f32[1, 32, 512, 512]"
      # t2568 = ltorch.to(t2567, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t2568: "cuda:0 bf16[1, 32, 512, 512]"
        # t2568 = prims.convert_element_type(t2567, dtypes.bfloat16)  # t2568: "cuda:0 bf16[1, 32, 512, 512]"
    # t2569 = ltorch.matmul(t2568, t2505)  # t2569: "cuda:0 bf16[1, 32, 512, 128]"
      # t2569 = prims.matmul(t2568, t2505)  # t2569: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t2570 = ltorch.transpose(t2569, 1, 2)  # t2570: "cuda:0 bf16[1, 512, 32, 128]"
    # t2570 = prims.transpose(t2569, (0, 2, 1, 3))  # t2570: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t2571 = ltorch.reshape(t2570, 1, 512, 4096)  # t2571: "cuda:0 bf16[1, 512, 4096]"
    # t2571 = prims.reshape(t2570, (1, 512, 4096))  # t2571: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2575 = ltorch.linear(t2571, t_transformer_h_15_attn_proj_weight, None)  # t2575: "cuda:0 bf16[1, 512, 4096]"
    # t2575 = prims.linear(t2571, t_transformer_h_15_attn_proj_weight, None)  # t2575: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t2579 = ltorch.add(t2575, t2469, alpha=None)  # t2579: "cuda:0 bf16[1, 512, 4096]"
    # t2576 = prims.convert_element_type(t2575, dtypes.float32)  # t2576: "cuda:0 f32[1, 512, 4096]"
    # t2577 = prims.convert_element_type(t2469, dtypes.float32)  # t2577: "cuda:0 f32[1, 512, 4096]"
    # t2578 = prims.add(t2576, t2577)  # t2578: "cuda:0 f32[1, 512, 4096]"
    # t2579 = prims.convert_element_type(t2578, dtypes.bfloat16)  # t2579: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2580 = prims.convert_element_type(t2579, dtypes.float32)  # t2580: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2581 = ltorch.mul(t2580, t2580)  # t2581: "cuda:0 f32[1, 512, 4096]"
    # t2581 = prims.mul(t2580, t2580)  # t2581: "cuda:0 f32[1, 512, 4096]"
  t2585 = ltorch.mean(t2581, -1, True, dtype=None)  # t2585: "cuda:0 f32[1, 512, 1]"
    # t2583 = prims.sum(t2581, (2,))  # t2583: "cuda:0 f32[1, 512]"
    # t2584 = prims.broadcast_in_dim(t2583, [1, 512, 1], [0, 1])  # t2584: "cuda:0 f32[1, 512, 1]"
    # t2585 = ltorch.true_divide(t2584, 4096)  # t2585: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2585 = prims.div(t2584, 4096.0)  # t2585: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2587 = ltorch.add(t2585, 1e-05, alpha=None)  # t2587: "cuda:0 f32[1, 512, 1]"
    # t2587 = prims.add(t2585, 1e-05)  # t2587: "cuda:0 f32[1, 512, 1]"
  t2588 = ltorch.rsqrt(t2587)  # t2588: "cuda:0 f32[1, 512, 1]"
    # t2588 = prims.rsqrt(t2587)  # t2588: "cuda:0 f32[1, 512, 1]"
  t2590 = ltorch.mul(t2580, t2588)  # t2590: "cuda:0 f32[1, 512, 4096]"
    # t2589 = prims.broadcast_in_dim(t2588, (1, 512, 4096), (0, 1, 2))  # t2589: "cuda:0 f32[1, 512, 4096]"
    # t2590 = prims.mul(t2580, t2589)  # t2590: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2591 = ltorch.to(t2590, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2591: "cuda:0 bf16[1, 512, 4096]"
    # t2591 = prims.convert_element_type(t2590, dtypes.bfloat16)  # t2591: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2601 = ltorch.mul(t2591, t_transformer_h_15_norm_2_weight)  # t2601: "cuda:0 bf16[1, 512, 4096]"
    # t2597 = prims.broadcast_in_dim(t_transformer_h_15_norm_2_weight, (1, 512, 4096), (2,))  # t2597: "cuda:0 bf16[1, 512, 4096]"
    # t2598 = prims.convert_element_type(t2591, dtypes.float32)  # t2598: "cuda:0 f32[1, 512, 4096]"
    # t2599 = prims.convert_element_type(t2597, dtypes.float32)  # t2599: "cuda:0 f32[1, 512, 4096]"
    # t2600 = prims.mul(t2598, t2599)  # t2600: "cuda:0 f32[1, 512, 4096]"
    # t2601 = prims.convert_element_type(t2600, dtypes.bfloat16)  # t2601: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2606 = ltorch.linear(t2601, t_transformer_h_15_mlp_fc_1_weight, None)  # t2606: "cuda:0 bf16[1, 512, 11008]"
    # t2606 = prims.linear(t2601, t_transformer_h_15_mlp_fc_1_weight, None)  # t2606: "cuda:0 bf16[1, 512, 11008]"
  t2610 = ltorch.linear(t2601, t_transformer_h_15_mlp_fc_2_weight, None)  # t2610: "cuda:0 bf16[1, 512, 11008]"
    # t2610 = prims.linear(t2601, t_transformer_h_15_mlp_fc_2_weight, None)  # t2610: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t2620 = ltorch.silu(t2606, False)  # t2620: "cuda:0 bf16[1, 512, 11008]"
    # t2611 = prims.convert_element_type(t2606, dtypes.float32)  # t2611: "cuda:0 f32[1, 512, 11008]"
    # t2612 = prims.neg(t2611)  # t2612: "cuda:0 f32[1, 512, 11008]"
    # t2613 = prims.exp(t2612)  # t2613: "cuda:0 f32[1, 512, 11008]"
    # t2614 = prims.add(1.0, t2613)  # t2614: "cuda:0 f32[1, 512, 11008]"
    # t2615 = prims.reciprocal(t2614)  # t2615: "cuda:0 f32[1, 512, 11008]"
    # t2616 = prims.convert_element_type(t2615, dtypes.bfloat16)  # t2616: "cuda:0 bf16[1, 512, 11008]"
    # t2617 = prims.convert_element_type(t2606, dtypes.float32)  # t2617: "cuda:0 f32[1, 512, 11008]"
    # t2618 = prims.convert_element_type(t2616, dtypes.float32)  # t2618: "cuda:0 f32[1, 512, 11008]"
    # t2619 = prims.mul(t2617, t2618)  # t2619: "cuda:0 f32[1, 512, 11008]"
    # t2620 = prims.convert_element_type(t2619, dtypes.bfloat16)  # t2620: "cuda:0 bf16[1, 512, 11008]"
  t2624 = ltorch.mul(t2620, t2610)  # t2624: "cuda:0 bf16[1, 512, 11008]"
    # t2621 = prims.convert_element_type(t2620, dtypes.float32)  # t2621: "cuda:0 f32[1, 512, 11008]"
    # t2622 = prims.convert_element_type(t2610, dtypes.float32)  # t2622: "cuda:0 f32[1, 512, 11008]"
    # t2623 = prims.mul(t2621, t2622)  # t2623: "cuda:0 f32[1, 512, 11008]"
    # t2624 = prims.convert_element_type(t2623, dtypes.bfloat16)  # t2624: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2628 = ltorch.linear(t2624, t_transformer_h_15_mlp_proj_weight, None)  # t2628: "cuda:0 bf16[1, 512, 4096]"
    # t2628 = prims.linear(t2624, t_transformer_h_15_mlp_proj_weight, None)  # t2628: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t2632 = ltorch.add(t2628, t2579, alpha=None)  # t2632: "cuda:0 bf16[1, 512, 4096]"
    # t2629 = prims.convert_element_type(t2628, dtypes.float32)  # t2629: "cuda:0 f32[1, 512, 4096]"
    # t2630 = prims.convert_element_type(t2579, dtypes.float32)  # t2630: "cuda:0 f32[1, 512, 4096]"
    # t2631 = prims.add(t2629, t2630)  # t2631: "cuda:0 f32[1, 512, 4096]"
    # t2632 = prims.convert_element_type(t2631, dtypes.bfloat16)  # t2632: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2633 = prims.convert_element_type(t2632, dtypes.float32)  # t2633: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2634 = ltorch.mul(t2633, t2633)  # t2634: "cuda:0 f32[1, 512, 4096]"
    # t2634 = prims.mul(t2633, t2633)  # t2634: "cuda:0 f32[1, 512, 4096]"
  t2638 = ltorch.mean(t2634, -1, True, dtype=None)  # t2638: "cuda:0 f32[1, 512, 1]"
    # t2636 = prims.sum(t2634, (2,))  # t2636: "cuda:0 f32[1, 512]"
    # t2637 = prims.broadcast_in_dim(t2636, [1, 512, 1], [0, 1])  # t2637: "cuda:0 f32[1, 512, 1]"
    # t2638 = ltorch.true_divide(t2637, 4096)  # t2638: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2638 = prims.div(t2637, 4096.0)  # t2638: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2640 = ltorch.add(t2638, 1e-05, alpha=None)  # t2640: "cuda:0 f32[1, 512, 1]"
    # t2640 = prims.add(t2638, 1e-05)  # t2640: "cuda:0 f32[1, 512, 1]"
  t2641 = ltorch.rsqrt(t2640)  # t2641: "cuda:0 f32[1, 512, 1]"
    # t2641 = prims.rsqrt(t2640)  # t2641: "cuda:0 f32[1, 512, 1]"
  t2643 = ltorch.mul(t2633, t2641)  # t2643: "cuda:0 f32[1, 512, 4096]"
    # t2642 = prims.broadcast_in_dim(t2641, (1, 512, 4096), (0, 1, 2))  # t2642: "cuda:0 f32[1, 512, 4096]"
    # t2643 = prims.mul(t2633, t2642)  # t2643: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2644 = ltorch.to(t2643, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2644: "cuda:0 bf16[1, 512, 4096]"
    # t2644 = prims.convert_element_type(t2643, dtypes.bfloat16)  # t2644: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2654 = ltorch.mul(t2644, t_transformer_ln_f_weight)  # t2654: "cuda:0 bf16[1, 512, 4096]"
    # t2650 = prims.broadcast_in_dim(t_transformer_ln_f_weight, (1, 512, 4096), (2,))  # t2650: "cuda:0 bf16[1, 512, 4096]"
    # t2651 = prims.convert_element_type(t2644, dtypes.float32)  # t2651: "cuda:0 f32[1, 512, 4096]"
    # t2652 = prims.convert_element_type(t2650, dtypes.float32)  # t2652: "cuda:0 f32[1, 512, 4096]"
    # t2653 = prims.mul(t2651, t2652)  # t2653: "cuda:0 f32[1, 512, 4096]"
    # t2654 = prims.convert_element_type(t2653, dtypes.bfloat16)  # t2654: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2658 = ltorch.linear(t2654, t_lm_head_weight, None)  # t2658: "cuda:0 bf16[1, 512, 32000]"
    # t2658 = prims.linear(t2654, t_lm_head_weight, None)  # t2658: "cuda:0 bf16[1, 512, 32000]"
  return t2658
============================================ END: computation_trc split_forward_backward
============================================ START: primal_trace sort_data_parallel_syncs
# Constructed by Dead Code Elimination (took 4 milliseconds)
import thunder
import thunder.core.dtypes as dtypes
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight):
  # idx: "cuda:0 i64[1, 512]"
  # tos1: "cuda:0 f32[4096, 128]"
  # t_lm_head_weight: "cuda:0 bf16[32000, 4096]"
  # t_sin: "cuda:0 f32[4096, 128]"
  # t_transformer_h_0_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_0_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_0_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_0_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_0_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_1_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_1_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_1_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_2_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_2_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_2_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_3_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_3_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_3_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_4_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_4_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_4_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_5_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_5_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_5_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_6_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_6_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_6_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_7_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_7_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_7_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_8_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_8_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_8_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_9_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_9_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_9_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_10_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_10_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_10_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_11_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_11_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_11_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_12_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_12_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_12_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_13_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_13_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_13_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_14_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_14_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_14_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_15_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_15_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_15_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_ln_f_weight: "cuda:0 bf16[4096]"
  # t_transformer_wte_weight: "cuda:0 bf16[32000, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:85: 	            cos = self.cos[:T]
  cos = ltorch.getitem(tos1, slice(None, 512, None))  # cos: "cuda:0 f32[512, 128]"
    # cos = prims.slice_prim(tos1, [0, 0], [512, 128], [1, 1])  # cos: "cuda:0 f32[512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:86: 	            sin = self.sin[:T]
  sin = ltorch.getitem(t_sin, slice(None, 512, None))  # sin: "cuda:0 f32[512, 128]"
    # sin = prims.slice_prim(t_sin, [0, 0], [512, 128], [1, 1])  # sin: "cuda:0 f32[512, 128]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py:190: 	        return F.embedding(
  x = ltorch.embedding(idx, t_transformer_wte_weight, None, None, 2.0, False, False)  # x: "cuda:0 bf16[1, 512, 4096]"
    # t16 = ltorch.reshape(idx, [512])  # t16: "cuda:0 i64[512]"
      # t16 = prims.reshape(idx, (512,))  # t16: "cuda:0 i64[512]"
    # t17 = prims.take(t_transformer_wte_weight, t16, 0)  # t17: "cuda:0 bf16[512, 4096]"
    # x = ltorch.reshape(t17, [1, 512, 4096])  # x: "cuda:0 bf16[1, 512, 4096]"
      # x = prims.reshape(t17, (1, 512, 4096))  # x: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  a = prims.convert_element_type(x, dtypes.float32)  # a: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  result = ltorch.mul(a, a)  # result: "cuda:0 f32[1, 512, 4096]"
    # result = prims.mul(a, a)  # result: "cuda:0 f32[1, 512, 4096]"
  norm_x = ltorch.mean(result, -1, True, dtype=None)  # norm_x: "cuda:0 f32[1, 512, 1]"
    # t24 = prims.sum(result, (2,))  # t24: "cuda:0 f32[1, 512]"
    # t25 = prims.broadcast_in_dim(t24, [1, 512, 1], [0, 1])  # t25: "cuda:0 f32[1, 512, 1]"
    # norm_x = ltorch.true_divide(t25, 4096)  # norm_x: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # norm_x = prims.div(t25, 4096.0)  # norm_x: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t28 = ltorch.add(norm_x, 1e-05, alpha=None)  # t28: "cuda:0 f32[1, 512, 1]"
    # t28 = prims.add(norm_x, 1e-05)  # t28: "cuda:0 f32[1, 512, 1]"
  b = ltorch.rsqrt(t28)  # b: "cuda:0 f32[1, 512, 1]"
    # b = prims.rsqrt(t28)  # b: "cuda:0 f32[1, 512, 1]"
  x_normed = ltorch.mul(a, b)  # x_normed: "cuda:0 f32[1, 512, 4096]"
    # t30 = prims.broadcast_in_dim(b, (1, 512, 4096), (0, 1, 2))  # t30: "cuda:0 f32[1, 512, 4096]"
    # x_normed = prims.mul(a, t30)  # x_normed: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t32 = ltorch.to(x_normed, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t32: "cuda:0 bf16[1, 512, 4096]"
    # t32 = prims.convert_element_type(x_normed, dtypes.bfloat16)  # t32: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  input = ltorch.mul(t32, t_transformer_h_0_norm_1_weight)  # input: "cuda:0 bf16[1, 512, 4096]"
    # t38 = prims.broadcast_in_dim(t_transformer_h_0_norm_1_weight, (1, 512, 4096), (2,))  # t38: "cuda:0 bf16[1, 512, 4096]"
    # t39 = prims.convert_element_type(t32, dtypes.float32)  # t39: "cuda:0 f32[1, 512, 4096]"
    # t40 = prims.convert_element_type(t38, dtypes.float32)  # t40: "cuda:0 f32[1, 512, 4096]"
    # t41 = prims.mul(t39, t40)  # t41: "cuda:0 f32[1, 512, 4096]"
    # input = prims.convert_element_type(t41, dtypes.bfloat16)  # input: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  qkv = ltorch.linear(input, t_transformer_h_0_attn_attn_weight, None)  # qkv: "cuda:0 bf16[1, 512, 12288]"
    # qkv = prims.linear(input, t_transformer_h_0_attn_attn_weight, None)  # qkv: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t51 = ltorch.view(qkv, 1, 512, 32, 3, 128)  # t51: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t51 = ltorch.reshape(qkv, (1, 512, 32, 3, 128))  # t51: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t51 = prims.reshape(qkv, (1, 512, 32, 3, 128))  # t51: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t52 = ltorch.permute(t51, 0, 2, 3, 1, 4)  # t52: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t52 = prims.transpose(t51, (0, 2, 3, 1, 4))  # t52: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (res, k, v) = ltorch.split(t52, (1, 1, 1), 2)
    # res = prims.slice_prim(t52, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # res: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # k = prims.slice_prim(t52, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # k: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # v = prims.slice_prim(t52, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # v: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  q = ltorch.reshape(res, 1, -1, 512, 128)  # q: "cuda:0 bf16[1, 32, 512, 128]"
    # q = prims.reshape(res, (1, 32, 512, 128))  # q: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t57 = ltorch.reshape(k, 1, -1, 512, 128)  # t57: "cuda:0 bf16[1, 32, 512, 128]"
    # t57 = prims.reshape(k, (1, 32, 512, 128))  # t57: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t58 = ltorch.reshape(v, 1, -1, 512, 128)  # t58: "cuda:0 bf16[1, 32, 512, 128]"
    # t58 = prims.reshape(v, (1, 32, 512, 128))  # t58: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t60 = ltorch.getitem(q, (..., slice(None, 128, None)))  # t60: "cuda:0 bf16[1, 32, 512, 128]"
    # t60 = prims.slice_prim(q, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t60: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  x1 = ltorch.getitem(t60, (..., slice(None, 64, None)))  # x1: "cuda:0 bf16[1, 32, 512, 64]"
    # x1 = prims.slice_prim(t60, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # x1: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  x2 = ltorch.getitem(t60, (..., slice(64, None, None)))  # x2: "cuda:0 bf16[1, 32, 512, 64]"
    # x2 = prims.slice_prim(t60, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # x2: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t65 = ltorch.neg(x2)  # t65: "cuda:0 bf16[1, 32, 512, 64]"
    # t63 = prims.convert_element_type(x2, dtypes.float32)  # t63: "cuda:0 f32[1, 32, 512, 64]"
    # t64 = prims.neg(t63)  # t64: "cuda:0 f32[1, 32, 512, 64]"
    # t65 = prims.convert_element_type(t64, dtypes.bfloat16)  # t65: "cuda:0 bf16[1, 32, 512, 64]"
  rotated = ltorch.cat((t65, x1), -1)  # rotated: "cuda:0 bf16[1, 32, 512, 128]"
    # rotated = prims.cat((t65, x1), -1)  # rotated: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t69 = ltorch.mul(t60, cos)  # t69: "cuda:0 f32[1, 32, 512, 128]"
    # t67 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t67: "cuda:0 f32[1, 32, 512, 128]"
    # t68 = prims.convert_element_type(t60, dtypes.float32)  # t68: "cuda:0 f32[1, 32, 512, 128]"
    # t69 = prims.mul(t68, t67)  # t69: "cuda:0 f32[1, 32, 512, 128]"
  t72 = ltorch.mul(rotated, sin)  # t72: "cuda:0 f32[1, 32, 512, 128]"
    # t70 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t70: "cuda:0 f32[1, 32, 512, 128]"
    # t71 = prims.convert_element_type(rotated, dtypes.float32)  # t71: "cuda:0 f32[1, 32, 512, 128]"
    # t72 = prims.mul(t71, t70)  # t72: "cuda:0 f32[1, 32, 512, 128]"
  roped = ltorch.add(t69, t72, alpha=None)  # roped: "cuda:0 f32[1, 32, 512, 128]"
    # roped = prims.add(t69, t72)  # roped: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  q_roped = ltorch.to(roped, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # q_roped: "cuda:0 bf16[1, 32, 512, 128]"
    # q_roped = prims.convert_element_type(roped, dtypes.bfloat16)  # q_roped: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t75 = ltorch.getitem(t57, (..., slice(None, 128, None)))  # t75: "cuda:0 bf16[1, 32, 512, 128]"
    # t75 = prims.slice_prim(t57, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t75: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t76 = ltorch.getitem(t75, (..., slice(None, 64, None)))  # t76: "cuda:0 bf16[1, 32, 512, 64]"
    # t76 = prims.slice_prim(t75, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t76: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  tos = ltorch.getitem(t75, (..., slice(64, None, None)))  # tos: "cuda:0 bf16[1, 32, 512, 64]"
    # tos = prims.slice_prim(t75, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # tos: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t80 = ltorch.neg(tos)  # t80: "cuda:0 bf16[1, 32, 512, 64]"
    # t78 = prims.convert_element_type(tos, dtypes.float32)  # t78: "cuda:0 f32[1, 32, 512, 64]"
    # t79 = prims.neg(t78)  # t79: "cuda:0 f32[1, 32, 512, 64]"
    # t80 = prims.convert_element_type(t79, dtypes.bfloat16)  # t80: "cuda:0 bf16[1, 32, 512, 64]"
  t81 = ltorch.cat((t80, t76), -1)  # t81: "cuda:0 bf16[1, 32, 512, 128]"
    # t81 = prims.cat((t80, t76), -1)  # t81: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t84 = ltorch.mul(t75, cos)  # t84: "cuda:0 f32[1, 32, 512, 128]"
    # t82 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t82: "cuda:0 f32[1, 32, 512, 128]"
    # t83 = prims.convert_element_type(t75, dtypes.float32)  # t83: "cuda:0 f32[1, 32, 512, 128]"
    # t84 = prims.mul(t83, t82)  # t84: "cuda:0 f32[1, 32, 512, 128]"
  t87 = ltorch.mul(t81, sin)  # t87: "cuda:0 f32[1, 32, 512, 128]"
    # t85 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t85: "cuda:0 f32[1, 32, 512, 128]"
    # t86 = prims.convert_element_type(t81, dtypes.float32)  # t86: "cuda:0 f32[1, 32, 512, 128]"
    # t87 = prims.mul(t86, t85)  # t87: "cuda:0 f32[1, 32, 512, 128]"
  t88 = ltorch.add(t84, t87, alpha=None)  # t88: "cuda:0 f32[1, 32, 512, 128]"
    # t88 = prims.add(t84, t87)  # t88: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  k_roped = ltorch.to(t88, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # k_roped: "cuda:0 bf16[1, 32, 512, 128]"
    # k_roped = prims.convert_element_type(t88, dtypes.bfloat16)  # k_roped: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t90 = ltorch.getitem(q, (..., slice(128, None, None)))  # t90: "cuda:0 bf16[1, 32, 512, 0]"
    # t90 = prims.slice_prim(q, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t90: "cuda:0 bf16[1, 32, 512, 0]"
  t91 = ltorch.cat((q_roped, t90), -1)  # t91: "cuda:0 bf16[1, 32, 512, 128]"
    # t91 = prims.cat((q_roped, t90), -1)  # t91: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t92 = ltorch.getitem(t57, (..., slice(128, None, None)))  # t92: "cuda:0 bf16[1, 32, 512, 0]"
    # t92 = prims.slice_prim(t57, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t92: "cuda:0 bf16[1, 32, 512, 0]"
  t93 = ltorch.cat((k_roped, t92), -1)  # t93: "cuda:0 bf16[1, 32, 512, 128]"
    # t93 = prims.cat((k_roped, t92), -1)  # t93: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  y = ltorch.scaled_dot_product_attention(t91, t93, t58, None, 0.0, True, scale=0.08838834764831843)  # y: "cuda:0 bf16[1, 32, 512, 128]"
    # t96 = ltorch.mul(t91, 0.29730177875068026)  # t96: "cuda:0 bf16[1, 32, 512, 128]"
      # t94 = prims.convert_element_type(t91, dtypes.float32)  # t94: "cuda:0 f32[1, 32, 512, 128]"
      # t95 = prims.mul(t94, 0.29730177875068026)  # t95: "cuda:0 f32[1, 32, 512, 128]"
      # t96 = prims.convert_element_type(t95, dtypes.bfloat16)  # t96: "cuda:0 bf16[1, 32, 512, 128]"
    # t97 = ltorch.transpose(t93, -2, -1)  # t97: "cuda:0 bf16[1, 32, 128, 512]"
      # t97 = prims.transpose(t93, (0, 1, 3, 2))  # t97: "cuda:0 bf16[1, 32, 128, 512]"
    # t100 = ltorch.mul(t97, 0.29730177875068026)  # t100: "cuda:0 bf16[1, 32, 128, 512]"
      # t98 = prims.convert_element_type(t97, dtypes.float32)  # t98: "cuda:0 f32[1, 32, 128, 512]"
      # t99 = prims.mul(t98, 0.29730177875068026)  # t99: "cuda:0 f32[1, 32, 128, 512]"
      # t100 = prims.convert_element_type(t99, dtypes.bfloat16)  # t100: "cuda:0 bf16[1, 32, 128, 512]"
    # t101 = ltorch.matmul(t96, t100)  # t101: "cuda:0 bf16[1, 32, 512, 512]"
      # t101 = prims.matmul(t96, t100)  # t101: "cuda:0 bf16[1, 32, 512, 512]"
    # t111 = ltorch.tril(t101, 0, fill_value=-float('inf'))  # t111: "cuda:0 bf16[1, 32, 512, 512]"
      # t102 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t102: "cuda:0 i64[512]"
        # t102 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t102: "cuda:0 i64[512]"
      # t103 = ltorch.unsqueeze(t102, -1)  # t103: "cuda:0 i64[512, 1]"
        # t103 = prims.broadcast_in_dim(t102, [512, 1], [0])  # t103: "cuda:0 i64[512, 1]"
      # t104 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t104: "cuda:0 i64[512]"
        # t104 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t104: "cuda:0 i64[512]"
      # t105 = ltorch.unsqueeze(t104, -2)  # t105: "cuda:0 i64[1, 512]"
        # t105 = prims.broadcast_in_dim(t104, [1, 512], [1])  # t105: "cuda:0 i64[1, 512]"
      # t106 = ltorch.add(t103, 0, alpha=None)  # t106: "cuda:0 i64[512, 1]"
        # t106 = prims.add(t103, 0)  # t106: "cuda:0 i64[512, 1]"
      # t109 = ltorch.ge(t106, t105)  # t109: "cuda:0 b8[512, 512]"
        # t107 = prims.broadcast_in_dim(t106, (512, 512), (0, 1))  # t107: "cuda:0 i64[512, 512]"
        # t108 = prims.broadcast_in_dim(t105, (512, 512), (0, 1))  # t108: "cuda:0 i64[512, 512]"
        # t109 = prims.ge(t107, t108)  # t109: "cuda:0 b8[512, 512]"
      # t111 = ltorch.where(t109, t101, -float('inf'))  # t111: "cuda:0 bf16[1, 32, 512, 512]"
        # t110 = prims.broadcast_in_dim(t109, (1, 32, 512, 512), (2, 3))  # t110: "cuda:0 b8[1, 32, 512, 512]"
        # t111 = prims.where(t110, t101, -float('inf'))  # t111: "cuda:0 bf16[1, 32, 512, 512]"
    # t122 = ltorch._softmax(t111, -1, dtype=None)  # t122: "cuda:0 bf16[1, 32, 512, 512]"
      # t112 = ltorch.to(t111, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t112: "cuda:0 f32[1, 32, 512, 512]"
        # t112 = prims.convert_element_type(t111, dtypes.float32)  # t112: "cuda:0 f32[1, 32, 512, 512]"
      # t114 = ltorch.amax(t112, -1, True)  # t114: "cuda:0 f32[1, 32, 512, 1]"
        # t113 = prims.amax(t112, (3,))  # t113: "cuda:0 f32[1, 32, 512]"
        # t114 = prims.broadcast_in_dim(t113, [1, 32, 512, 1], [0, 1, 2])  # t114: "cuda:0 f32[1, 32, 512, 1]"
      # t116 = ltorch.sub(t112, t114, alpha=None)  # t116: "cuda:0 f32[1, 32, 512, 512]"
        # t115 = prims.broadcast_in_dim(t114, (1, 32, 512, 512), (0, 1, 2, 3))  # t115: "cuda:0 f32[1, 32, 512, 512]"
        # t116 = prims.sub(t112, t115)  # t116: "cuda:0 f32[1, 32, 512, 512]"
      # t117 = ltorch.exp(t116)  # t117: "cuda:0 f32[1, 32, 512, 512]"
        # t117 = prims.exp(t116)  # t117: "cuda:0 f32[1, 32, 512, 512]"
      # t119 = ltorch.sum(t117, -1, True, dtype=None)  # t119: "cuda:0 f32[1, 32, 512, 1]"
        # t118 = prims.sum(t117, (3,))  # t118: "cuda:0 f32[1, 32, 512]"
        # t119 = prims.broadcast_in_dim(t118, [1, 32, 512, 1], [0, 1, 2])  # t119: "cuda:0 f32[1, 32, 512, 1]"
      # t121 = ltorch.true_divide(t117, t119)  # t121: "cuda:0 f32[1, 32, 512, 512]"
        # t120 = prims.broadcast_in_dim(t119, (1, 32, 512, 512), (0, 1, 2, 3))  # t120: "cuda:0 f32[1, 32, 512, 512]"
        # t121 = prims.div(t117, t120)  # t121: "cuda:0 f32[1, 32, 512, 512]"
      # t122 = ltorch.to(t121, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t122: "cuda:0 bf16[1, 32, 512, 512]"
        # t122 = prims.convert_element_type(t121, dtypes.bfloat16)  # t122: "cuda:0 bf16[1, 32, 512, 512]"
    # y = ltorch.matmul(t122, t58)  # y: "cuda:0 bf16[1, 32, 512, 128]"
      # y = prims.matmul(t122, t58)  # y: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t124 = ltorch.transpose(y, 1, 2)  # t124: "cuda:0 bf16[1, 512, 32, 128]"
    # t124 = prims.transpose(y, (0, 2, 1, 3))  # t124: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t125 = ltorch.reshape(t124, 1, 512, 4096)  # t125: "cuda:0 bf16[1, 512, 4096]"
    # t125 = prims.reshape(t124, (1, 512, 4096))  # t125: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  attention_output = ltorch.linear(t125, t_transformer_h_0_attn_proj_weight, None)  # attention_output: "cuda:0 bf16[1, 512, 4096]"
    # attention_output = prims.linear(t125, t_transformer_h_0_attn_proj_weight, None)  # attention_output: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t134 = ltorch.add(attention_output, x, alpha=None)  # t134: "cuda:0 bf16[1, 512, 4096]"
    # t131 = prims.convert_element_type(attention_output, dtypes.float32)  # t131: "cuda:0 f32[1, 512, 4096]"
    # t132 = prims.convert_element_type(x, dtypes.float32)  # t132: "cuda:0 f32[1, 512, 4096]"
    # t133 = prims.add(t131, t132)  # t133: "cuda:0 f32[1, 512, 4096]"
    # t134 = prims.convert_element_type(t133, dtypes.bfloat16)  # t134: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t135 = prims.convert_element_type(t134, dtypes.float32)  # t135: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t136 = ltorch.mul(t135, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
    # t136 = prims.mul(t135, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
  t140 = ltorch.mean(t136, -1, True, dtype=None)  # t140: "cuda:0 f32[1, 512, 1]"
    # t138 = prims.sum(t136, (2,))  # t138: "cuda:0 f32[1, 512]"
    # t139 = prims.broadcast_in_dim(t138, [1, 512, 1], [0, 1])  # t139: "cuda:0 f32[1, 512, 1]"
    # t140 = ltorch.true_divide(t139, 4096)  # t140: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t140 = prims.div(t139, 4096.0)  # t140: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t142 = ltorch.add(t140, 1e-05, alpha=None)  # t142: "cuda:0 f32[1, 512, 1]"
    # t142 = prims.add(t140, 1e-05)  # t142: "cuda:0 f32[1, 512, 1]"
  t143 = ltorch.rsqrt(t142)  # t143: "cuda:0 f32[1, 512, 1]"
    # t143 = prims.rsqrt(t142)  # t143: "cuda:0 f32[1, 512, 1]"
  t145 = ltorch.mul(t135, t143)  # t145: "cuda:0 f32[1, 512, 4096]"
    # t144 = prims.broadcast_in_dim(t143, (1, 512, 4096), (0, 1, 2))  # t144: "cuda:0 f32[1, 512, 4096]"
    # t145 = prims.mul(t135, t144)  # t145: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t146 = ltorch.to(t145, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t146: "cuda:0 bf16[1, 512, 4096]"
    # t146 = prims.convert_element_type(t145, dtypes.bfloat16)  # t146: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t156 = ltorch.mul(t146, t_transformer_h_0_norm_2_weight)  # t156: "cuda:0 bf16[1, 512, 4096]"
    # t152 = prims.broadcast_in_dim(t_transformer_h_0_norm_2_weight, (1, 512, 4096), (2,))  # t152: "cuda:0 bf16[1, 512, 4096]"
    # t153 = prims.convert_element_type(t146, dtypes.float32)  # t153: "cuda:0 f32[1, 512, 4096]"
    # t154 = prims.convert_element_type(t152, dtypes.float32)  # t154: "cuda:0 f32[1, 512, 4096]"
    # t155 = prims.mul(t153, t154)  # t155: "cuda:0 f32[1, 512, 4096]"
    # t156 = prims.convert_element_type(t155, dtypes.bfloat16)  # t156: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(t156, t_transformer_h_0_mlp_fc_1_weight, None)  # x_fc_1: "cuda:0 bf16[1, 512, 11008]"
    # x_fc_1 = prims.linear(t156, t_transformer_h_0_mlp_fc_1_weight, None)  # x_fc_1: "cuda:0 bf16[1, 512, 11008]"
  x_fc_2 = ltorch.linear(t156, t_transformer_h_0_mlp_fc_2_weight, None)  # x_fc_2: "cuda:0 bf16[1, 512, 11008]"
    # x_fc_2 = prims.linear(t156, t_transformer_h_0_mlp_fc_2_weight, None)  # x_fc_2: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t175 = ltorch.silu(x_fc_1, False)  # t175: "cuda:0 bf16[1, 512, 11008]"
    # t166 = prims.convert_element_type(x_fc_1, dtypes.float32)  # t166: "cuda:0 f32[1, 512, 11008]"
    # t167 = prims.neg(t166)  # t167: "cuda:0 f32[1, 512, 11008]"
    # t168 = prims.exp(t167)  # t168: "cuda:0 f32[1, 512, 11008]"
    # t169 = prims.add(1.0, t168)  # t169: "cuda:0 f32[1, 512, 11008]"
    # t170 = prims.reciprocal(t169)  # t170: "cuda:0 f32[1, 512, 11008]"
    # t171 = prims.convert_element_type(t170, dtypes.bfloat16)  # t171: "cuda:0 bf16[1, 512, 11008]"
    # t172 = prims.convert_element_type(x_fc_1, dtypes.float32)  # t172: "cuda:0 f32[1, 512, 11008]"
    # t173 = prims.convert_element_type(t171, dtypes.float32)  # t173: "cuda:0 f32[1, 512, 11008]"
    # t174 = prims.mul(t172, t173)  # t174: "cuda:0 f32[1, 512, 11008]"
    # t175 = prims.convert_element_type(t174, dtypes.bfloat16)  # t175: "cuda:0 bf16[1, 512, 11008]"
  t179 = ltorch.mul(t175, x_fc_2)  # t179: "cuda:0 bf16[1, 512, 11008]"
    # t176 = prims.convert_element_type(t175, dtypes.float32)  # t176: "cuda:0 f32[1, 512, 11008]"
    # t177 = prims.convert_element_type(x_fc_2, dtypes.float32)  # t177: "cuda:0 f32[1, 512, 11008]"
    # t178 = prims.mul(t176, t177)  # t178: "cuda:0 f32[1, 512, 11008]"
    # t179 = prims.convert_element_type(t178, dtypes.bfloat16)  # t179: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t183 = ltorch.linear(t179, t_transformer_h_0_mlp_proj_weight, None)  # t183: "cuda:0 bf16[1, 512, 4096]"
    # t183 = prims.linear(t179, t_transformer_h_0_mlp_proj_weight, None)  # t183: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t187 = ltorch.add(t183, t134, alpha=None)  # t187: "cuda:0 bf16[1, 512, 4096]"
    # t184 = prims.convert_element_type(t183, dtypes.float32)  # t184: "cuda:0 f32[1, 512, 4096]"
    # t185 = prims.convert_element_type(t134, dtypes.float32)  # t185: "cuda:0 f32[1, 512, 4096]"
    # t186 = prims.add(t184, t185)  # t186: "cuda:0 f32[1, 512, 4096]"
    # t187 = prims.convert_element_type(t186, dtypes.bfloat16)  # t187: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t189 = prims.convert_element_type(t187, dtypes.float32)  # t189: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t190 = ltorch.mul(t189, t189)  # t190: "cuda:0 f32[1, 512, 4096]"
    # t190 = prims.mul(t189, t189)  # t190: "cuda:0 f32[1, 512, 4096]"
  t194 = ltorch.mean(t190, -1, True, dtype=None)  # t194: "cuda:0 f32[1, 512, 1]"
    # t192 = prims.sum(t190, (2,))  # t192: "cuda:0 f32[1, 512]"
    # t193 = prims.broadcast_in_dim(t192, [1, 512, 1], [0, 1])  # t193: "cuda:0 f32[1, 512, 1]"
    # t194 = ltorch.true_divide(t193, 4096)  # t194: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t194 = prims.div(t193, 4096.0)  # t194: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t196 = ltorch.add(t194, 1e-05, alpha=None)  # t196: "cuda:0 f32[1, 512, 1]"
    # t196 = prims.add(t194, 1e-05)  # t196: "cuda:0 f32[1, 512, 1]"
  t197 = ltorch.rsqrt(t196)  # t197: "cuda:0 f32[1, 512, 1]"
    # t197 = prims.rsqrt(t196)  # t197: "cuda:0 f32[1, 512, 1]"
  t199 = ltorch.mul(t189, t197)  # t199: "cuda:0 f32[1, 512, 4096]"
    # t198 = prims.broadcast_in_dim(t197, (1, 512, 4096), (0, 1, 2))  # t198: "cuda:0 f32[1, 512, 4096]"
    # t199 = prims.mul(t189, t198)  # t199: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t200 = ltorch.to(t199, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t200: "cuda:0 bf16[1, 512, 4096]"
    # t200 = prims.convert_element_type(t199, dtypes.bfloat16)  # t200: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t210 = ltorch.mul(t200, t_transformer_h_1_norm_1_weight)  # t210: "cuda:0 bf16[1, 512, 4096]"
    # t206 = prims.broadcast_in_dim(t_transformer_h_1_norm_1_weight, (1, 512, 4096), (2,))  # t206: "cuda:0 bf16[1, 512, 4096]"
    # t207 = prims.convert_element_type(t200, dtypes.float32)  # t207: "cuda:0 f32[1, 512, 4096]"
    # t208 = prims.convert_element_type(t206, dtypes.float32)  # t208: "cuda:0 f32[1, 512, 4096]"
    # t209 = prims.mul(t207, t208)  # t209: "cuda:0 f32[1, 512, 4096]"
    # t210 = prims.convert_element_type(t209, dtypes.bfloat16)  # t210: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t215 = ltorch.linear(t210, t_transformer_h_1_attn_attn_weight, None)  # t215: "cuda:0 bf16[1, 512, 12288]"
    # t215 = prims.linear(t210, t_transformer_h_1_attn_attn_weight, None)  # t215: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t216 = ltorch.view(t215, 1, 512, 32, 3, 128)  # t216: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t216 = ltorch.reshape(t215, (1, 512, 32, 3, 128))  # t216: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t216 = prims.reshape(t215, (1, 512, 32, 3, 128))  # t216: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t217 = ltorch.permute(t216, 0, 2, 3, 1, 4)  # t217: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t217 = prims.transpose(t216, (0, 2, 3, 1, 4))  # t217: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t218, t219, t220) = ltorch.split(t217, (1, 1, 1), 2)
    # t218 = prims.slice_prim(t217, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t218: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t219 = prims.slice_prim(t217, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t219: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t220 = prims.slice_prim(t217, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t220: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t221 = ltorch.reshape(t218, 1, -1, 512, 128)  # t221: "cuda:0 bf16[1, 32, 512, 128]"
    # t221 = prims.reshape(t218, (1, 32, 512, 128))  # t221: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t222 = ltorch.reshape(t219, 1, -1, 512, 128)  # t222: "cuda:0 bf16[1, 32, 512, 128]"
    # t222 = prims.reshape(t219, (1, 32, 512, 128))  # t222: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t223 = ltorch.reshape(t220, 1, -1, 512, 128)  # t223: "cuda:0 bf16[1, 32, 512, 128]"
    # t223 = prims.reshape(t220, (1, 32, 512, 128))  # t223: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t224 = ltorch.getitem(t221, (..., slice(None, 128, None)))  # t224: "cuda:0 bf16[1, 32, 512, 128]"
    # t224 = prims.slice_prim(t221, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t224: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t225 = ltorch.getitem(t224, (..., slice(None, 64, None)))  # t225: "cuda:0 bf16[1, 32, 512, 64]"
    # t225 = prims.slice_prim(t224, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t225: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t226 = ltorch.getitem(t224, (..., slice(64, None, None)))  # t226: "cuda:0 bf16[1, 32, 512, 64]"
    # t226 = prims.slice_prim(t224, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t226: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t229 = ltorch.neg(t226)  # t229: "cuda:0 bf16[1, 32, 512, 64]"
    # t227 = prims.convert_element_type(t226, dtypes.float32)  # t227: "cuda:0 f32[1, 32, 512, 64]"
    # t228 = prims.neg(t227)  # t228: "cuda:0 f32[1, 32, 512, 64]"
    # t229 = prims.convert_element_type(t228, dtypes.bfloat16)  # t229: "cuda:0 bf16[1, 32, 512, 64]"
  t230 = ltorch.cat((t229, t225), -1)  # t230: "cuda:0 bf16[1, 32, 512, 128]"
    # t230 = prims.cat((t229, t225), -1)  # t230: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t233 = ltorch.mul(t224, cos)  # t233: "cuda:0 f32[1, 32, 512, 128]"
    # t231 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t231: "cuda:0 f32[1, 32, 512, 128]"
    # t232 = prims.convert_element_type(t224, dtypes.float32)  # t232: "cuda:0 f32[1, 32, 512, 128]"
    # t233 = prims.mul(t232, t231)  # t233: "cuda:0 f32[1, 32, 512, 128]"
  t236 = ltorch.mul(t230, sin)  # t236: "cuda:0 f32[1, 32, 512, 128]"
    # t234 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t234: "cuda:0 f32[1, 32, 512, 128]"
    # t235 = prims.convert_element_type(t230, dtypes.float32)  # t235: "cuda:0 f32[1, 32, 512, 128]"
    # t236 = prims.mul(t235, t234)  # t236: "cuda:0 f32[1, 32, 512, 128]"
  t237 = ltorch.add(t233, t236, alpha=None)  # t237: "cuda:0 f32[1, 32, 512, 128]"
    # t237 = prims.add(t233, t236)  # t237: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t238 = ltorch.to(t237, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t238: "cuda:0 bf16[1, 32, 512, 128]"
    # t238 = prims.convert_element_type(t237, dtypes.bfloat16)  # t238: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t239 = ltorch.getitem(t222, (..., slice(None, 128, None)))  # t239: "cuda:0 bf16[1, 32, 512, 128]"
    # t239 = prims.slice_prim(t222, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t239: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t240 = ltorch.getitem(t239, (..., slice(None, 64, None)))  # t240: "cuda:0 bf16[1, 32, 512, 64]"
    # t240 = prims.slice_prim(t239, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t240: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t241 = ltorch.getitem(t239, (..., slice(64, None, None)))  # t241: "cuda:0 bf16[1, 32, 512, 64]"
    # t241 = prims.slice_prim(t239, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t241: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t244 = ltorch.neg(t241)  # t244: "cuda:0 bf16[1, 32, 512, 64]"
    # t242 = prims.convert_element_type(t241, dtypes.float32)  # t242: "cuda:0 f32[1, 32, 512, 64]"
    # t243 = prims.neg(t242)  # t243: "cuda:0 f32[1, 32, 512, 64]"
    # t244 = prims.convert_element_type(t243, dtypes.bfloat16)  # t244: "cuda:0 bf16[1, 32, 512, 64]"
  t245 = ltorch.cat((t244, t240), -1)  # t245: "cuda:0 bf16[1, 32, 512, 128]"
    # t245 = prims.cat((t244, t240), -1)  # t245: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t248 = ltorch.mul(t239, cos)  # t248: "cuda:0 f32[1, 32, 512, 128]"
    # t246 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t246: "cuda:0 f32[1, 32, 512, 128]"
    # t247 = prims.convert_element_type(t239, dtypes.float32)  # t247: "cuda:0 f32[1, 32, 512, 128]"
    # t248 = prims.mul(t247, t246)  # t248: "cuda:0 f32[1, 32, 512, 128]"
  t251 = ltorch.mul(t245, sin)  # t251: "cuda:0 f32[1, 32, 512, 128]"
    # t249 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t249: "cuda:0 f32[1, 32, 512, 128]"
    # t250 = prims.convert_element_type(t245, dtypes.float32)  # t250: "cuda:0 f32[1, 32, 512, 128]"
    # t251 = prims.mul(t250, t249)  # t251: "cuda:0 f32[1, 32, 512, 128]"
  t252 = ltorch.add(t248, t251, alpha=None)  # t252: "cuda:0 f32[1, 32, 512, 128]"
    # t252 = prims.add(t248, t251)  # t252: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t253 = ltorch.to(t252, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t253: "cuda:0 bf16[1, 32, 512, 128]"
    # t253 = prims.convert_element_type(t252, dtypes.bfloat16)  # t253: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t254 = ltorch.getitem(t221, (..., slice(128, None, None)))  # t254: "cuda:0 bf16[1, 32, 512, 0]"
    # t254 = prims.slice_prim(t221, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t254: "cuda:0 bf16[1, 32, 512, 0]"
  t255 = ltorch.cat((t238, t254), -1)  # t255: "cuda:0 bf16[1, 32, 512, 128]"
    # t255 = prims.cat((t238, t254), -1)  # t255: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t256 = ltorch.getitem(t222, (..., slice(128, None, None)))  # t256: "cuda:0 bf16[1, 32, 512, 0]"
    # t256 = prims.slice_prim(t222, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t256: "cuda:0 bf16[1, 32, 512, 0]"
  t257 = ltorch.cat((t253, t256), -1)  # t257: "cuda:0 bf16[1, 32, 512, 128]"
    # t257 = prims.cat((t253, t256), -1)  # t257: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t287 = ltorch.scaled_dot_product_attention(t255, t257, t223, None, 0.0, True, scale=0.08838834764831843)  # t287: "cuda:0 bf16[1, 32, 512, 128]"
    # t260 = ltorch.mul(t255, 0.29730177875068026)  # t260: "cuda:0 bf16[1, 32, 512, 128]"
      # t258 = prims.convert_element_type(t255, dtypes.float32)  # t258: "cuda:0 f32[1, 32, 512, 128]"
      # t259 = prims.mul(t258, 0.29730177875068026)  # t259: "cuda:0 f32[1, 32, 512, 128]"
      # t260 = prims.convert_element_type(t259, dtypes.bfloat16)  # t260: "cuda:0 bf16[1, 32, 512, 128]"
    # t261 = ltorch.transpose(t257, -2, -1)  # t261: "cuda:0 bf16[1, 32, 128, 512]"
      # t261 = prims.transpose(t257, (0, 1, 3, 2))  # t261: "cuda:0 bf16[1, 32, 128, 512]"
    # t264 = ltorch.mul(t261, 0.29730177875068026)  # t264: "cuda:0 bf16[1, 32, 128, 512]"
      # t262 = prims.convert_element_type(t261, dtypes.float32)  # t262: "cuda:0 f32[1, 32, 128, 512]"
      # t263 = prims.mul(t262, 0.29730177875068026)  # t263: "cuda:0 f32[1, 32, 128, 512]"
      # t264 = prims.convert_element_type(t263, dtypes.bfloat16)  # t264: "cuda:0 bf16[1, 32, 128, 512]"
    # t265 = ltorch.matmul(t260, t264)  # t265: "cuda:0 bf16[1, 32, 512, 512]"
      # t265 = prims.matmul(t260, t264)  # t265: "cuda:0 bf16[1, 32, 512, 512]"
    # t275 = ltorch.tril(t265, 0, fill_value=-float('inf'))  # t275: "cuda:0 bf16[1, 32, 512, 512]"
      # t266 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t266: "cuda:0 i64[512]"
        # t266 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t266: "cuda:0 i64[512]"
      # t267 = ltorch.unsqueeze(t266, -1)  # t267: "cuda:0 i64[512, 1]"
        # t267 = prims.broadcast_in_dim(t266, [512, 1], [0])  # t267: "cuda:0 i64[512, 1]"
      # t268 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t268: "cuda:0 i64[512]"
        # t268 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t268: "cuda:0 i64[512]"
      # t269 = ltorch.unsqueeze(t268, -2)  # t269: "cuda:0 i64[1, 512]"
        # t269 = prims.broadcast_in_dim(t268, [1, 512], [1])  # t269: "cuda:0 i64[1, 512]"
      # t270 = ltorch.add(t267, 0, alpha=None)  # t270: "cuda:0 i64[512, 1]"
        # t270 = prims.add(t267, 0)  # t270: "cuda:0 i64[512, 1]"
      # t273 = ltorch.ge(t270, t269)  # t273: "cuda:0 b8[512, 512]"
        # t271 = prims.broadcast_in_dim(t270, (512, 512), (0, 1))  # t271: "cuda:0 i64[512, 512]"
        # t272 = prims.broadcast_in_dim(t269, (512, 512), (0, 1))  # t272: "cuda:0 i64[512, 512]"
        # t273 = prims.ge(t271, t272)  # t273: "cuda:0 b8[512, 512]"
      # t275 = ltorch.where(t273, t265, -float('inf'))  # t275: "cuda:0 bf16[1, 32, 512, 512]"
        # t274 = prims.broadcast_in_dim(t273, (1, 32, 512, 512), (2, 3))  # t274: "cuda:0 b8[1, 32, 512, 512]"
        # t275 = prims.where(t274, t265, -float('inf'))  # t275: "cuda:0 bf16[1, 32, 512, 512]"
    # t286 = ltorch._softmax(t275, -1, dtype=None)  # t286: "cuda:0 bf16[1, 32, 512, 512]"
      # t276 = ltorch.to(t275, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t276: "cuda:0 f32[1, 32, 512, 512]"
        # t276 = prims.convert_element_type(t275, dtypes.float32)  # t276: "cuda:0 f32[1, 32, 512, 512]"
      # t278 = ltorch.amax(t276, -1, True)  # t278: "cuda:0 f32[1, 32, 512, 1]"
        # t277 = prims.amax(t276, (3,))  # t277: "cuda:0 f32[1, 32, 512]"
        # t278 = prims.broadcast_in_dim(t277, [1, 32, 512, 1], [0, 1, 2])  # t278: "cuda:0 f32[1, 32, 512, 1]"
      # t280 = ltorch.sub(t276, t278, alpha=None)  # t280: "cuda:0 f32[1, 32, 512, 512]"
        # t279 = prims.broadcast_in_dim(t278, (1, 32, 512, 512), (0, 1, 2, 3))  # t279: "cuda:0 f32[1, 32, 512, 512]"
        # t280 = prims.sub(t276, t279)  # t280: "cuda:0 f32[1, 32, 512, 512]"
      # t281 = ltorch.exp(t280)  # t281: "cuda:0 f32[1, 32, 512, 512]"
        # t281 = prims.exp(t280)  # t281: "cuda:0 f32[1, 32, 512, 512]"
      # t283 = ltorch.sum(t281, -1, True, dtype=None)  # t283: "cuda:0 f32[1, 32, 512, 1]"
        # t282 = prims.sum(t281, (3,))  # t282: "cuda:0 f32[1, 32, 512]"
        # t283 = prims.broadcast_in_dim(t282, [1, 32, 512, 1], [0, 1, 2])  # t283: "cuda:0 f32[1, 32, 512, 1]"
      # t285 = ltorch.true_divide(t281, t283)  # t285: "cuda:0 f32[1, 32, 512, 512]"
        # t284 = prims.broadcast_in_dim(t283, (1, 32, 512, 512), (0, 1, 2, 3))  # t284: "cuda:0 f32[1, 32, 512, 512]"
        # t285 = prims.div(t281, t284)  # t285: "cuda:0 f32[1, 32, 512, 512]"
      # t286 = ltorch.to(t285, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t286: "cuda:0 bf16[1, 32, 512, 512]"
        # t286 = prims.convert_element_type(t285, dtypes.bfloat16)  # t286: "cuda:0 bf16[1, 32, 512, 512]"
    # t287 = ltorch.matmul(t286, t223)  # t287: "cuda:0 bf16[1, 32, 512, 128]"
      # t287 = prims.matmul(t286, t223)  # t287: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t288 = ltorch.transpose(t287, 1, 2)  # t288: "cuda:0 bf16[1, 512, 32, 128]"
    # t288 = prims.transpose(t287, (0, 2, 1, 3))  # t288: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t289 = ltorch.reshape(t288, 1, 512, 4096)  # t289: "cuda:0 bf16[1, 512, 4096]"
    # t289 = prims.reshape(t288, (1, 512, 4096))  # t289: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t293 = ltorch.linear(t289, t_transformer_h_1_attn_proj_weight, None)  # t293: "cuda:0 bf16[1, 512, 4096]"
    # t293 = prims.linear(t289, t_transformer_h_1_attn_proj_weight, None)  # t293: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t297 = ltorch.add(t293, t187, alpha=None)  # t297: "cuda:0 bf16[1, 512, 4096]"
    # t294 = prims.convert_element_type(t293, dtypes.float32)  # t294: "cuda:0 f32[1, 512, 4096]"
    # t295 = prims.convert_element_type(t187, dtypes.float32)  # t295: "cuda:0 f32[1, 512, 4096]"
    # t296 = prims.add(t294, t295)  # t296: "cuda:0 f32[1, 512, 4096]"
    # t297 = prims.convert_element_type(t296, dtypes.bfloat16)  # t297: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t298 = prims.convert_element_type(t297, dtypes.float32)  # t298: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t299 = ltorch.mul(t298, t298)  # t299: "cuda:0 f32[1, 512, 4096]"
    # t299 = prims.mul(t298, t298)  # t299: "cuda:0 f32[1, 512, 4096]"
  t303 = ltorch.mean(t299, -1, True, dtype=None)  # t303: "cuda:0 f32[1, 512, 1]"
    # t301 = prims.sum(t299, (2,))  # t301: "cuda:0 f32[1, 512]"
    # t302 = prims.broadcast_in_dim(t301, [1, 512, 1], [0, 1])  # t302: "cuda:0 f32[1, 512, 1]"
    # t303 = ltorch.true_divide(t302, 4096)  # t303: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t303 = prims.div(t302, 4096.0)  # t303: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t305 = ltorch.add(t303, 1e-05, alpha=None)  # t305: "cuda:0 f32[1, 512, 1]"
    # t305 = prims.add(t303, 1e-05)  # t305: "cuda:0 f32[1, 512, 1]"
  t306 = ltorch.rsqrt(t305)  # t306: "cuda:0 f32[1, 512, 1]"
    # t306 = prims.rsqrt(t305)  # t306: "cuda:0 f32[1, 512, 1]"
  t308 = ltorch.mul(t298, t306)  # t308: "cuda:0 f32[1, 512, 4096]"
    # t307 = prims.broadcast_in_dim(t306, (1, 512, 4096), (0, 1, 2))  # t307: "cuda:0 f32[1, 512, 4096]"
    # t308 = prims.mul(t298, t307)  # t308: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t309 = ltorch.to(t308, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t309: "cuda:0 bf16[1, 512, 4096]"
    # t309 = prims.convert_element_type(t308, dtypes.bfloat16)  # t309: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t319 = ltorch.mul(t309, t_transformer_h_1_norm_2_weight)  # t319: "cuda:0 bf16[1, 512, 4096]"
    # t315 = prims.broadcast_in_dim(t_transformer_h_1_norm_2_weight, (1, 512, 4096), (2,))  # t315: "cuda:0 bf16[1, 512, 4096]"
    # t316 = prims.convert_element_type(t309, dtypes.float32)  # t316: "cuda:0 f32[1, 512, 4096]"
    # t317 = prims.convert_element_type(t315, dtypes.float32)  # t317: "cuda:0 f32[1, 512, 4096]"
    # t318 = prims.mul(t316, t317)  # t318: "cuda:0 f32[1, 512, 4096]"
    # t319 = prims.convert_element_type(t318, dtypes.bfloat16)  # t319: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t324 = ltorch.linear(t319, t_transformer_h_1_mlp_fc_1_weight, None)  # t324: "cuda:0 bf16[1, 512, 11008]"
    # t324 = prims.linear(t319, t_transformer_h_1_mlp_fc_1_weight, None)  # t324: "cuda:0 bf16[1, 512, 11008]"
  t328 = ltorch.linear(t319, t_transformer_h_1_mlp_fc_2_weight, None)  # t328: "cuda:0 bf16[1, 512, 11008]"
    # t328 = prims.linear(t319, t_transformer_h_1_mlp_fc_2_weight, None)  # t328: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t338 = ltorch.silu(t324, False)  # t338: "cuda:0 bf16[1, 512, 11008]"
    # t329 = prims.convert_element_type(t324, dtypes.float32)  # t329: "cuda:0 f32[1, 512, 11008]"
    # t330 = prims.neg(t329)  # t330: "cuda:0 f32[1, 512, 11008]"
    # t331 = prims.exp(t330)  # t331: "cuda:0 f32[1, 512, 11008]"
    # t332 = prims.add(1.0, t331)  # t332: "cuda:0 f32[1, 512, 11008]"
    # t333 = prims.reciprocal(t332)  # t333: "cuda:0 f32[1, 512, 11008]"
    # t334 = prims.convert_element_type(t333, dtypes.bfloat16)  # t334: "cuda:0 bf16[1, 512, 11008]"
    # t335 = prims.convert_element_type(t324, dtypes.float32)  # t335: "cuda:0 f32[1, 512, 11008]"
    # t336 = prims.convert_element_type(t334, dtypes.float32)  # t336: "cuda:0 f32[1, 512, 11008]"
    # t337 = prims.mul(t335, t336)  # t337: "cuda:0 f32[1, 512, 11008]"
    # t338 = prims.convert_element_type(t337, dtypes.bfloat16)  # t338: "cuda:0 bf16[1, 512, 11008]"
  t342 = ltorch.mul(t338, t328)  # t342: "cuda:0 bf16[1, 512, 11008]"
    # t339 = prims.convert_element_type(t338, dtypes.float32)  # t339: "cuda:0 f32[1, 512, 11008]"
    # t340 = prims.convert_element_type(t328, dtypes.float32)  # t340: "cuda:0 f32[1, 512, 11008]"
    # t341 = prims.mul(t339, t340)  # t341: "cuda:0 f32[1, 512, 11008]"
    # t342 = prims.convert_element_type(t341, dtypes.bfloat16)  # t342: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t346 = ltorch.linear(t342, t_transformer_h_1_mlp_proj_weight, None)  # t346: "cuda:0 bf16[1, 512, 4096]"
    # t346 = prims.linear(t342, t_transformer_h_1_mlp_proj_weight, None)  # t346: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t350 = ltorch.add(t346, t297, alpha=None)  # t350: "cuda:0 bf16[1, 512, 4096]"
    # t347 = prims.convert_element_type(t346, dtypes.float32)  # t347: "cuda:0 f32[1, 512, 4096]"
    # t348 = prims.convert_element_type(t297, dtypes.float32)  # t348: "cuda:0 f32[1, 512, 4096]"
    # t349 = prims.add(t347, t348)  # t349: "cuda:0 f32[1, 512, 4096]"
    # t350 = prims.convert_element_type(t349, dtypes.bfloat16)  # t350: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t352 = prims.convert_element_type(t350, dtypes.float32)  # t352: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t353 = ltorch.mul(t352, t352)  # t353: "cuda:0 f32[1, 512, 4096]"
    # t353 = prims.mul(t352, t352)  # t353: "cuda:0 f32[1, 512, 4096]"
  t357 = ltorch.mean(t353, -1, True, dtype=None)  # t357: "cuda:0 f32[1, 512, 1]"
    # t355 = prims.sum(t353, (2,))  # t355: "cuda:0 f32[1, 512]"
    # t356 = prims.broadcast_in_dim(t355, [1, 512, 1], [0, 1])  # t356: "cuda:0 f32[1, 512, 1]"
    # t357 = ltorch.true_divide(t356, 4096)  # t357: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t357 = prims.div(t356, 4096.0)  # t357: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t359 = ltorch.add(t357, 1e-05, alpha=None)  # t359: "cuda:0 f32[1, 512, 1]"
    # t359 = prims.add(t357, 1e-05)  # t359: "cuda:0 f32[1, 512, 1]"
  t360 = ltorch.rsqrt(t359)  # t360: "cuda:0 f32[1, 512, 1]"
    # t360 = prims.rsqrt(t359)  # t360: "cuda:0 f32[1, 512, 1]"
  t362 = ltorch.mul(t352, t360)  # t362: "cuda:0 f32[1, 512, 4096]"
    # t361 = prims.broadcast_in_dim(t360, (1, 512, 4096), (0, 1, 2))  # t361: "cuda:0 f32[1, 512, 4096]"
    # t362 = prims.mul(t352, t361)  # t362: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t363 = ltorch.to(t362, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t363: "cuda:0 bf16[1, 512, 4096]"
    # t363 = prims.convert_element_type(t362, dtypes.bfloat16)  # t363: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t373 = ltorch.mul(t363, t_transformer_h_2_norm_1_weight)  # t373: "cuda:0 bf16[1, 512, 4096]"
    # t369 = prims.broadcast_in_dim(t_transformer_h_2_norm_1_weight, (1, 512, 4096), (2,))  # t369: "cuda:0 bf16[1, 512, 4096]"
    # t370 = prims.convert_element_type(t363, dtypes.float32)  # t370: "cuda:0 f32[1, 512, 4096]"
    # t371 = prims.convert_element_type(t369, dtypes.float32)  # t371: "cuda:0 f32[1, 512, 4096]"
    # t372 = prims.mul(t370, t371)  # t372: "cuda:0 f32[1, 512, 4096]"
    # t373 = prims.convert_element_type(t372, dtypes.bfloat16)  # t373: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t378 = ltorch.linear(t373, t_transformer_h_2_attn_attn_weight, None)  # t378: "cuda:0 bf16[1, 512, 12288]"
    # t378 = prims.linear(t373, t_transformer_h_2_attn_attn_weight, None)  # t378: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t379 = ltorch.view(t378, 1, 512, 32, 3, 128)  # t379: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t379 = ltorch.reshape(t378, (1, 512, 32, 3, 128))  # t379: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t379 = prims.reshape(t378, (1, 512, 32, 3, 128))  # t379: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t380 = ltorch.permute(t379, 0, 2, 3, 1, 4)  # t380: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t380 = prims.transpose(t379, (0, 2, 3, 1, 4))  # t380: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t381, t382, t383) = ltorch.split(t380, (1, 1, 1), 2)
    # t381 = prims.slice_prim(t380, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t381: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t382 = prims.slice_prim(t380, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t382: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t383 = prims.slice_prim(t380, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t383: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t384 = ltorch.reshape(t381, 1, -1, 512, 128)  # t384: "cuda:0 bf16[1, 32, 512, 128]"
    # t384 = prims.reshape(t381, (1, 32, 512, 128))  # t384: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t385 = ltorch.reshape(t382, 1, -1, 512, 128)  # t385: "cuda:0 bf16[1, 32, 512, 128]"
    # t385 = prims.reshape(t382, (1, 32, 512, 128))  # t385: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t386 = ltorch.reshape(t383, 1, -1, 512, 128)  # t386: "cuda:0 bf16[1, 32, 512, 128]"
    # t386 = prims.reshape(t383, (1, 32, 512, 128))  # t386: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t387 = ltorch.getitem(t384, (..., slice(None, 128, None)))  # t387: "cuda:0 bf16[1, 32, 512, 128]"
    # t387 = prims.slice_prim(t384, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t387: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t388 = ltorch.getitem(t387, (..., slice(None, 64, None)))  # t388: "cuda:0 bf16[1, 32, 512, 64]"
    # t388 = prims.slice_prim(t387, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t388: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t389 = ltorch.getitem(t387, (..., slice(64, None, None)))  # t389: "cuda:0 bf16[1, 32, 512, 64]"
    # t389 = prims.slice_prim(t387, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t389: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t392 = ltorch.neg(t389)  # t392: "cuda:0 bf16[1, 32, 512, 64]"
    # t390 = prims.convert_element_type(t389, dtypes.float32)  # t390: "cuda:0 f32[1, 32, 512, 64]"
    # t391 = prims.neg(t390)  # t391: "cuda:0 f32[1, 32, 512, 64]"
    # t392 = prims.convert_element_type(t391, dtypes.bfloat16)  # t392: "cuda:0 bf16[1, 32, 512, 64]"
  t393 = ltorch.cat((t392, t388), -1)  # t393: "cuda:0 bf16[1, 32, 512, 128]"
    # t393 = prims.cat((t392, t388), -1)  # t393: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t396 = ltorch.mul(t387, cos)  # t396: "cuda:0 f32[1, 32, 512, 128]"
    # t394 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t394: "cuda:0 f32[1, 32, 512, 128]"
    # t395 = prims.convert_element_type(t387, dtypes.float32)  # t395: "cuda:0 f32[1, 32, 512, 128]"
    # t396 = prims.mul(t395, t394)  # t396: "cuda:0 f32[1, 32, 512, 128]"
  t399 = ltorch.mul(t393, sin)  # t399: "cuda:0 f32[1, 32, 512, 128]"
    # t397 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t397: "cuda:0 f32[1, 32, 512, 128]"
    # t398 = prims.convert_element_type(t393, dtypes.float32)  # t398: "cuda:0 f32[1, 32, 512, 128]"
    # t399 = prims.mul(t398, t397)  # t399: "cuda:0 f32[1, 32, 512, 128]"
  t400 = ltorch.add(t396, t399, alpha=None)  # t400: "cuda:0 f32[1, 32, 512, 128]"
    # t400 = prims.add(t396, t399)  # t400: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t401 = ltorch.to(t400, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t401: "cuda:0 bf16[1, 32, 512, 128]"
    # t401 = prims.convert_element_type(t400, dtypes.bfloat16)  # t401: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t402 = ltorch.getitem(t385, (..., slice(None, 128, None)))  # t402: "cuda:0 bf16[1, 32, 512, 128]"
    # t402 = prims.slice_prim(t385, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t402: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t403 = ltorch.getitem(t402, (..., slice(None, 64, None)))  # t403: "cuda:0 bf16[1, 32, 512, 64]"
    # t403 = prims.slice_prim(t402, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t403: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t404 = ltorch.getitem(t402, (..., slice(64, None, None)))  # t404: "cuda:0 bf16[1, 32, 512, 64]"
    # t404 = prims.slice_prim(t402, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t404: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t407 = ltorch.neg(t404)  # t407: "cuda:0 bf16[1, 32, 512, 64]"
    # t405 = prims.convert_element_type(t404, dtypes.float32)  # t405: "cuda:0 f32[1, 32, 512, 64]"
    # t406 = prims.neg(t405)  # t406: "cuda:0 f32[1, 32, 512, 64]"
    # t407 = prims.convert_element_type(t406, dtypes.bfloat16)  # t407: "cuda:0 bf16[1, 32, 512, 64]"
  t408 = ltorch.cat((t407, t403), -1)  # t408: "cuda:0 bf16[1, 32, 512, 128]"
    # t408 = prims.cat((t407, t403), -1)  # t408: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t411 = ltorch.mul(t402, cos)  # t411: "cuda:0 f32[1, 32, 512, 128]"
    # t409 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t409: "cuda:0 f32[1, 32, 512, 128]"
    # t410 = prims.convert_element_type(t402, dtypes.float32)  # t410: "cuda:0 f32[1, 32, 512, 128]"
    # t411 = prims.mul(t410, t409)  # t411: "cuda:0 f32[1, 32, 512, 128]"
  t414 = ltorch.mul(t408, sin)  # t414: "cuda:0 f32[1, 32, 512, 128]"
    # t412 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t412: "cuda:0 f32[1, 32, 512, 128]"
    # t413 = prims.convert_element_type(t408, dtypes.float32)  # t413: "cuda:0 f32[1, 32, 512, 128]"
    # t414 = prims.mul(t413, t412)  # t414: "cuda:0 f32[1, 32, 512, 128]"
  t415 = ltorch.add(t411, t414, alpha=None)  # t415: "cuda:0 f32[1, 32, 512, 128]"
    # t415 = prims.add(t411, t414)  # t415: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t416 = ltorch.to(t415, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t416: "cuda:0 bf16[1, 32, 512, 128]"
    # t416 = prims.convert_element_type(t415, dtypes.bfloat16)  # t416: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t417 = ltorch.getitem(t384, (..., slice(128, None, None)))  # t417: "cuda:0 bf16[1, 32, 512, 0]"
    # t417 = prims.slice_prim(t384, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t417: "cuda:0 bf16[1, 32, 512, 0]"
  t418 = ltorch.cat((t401, t417), -1)  # t418: "cuda:0 bf16[1, 32, 512, 128]"
    # t418 = prims.cat((t401, t417), -1)  # t418: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t419 = ltorch.getitem(t385, (..., slice(128, None, None)))  # t419: "cuda:0 bf16[1, 32, 512, 0]"
    # t419 = prims.slice_prim(t385, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t419: "cuda:0 bf16[1, 32, 512, 0]"
  t420 = ltorch.cat((t416, t419), -1)  # t420: "cuda:0 bf16[1, 32, 512, 128]"
    # t420 = prims.cat((t416, t419), -1)  # t420: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t450 = ltorch.scaled_dot_product_attention(t418, t420, t386, None, 0.0, True, scale=0.08838834764831843)  # t450: "cuda:0 bf16[1, 32, 512, 128]"
    # t423 = ltorch.mul(t418, 0.29730177875068026)  # t423: "cuda:0 bf16[1, 32, 512, 128]"
      # t421 = prims.convert_element_type(t418, dtypes.float32)  # t421: "cuda:0 f32[1, 32, 512, 128]"
      # t422 = prims.mul(t421, 0.29730177875068026)  # t422: "cuda:0 f32[1, 32, 512, 128]"
      # t423 = prims.convert_element_type(t422, dtypes.bfloat16)  # t423: "cuda:0 bf16[1, 32, 512, 128]"
    # t424 = ltorch.transpose(t420, -2, -1)  # t424: "cuda:0 bf16[1, 32, 128, 512]"
      # t424 = prims.transpose(t420, (0, 1, 3, 2))  # t424: "cuda:0 bf16[1, 32, 128, 512]"
    # t427 = ltorch.mul(t424, 0.29730177875068026)  # t427: "cuda:0 bf16[1, 32, 128, 512]"
      # t425 = prims.convert_element_type(t424, dtypes.float32)  # t425: "cuda:0 f32[1, 32, 128, 512]"
      # t426 = prims.mul(t425, 0.29730177875068026)  # t426: "cuda:0 f32[1, 32, 128, 512]"
      # t427 = prims.convert_element_type(t426, dtypes.bfloat16)  # t427: "cuda:0 bf16[1, 32, 128, 512]"
    # t428 = ltorch.matmul(t423, t427)  # t428: "cuda:0 bf16[1, 32, 512, 512]"
      # t428 = prims.matmul(t423, t427)  # t428: "cuda:0 bf16[1, 32, 512, 512]"
    # t438 = ltorch.tril(t428, 0, fill_value=-float('inf'))  # t438: "cuda:0 bf16[1, 32, 512, 512]"
      # t429 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t429: "cuda:0 i64[512]"
        # t429 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t429: "cuda:0 i64[512]"
      # t430 = ltorch.unsqueeze(t429, -1)  # t430: "cuda:0 i64[512, 1]"
        # t430 = prims.broadcast_in_dim(t429, [512, 1], [0])  # t430: "cuda:0 i64[512, 1]"
      # t431 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t431: "cuda:0 i64[512]"
        # t431 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t431: "cuda:0 i64[512]"
      # t432 = ltorch.unsqueeze(t431, -2)  # t432: "cuda:0 i64[1, 512]"
        # t432 = prims.broadcast_in_dim(t431, [1, 512], [1])  # t432: "cuda:0 i64[1, 512]"
      # t433 = ltorch.add(t430, 0, alpha=None)  # t433: "cuda:0 i64[512, 1]"
        # t433 = prims.add(t430, 0)  # t433: "cuda:0 i64[512, 1]"
      # t436 = ltorch.ge(t433, t432)  # t436: "cuda:0 b8[512, 512]"
        # t434 = prims.broadcast_in_dim(t433, (512, 512), (0, 1))  # t434: "cuda:0 i64[512, 512]"
        # t435 = prims.broadcast_in_dim(t432, (512, 512), (0, 1))  # t435: "cuda:0 i64[512, 512]"
        # t436 = prims.ge(t434, t435)  # t436: "cuda:0 b8[512, 512]"
      # t438 = ltorch.where(t436, t428, -float('inf'))  # t438: "cuda:0 bf16[1, 32, 512, 512]"
        # t437 = prims.broadcast_in_dim(t436, (1, 32, 512, 512), (2, 3))  # t437: "cuda:0 b8[1, 32, 512, 512]"
        # t438 = prims.where(t437, t428, -float('inf'))  # t438: "cuda:0 bf16[1, 32, 512, 512]"
    # t449 = ltorch._softmax(t438, -1, dtype=None)  # t449: "cuda:0 bf16[1, 32, 512, 512]"
      # t439 = ltorch.to(t438, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t439: "cuda:0 f32[1, 32, 512, 512]"
        # t439 = prims.convert_element_type(t438, dtypes.float32)  # t439: "cuda:0 f32[1, 32, 512, 512]"
      # t441 = ltorch.amax(t439, -1, True)  # t441: "cuda:0 f32[1, 32, 512, 1]"
        # t440 = prims.amax(t439, (3,))  # t440: "cuda:0 f32[1, 32, 512]"
        # t441 = prims.broadcast_in_dim(t440, [1, 32, 512, 1], [0, 1, 2])  # t441: "cuda:0 f32[1, 32, 512, 1]"
      # t443 = ltorch.sub(t439, t441, alpha=None)  # t443: "cuda:0 f32[1, 32, 512, 512]"
        # t442 = prims.broadcast_in_dim(t441, (1, 32, 512, 512), (0, 1, 2, 3))  # t442: "cuda:0 f32[1, 32, 512, 512]"
        # t443 = prims.sub(t439, t442)  # t443: "cuda:0 f32[1, 32, 512, 512]"
      # t444 = ltorch.exp(t443)  # t444: "cuda:0 f32[1, 32, 512, 512]"
        # t444 = prims.exp(t443)  # t444: "cuda:0 f32[1, 32, 512, 512]"
      # t446 = ltorch.sum(t444, -1, True, dtype=None)  # t446: "cuda:0 f32[1, 32, 512, 1]"
        # t445 = prims.sum(t444, (3,))  # t445: "cuda:0 f32[1, 32, 512]"
        # t446 = prims.broadcast_in_dim(t445, [1, 32, 512, 1], [0, 1, 2])  # t446: "cuda:0 f32[1, 32, 512, 1]"
      # t448 = ltorch.true_divide(t444, t446)  # t448: "cuda:0 f32[1, 32, 512, 512]"
        # t447 = prims.broadcast_in_dim(t446, (1, 32, 512, 512), (0, 1, 2, 3))  # t447: "cuda:0 f32[1, 32, 512, 512]"
        # t448 = prims.div(t444, t447)  # t448: "cuda:0 f32[1, 32, 512, 512]"
      # t449 = ltorch.to(t448, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t449: "cuda:0 bf16[1, 32, 512, 512]"
        # t449 = prims.convert_element_type(t448, dtypes.bfloat16)  # t449: "cuda:0 bf16[1, 32, 512, 512]"
    # t450 = ltorch.matmul(t449, t386)  # t450: "cuda:0 bf16[1, 32, 512, 128]"
      # t450 = prims.matmul(t449, t386)  # t450: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t451 = ltorch.transpose(t450, 1, 2)  # t451: "cuda:0 bf16[1, 512, 32, 128]"
    # t451 = prims.transpose(t450, (0, 2, 1, 3))  # t451: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t452 = ltorch.reshape(t451, 1, 512, 4096)  # t452: "cuda:0 bf16[1, 512, 4096]"
    # t452 = prims.reshape(t451, (1, 512, 4096))  # t452: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t456 = ltorch.linear(t452, t_transformer_h_2_attn_proj_weight, None)  # t456: "cuda:0 bf16[1, 512, 4096]"
    # t456 = prims.linear(t452, t_transformer_h_2_attn_proj_weight, None)  # t456: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t460 = ltorch.add(t456, t350, alpha=None)  # t460: "cuda:0 bf16[1, 512, 4096]"
    # t457 = prims.convert_element_type(t456, dtypes.float32)  # t457: "cuda:0 f32[1, 512, 4096]"
    # t458 = prims.convert_element_type(t350, dtypes.float32)  # t458: "cuda:0 f32[1, 512, 4096]"
    # t459 = prims.add(t457, t458)  # t459: "cuda:0 f32[1, 512, 4096]"
    # t460 = prims.convert_element_type(t459, dtypes.bfloat16)  # t460: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t461 = prims.convert_element_type(t460, dtypes.float32)  # t461: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t462 = ltorch.mul(t461, t461)  # t462: "cuda:0 f32[1, 512, 4096]"
    # t462 = prims.mul(t461, t461)  # t462: "cuda:0 f32[1, 512, 4096]"
  t466 = ltorch.mean(t462, -1, True, dtype=None)  # t466: "cuda:0 f32[1, 512, 1]"
    # t464 = prims.sum(t462, (2,))  # t464: "cuda:0 f32[1, 512]"
    # t465 = prims.broadcast_in_dim(t464, [1, 512, 1], [0, 1])  # t465: "cuda:0 f32[1, 512, 1]"
    # t466 = ltorch.true_divide(t465, 4096)  # t466: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t466 = prims.div(t465, 4096.0)  # t466: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t468 = ltorch.add(t466, 1e-05, alpha=None)  # t468: "cuda:0 f32[1, 512, 1]"
    # t468 = prims.add(t466, 1e-05)  # t468: "cuda:0 f32[1, 512, 1]"
  t469 = ltorch.rsqrt(t468)  # t469: "cuda:0 f32[1, 512, 1]"
    # t469 = prims.rsqrt(t468)  # t469: "cuda:0 f32[1, 512, 1]"
  t471 = ltorch.mul(t461, t469)  # t471: "cuda:0 f32[1, 512, 4096]"
    # t470 = prims.broadcast_in_dim(t469, (1, 512, 4096), (0, 1, 2))  # t470: "cuda:0 f32[1, 512, 4096]"
    # t471 = prims.mul(t461, t470)  # t471: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t472 = ltorch.to(t471, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t472: "cuda:0 bf16[1, 512, 4096]"
    # t472 = prims.convert_element_type(t471, dtypes.bfloat16)  # t472: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t482 = ltorch.mul(t472, t_transformer_h_2_norm_2_weight)  # t482: "cuda:0 bf16[1, 512, 4096]"
    # t478 = prims.broadcast_in_dim(t_transformer_h_2_norm_2_weight, (1, 512, 4096), (2,))  # t478: "cuda:0 bf16[1, 512, 4096]"
    # t479 = prims.convert_element_type(t472, dtypes.float32)  # t479: "cuda:0 f32[1, 512, 4096]"
    # t480 = prims.convert_element_type(t478, dtypes.float32)  # t480: "cuda:0 f32[1, 512, 4096]"
    # t481 = prims.mul(t479, t480)  # t481: "cuda:0 f32[1, 512, 4096]"
    # t482 = prims.convert_element_type(t481, dtypes.bfloat16)  # t482: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t487 = ltorch.linear(t482, t_transformer_h_2_mlp_fc_1_weight, None)  # t487: "cuda:0 bf16[1, 512, 11008]"
    # t487 = prims.linear(t482, t_transformer_h_2_mlp_fc_1_weight, None)  # t487: "cuda:0 bf16[1, 512, 11008]"
  t491 = ltorch.linear(t482, t_transformer_h_2_mlp_fc_2_weight, None)  # t491: "cuda:0 bf16[1, 512, 11008]"
    # t491 = prims.linear(t482, t_transformer_h_2_mlp_fc_2_weight, None)  # t491: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t501 = ltorch.silu(t487, False)  # t501: "cuda:0 bf16[1, 512, 11008]"
    # t492 = prims.convert_element_type(t487, dtypes.float32)  # t492: "cuda:0 f32[1, 512, 11008]"
    # t493 = prims.neg(t492)  # t493: "cuda:0 f32[1, 512, 11008]"
    # t494 = prims.exp(t493)  # t494: "cuda:0 f32[1, 512, 11008]"
    # t495 = prims.add(1.0, t494)  # t495: "cuda:0 f32[1, 512, 11008]"
    # t496 = prims.reciprocal(t495)  # t496: "cuda:0 f32[1, 512, 11008]"
    # t497 = prims.convert_element_type(t496, dtypes.bfloat16)  # t497: "cuda:0 bf16[1, 512, 11008]"
    # t498 = prims.convert_element_type(t487, dtypes.float32)  # t498: "cuda:0 f32[1, 512, 11008]"
    # t499 = prims.convert_element_type(t497, dtypes.float32)  # t499: "cuda:0 f32[1, 512, 11008]"
    # t500 = prims.mul(t498, t499)  # t500: "cuda:0 f32[1, 512, 11008]"
    # t501 = prims.convert_element_type(t500, dtypes.bfloat16)  # t501: "cuda:0 bf16[1, 512, 11008]"
  t505 = ltorch.mul(t501, t491)  # t505: "cuda:0 bf16[1, 512, 11008]"
    # t502 = prims.convert_element_type(t501, dtypes.float32)  # t502: "cuda:0 f32[1, 512, 11008]"
    # t503 = prims.convert_element_type(t491, dtypes.float32)  # t503: "cuda:0 f32[1, 512, 11008]"
    # t504 = prims.mul(t502, t503)  # t504: "cuda:0 f32[1, 512, 11008]"
    # t505 = prims.convert_element_type(t504, dtypes.bfloat16)  # t505: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t509 = ltorch.linear(t505, t_transformer_h_2_mlp_proj_weight, None)  # t509: "cuda:0 bf16[1, 512, 4096]"
    # t509 = prims.linear(t505, t_transformer_h_2_mlp_proj_weight, None)  # t509: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t513 = ltorch.add(t509, t460, alpha=None)  # t513: "cuda:0 bf16[1, 512, 4096]"
    # t510 = prims.convert_element_type(t509, dtypes.float32)  # t510: "cuda:0 f32[1, 512, 4096]"
    # t511 = prims.convert_element_type(t460, dtypes.float32)  # t511: "cuda:0 f32[1, 512, 4096]"
    # t512 = prims.add(t510, t511)  # t512: "cuda:0 f32[1, 512, 4096]"
    # t513 = prims.convert_element_type(t512, dtypes.bfloat16)  # t513: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t515 = prims.convert_element_type(t513, dtypes.float32)  # t515: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t516 = ltorch.mul(t515, t515)  # t516: "cuda:0 f32[1, 512, 4096]"
    # t516 = prims.mul(t515, t515)  # t516: "cuda:0 f32[1, 512, 4096]"
  t520 = ltorch.mean(t516, -1, True, dtype=None)  # t520: "cuda:0 f32[1, 512, 1]"
    # t518 = prims.sum(t516, (2,))  # t518: "cuda:0 f32[1, 512]"
    # t519 = prims.broadcast_in_dim(t518, [1, 512, 1], [0, 1])  # t519: "cuda:0 f32[1, 512, 1]"
    # t520 = ltorch.true_divide(t519, 4096)  # t520: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t520 = prims.div(t519, 4096.0)  # t520: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t522 = ltorch.add(t520, 1e-05, alpha=None)  # t522: "cuda:0 f32[1, 512, 1]"
    # t522 = prims.add(t520, 1e-05)  # t522: "cuda:0 f32[1, 512, 1]"
  t523 = ltorch.rsqrt(t522)  # t523: "cuda:0 f32[1, 512, 1]"
    # t523 = prims.rsqrt(t522)  # t523: "cuda:0 f32[1, 512, 1]"
  t525 = ltorch.mul(t515, t523)  # t525: "cuda:0 f32[1, 512, 4096]"
    # t524 = prims.broadcast_in_dim(t523, (1, 512, 4096), (0, 1, 2))  # t524: "cuda:0 f32[1, 512, 4096]"
    # t525 = prims.mul(t515, t524)  # t525: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t526 = ltorch.to(t525, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t526: "cuda:0 bf16[1, 512, 4096]"
    # t526 = prims.convert_element_type(t525, dtypes.bfloat16)  # t526: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t536 = ltorch.mul(t526, t_transformer_h_3_norm_1_weight)  # t536: "cuda:0 bf16[1, 512, 4096]"
    # t532 = prims.broadcast_in_dim(t_transformer_h_3_norm_1_weight, (1, 512, 4096), (2,))  # t532: "cuda:0 bf16[1, 512, 4096]"
    # t533 = prims.convert_element_type(t526, dtypes.float32)  # t533: "cuda:0 f32[1, 512, 4096]"
    # t534 = prims.convert_element_type(t532, dtypes.float32)  # t534: "cuda:0 f32[1, 512, 4096]"
    # t535 = prims.mul(t533, t534)  # t535: "cuda:0 f32[1, 512, 4096]"
    # t536 = prims.convert_element_type(t535, dtypes.bfloat16)  # t536: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t541 = ltorch.linear(t536, t_transformer_h_3_attn_attn_weight, None)  # t541: "cuda:0 bf16[1, 512, 12288]"
    # t541 = prims.linear(t536, t_transformer_h_3_attn_attn_weight, None)  # t541: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t542 = ltorch.view(t541, 1, 512, 32, 3, 128)  # t542: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t542 = ltorch.reshape(t541, (1, 512, 32, 3, 128))  # t542: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t542 = prims.reshape(t541, (1, 512, 32, 3, 128))  # t542: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t543 = ltorch.permute(t542, 0, 2, 3, 1, 4)  # t543: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t543 = prims.transpose(t542, (0, 2, 3, 1, 4))  # t543: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t544, t545, t546) = ltorch.split(t543, (1, 1, 1), 2)
    # t544 = prims.slice_prim(t543, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t544: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t545 = prims.slice_prim(t543, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t545: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t546 = prims.slice_prim(t543, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t546: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t547 = ltorch.reshape(t544, 1, -1, 512, 128)  # t547: "cuda:0 bf16[1, 32, 512, 128]"
    # t547 = prims.reshape(t544, (1, 32, 512, 128))  # t547: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t548 = ltorch.reshape(t545, 1, -1, 512, 128)  # t548: "cuda:0 bf16[1, 32, 512, 128]"
    # t548 = prims.reshape(t545, (1, 32, 512, 128))  # t548: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t549 = ltorch.reshape(t546, 1, -1, 512, 128)  # t549: "cuda:0 bf16[1, 32, 512, 128]"
    # t549 = prims.reshape(t546, (1, 32, 512, 128))  # t549: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t550 = ltorch.getitem(t547, (..., slice(None, 128, None)))  # t550: "cuda:0 bf16[1, 32, 512, 128]"
    # t550 = prims.slice_prim(t547, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t550: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t551 = ltorch.getitem(t550, (..., slice(None, 64, None)))  # t551: "cuda:0 bf16[1, 32, 512, 64]"
    # t551 = prims.slice_prim(t550, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t551: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t552 = ltorch.getitem(t550, (..., slice(64, None, None)))  # t552: "cuda:0 bf16[1, 32, 512, 64]"
    # t552 = prims.slice_prim(t550, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t552: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t555 = ltorch.neg(t552)  # t555: "cuda:0 bf16[1, 32, 512, 64]"
    # t553 = prims.convert_element_type(t552, dtypes.float32)  # t553: "cuda:0 f32[1, 32, 512, 64]"
    # t554 = prims.neg(t553)  # t554: "cuda:0 f32[1, 32, 512, 64]"
    # t555 = prims.convert_element_type(t554, dtypes.bfloat16)  # t555: "cuda:0 bf16[1, 32, 512, 64]"
  t556 = ltorch.cat((t555, t551), -1)  # t556: "cuda:0 bf16[1, 32, 512, 128]"
    # t556 = prims.cat((t555, t551), -1)  # t556: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t559 = ltorch.mul(t550, cos)  # t559: "cuda:0 f32[1, 32, 512, 128]"
    # t557 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t557: "cuda:0 f32[1, 32, 512, 128]"
    # t558 = prims.convert_element_type(t550, dtypes.float32)  # t558: "cuda:0 f32[1, 32, 512, 128]"
    # t559 = prims.mul(t558, t557)  # t559: "cuda:0 f32[1, 32, 512, 128]"
  t562 = ltorch.mul(t556, sin)  # t562: "cuda:0 f32[1, 32, 512, 128]"
    # t560 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t560: "cuda:0 f32[1, 32, 512, 128]"
    # t561 = prims.convert_element_type(t556, dtypes.float32)  # t561: "cuda:0 f32[1, 32, 512, 128]"
    # t562 = prims.mul(t561, t560)  # t562: "cuda:0 f32[1, 32, 512, 128]"
  t563 = ltorch.add(t559, t562, alpha=None)  # t563: "cuda:0 f32[1, 32, 512, 128]"
    # t563 = prims.add(t559, t562)  # t563: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t564 = ltorch.to(t563, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t564: "cuda:0 bf16[1, 32, 512, 128]"
    # t564 = prims.convert_element_type(t563, dtypes.bfloat16)  # t564: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t565 = ltorch.getitem(t548, (..., slice(None, 128, None)))  # t565: "cuda:0 bf16[1, 32, 512, 128]"
    # t565 = prims.slice_prim(t548, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t565: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t566 = ltorch.getitem(t565, (..., slice(None, 64, None)))  # t566: "cuda:0 bf16[1, 32, 512, 64]"
    # t566 = prims.slice_prim(t565, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t566: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t567 = ltorch.getitem(t565, (..., slice(64, None, None)))  # t567: "cuda:0 bf16[1, 32, 512, 64]"
    # t567 = prims.slice_prim(t565, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t567: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t570 = ltorch.neg(t567)  # t570: "cuda:0 bf16[1, 32, 512, 64]"
    # t568 = prims.convert_element_type(t567, dtypes.float32)  # t568: "cuda:0 f32[1, 32, 512, 64]"
    # t569 = prims.neg(t568)  # t569: "cuda:0 f32[1, 32, 512, 64]"
    # t570 = prims.convert_element_type(t569, dtypes.bfloat16)  # t570: "cuda:0 bf16[1, 32, 512, 64]"
  t571 = ltorch.cat((t570, t566), -1)  # t571: "cuda:0 bf16[1, 32, 512, 128]"
    # t571 = prims.cat((t570, t566), -1)  # t571: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t574 = ltorch.mul(t565, cos)  # t574: "cuda:0 f32[1, 32, 512, 128]"
    # t572 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t572: "cuda:0 f32[1, 32, 512, 128]"
    # t573 = prims.convert_element_type(t565, dtypes.float32)  # t573: "cuda:0 f32[1, 32, 512, 128]"
    # t574 = prims.mul(t573, t572)  # t574: "cuda:0 f32[1, 32, 512, 128]"
  t577 = ltorch.mul(t571, sin)  # t577: "cuda:0 f32[1, 32, 512, 128]"
    # t575 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t575: "cuda:0 f32[1, 32, 512, 128]"
    # t576 = prims.convert_element_type(t571, dtypes.float32)  # t576: "cuda:0 f32[1, 32, 512, 128]"
    # t577 = prims.mul(t576, t575)  # t577: "cuda:0 f32[1, 32, 512, 128]"
  t578 = ltorch.add(t574, t577, alpha=None)  # t578: "cuda:0 f32[1, 32, 512, 128]"
    # t578 = prims.add(t574, t577)  # t578: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t579 = ltorch.to(t578, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t579: "cuda:0 bf16[1, 32, 512, 128]"
    # t579 = prims.convert_element_type(t578, dtypes.bfloat16)  # t579: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t580 = ltorch.getitem(t547, (..., slice(128, None, None)))  # t580: "cuda:0 bf16[1, 32, 512, 0]"
    # t580 = prims.slice_prim(t547, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t580: "cuda:0 bf16[1, 32, 512, 0]"
  t581 = ltorch.cat((t564, t580), -1)  # t581: "cuda:0 bf16[1, 32, 512, 128]"
    # t581 = prims.cat((t564, t580), -1)  # t581: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t582 = ltorch.getitem(t548, (..., slice(128, None, None)))  # t582: "cuda:0 bf16[1, 32, 512, 0]"
    # t582 = prims.slice_prim(t548, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t582: "cuda:0 bf16[1, 32, 512, 0]"
  t583 = ltorch.cat((t579, t582), -1)  # t583: "cuda:0 bf16[1, 32, 512, 128]"
    # t583 = prims.cat((t579, t582), -1)  # t583: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t613 = ltorch.scaled_dot_product_attention(t581, t583, t549, None, 0.0, True, scale=0.08838834764831843)  # t613: "cuda:0 bf16[1, 32, 512, 128]"
    # t586 = ltorch.mul(t581, 0.29730177875068026)  # t586: "cuda:0 bf16[1, 32, 512, 128]"
      # t584 = prims.convert_element_type(t581, dtypes.float32)  # t584: "cuda:0 f32[1, 32, 512, 128]"
      # t585 = prims.mul(t584, 0.29730177875068026)  # t585: "cuda:0 f32[1, 32, 512, 128]"
      # t586 = prims.convert_element_type(t585, dtypes.bfloat16)  # t586: "cuda:0 bf16[1, 32, 512, 128]"
    # t587 = ltorch.transpose(t583, -2, -1)  # t587: "cuda:0 bf16[1, 32, 128, 512]"
      # t587 = prims.transpose(t583, (0, 1, 3, 2))  # t587: "cuda:0 bf16[1, 32, 128, 512]"
    # t590 = ltorch.mul(t587, 0.29730177875068026)  # t590: "cuda:0 bf16[1, 32, 128, 512]"
      # t588 = prims.convert_element_type(t587, dtypes.float32)  # t588: "cuda:0 f32[1, 32, 128, 512]"
      # t589 = prims.mul(t588, 0.29730177875068026)  # t589: "cuda:0 f32[1, 32, 128, 512]"
      # t590 = prims.convert_element_type(t589, dtypes.bfloat16)  # t590: "cuda:0 bf16[1, 32, 128, 512]"
    # t591 = ltorch.matmul(t586, t590)  # t591: "cuda:0 bf16[1, 32, 512, 512]"
      # t591 = prims.matmul(t586, t590)  # t591: "cuda:0 bf16[1, 32, 512, 512]"
    # t601 = ltorch.tril(t591, 0, fill_value=-float('inf'))  # t601: "cuda:0 bf16[1, 32, 512, 512]"
      # t592 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t592: "cuda:0 i64[512]"
        # t592 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t592: "cuda:0 i64[512]"
      # t593 = ltorch.unsqueeze(t592, -1)  # t593: "cuda:0 i64[512, 1]"
        # t593 = prims.broadcast_in_dim(t592, [512, 1], [0])  # t593: "cuda:0 i64[512, 1]"
      # t594 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t594: "cuda:0 i64[512]"
        # t594 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t594: "cuda:0 i64[512]"
      # t595 = ltorch.unsqueeze(t594, -2)  # t595: "cuda:0 i64[1, 512]"
        # t595 = prims.broadcast_in_dim(t594, [1, 512], [1])  # t595: "cuda:0 i64[1, 512]"
      # t596 = ltorch.add(t593, 0, alpha=None)  # t596: "cuda:0 i64[512, 1]"
        # t596 = prims.add(t593, 0)  # t596: "cuda:0 i64[512, 1]"
      # t599 = ltorch.ge(t596, t595)  # t599: "cuda:0 b8[512, 512]"
        # t597 = prims.broadcast_in_dim(t596, (512, 512), (0, 1))  # t597: "cuda:0 i64[512, 512]"
        # t598 = prims.broadcast_in_dim(t595, (512, 512), (0, 1))  # t598: "cuda:0 i64[512, 512]"
        # t599 = prims.ge(t597, t598)  # t599: "cuda:0 b8[512, 512]"
      # t601 = ltorch.where(t599, t591, -float('inf'))  # t601: "cuda:0 bf16[1, 32, 512, 512]"
        # t600 = prims.broadcast_in_dim(t599, (1, 32, 512, 512), (2, 3))  # t600: "cuda:0 b8[1, 32, 512, 512]"
        # t601 = prims.where(t600, t591, -float('inf'))  # t601: "cuda:0 bf16[1, 32, 512, 512]"
    # t612 = ltorch._softmax(t601, -1, dtype=None)  # t612: "cuda:0 bf16[1, 32, 512, 512]"
      # t602 = ltorch.to(t601, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t602: "cuda:0 f32[1, 32, 512, 512]"
        # t602 = prims.convert_element_type(t601, dtypes.float32)  # t602: "cuda:0 f32[1, 32, 512, 512]"
      # t604 = ltorch.amax(t602, -1, True)  # t604: "cuda:0 f32[1, 32, 512, 1]"
        # t603 = prims.amax(t602, (3,))  # t603: "cuda:0 f32[1, 32, 512]"
        # t604 = prims.broadcast_in_dim(t603, [1, 32, 512, 1], [0, 1, 2])  # t604: "cuda:0 f32[1, 32, 512, 1]"
      # t606 = ltorch.sub(t602, t604, alpha=None)  # t606: "cuda:0 f32[1, 32, 512, 512]"
        # t605 = prims.broadcast_in_dim(t604, (1, 32, 512, 512), (0, 1, 2, 3))  # t605: "cuda:0 f32[1, 32, 512, 512]"
        # t606 = prims.sub(t602, t605)  # t606: "cuda:0 f32[1, 32, 512, 512]"
      # t607 = ltorch.exp(t606)  # t607: "cuda:0 f32[1, 32, 512, 512]"
        # t607 = prims.exp(t606)  # t607: "cuda:0 f32[1, 32, 512, 512]"
      # t609 = ltorch.sum(t607, -1, True, dtype=None)  # t609: "cuda:0 f32[1, 32, 512, 1]"
        # t608 = prims.sum(t607, (3,))  # t608: "cuda:0 f32[1, 32, 512]"
        # t609 = prims.broadcast_in_dim(t608, [1, 32, 512, 1], [0, 1, 2])  # t609: "cuda:0 f32[1, 32, 512, 1]"
      # t611 = ltorch.true_divide(t607, t609)  # t611: "cuda:0 f32[1, 32, 512, 512]"
        # t610 = prims.broadcast_in_dim(t609, (1, 32, 512, 512), (0, 1, 2, 3))  # t610: "cuda:0 f32[1, 32, 512, 512]"
        # t611 = prims.div(t607, t610)  # t611: "cuda:0 f32[1, 32, 512, 512]"
      # t612 = ltorch.to(t611, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t612: "cuda:0 bf16[1, 32, 512, 512]"
        # t612 = prims.convert_element_type(t611, dtypes.bfloat16)  # t612: "cuda:0 bf16[1, 32, 512, 512]"
    # t613 = ltorch.matmul(t612, t549)  # t613: "cuda:0 bf16[1, 32, 512, 128]"
      # t613 = prims.matmul(t612, t549)  # t613: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t614 = ltorch.transpose(t613, 1, 2)  # t614: "cuda:0 bf16[1, 512, 32, 128]"
    # t614 = prims.transpose(t613, (0, 2, 1, 3))  # t614: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t615 = ltorch.reshape(t614, 1, 512, 4096)  # t615: "cuda:0 bf16[1, 512, 4096]"
    # t615 = prims.reshape(t614, (1, 512, 4096))  # t615: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t619 = ltorch.linear(t615, t_transformer_h_3_attn_proj_weight, None)  # t619: "cuda:0 bf16[1, 512, 4096]"
    # t619 = prims.linear(t615, t_transformer_h_3_attn_proj_weight, None)  # t619: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t623 = ltorch.add(t619, t513, alpha=None)  # t623: "cuda:0 bf16[1, 512, 4096]"
    # t620 = prims.convert_element_type(t619, dtypes.float32)  # t620: "cuda:0 f32[1, 512, 4096]"
    # t621 = prims.convert_element_type(t513, dtypes.float32)  # t621: "cuda:0 f32[1, 512, 4096]"
    # t622 = prims.add(t620, t621)  # t622: "cuda:0 f32[1, 512, 4096]"
    # t623 = prims.convert_element_type(t622, dtypes.bfloat16)  # t623: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t624 = prims.convert_element_type(t623, dtypes.float32)  # t624: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t625 = ltorch.mul(t624, t624)  # t625: "cuda:0 f32[1, 512, 4096]"
    # t625 = prims.mul(t624, t624)  # t625: "cuda:0 f32[1, 512, 4096]"
  t629 = ltorch.mean(t625, -1, True, dtype=None)  # t629: "cuda:0 f32[1, 512, 1]"
    # t627 = prims.sum(t625, (2,))  # t627: "cuda:0 f32[1, 512]"
    # t628 = prims.broadcast_in_dim(t627, [1, 512, 1], [0, 1])  # t628: "cuda:0 f32[1, 512, 1]"
    # t629 = ltorch.true_divide(t628, 4096)  # t629: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t629 = prims.div(t628, 4096.0)  # t629: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t631 = ltorch.add(t629, 1e-05, alpha=None)  # t631: "cuda:0 f32[1, 512, 1]"
    # t631 = prims.add(t629, 1e-05)  # t631: "cuda:0 f32[1, 512, 1]"
  t632 = ltorch.rsqrt(t631)  # t632: "cuda:0 f32[1, 512, 1]"
    # t632 = prims.rsqrt(t631)  # t632: "cuda:0 f32[1, 512, 1]"
  t634 = ltorch.mul(t624, t632)  # t634: "cuda:0 f32[1, 512, 4096]"
    # t633 = prims.broadcast_in_dim(t632, (1, 512, 4096), (0, 1, 2))  # t633: "cuda:0 f32[1, 512, 4096]"
    # t634 = prims.mul(t624, t633)  # t634: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t635 = ltorch.to(t634, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t635: "cuda:0 bf16[1, 512, 4096]"
    # t635 = prims.convert_element_type(t634, dtypes.bfloat16)  # t635: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t645 = ltorch.mul(t635, t_transformer_h_3_norm_2_weight)  # t645: "cuda:0 bf16[1, 512, 4096]"
    # t641 = prims.broadcast_in_dim(t_transformer_h_3_norm_2_weight, (1, 512, 4096), (2,))  # t641: "cuda:0 bf16[1, 512, 4096]"
    # t642 = prims.convert_element_type(t635, dtypes.float32)  # t642: "cuda:0 f32[1, 512, 4096]"
    # t643 = prims.convert_element_type(t641, dtypes.float32)  # t643: "cuda:0 f32[1, 512, 4096]"
    # t644 = prims.mul(t642, t643)  # t644: "cuda:0 f32[1, 512, 4096]"
    # t645 = prims.convert_element_type(t644, dtypes.bfloat16)  # t645: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t650 = ltorch.linear(t645, t_transformer_h_3_mlp_fc_1_weight, None)  # t650: "cuda:0 bf16[1, 512, 11008]"
    # t650 = prims.linear(t645, t_transformer_h_3_mlp_fc_1_weight, None)  # t650: "cuda:0 bf16[1, 512, 11008]"
  t654 = ltorch.linear(t645, t_transformer_h_3_mlp_fc_2_weight, None)  # t654: "cuda:0 bf16[1, 512, 11008]"
    # t654 = prims.linear(t645, t_transformer_h_3_mlp_fc_2_weight, None)  # t654: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t664 = ltorch.silu(t650, False)  # t664: "cuda:0 bf16[1, 512, 11008]"
    # t655 = prims.convert_element_type(t650, dtypes.float32)  # t655: "cuda:0 f32[1, 512, 11008]"
    # t656 = prims.neg(t655)  # t656: "cuda:0 f32[1, 512, 11008]"
    # t657 = prims.exp(t656)  # t657: "cuda:0 f32[1, 512, 11008]"
    # t658 = prims.add(1.0, t657)  # t658: "cuda:0 f32[1, 512, 11008]"
    # t659 = prims.reciprocal(t658)  # t659: "cuda:0 f32[1, 512, 11008]"
    # t660 = prims.convert_element_type(t659, dtypes.bfloat16)  # t660: "cuda:0 bf16[1, 512, 11008]"
    # t661 = prims.convert_element_type(t650, dtypes.float32)  # t661: "cuda:0 f32[1, 512, 11008]"
    # t662 = prims.convert_element_type(t660, dtypes.float32)  # t662: "cuda:0 f32[1, 512, 11008]"
    # t663 = prims.mul(t661, t662)  # t663: "cuda:0 f32[1, 512, 11008]"
    # t664 = prims.convert_element_type(t663, dtypes.bfloat16)  # t664: "cuda:0 bf16[1, 512, 11008]"
  t668 = ltorch.mul(t664, t654)  # t668: "cuda:0 bf16[1, 512, 11008]"
    # t665 = prims.convert_element_type(t664, dtypes.float32)  # t665: "cuda:0 f32[1, 512, 11008]"
    # t666 = prims.convert_element_type(t654, dtypes.float32)  # t666: "cuda:0 f32[1, 512, 11008]"
    # t667 = prims.mul(t665, t666)  # t667: "cuda:0 f32[1, 512, 11008]"
    # t668 = prims.convert_element_type(t667, dtypes.bfloat16)  # t668: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t672 = ltorch.linear(t668, t_transformer_h_3_mlp_proj_weight, None)  # t672: "cuda:0 bf16[1, 512, 4096]"
    # t672 = prims.linear(t668, t_transformer_h_3_mlp_proj_weight, None)  # t672: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t676 = ltorch.add(t672, t623, alpha=None)  # t676: "cuda:0 bf16[1, 512, 4096]"
    # t673 = prims.convert_element_type(t672, dtypes.float32)  # t673: "cuda:0 f32[1, 512, 4096]"
    # t674 = prims.convert_element_type(t623, dtypes.float32)  # t674: "cuda:0 f32[1, 512, 4096]"
    # t675 = prims.add(t673, t674)  # t675: "cuda:0 f32[1, 512, 4096]"
    # t676 = prims.convert_element_type(t675, dtypes.bfloat16)  # t676: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t678 = prims.convert_element_type(t676, dtypes.float32)  # t678: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t679 = ltorch.mul(t678, t678)  # t679: "cuda:0 f32[1, 512, 4096]"
    # t679 = prims.mul(t678, t678)  # t679: "cuda:0 f32[1, 512, 4096]"
  t683 = ltorch.mean(t679, -1, True, dtype=None)  # t683: "cuda:0 f32[1, 512, 1]"
    # t681 = prims.sum(t679, (2,))  # t681: "cuda:0 f32[1, 512]"
    # t682 = prims.broadcast_in_dim(t681, [1, 512, 1], [0, 1])  # t682: "cuda:0 f32[1, 512, 1]"
    # t683 = ltorch.true_divide(t682, 4096)  # t683: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t683 = prims.div(t682, 4096.0)  # t683: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t685 = ltorch.add(t683, 1e-05, alpha=None)  # t685: "cuda:0 f32[1, 512, 1]"
    # t685 = prims.add(t683, 1e-05)  # t685: "cuda:0 f32[1, 512, 1]"
  t686 = ltorch.rsqrt(t685)  # t686: "cuda:0 f32[1, 512, 1]"
    # t686 = prims.rsqrt(t685)  # t686: "cuda:0 f32[1, 512, 1]"
  t688 = ltorch.mul(t678, t686)  # t688: "cuda:0 f32[1, 512, 4096]"
    # t687 = prims.broadcast_in_dim(t686, (1, 512, 4096), (0, 1, 2))  # t687: "cuda:0 f32[1, 512, 4096]"
    # t688 = prims.mul(t678, t687)  # t688: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t689 = ltorch.to(t688, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t689: "cuda:0 bf16[1, 512, 4096]"
    # t689 = prims.convert_element_type(t688, dtypes.bfloat16)  # t689: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t699 = ltorch.mul(t689, t_transformer_h_4_norm_1_weight)  # t699: "cuda:0 bf16[1, 512, 4096]"
    # t695 = prims.broadcast_in_dim(t_transformer_h_4_norm_1_weight, (1, 512, 4096), (2,))  # t695: "cuda:0 bf16[1, 512, 4096]"
    # t696 = prims.convert_element_type(t689, dtypes.float32)  # t696: "cuda:0 f32[1, 512, 4096]"
    # t697 = prims.convert_element_type(t695, dtypes.float32)  # t697: "cuda:0 f32[1, 512, 4096]"
    # t698 = prims.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 4096]"
    # t699 = prims.convert_element_type(t698, dtypes.bfloat16)  # t699: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t704 = ltorch.linear(t699, t_transformer_h_4_attn_attn_weight, None)  # t704: "cuda:0 bf16[1, 512, 12288]"
    # t704 = prims.linear(t699, t_transformer_h_4_attn_attn_weight, None)  # t704: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t705 = ltorch.view(t704, 1, 512, 32, 3, 128)  # t705: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t705 = ltorch.reshape(t704, (1, 512, 32, 3, 128))  # t705: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t705 = prims.reshape(t704, (1, 512, 32, 3, 128))  # t705: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t706 = ltorch.permute(t705, 0, 2, 3, 1, 4)  # t706: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t706 = prims.transpose(t705, (0, 2, 3, 1, 4))  # t706: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t707, t708, t709) = ltorch.split(t706, (1, 1, 1), 2)
    # t707 = prims.slice_prim(t706, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t707: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t708 = prims.slice_prim(t706, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t708: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t709 = prims.slice_prim(t706, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t709: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t710 = ltorch.reshape(t707, 1, -1, 512, 128)  # t710: "cuda:0 bf16[1, 32, 512, 128]"
    # t710 = prims.reshape(t707, (1, 32, 512, 128))  # t710: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t711 = ltorch.reshape(t708, 1, -1, 512, 128)  # t711: "cuda:0 bf16[1, 32, 512, 128]"
    # t711 = prims.reshape(t708, (1, 32, 512, 128))  # t711: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t712 = ltorch.reshape(t709, 1, -1, 512, 128)  # t712: "cuda:0 bf16[1, 32, 512, 128]"
    # t712 = prims.reshape(t709, (1, 32, 512, 128))  # t712: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t713 = ltorch.getitem(t710, (..., slice(None, 128, None)))  # t713: "cuda:0 bf16[1, 32, 512, 128]"
    # t713 = prims.slice_prim(t710, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t713: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t714 = ltorch.getitem(t713, (..., slice(None, 64, None)))  # t714: "cuda:0 bf16[1, 32, 512, 64]"
    # t714 = prims.slice_prim(t713, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t714: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t715 = ltorch.getitem(t713, (..., slice(64, None, None)))  # t715: "cuda:0 bf16[1, 32, 512, 64]"
    # t715 = prims.slice_prim(t713, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t715: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t718 = ltorch.neg(t715)  # t718: "cuda:0 bf16[1, 32, 512, 64]"
    # t716 = prims.convert_element_type(t715, dtypes.float32)  # t716: "cuda:0 f32[1, 32, 512, 64]"
    # t717 = prims.neg(t716)  # t717: "cuda:0 f32[1, 32, 512, 64]"
    # t718 = prims.convert_element_type(t717, dtypes.bfloat16)  # t718: "cuda:0 bf16[1, 32, 512, 64]"
  t719 = ltorch.cat((t718, t714), -1)  # t719: "cuda:0 bf16[1, 32, 512, 128]"
    # t719 = prims.cat((t718, t714), -1)  # t719: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t722 = ltorch.mul(t713, cos)  # t722: "cuda:0 f32[1, 32, 512, 128]"
    # t720 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t720: "cuda:0 f32[1, 32, 512, 128]"
    # t721 = prims.convert_element_type(t713, dtypes.float32)  # t721: "cuda:0 f32[1, 32, 512, 128]"
    # t722 = prims.mul(t721, t720)  # t722: "cuda:0 f32[1, 32, 512, 128]"
  t725 = ltorch.mul(t719, sin)  # t725: "cuda:0 f32[1, 32, 512, 128]"
    # t723 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t723: "cuda:0 f32[1, 32, 512, 128]"
    # t724 = prims.convert_element_type(t719, dtypes.float32)  # t724: "cuda:0 f32[1, 32, 512, 128]"
    # t725 = prims.mul(t724, t723)  # t725: "cuda:0 f32[1, 32, 512, 128]"
  t726 = ltorch.add(t722, t725, alpha=None)  # t726: "cuda:0 f32[1, 32, 512, 128]"
    # t726 = prims.add(t722, t725)  # t726: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t727 = ltorch.to(t726, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t727: "cuda:0 bf16[1, 32, 512, 128]"
    # t727 = prims.convert_element_type(t726, dtypes.bfloat16)  # t727: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t728 = ltorch.getitem(t711, (..., slice(None, 128, None)))  # t728: "cuda:0 bf16[1, 32, 512, 128]"
    # t728 = prims.slice_prim(t711, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t728: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t729 = ltorch.getitem(t728, (..., slice(None, 64, None)))  # t729: "cuda:0 bf16[1, 32, 512, 64]"
    # t729 = prims.slice_prim(t728, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t729: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t730 = ltorch.getitem(t728, (..., slice(64, None, None)))  # t730: "cuda:0 bf16[1, 32, 512, 64]"
    # t730 = prims.slice_prim(t728, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t730: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t733 = ltorch.neg(t730)  # t733: "cuda:0 bf16[1, 32, 512, 64]"
    # t731 = prims.convert_element_type(t730, dtypes.float32)  # t731: "cuda:0 f32[1, 32, 512, 64]"
    # t732 = prims.neg(t731)  # t732: "cuda:0 f32[1, 32, 512, 64]"
    # t733 = prims.convert_element_type(t732, dtypes.bfloat16)  # t733: "cuda:0 bf16[1, 32, 512, 64]"
  t734 = ltorch.cat((t733, t729), -1)  # t734: "cuda:0 bf16[1, 32, 512, 128]"
    # t734 = prims.cat((t733, t729), -1)  # t734: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t737 = ltorch.mul(t728, cos)  # t737: "cuda:0 f32[1, 32, 512, 128]"
    # t735 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t735: "cuda:0 f32[1, 32, 512, 128]"
    # t736 = prims.convert_element_type(t728, dtypes.float32)  # t736: "cuda:0 f32[1, 32, 512, 128]"
    # t737 = prims.mul(t736, t735)  # t737: "cuda:0 f32[1, 32, 512, 128]"
  t740 = ltorch.mul(t734, sin)  # t740: "cuda:0 f32[1, 32, 512, 128]"
    # t738 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t738: "cuda:0 f32[1, 32, 512, 128]"
    # t739 = prims.convert_element_type(t734, dtypes.float32)  # t739: "cuda:0 f32[1, 32, 512, 128]"
    # t740 = prims.mul(t739, t738)  # t740: "cuda:0 f32[1, 32, 512, 128]"
  t741 = ltorch.add(t737, t740, alpha=None)  # t741: "cuda:0 f32[1, 32, 512, 128]"
    # t741 = prims.add(t737, t740)  # t741: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t742 = ltorch.to(t741, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t742: "cuda:0 bf16[1, 32, 512, 128]"
    # t742 = prims.convert_element_type(t741, dtypes.bfloat16)  # t742: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t743 = ltorch.getitem(t710, (..., slice(128, None, None)))  # t743: "cuda:0 bf16[1, 32, 512, 0]"
    # t743 = prims.slice_prim(t710, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t743: "cuda:0 bf16[1, 32, 512, 0]"
  t744 = ltorch.cat((t727, t743), -1)  # t744: "cuda:0 bf16[1, 32, 512, 128]"
    # t744 = prims.cat((t727, t743), -1)  # t744: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t745 = ltorch.getitem(t711, (..., slice(128, None, None)))  # t745: "cuda:0 bf16[1, 32, 512, 0]"
    # t745 = prims.slice_prim(t711, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t745: "cuda:0 bf16[1, 32, 512, 0]"
  t746 = ltorch.cat((t742, t745), -1)  # t746: "cuda:0 bf16[1, 32, 512, 128]"
    # t746 = prims.cat((t742, t745), -1)  # t746: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t776 = ltorch.scaled_dot_product_attention(t744, t746, t712, None, 0.0, True, scale=0.08838834764831843)  # t776: "cuda:0 bf16[1, 32, 512, 128]"
    # t749 = ltorch.mul(t744, 0.29730177875068026)  # t749: "cuda:0 bf16[1, 32, 512, 128]"
      # t747 = prims.convert_element_type(t744, dtypes.float32)  # t747: "cuda:0 f32[1, 32, 512, 128]"
      # t748 = prims.mul(t747, 0.29730177875068026)  # t748: "cuda:0 f32[1, 32, 512, 128]"
      # t749 = prims.convert_element_type(t748, dtypes.bfloat16)  # t749: "cuda:0 bf16[1, 32, 512, 128]"
    # t750 = ltorch.transpose(t746, -2, -1)  # t750: "cuda:0 bf16[1, 32, 128, 512]"
      # t750 = prims.transpose(t746, (0, 1, 3, 2))  # t750: "cuda:0 bf16[1, 32, 128, 512]"
    # t753 = ltorch.mul(t750, 0.29730177875068026)  # t753: "cuda:0 bf16[1, 32, 128, 512]"
      # t751 = prims.convert_element_type(t750, dtypes.float32)  # t751: "cuda:0 f32[1, 32, 128, 512]"
      # t752 = prims.mul(t751, 0.29730177875068026)  # t752: "cuda:0 f32[1, 32, 128, 512]"
      # t753 = prims.convert_element_type(t752, dtypes.bfloat16)  # t753: "cuda:0 bf16[1, 32, 128, 512]"
    # t754 = ltorch.matmul(t749, t753)  # t754: "cuda:0 bf16[1, 32, 512, 512]"
      # t754 = prims.matmul(t749, t753)  # t754: "cuda:0 bf16[1, 32, 512, 512]"
    # t764 = ltorch.tril(t754, 0, fill_value=-float('inf'))  # t764: "cuda:0 bf16[1, 32, 512, 512]"
      # t755 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t755: "cuda:0 i64[512]"
        # t755 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t755: "cuda:0 i64[512]"
      # t756 = ltorch.unsqueeze(t755, -1)  # t756: "cuda:0 i64[512, 1]"
        # t756 = prims.broadcast_in_dim(t755, [512, 1], [0])  # t756: "cuda:0 i64[512, 1]"
      # t757 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t757: "cuda:0 i64[512]"
        # t757 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t757: "cuda:0 i64[512]"
      # t758 = ltorch.unsqueeze(t757, -2)  # t758: "cuda:0 i64[1, 512]"
        # t758 = prims.broadcast_in_dim(t757, [1, 512], [1])  # t758: "cuda:0 i64[1, 512]"
      # t759 = ltorch.add(t756, 0, alpha=None)  # t759: "cuda:0 i64[512, 1]"
        # t759 = prims.add(t756, 0)  # t759: "cuda:0 i64[512, 1]"
      # t762 = ltorch.ge(t759, t758)  # t762: "cuda:0 b8[512, 512]"
        # t760 = prims.broadcast_in_dim(t759, (512, 512), (0, 1))  # t760: "cuda:0 i64[512, 512]"
        # t761 = prims.broadcast_in_dim(t758, (512, 512), (0, 1))  # t761: "cuda:0 i64[512, 512]"
        # t762 = prims.ge(t760, t761)  # t762: "cuda:0 b8[512, 512]"
      # t764 = ltorch.where(t762, t754, -float('inf'))  # t764: "cuda:0 bf16[1, 32, 512, 512]"
        # t763 = prims.broadcast_in_dim(t762, (1, 32, 512, 512), (2, 3))  # t763: "cuda:0 b8[1, 32, 512, 512]"
        # t764 = prims.where(t763, t754, -float('inf'))  # t764: "cuda:0 bf16[1, 32, 512, 512]"
    # t775 = ltorch._softmax(t764, -1, dtype=None)  # t775: "cuda:0 bf16[1, 32, 512, 512]"
      # t765 = ltorch.to(t764, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t765: "cuda:0 f32[1, 32, 512, 512]"
        # t765 = prims.convert_element_type(t764, dtypes.float32)  # t765: "cuda:0 f32[1, 32, 512, 512]"
      # t767 = ltorch.amax(t765, -1, True)  # t767: "cuda:0 f32[1, 32, 512, 1]"
        # t766 = prims.amax(t765, (3,))  # t766: "cuda:0 f32[1, 32, 512]"
        # t767 = prims.broadcast_in_dim(t766, [1, 32, 512, 1], [0, 1, 2])  # t767: "cuda:0 f32[1, 32, 512, 1]"
      # t769 = ltorch.sub(t765, t767, alpha=None)  # t769: "cuda:0 f32[1, 32, 512, 512]"
        # t768 = prims.broadcast_in_dim(t767, (1, 32, 512, 512), (0, 1, 2, 3))  # t768: "cuda:0 f32[1, 32, 512, 512]"
        # t769 = prims.sub(t765, t768)  # t769: "cuda:0 f32[1, 32, 512, 512]"
      # t770 = ltorch.exp(t769)  # t770: "cuda:0 f32[1, 32, 512, 512]"
        # t770 = prims.exp(t769)  # t770: "cuda:0 f32[1, 32, 512, 512]"
      # t772 = ltorch.sum(t770, -1, True, dtype=None)  # t772: "cuda:0 f32[1, 32, 512, 1]"
        # t771 = prims.sum(t770, (3,))  # t771: "cuda:0 f32[1, 32, 512]"
        # t772 = prims.broadcast_in_dim(t771, [1, 32, 512, 1], [0, 1, 2])  # t772: "cuda:0 f32[1, 32, 512, 1]"
      # t774 = ltorch.true_divide(t770, t772)  # t774: "cuda:0 f32[1, 32, 512, 512]"
        # t773 = prims.broadcast_in_dim(t772, (1, 32, 512, 512), (0, 1, 2, 3))  # t773: "cuda:0 f32[1, 32, 512, 512]"
        # t774 = prims.div(t770, t773)  # t774: "cuda:0 f32[1, 32, 512, 512]"
      # t775 = ltorch.to(t774, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t775: "cuda:0 bf16[1, 32, 512, 512]"
        # t775 = prims.convert_element_type(t774, dtypes.bfloat16)  # t775: "cuda:0 bf16[1, 32, 512, 512]"
    # t776 = ltorch.matmul(t775, t712)  # t776: "cuda:0 bf16[1, 32, 512, 128]"
      # t776 = prims.matmul(t775, t712)  # t776: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t777 = ltorch.transpose(t776, 1, 2)  # t777: "cuda:0 bf16[1, 512, 32, 128]"
    # t777 = prims.transpose(t776, (0, 2, 1, 3))  # t777: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t778 = ltorch.reshape(t777, 1, 512, 4096)  # t778: "cuda:0 bf16[1, 512, 4096]"
    # t778 = prims.reshape(t777, (1, 512, 4096))  # t778: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t782 = ltorch.linear(t778, t_transformer_h_4_attn_proj_weight, None)  # t782: "cuda:0 bf16[1, 512, 4096]"
    # t782 = prims.linear(t778, t_transformer_h_4_attn_proj_weight, None)  # t782: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t786 = ltorch.add(t782, t676, alpha=None)  # t786: "cuda:0 bf16[1, 512, 4096]"
    # t783 = prims.convert_element_type(t782, dtypes.float32)  # t783: "cuda:0 f32[1, 512, 4096]"
    # t784 = prims.convert_element_type(t676, dtypes.float32)  # t784: "cuda:0 f32[1, 512, 4096]"
    # t785 = prims.add(t783, t784)  # t785: "cuda:0 f32[1, 512, 4096]"
    # t786 = prims.convert_element_type(t785, dtypes.bfloat16)  # t786: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t787 = prims.convert_element_type(t786, dtypes.float32)  # t787: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t788 = ltorch.mul(t787, t787)  # t788: "cuda:0 f32[1, 512, 4096]"
    # t788 = prims.mul(t787, t787)  # t788: "cuda:0 f32[1, 512, 4096]"
  t792 = ltorch.mean(t788, -1, True, dtype=None)  # t792: "cuda:0 f32[1, 512, 1]"
    # t790 = prims.sum(t788, (2,))  # t790: "cuda:0 f32[1, 512]"
    # t791 = prims.broadcast_in_dim(t790, [1, 512, 1], [0, 1])  # t791: "cuda:0 f32[1, 512, 1]"
    # t792 = ltorch.true_divide(t791, 4096)  # t792: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t792 = prims.div(t791, 4096.0)  # t792: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t794 = ltorch.add(t792, 1e-05, alpha=None)  # t794: "cuda:0 f32[1, 512, 1]"
    # t794 = prims.add(t792, 1e-05)  # t794: "cuda:0 f32[1, 512, 1]"
  t795 = ltorch.rsqrt(t794)  # t795: "cuda:0 f32[1, 512, 1]"
    # t795 = prims.rsqrt(t794)  # t795: "cuda:0 f32[1, 512, 1]"
  t797 = ltorch.mul(t787, t795)  # t797: "cuda:0 f32[1, 512, 4096]"
    # t796 = prims.broadcast_in_dim(t795, (1, 512, 4096), (0, 1, 2))  # t796: "cuda:0 f32[1, 512, 4096]"
    # t797 = prims.mul(t787, t796)  # t797: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t798 = ltorch.to(t797, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t798: "cuda:0 bf16[1, 512, 4096]"
    # t798 = prims.convert_element_type(t797, dtypes.bfloat16)  # t798: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t808 = ltorch.mul(t798, t_transformer_h_4_norm_2_weight)  # t808: "cuda:0 bf16[1, 512, 4096]"
    # t804 = prims.broadcast_in_dim(t_transformer_h_4_norm_2_weight, (1, 512, 4096), (2,))  # t804: "cuda:0 bf16[1, 512, 4096]"
    # t805 = prims.convert_element_type(t798, dtypes.float32)  # t805: "cuda:0 f32[1, 512, 4096]"
    # t806 = prims.convert_element_type(t804, dtypes.float32)  # t806: "cuda:0 f32[1, 512, 4096]"
    # t807 = prims.mul(t805, t806)  # t807: "cuda:0 f32[1, 512, 4096]"
    # t808 = prims.convert_element_type(t807, dtypes.bfloat16)  # t808: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t813 = ltorch.linear(t808, t_transformer_h_4_mlp_fc_1_weight, None)  # t813: "cuda:0 bf16[1, 512, 11008]"
    # t813 = prims.linear(t808, t_transformer_h_4_mlp_fc_1_weight, None)  # t813: "cuda:0 bf16[1, 512, 11008]"
  t817 = ltorch.linear(t808, t_transformer_h_4_mlp_fc_2_weight, None)  # t817: "cuda:0 bf16[1, 512, 11008]"
    # t817 = prims.linear(t808, t_transformer_h_4_mlp_fc_2_weight, None)  # t817: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t827 = ltorch.silu(t813, False)  # t827: "cuda:0 bf16[1, 512, 11008]"
    # t818 = prims.convert_element_type(t813, dtypes.float32)  # t818: "cuda:0 f32[1, 512, 11008]"
    # t819 = prims.neg(t818)  # t819: "cuda:0 f32[1, 512, 11008]"
    # t820 = prims.exp(t819)  # t820: "cuda:0 f32[1, 512, 11008]"
    # t821 = prims.add(1.0, t820)  # t821: "cuda:0 f32[1, 512, 11008]"
    # t822 = prims.reciprocal(t821)  # t822: "cuda:0 f32[1, 512, 11008]"
    # t823 = prims.convert_element_type(t822, dtypes.bfloat16)  # t823: "cuda:0 bf16[1, 512, 11008]"
    # t824 = prims.convert_element_type(t813, dtypes.float32)  # t824: "cuda:0 f32[1, 512, 11008]"
    # t825 = prims.convert_element_type(t823, dtypes.float32)  # t825: "cuda:0 f32[1, 512, 11008]"
    # t826 = prims.mul(t824, t825)  # t826: "cuda:0 f32[1, 512, 11008]"
    # t827 = prims.convert_element_type(t826, dtypes.bfloat16)  # t827: "cuda:0 bf16[1, 512, 11008]"
  t831 = ltorch.mul(t827, t817)  # t831: "cuda:0 bf16[1, 512, 11008]"
    # t828 = prims.convert_element_type(t827, dtypes.float32)  # t828: "cuda:0 f32[1, 512, 11008]"
    # t829 = prims.convert_element_type(t817, dtypes.float32)  # t829: "cuda:0 f32[1, 512, 11008]"
    # t830 = prims.mul(t828, t829)  # t830: "cuda:0 f32[1, 512, 11008]"
    # t831 = prims.convert_element_type(t830, dtypes.bfloat16)  # t831: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t835 = ltorch.linear(t831, t_transformer_h_4_mlp_proj_weight, None)  # t835: "cuda:0 bf16[1, 512, 4096]"
    # t835 = prims.linear(t831, t_transformer_h_4_mlp_proj_weight, None)  # t835: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t839 = ltorch.add(t835, t786, alpha=None)  # t839: "cuda:0 bf16[1, 512, 4096]"
    # t836 = prims.convert_element_type(t835, dtypes.float32)  # t836: "cuda:0 f32[1, 512, 4096]"
    # t837 = prims.convert_element_type(t786, dtypes.float32)  # t837: "cuda:0 f32[1, 512, 4096]"
    # t838 = prims.add(t836, t837)  # t838: "cuda:0 f32[1, 512, 4096]"
    # t839 = prims.convert_element_type(t838, dtypes.bfloat16)  # t839: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t841 = prims.convert_element_type(t839, dtypes.float32)  # t841: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t842 = ltorch.mul(t841, t841)  # t842: "cuda:0 f32[1, 512, 4096]"
    # t842 = prims.mul(t841, t841)  # t842: "cuda:0 f32[1, 512, 4096]"
  t846 = ltorch.mean(t842, -1, True, dtype=None)  # t846: "cuda:0 f32[1, 512, 1]"
    # t844 = prims.sum(t842, (2,))  # t844: "cuda:0 f32[1, 512]"
    # t845 = prims.broadcast_in_dim(t844, [1, 512, 1], [0, 1])  # t845: "cuda:0 f32[1, 512, 1]"
    # t846 = ltorch.true_divide(t845, 4096)  # t846: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t846 = prims.div(t845, 4096.0)  # t846: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t848 = ltorch.add(t846, 1e-05, alpha=None)  # t848: "cuda:0 f32[1, 512, 1]"
    # t848 = prims.add(t846, 1e-05)  # t848: "cuda:0 f32[1, 512, 1]"
  t849 = ltorch.rsqrt(t848)  # t849: "cuda:0 f32[1, 512, 1]"
    # t849 = prims.rsqrt(t848)  # t849: "cuda:0 f32[1, 512, 1]"
  t851 = ltorch.mul(t841, t849)  # t851: "cuda:0 f32[1, 512, 4096]"
    # t850 = prims.broadcast_in_dim(t849, (1, 512, 4096), (0, 1, 2))  # t850: "cuda:0 f32[1, 512, 4096]"
    # t851 = prims.mul(t841, t850)  # t851: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t852 = ltorch.to(t851, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t852: "cuda:0 bf16[1, 512, 4096]"
    # t852 = prims.convert_element_type(t851, dtypes.bfloat16)  # t852: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t862 = ltorch.mul(t852, t_transformer_h_5_norm_1_weight)  # t862: "cuda:0 bf16[1, 512, 4096]"
    # t858 = prims.broadcast_in_dim(t_transformer_h_5_norm_1_weight, (1, 512, 4096), (2,))  # t858: "cuda:0 bf16[1, 512, 4096]"
    # t859 = prims.convert_element_type(t852, dtypes.float32)  # t859: "cuda:0 f32[1, 512, 4096]"
    # t860 = prims.convert_element_type(t858, dtypes.float32)  # t860: "cuda:0 f32[1, 512, 4096]"
    # t861 = prims.mul(t859, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
    # t862 = prims.convert_element_type(t861, dtypes.bfloat16)  # t862: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t867 = ltorch.linear(t862, t_transformer_h_5_attn_attn_weight, None)  # t867: "cuda:0 bf16[1, 512, 12288]"
    # t867 = prims.linear(t862, t_transformer_h_5_attn_attn_weight, None)  # t867: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t868 = ltorch.view(t867, 1, 512, 32, 3, 128)  # t868: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t868 = ltorch.reshape(t867, (1, 512, 32, 3, 128))  # t868: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t868 = prims.reshape(t867, (1, 512, 32, 3, 128))  # t868: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t869 = ltorch.permute(t868, 0, 2, 3, 1, 4)  # t869: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t869 = prims.transpose(t868, (0, 2, 3, 1, 4))  # t869: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t870, t871, t872) = ltorch.split(t869, (1, 1, 1), 2)
    # t870 = prims.slice_prim(t869, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t870: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t871 = prims.slice_prim(t869, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t871: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t872 = prims.slice_prim(t869, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t872: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t873 = ltorch.reshape(t870, 1, -1, 512, 128)  # t873: "cuda:0 bf16[1, 32, 512, 128]"
    # t873 = prims.reshape(t870, (1, 32, 512, 128))  # t873: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t874 = ltorch.reshape(t871, 1, -1, 512, 128)  # t874: "cuda:0 bf16[1, 32, 512, 128]"
    # t874 = prims.reshape(t871, (1, 32, 512, 128))  # t874: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t875 = ltorch.reshape(t872, 1, -1, 512, 128)  # t875: "cuda:0 bf16[1, 32, 512, 128]"
    # t875 = prims.reshape(t872, (1, 32, 512, 128))  # t875: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t876 = ltorch.getitem(t873, (..., slice(None, 128, None)))  # t876: "cuda:0 bf16[1, 32, 512, 128]"
    # t876 = prims.slice_prim(t873, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t876: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t877 = ltorch.getitem(t876, (..., slice(None, 64, None)))  # t877: "cuda:0 bf16[1, 32, 512, 64]"
    # t877 = prims.slice_prim(t876, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t877: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t878 = ltorch.getitem(t876, (..., slice(64, None, None)))  # t878: "cuda:0 bf16[1, 32, 512, 64]"
    # t878 = prims.slice_prim(t876, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t878: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t881 = ltorch.neg(t878)  # t881: "cuda:0 bf16[1, 32, 512, 64]"
    # t879 = prims.convert_element_type(t878, dtypes.float32)  # t879: "cuda:0 f32[1, 32, 512, 64]"
    # t880 = prims.neg(t879)  # t880: "cuda:0 f32[1, 32, 512, 64]"
    # t881 = prims.convert_element_type(t880, dtypes.bfloat16)  # t881: "cuda:0 bf16[1, 32, 512, 64]"
  t882 = ltorch.cat((t881, t877), -1)  # t882: "cuda:0 bf16[1, 32, 512, 128]"
    # t882 = prims.cat((t881, t877), -1)  # t882: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t885 = ltorch.mul(t876, cos)  # t885: "cuda:0 f32[1, 32, 512, 128]"
    # t883 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t883: "cuda:0 f32[1, 32, 512, 128]"
    # t884 = prims.convert_element_type(t876, dtypes.float32)  # t884: "cuda:0 f32[1, 32, 512, 128]"
    # t885 = prims.mul(t884, t883)  # t885: "cuda:0 f32[1, 32, 512, 128]"
  t888 = ltorch.mul(t882, sin)  # t888: "cuda:0 f32[1, 32, 512, 128]"
    # t886 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t886: "cuda:0 f32[1, 32, 512, 128]"
    # t887 = prims.convert_element_type(t882, dtypes.float32)  # t887: "cuda:0 f32[1, 32, 512, 128]"
    # t888 = prims.mul(t887, t886)  # t888: "cuda:0 f32[1, 32, 512, 128]"
  t889 = ltorch.add(t885, t888, alpha=None)  # t889: "cuda:0 f32[1, 32, 512, 128]"
    # t889 = prims.add(t885, t888)  # t889: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t890 = ltorch.to(t889, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t890: "cuda:0 bf16[1, 32, 512, 128]"
    # t890 = prims.convert_element_type(t889, dtypes.bfloat16)  # t890: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t891 = ltorch.getitem(t874, (..., slice(None, 128, None)))  # t891: "cuda:0 bf16[1, 32, 512, 128]"
    # t891 = prims.slice_prim(t874, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t891: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t892 = ltorch.getitem(t891, (..., slice(None, 64, None)))  # t892: "cuda:0 bf16[1, 32, 512, 64]"
    # t892 = prims.slice_prim(t891, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t892: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t893 = ltorch.getitem(t891, (..., slice(64, None, None)))  # t893: "cuda:0 bf16[1, 32, 512, 64]"
    # t893 = prims.slice_prim(t891, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t893: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t896 = ltorch.neg(t893)  # t896: "cuda:0 bf16[1, 32, 512, 64]"
    # t894 = prims.convert_element_type(t893, dtypes.float32)  # t894: "cuda:0 f32[1, 32, 512, 64]"
    # t895 = prims.neg(t894)  # t895: "cuda:0 f32[1, 32, 512, 64]"
    # t896 = prims.convert_element_type(t895, dtypes.bfloat16)  # t896: "cuda:0 bf16[1, 32, 512, 64]"
  t897 = ltorch.cat((t896, t892), -1)  # t897: "cuda:0 bf16[1, 32, 512, 128]"
    # t897 = prims.cat((t896, t892), -1)  # t897: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t900 = ltorch.mul(t891, cos)  # t900: "cuda:0 f32[1, 32, 512, 128]"
    # t898 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t898: "cuda:0 f32[1, 32, 512, 128]"
    # t899 = prims.convert_element_type(t891, dtypes.float32)  # t899: "cuda:0 f32[1, 32, 512, 128]"
    # t900 = prims.mul(t899, t898)  # t900: "cuda:0 f32[1, 32, 512, 128]"
  t903 = ltorch.mul(t897, sin)  # t903: "cuda:0 f32[1, 32, 512, 128]"
    # t901 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t901: "cuda:0 f32[1, 32, 512, 128]"
    # t902 = prims.convert_element_type(t897, dtypes.float32)  # t902: "cuda:0 f32[1, 32, 512, 128]"
    # t903 = prims.mul(t902, t901)  # t903: "cuda:0 f32[1, 32, 512, 128]"
  t904 = ltorch.add(t900, t903, alpha=None)  # t904: "cuda:0 f32[1, 32, 512, 128]"
    # t904 = prims.add(t900, t903)  # t904: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t905 = ltorch.to(t904, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t905: "cuda:0 bf16[1, 32, 512, 128]"
    # t905 = prims.convert_element_type(t904, dtypes.bfloat16)  # t905: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t906 = ltorch.getitem(t873, (..., slice(128, None, None)))  # t906: "cuda:0 bf16[1, 32, 512, 0]"
    # t906 = prims.slice_prim(t873, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t906: "cuda:0 bf16[1, 32, 512, 0]"
  t907 = ltorch.cat((t890, t906), -1)  # t907: "cuda:0 bf16[1, 32, 512, 128]"
    # t907 = prims.cat((t890, t906), -1)  # t907: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t908 = ltorch.getitem(t874, (..., slice(128, None, None)))  # t908: "cuda:0 bf16[1, 32, 512, 0]"
    # t908 = prims.slice_prim(t874, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t908: "cuda:0 bf16[1, 32, 512, 0]"
  t909 = ltorch.cat((t905, t908), -1)  # t909: "cuda:0 bf16[1, 32, 512, 128]"
    # t909 = prims.cat((t905, t908), -1)  # t909: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t939 = ltorch.scaled_dot_product_attention(t907, t909, t875, None, 0.0, True, scale=0.08838834764831843)  # t939: "cuda:0 bf16[1, 32, 512, 128]"
    # t912 = ltorch.mul(t907, 0.29730177875068026)  # t912: "cuda:0 bf16[1, 32, 512, 128]"
      # t910 = prims.convert_element_type(t907, dtypes.float32)  # t910: "cuda:0 f32[1, 32, 512, 128]"
      # t911 = prims.mul(t910, 0.29730177875068026)  # t911: "cuda:0 f32[1, 32, 512, 128]"
      # t912 = prims.convert_element_type(t911, dtypes.bfloat16)  # t912: "cuda:0 bf16[1, 32, 512, 128]"
    # t913 = ltorch.transpose(t909, -2, -1)  # t913: "cuda:0 bf16[1, 32, 128, 512]"
      # t913 = prims.transpose(t909, (0, 1, 3, 2))  # t913: "cuda:0 bf16[1, 32, 128, 512]"
    # t916 = ltorch.mul(t913, 0.29730177875068026)  # t916: "cuda:0 bf16[1, 32, 128, 512]"
      # t914 = prims.convert_element_type(t913, dtypes.float32)  # t914: "cuda:0 f32[1, 32, 128, 512]"
      # t915 = prims.mul(t914, 0.29730177875068026)  # t915: "cuda:0 f32[1, 32, 128, 512]"
      # t916 = prims.convert_element_type(t915, dtypes.bfloat16)  # t916: "cuda:0 bf16[1, 32, 128, 512]"
    # t917 = ltorch.matmul(t912, t916)  # t917: "cuda:0 bf16[1, 32, 512, 512]"
      # t917 = prims.matmul(t912, t916)  # t917: "cuda:0 bf16[1, 32, 512, 512]"
    # t927 = ltorch.tril(t917, 0, fill_value=-float('inf'))  # t927: "cuda:0 bf16[1, 32, 512, 512]"
      # t918 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t918: "cuda:0 i64[512]"
        # t918 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t918: "cuda:0 i64[512]"
      # t919 = ltorch.unsqueeze(t918, -1)  # t919: "cuda:0 i64[512, 1]"
        # t919 = prims.broadcast_in_dim(t918, [512, 1], [0])  # t919: "cuda:0 i64[512, 1]"
      # t920 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t920: "cuda:0 i64[512]"
        # t920 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t920: "cuda:0 i64[512]"
      # t921 = ltorch.unsqueeze(t920, -2)  # t921: "cuda:0 i64[1, 512]"
        # t921 = prims.broadcast_in_dim(t920, [1, 512], [1])  # t921: "cuda:0 i64[1, 512]"
      # t922 = ltorch.add(t919, 0, alpha=None)  # t922: "cuda:0 i64[512, 1]"
        # t922 = prims.add(t919, 0)  # t922: "cuda:0 i64[512, 1]"
      # t925 = ltorch.ge(t922, t921)  # t925: "cuda:0 b8[512, 512]"
        # t923 = prims.broadcast_in_dim(t922, (512, 512), (0, 1))  # t923: "cuda:0 i64[512, 512]"
        # t924 = prims.broadcast_in_dim(t921, (512, 512), (0, 1))  # t924: "cuda:0 i64[512, 512]"
        # t925 = prims.ge(t923, t924)  # t925: "cuda:0 b8[512, 512]"
      # t927 = ltorch.where(t925, t917, -float('inf'))  # t927: "cuda:0 bf16[1, 32, 512, 512]"
        # t926 = prims.broadcast_in_dim(t925, (1, 32, 512, 512), (2, 3))  # t926: "cuda:0 b8[1, 32, 512, 512]"
        # t927 = prims.where(t926, t917, -float('inf'))  # t927: "cuda:0 bf16[1, 32, 512, 512]"
    # t938 = ltorch._softmax(t927, -1, dtype=None)  # t938: "cuda:0 bf16[1, 32, 512, 512]"
      # t928 = ltorch.to(t927, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t928: "cuda:0 f32[1, 32, 512, 512]"
        # t928 = prims.convert_element_type(t927, dtypes.float32)  # t928: "cuda:0 f32[1, 32, 512, 512]"
      # t930 = ltorch.amax(t928, -1, True)  # t930: "cuda:0 f32[1, 32, 512, 1]"
        # t929 = prims.amax(t928, (3,))  # t929: "cuda:0 f32[1, 32, 512]"
        # t930 = prims.broadcast_in_dim(t929, [1, 32, 512, 1], [0, 1, 2])  # t930: "cuda:0 f32[1, 32, 512, 1]"
      # t932 = ltorch.sub(t928, t930, alpha=None)  # t932: "cuda:0 f32[1, 32, 512, 512]"
        # t931 = prims.broadcast_in_dim(t930, (1, 32, 512, 512), (0, 1, 2, 3))  # t931: "cuda:0 f32[1, 32, 512, 512]"
        # t932 = prims.sub(t928, t931)  # t932: "cuda:0 f32[1, 32, 512, 512]"
      # t933 = ltorch.exp(t932)  # t933: "cuda:0 f32[1, 32, 512, 512]"
        # t933 = prims.exp(t932)  # t933: "cuda:0 f32[1, 32, 512, 512]"
      # t935 = ltorch.sum(t933, -1, True, dtype=None)  # t935: "cuda:0 f32[1, 32, 512, 1]"
        # t934 = prims.sum(t933, (3,))  # t934: "cuda:0 f32[1, 32, 512]"
        # t935 = prims.broadcast_in_dim(t934, [1, 32, 512, 1], [0, 1, 2])  # t935: "cuda:0 f32[1, 32, 512, 1]"
      # t937 = ltorch.true_divide(t933, t935)  # t937: "cuda:0 f32[1, 32, 512, 512]"
        # t936 = prims.broadcast_in_dim(t935, (1, 32, 512, 512), (0, 1, 2, 3))  # t936: "cuda:0 f32[1, 32, 512, 512]"
        # t937 = prims.div(t933, t936)  # t937: "cuda:0 f32[1, 32, 512, 512]"
      # t938 = ltorch.to(t937, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t938: "cuda:0 bf16[1, 32, 512, 512]"
        # t938 = prims.convert_element_type(t937, dtypes.bfloat16)  # t938: "cuda:0 bf16[1, 32, 512, 512]"
    # t939 = ltorch.matmul(t938, t875)  # t939: "cuda:0 bf16[1, 32, 512, 128]"
      # t939 = prims.matmul(t938, t875)  # t939: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t940 = ltorch.transpose(t939, 1, 2)  # t940: "cuda:0 bf16[1, 512, 32, 128]"
    # t940 = prims.transpose(t939, (0, 2, 1, 3))  # t940: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t941 = ltorch.reshape(t940, 1, 512, 4096)  # t941: "cuda:0 bf16[1, 512, 4096]"
    # t941 = prims.reshape(t940, (1, 512, 4096))  # t941: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t945 = ltorch.linear(t941, t_transformer_h_5_attn_proj_weight, None)  # t945: "cuda:0 bf16[1, 512, 4096]"
    # t945 = prims.linear(t941, t_transformer_h_5_attn_proj_weight, None)  # t945: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t949 = ltorch.add(t945, t839, alpha=None)  # t949: "cuda:0 bf16[1, 512, 4096]"
    # t946 = prims.convert_element_type(t945, dtypes.float32)  # t946: "cuda:0 f32[1, 512, 4096]"
    # t947 = prims.convert_element_type(t839, dtypes.float32)  # t947: "cuda:0 f32[1, 512, 4096]"
    # t948 = prims.add(t946, t947)  # t948: "cuda:0 f32[1, 512, 4096]"
    # t949 = prims.convert_element_type(t948, dtypes.bfloat16)  # t949: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t950 = prims.convert_element_type(t949, dtypes.float32)  # t950: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t951 = ltorch.mul(t950, t950)  # t951: "cuda:0 f32[1, 512, 4096]"
    # t951 = prims.mul(t950, t950)  # t951: "cuda:0 f32[1, 512, 4096]"
  t955 = ltorch.mean(t951, -1, True, dtype=None)  # t955: "cuda:0 f32[1, 512, 1]"
    # t953 = prims.sum(t951, (2,))  # t953: "cuda:0 f32[1, 512]"
    # t954 = prims.broadcast_in_dim(t953, [1, 512, 1], [0, 1])  # t954: "cuda:0 f32[1, 512, 1]"
    # t955 = ltorch.true_divide(t954, 4096)  # t955: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t955 = prims.div(t954, 4096.0)  # t955: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t957 = ltorch.add(t955, 1e-05, alpha=None)  # t957: "cuda:0 f32[1, 512, 1]"
    # t957 = prims.add(t955, 1e-05)  # t957: "cuda:0 f32[1, 512, 1]"
  t958 = ltorch.rsqrt(t957)  # t958: "cuda:0 f32[1, 512, 1]"
    # t958 = prims.rsqrt(t957)  # t958: "cuda:0 f32[1, 512, 1]"
  t960 = ltorch.mul(t950, t958)  # t960: "cuda:0 f32[1, 512, 4096]"
    # t959 = prims.broadcast_in_dim(t958, (1, 512, 4096), (0, 1, 2))  # t959: "cuda:0 f32[1, 512, 4096]"
    # t960 = prims.mul(t950, t959)  # t960: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t961 = ltorch.to(t960, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t961: "cuda:0 bf16[1, 512, 4096]"
    # t961 = prims.convert_element_type(t960, dtypes.bfloat16)  # t961: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t971 = ltorch.mul(t961, t_transformer_h_5_norm_2_weight)  # t971: "cuda:0 bf16[1, 512, 4096]"
    # t967 = prims.broadcast_in_dim(t_transformer_h_5_norm_2_weight, (1, 512, 4096), (2,))  # t967: "cuda:0 bf16[1, 512, 4096]"
    # t968 = prims.convert_element_type(t961, dtypes.float32)  # t968: "cuda:0 f32[1, 512, 4096]"
    # t969 = prims.convert_element_type(t967, dtypes.float32)  # t969: "cuda:0 f32[1, 512, 4096]"
    # t970 = prims.mul(t968, t969)  # t970: "cuda:0 f32[1, 512, 4096]"
    # t971 = prims.convert_element_type(t970, dtypes.bfloat16)  # t971: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t976 = ltorch.linear(t971, t_transformer_h_5_mlp_fc_1_weight, None)  # t976: "cuda:0 bf16[1, 512, 11008]"
    # t976 = prims.linear(t971, t_transformer_h_5_mlp_fc_1_weight, None)  # t976: "cuda:0 bf16[1, 512, 11008]"
  t980 = ltorch.linear(t971, t_transformer_h_5_mlp_fc_2_weight, None)  # t980: "cuda:0 bf16[1, 512, 11008]"
    # t980 = prims.linear(t971, t_transformer_h_5_mlp_fc_2_weight, None)  # t980: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t990 = ltorch.silu(t976, False)  # t990: "cuda:0 bf16[1, 512, 11008]"
    # t981 = prims.convert_element_type(t976, dtypes.float32)  # t981: "cuda:0 f32[1, 512, 11008]"
    # t982 = prims.neg(t981)  # t982: "cuda:0 f32[1, 512, 11008]"
    # t983 = prims.exp(t982)  # t983: "cuda:0 f32[1, 512, 11008]"
    # t984 = prims.add(1.0, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
    # t985 = prims.reciprocal(t984)  # t985: "cuda:0 f32[1, 512, 11008]"
    # t986 = prims.convert_element_type(t985, dtypes.bfloat16)  # t986: "cuda:0 bf16[1, 512, 11008]"
    # t987 = prims.convert_element_type(t976, dtypes.float32)  # t987: "cuda:0 f32[1, 512, 11008]"
    # t988 = prims.convert_element_type(t986, dtypes.float32)  # t988: "cuda:0 f32[1, 512, 11008]"
    # t989 = prims.mul(t987, t988)  # t989: "cuda:0 f32[1, 512, 11008]"
    # t990 = prims.convert_element_type(t989, dtypes.bfloat16)  # t990: "cuda:0 bf16[1, 512, 11008]"
  t994 = ltorch.mul(t990, t980)  # t994: "cuda:0 bf16[1, 512, 11008]"
    # t991 = prims.convert_element_type(t990, dtypes.float32)  # t991: "cuda:0 f32[1, 512, 11008]"
    # t992 = prims.convert_element_type(t980, dtypes.float32)  # t992: "cuda:0 f32[1, 512, 11008]"
    # t993 = prims.mul(t991, t992)  # t993: "cuda:0 f32[1, 512, 11008]"
    # t994 = prims.convert_element_type(t993, dtypes.bfloat16)  # t994: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t998 = ltorch.linear(t994, t_transformer_h_5_mlp_proj_weight, None)  # t998: "cuda:0 bf16[1, 512, 4096]"
    # t998 = prims.linear(t994, t_transformer_h_5_mlp_proj_weight, None)  # t998: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1002 = ltorch.add(t998, t949, alpha=None)  # t1002: "cuda:0 bf16[1, 512, 4096]"
    # t999 = prims.convert_element_type(t998, dtypes.float32)  # t999: "cuda:0 f32[1, 512, 4096]"
    # t1000 = prims.convert_element_type(t949, dtypes.float32)  # t1000: "cuda:0 f32[1, 512, 4096]"
    # t1001 = prims.add(t999, t1000)  # t1001: "cuda:0 f32[1, 512, 4096]"
    # t1002 = prims.convert_element_type(t1001, dtypes.bfloat16)  # t1002: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1004 = prims.convert_element_type(t1002, dtypes.float32)  # t1004: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1005 = ltorch.mul(t1004, t1004)  # t1005: "cuda:0 f32[1, 512, 4096]"
    # t1005 = prims.mul(t1004, t1004)  # t1005: "cuda:0 f32[1, 512, 4096]"
  t1009 = ltorch.mean(t1005, -1, True, dtype=None)  # t1009: "cuda:0 f32[1, 512, 1]"
    # t1007 = prims.sum(t1005, (2,))  # t1007: "cuda:0 f32[1, 512]"
    # t1008 = prims.broadcast_in_dim(t1007, [1, 512, 1], [0, 1])  # t1008: "cuda:0 f32[1, 512, 1]"
    # t1009 = ltorch.true_divide(t1008, 4096)  # t1009: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1009 = prims.div(t1008, 4096.0)  # t1009: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1011 = ltorch.add(t1009, 1e-05, alpha=None)  # t1011: "cuda:0 f32[1, 512, 1]"
    # t1011 = prims.add(t1009, 1e-05)  # t1011: "cuda:0 f32[1, 512, 1]"
  t1012 = ltorch.rsqrt(t1011)  # t1012: "cuda:0 f32[1, 512, 1]"
    # t1012 = prims.rsqrt(t1011)  # t1012: "cuda:0 f32[1, 512, 1]"
  t1014 = ltorch.mul(t1004, t1012)  # t1014: "cuda:0 f32[1, 512, 4096]"
    # t1013 = prims.broadcast_in_dim(t1012, (1, 512, 4096), (0, 1, 2))  # t1013: "cuda:0 f32[1, 512, 4096]"
    # t1014 = prims.mul(t1004, t1013)  # t1014: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1015 = ltorch.to(t1014, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1015: "cuda:0 bf16[1, 512, 4096]"
    # t1015 = prims.convert_element_type(t1014, dtypes.bfloat16)  # t1015: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1025 = ltorch.mul(t1015, t_transformer_h_6_norm_1_weight)  # t1025: "cuda:0 bf16[1, 512, 4096]"
    # t1021 = prims.broadcast_in_dim(t_transformer_h_6_norm_1_weight, (1, 512, 4096), (2,))  # t1021: "cuda:0 bf16[1, 512, 4096]"
    # t1022 = prims.convert_element_type(t1015, dtypes.float32)  # t1022: "cuda:0 f32[1, 512, 4096]"
    # t1023 = prims.convert_element_type(t1021, dtypes.float32)  # t1023: "cuda:0 f32[1, 512, 4096]"
    # t1024 = prims.mul(t1022, t1023)  # t1024: "cuda:0 f32[1, 512, 4096]"
    # t1025 = prims.convert_element_type(t1024, dtypes.bfloat16)  # t1025: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1030 = ltorch.linear(t1025, t_transformer_h_6_attn_attn_weight, None)  # t1030: "cuda:0 bf16[1, 512, 12288]"
    # t1030 = prims.linear(t1025, t_transformer_h_6_attn_attn_weight, None)  # t1030: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1031 = ltorch.view(t1030, 1, 512, 32, 3, 128)  # t1031: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1031 = ltorch.reshape(t1030, (1, 512, 32, 3, 128))  # t1031: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1031 = prims.reshape(t1030, (1, 512, 32, 3, 128))  # t1031: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1032 = ltorch.permute(t1031, 0, 2, 3, 1, 4)  # t1032: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1032 = prims.transpose(t1031, (0, 2, 3, 1, 4))  # t1032: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1033, t1034, t1035) = ltorch.split(t1032, (1, 1, 1), 2)
    # t1033 = prims.slice_prim(t1032, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1033: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1034 = prims.slice_prim(t1032, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1034: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1035 = prims.slice_prim(t1032, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1035: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1036 = ltorch.reshape(t1033, 1, -1, 512, 128)  # t1036: "cuda:0 bf16[1, 32, 512, 128]"
    # t1036 = prims.reshape(t1033, (1, 32, 512, 128))  # t1036: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1037 = ltorch.reshape(t1034, 1, -1, 512, 128)  # t1037: "cuda:0 bf16[1, 32, 512, 128]"
    # t1037 = prims.reshape(t1034, (1, 32, 512, 128))  # t1037: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1038 = ltorch.reshape(t1035, 1, -1, 512, 128)  # t1038: "cuda:0 bf16[1, 32, 512, 128]"
    # t1038 = prims.reshape(t1035, (1, 32, 512, 128))  # t1038: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1039 = ltorch.getitem(t1036, (..., slice(None, 128, None)))  # t1039: "cuda:0 bf16[1, 32, 512, 128]"
    # t1039 = prims.slice_prim(t1036, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1039: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1040 = ltorch.getitem(t1039, (..., slice(None, 64, None)))  # t1040: "cuda:0 bf16[1, 32, 512, 64]"
    # t1040 = prims.slice_prim(t1039, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1040: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1041 = ltorch.getitem(t1039, (..., slice(64, None, None)))  # t1041: "cuda:0 bf16[1, 32, 512, 64]"
    # t1041 = prims.slice_prim(t1039, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1041: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1044 = ltorch.neg(t1041)  # t1044: "cuda:0 bf16[1, 32, 512, 64]"
    # t1042 = prims.convert_element_type(t1041, dtypes.float32)  # t1042: "cuda:0 f32[1, 32, 512, 64]"
    # t1043 = prims.neg(t1042)  # t1043: "cuda:0 f32[1, 32, 512, 64]"
    # t1044 = prims.convert_element_type(t1043, dtypes.bfloat16)  # t1044: "cuda:0 bf16[1, 32, 512, 64]"
  t1045 = ltorch.cat((t1044, t1040), -1)  # t1045: "cuda:0 bf16[1, 32, 512, 128]"
    # t1045 = prims.cat((t1044, t1040), -1)  # t1045: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1048 = ltorch.mul(t1039, cos)  # t1048: "cuda:0 f32[1, 32, 512, 128]"
    # t1046 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1046: "cuda:0 f32[1, 32, 512, 128]"
    # t1047 = prims.convert_element_type(t1039, dtypes.float32)  # t1047: "cuda:0 f32[1, 32, 512, 128]"
    # t1048 = prims.mul(t1047, t1046)  # t1048: "cuda:0 f32[1, 32, 512, 128]"
  t1051 = ltorch.mul(t1045, sin)  # t1051: "cuda:0 f32[1, 32, 512, 128]"
    # t1049 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1049: "cuda:0 f32[1, 32, 512, 128]"
    # t1050 = prims.convert_element_type(t1045, dtypes.float32)  # t1050: "cuda:0 f32[1, 32, 512, 128]"
    # t1051 = prims.mul(t1050, t1049)  # t1051: "cuda:0 f32[1, 32, 512, 128]"
  t1052 = ltorch.add(t1048, t1051, alpha=None)  # t1052: "cuda:0 f32[1, 32, 512, 128]"
    # t1052 = prims.add(t1048, t1051)  # t1052: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1053 = ltorch.to(t1052, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1053: "cuda:0 bf16[1, 32, 512, 128]"
    # t1053 = prims.convert_element_type(t1052, dtypes.bfloat16)  # t1053: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1054 = ltorch.getitem(t1037, (..., slice(None, 128, None)))  # t1054: "cuda:0 bf16[1, 32, 512, 128]"
    # t1054 = prims.slice_prim(t1037, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1054: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1055 = ltorch.getitem(t1054, (..., slice(None, 64, None)))  # t1055: "cuda:0 bf16[1, 32, 512, 64]"
    # t1055 = prims.slice_prim(t1054, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1055: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1056 = ltorch.getitem(t1054, (..., slice(64, None, None)))  # t1056: "cuda:0 bf16[1, 32, 512, 64]"
    # t1056 = prims.slice_prim(t1054, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1056: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1059 = ltorch.neg(t1056)  # t1059: "cuda:0 bf16[1, 32, 512, 64]"
    # t1057 = prims.convert_element_type(t1056, dtypes.float32)  # t1057: "cuda:0 f32[1, 32, 512, 64]"
    # t1058 = prims.neg(t1057)  # t1058: "cuda:0 f32[1, 32, 512, 64]"
    # t1059 = prims.convert_element_type(t1058, dtypes.bfloat16)  # t1059: "cuda:0 bf16[1, 32, 512, 64]"
  t1060 = ltorch.cat((t1059, t1055), -1)  # t1060: "cuda:0 bf16[1, 32, 512, 128]"
    # t1060 = prims.cat((t1059, t1055), -1)  # t1060: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1063 = ltorch.mul(t1054, cos)  # t1063: "cuda:0 f32[1, 32, 512, 128]"
    # t1061 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1061: "cuda:0 f32[1, 32, 512, 128]"
    # t1062 = prims.convert_element_type(t1054, dtypes.float32)  # t1062: "cuda:0 f32[1, 32, 512, 128]"
    # t1063 = prims.mul(t1062, t1061)  # t1063: "cuda:0 f32[1, 32, 512, 128]"
  t1066 = ltorch.mul(t1060, sin)  # t1066: "cuda:0 f32[1, 32, 512, 128]"
    # t1064 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1064: "cuda:0 f32[1, 32, 512, 128]"
    # t1065 = prims.convert_element_type(t1060, dtypes.float32)  # t1065: "cuda:0 f32[1, 32, 512, 128]"
    # t1066 = prims.mul(t1065, t1064)  # t1066: "cuda:0 f32[1, 32, 512, 128]"
  t1067 = ltorch.add(t1063, t1066, alpha=None)  # t1067: "cuda:0 f32[1, 32, 512, 128]"
    # t1067 = prims.add(t1063, t1066)  # t1067: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1068 = ltorch.to(t1067, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1068: "cuda:0 bf16[1, 32, 512, 128]"
    # t1068 = prims.convert_element_type(t1067, dtypes.bfloat16)  # t1068: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1069 = ltorch.getitem(t1036, (..., slice(128, None, None)))  # t1069: "cuda:0 bf16[1, 32, 512, 0]"
    # t1069 = prims.slice_prim(t1036, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1069: "cuda:0 bf16[1, 32, 512, 0]"
  t1070 = ltorch.cat((t1053, t1069), -1)  # t1070: "cuda:0 bf16[1, 32, 512, 128]"
    # t1070 = prims.cat((t1053, t1069), -1)  # t1070: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1071 = ltorch.getitem(t1037, (..., slice(128, None, None)))  # t1071: "cuda:0 bf16[1, 32, 512, 0]"
    # t1071 = prims.slice_prim(t1037, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1071: "cuda:0 bf16[1, 32, 512, 0]"
  t1072 = ltorch.cat((t1068, t1071), -1)  # t1072: "cuda:0 bf16[1, 32, 512, 128]"
    # t1072 = prims.cat((t1068, t1071), -1)  # t1072: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1102 = ltorch.scaled_dot_product_attention(t1070, t1072, t1038, None, 0.0, True, scale=0.08838834764831843)  # t1102: "cuda:0 bf16[1, 32, 512, 128]"
    # t1075 = ltorch.mul(t1070, 0.29730177875068026)  # t1075: "cuda:0 bf16[1, 32, 512, 128]"
      # t1073 = prims.convert_element_type(t1070, dtypes.float32)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
      # t1074 = prims.mul(t1073, 0.29730177875068026)  # t1074: "cuda:0 f32[1, 32, 512, 128]"
      # t1075 = prims.convert_element_type(t1074, dtypes.bfloat16)  # t1075: "cuda:0 bf16[1, 32, 512, 128]"
    # t1076 = ltorch.transpose(t1072, -2, -1)  # t1076: "cuda:0 bf16[1, 32, 128, 512]"
      # t1076 = prims.transpose(t1072, (0, 1, 3, 2))  # t1076: "cuda:0 bf16[1, 32, 128, 512]"
    # t1079 = ltorch.mul(t1076, 0.29730177875068026)  # t1079: "cuda:0 bf16[1, 32, 128, 512]"
      # t1077 = prims.convert_element_type(t1076, dtypes.float32)  # t1077: "cuda:0 f32[1, 32, 128, 512]"
      # t1078 = prims.mul(t1077, 0.29730177875068026)  # t1078: "cuda:0 f32[1, 32, 128, 512]"
      # t1079 = prims.convert_element_type(t1078, dtypes.bfloat16)  # t1079: "cuda:0 bf16[1, 32, 128, 512]"
    # t1080 = ltorch.matmul(t1075, t1079)  # t1080: "cuda:0 bf16[1, 32, 512, 512]"
      # t1080 = prims.matmul(t1075, t1079)  # t1080: "cuda:0 bf16[1, 32, 512, 512]"
    # t1090 = ltorch.tril(t1080, 0, fill_value=-float('inf'))  # t1090: "cuda:0 bf16[1, 32, 512, 512]"
      # t1081 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1081: "cuda:0 i64[512]"
        # t1081 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1081: "cuda:0 i64[512]"
      # t1082 = ltorch.unsqueeze(t1081, -1)  # t1082: "cuda:0 i64[512, 1]"
        # t1082 = prims.broadcast_in_dim(t1081, [512, 1], [0])  # t1082: "cuda:0 i64[512, 1]"
      # t1083 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1083: "cuda:0 i64[512]"
        # t1083 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1083: "cuda:0 i64[512]"
      # t1084 = ltorch.unsqueeze(t1083, -2)  # t1084: "cuda:0 i64[1, 512]"
        # t1084 = prims.broadcast_in_dim(t1083, [1, 512], [1])  # t1084: "cuda:0 i64[1, 512]"
      # t1085 = ltorch.add(t1082, 0, alpha=None)  # t1085: "cuda:0 i64[512, 1]"
        # t1085 = prims.add(t1082, 0)  # t1085: "cuda:0 i64[512, 1]"
      # t1088 = ltorch.ge(t1085, t1084)  # t1088: "cuda:0 b8[512, 512]"
        # t1086 = prims.broadcast_in_dim(t1085, (512, 512), (0, 1))  # t1086: "cuda:0 i64[512, 512]"
        # t1087 = prims.broadcast_in_dim(t1084, (512, 512), (0, 1))  # t1087: "cuda:0 i64[512, 512]"
        # t1088 = prims.ge(t1086, t1087)  # t1088: "cuda:0 b8[512, 512]"
      # t1090 = ltorch.where(t1088, t1080, -float('inf'))  # t1090: "cuda:0 bf16[1, 32, 512, 512]"
        # t1089 = prims.broadcast_in_dim(t1088, (1, 32, 512, 512), (2, 3))  # t1089: "cuda:0 b8[1, 32, 512, 512]"
        # t1090 = prims.where(t1089, t1080, -float('inf'))  # t1090: "cuda:0 bf16[1, 32, 512, 512]"
    # t1101 = ltorch._softmax(t1090, -1, dtype=None)  # t1101: "cuda:0 bf16[1, 32, 512, 512]"
      # t1091 = ltorch.to(t1090, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1091: "cuda:0 f32[1, 32, 512, 512]"
        # t1091 = prims.convert_element_type(t1090, dtypes.float32)  # t1091: "cuda:0 f32[1, 32, 512, 512]"
      # t1093 = ltorch.amax(t1091, -1, True)  # t1093: "cuda:0 f32[1, 32, 512, 1]"
        # t1092 = prims.amax(t1091, (3,))  # t1092: "cuda:0 f32[1, 32, 512]"
        # t1093 = prims.broadcast_in_dim(t1092, [1, 32, 512, 1], [0, 1, 2])  # t1093: "cuda:0 f32[1, 32, 512, 1]"
      # t1095 = ltorch.sub(t1091, t1093, alpha=None)  # t1095: "cuda:0 f32[1, 32, 512, 512]"
        # t1094 = prims.broadcast_in_dim(t1093, (1, 32, 512, 512), (0, 1, 2, 3))  # t1094: "cuda:0 f32[1, 32, 512, 512]"
        # t1095 = prims.sub(t1091, t1094)  # t1095: "cuda:0 f32[1, 32, 512, 512]"
      # t1096 = ltorch.exp(t1095)  # t1096: "cuda:0 f32[1, 32, 512, 512]"
        # t1096 = prims.exp(t1095)  # t1096: "cuda:0 f32[1, 32, 512, 512]"
      # t1098 = ltorch.sum(t1096, -1, True, dtype=None)  # t1098: "cuda:0 f32[1, 32, 512, 1]"
        # t1097 = prims.sum(t1096, (3,))  # t1097: "cuda:0 f32[1, 32, 512]"
        # t1098 = prims.broadcast_in_dim(t1097, [1, 32, 512, 1], [0, 1, 2])  # t1098: "cuda:0 f32[1, 32, 512, 1]"
      # t1100 = ltorch.true_divide(t1096, t1098)  # t1100: "cuda:0 f32[1, 32, 512, 512]"
        # t1099 = prims.broadcast_in_dim(t1098, (1, 32, 512, 512), (0, 1, 2, 3))  # t1099: "cuda:0 f32[1, 32, 512, 512]"
        # t1100 = prims.div(t1096, t1099)  # t1100: "cuda:0 f32[1, 32, 512, 512]"
      # t1101 = ltorch.to(t1100, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1101: "cuda:0 bf16[1, 32, 512, 512]"
        # t1101 = prims.convert_element_type(t1100, dtypes.bfloat16)  # t1101: "cuda:0 bf16[1, 32, 512, 512]"
    # t1102 = ltorch.matmul(t1101, t1038)  # t1102: "cuda:0 bf16[1, 32, 512, 128]"
      # t1102 = prims.matmul(t1101, t1038)  # t1102: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1103 = ltorch.transpose(t1102, 1, 2)  # t1103: "cuda:0 bf16[1, 512, 32, 128]"
    # t1103 = prims.transpose(t1102, (0, 2, 1, 3))  # t1103: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1104 = ltorch.reshape(t1103, 1, 512, 4096)  # t1104: "cuda:0 bf16[1, 512, 4096]"
    # t1104 = prims.reshape(t1103, (1, 512, 4096))  # t1104: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1108 = ltorch.linear(t1104, t_transformer_h_6_attn_proj_weight, None)  # t1108: "cuda:0 bf16[1, 512, 4096]"
    # t1108 = prims.linear(t1104, t_transformer_h_6_attn_proj_weight, None)  # t1108: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1112 = ltorch.add(t1108, t1002, alpha=None)  # t1112: "cuda:0 bf16[1, 512, 4096]"
    # t1109 = prims.convert_element_type(t1108, dtypes.float32)  # t1109: "cuda:0 f32[1, 512, 4096]"
    # t1110 = prims.convert_element_type(t1002, dtypes.float32)  # t1110: "cuda:0 f32[1, 512, 4096]"
    # t1111 = prims.add(t1109, t1110)  # t1111: "cuda:0 f32[1, 512, 4096]"
    # t1112 = prims.convert_element_type(t1111, dtypes.bfloat16)  # t1112: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1113 = prims.convert_element_type(t1112, dtypes.float32)  # t1113: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1114 = ltorch.mul(t1113, t1113)  # t1114: "cuda:0 f32[1, 512, 4096]"
    # t1114 = prims.mul(t1113, t1113)  # t1114: "cuda:0 f32[1, 512, 4096]"
  t1118 = ltorch.mean(t1114, -1, True, dtype=None)  # t1118: "cuda:0 f32[1, 512, 1]"
    # t1116 = prims.sum(t1114, (2,))  # t1116: "cuda:0 f32[1, 512]"
    # t1117 = prims.broadcast_in_dim(t1116, [1, 512, 1], [0, 1])  # t1117: "cuda:0 f32[1, 512, 1]"
    # t1118 = ltorch.true_divide(t1117, 4096)  # t1118: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1118 = prims.div(t1117, 4096.0)  # t1118: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1120 = ltorch.add(t1118, 1e-05, alpha=None)  # t1120: "cuda:0 f32[1, 512, 1]"
    # t1120 = prims.add(t1118, 1e-05)  # t1120: "cuda:0 f32[1, 512, 1]"
  t1121 = ltorch.rsqrt(t1120)  # t1121: "cuda:0 f32[1, 512, 1]"
    # t1121 = prims.rsqrt(t1120)  # t1121: "cuda:0 f32[1, 512, 1]"
  t1123 = ltorch.mul(t1113, t1121)  # t1123: "cuda:0 f32[1, 512, 4096]"
    # t1122 = prims.broadcast_in_dim(t1121, (1, 512, 4096), (0, 1, 2))  # t1122: "cuda:0 f32[1, 512, 4096]"
    # t1123 = prims.mul(t1113, t1122)  # t1123: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1124 = ltorch.to(t1123, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1124: "cuda:0 bf16[1, 512, 4096]"
    # t1124 = prims.convert_element_type(t1123, dtypes.bfloat16)  # t1124: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1134 = ltorch.mul(t1124, t_transformer_h_6_norm_2_weight)  # t1134: "cuda:0 bf16[1, 512, 4096]"
    # t1130 = prims.broadcast_in_dim(t_transformer_h_6_norm_2_weight, (1, 512, 4096), (2,))  # t1130: "cuda:0 bf16[1, 512, 4096]"
    # t1131 = prims.convert_element_type(t1124, dtypes.float32)  # t1131: "cuda:0 f32[1, 512, 4096]"
    # t1132 = prims.convert_element_type(t1130, dtypes.float32)  # t1132: "cuda:0 f32[1, 512, 4096]"
    # t1133 = prims.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 4096]"
    # t1134 = prims.convert_element_type(t1133, dtypes.bfloat16)  # t1134: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1139 = ltorch.linear(t1134, t_transformer_h_6_mlp_fc_1_weight, None)  # t1139: "cuda:0 bf16[1, 512, 11008]"
    # t1139 = prims.linear(t1134, t_transformer_h_6_mlp_fc_1_weight, None)  # t1139: "cuda:0 bf16[1, 512, 11008]"
  t1143 = ltorch.linear(t1134, t_transformer_h_6_mlp_fc_2_weight, None)  # t1143: "cuda:0 bf16[1, 512, 11008]"
    # t1143 = prims.linear(t1134, t_transformer_h_6_mlp_fc_2_weight, None)  # t1143: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1153 = ltorch.silu(t1139, False)  # t1153: "cuda:0 bf16[1, 512, 11008]"
    # t1144 = prims.convert_element_type(t1139, dtypes.float32)  # t1144: "cuda:0 f32[1, 512, 11008]"
    # t1145 = prims.neg(t1144)  # t1145: "cuda:0 f32[1, 512, 11008]"
    # t1146 = prims.exp(t1145)  # t1146: "cuda:0 f32[1, 512, 11008]"
    # t1147 = prims.add(1.0, t1146)  # t1147: "cuda:0 f32[1, 512, 11008]"
    # t1148 = prims.reciprocal(t1147)  # t1148: "cuda:0 f32[1, 512, 11008]"
    # t1149 = prims.convert_element_type(t1148, dtypes.bfloat16)  # t1149: "cuda:0 bf16[1, 512, 11008]"
    # t1150 = prims.convert_element_type(t1139, dtypes.float32)  # t1150: "cuda:0 f32[1, 512, 11008]"
    # t1151 = prims.convert_element_type(t1149, dtypes.float32)  # t1151: "cuda:0 f32[1, 512, 11008]"
    # t1152 = prims.mul(t1150, t1151)  # t1152: "cuda:0 f32[1, 512, 11008]"
    # t1153 = prims.convert_element_type(t1152, dtypes.bfloat16)  # t1153: "cuda:0 bf16[1, 512, 11008]"
  t1157 = ltorch.mul(t1153, t1143)  # t1157: "cuda:0 bf16[1, 512, 11008]"
    # t1154 = prims.convert_element_type(t1153, dtypes.float32)  # t1154: "cuda:0 f32[1, 512, 11008]"
    # t1155 = prims.convert_element_type(t1143, dtypes.float32)  # t1155: "cuda:0 f32[1, 512, 11008]"
    # t1156 = prims.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 11008]"
    # t1157 = prims.convert_element_type(t1156, dtypes.bfloat16)  # t1157: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1161 = ltorch.linear(t1157, t_transformer_h_6_mlp_proj_weight, None)  # t1161: "cuda:0 bf16[1, 512, 4096]"
    # t1161 = prims.linear(t1157, t_transformer_h_6_mlp_proj_weight, None)  # t1161: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1165 = ltorch.add(t1161, t1112, alpha=None)  # t1165: "cuda:0 bf16[1, 512, 4096]"
    # t1162 = prims.convert_element_type(t1161, dtypes.float32)  # t1162: "cuda:0 f32[1, 512, 4096]"
    # t1163 = prims.convert_element_type(t1112, dtypes.float32)  # t1163: "cuda:0 f32[1, 512, 4096]"
    # t1164 = prims.add(t1162, t1163)  # t1164: "cuda:0 f32[1, 512, 4096]"
    # t1165 = prims.convert_element_type(t1164, dtypes.bfloat16)  # t1165: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1167 = prims.convert_element_type(t1165, dtypes.float32)  # t1167: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1168 = ltorch.mul(t1167, t1167)  # t1168: "cuda:0 f32[1, 512, 4096]"
    # t1168 = prims.mul(t1167, t1167)  # t1168: "cuda:0 f32[1, 512, 4096]"
  t1172 = ltorch.mean(t1168, -1, True, dtype=None)  # t1172: "cuda:0 f32[1, 512, 1]"
    # t1170 = prims.sum(t1168, (2,))  # t1170: "cuda:0 f32[1, 512]"
    # t1171 = prims.broadcast_in_dim(t1170, [1, 512, 1], [0, 1])  # t1171: "cuda:0 f32[1, 512, 1]"
    # t1172 = ltorch.true_divide(t1171, 4096)  # t1172: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1172 = prims.div(t1171, 4096.0)  # t1172: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1174 = ltorch.add(t1172, 1e-05, alpha=None)  # t1174: "cuda:0 f32[1, 512, 1]"
    # t1174 = prims.add(t1172, 1e-05)  # t1174: "cuda:0 f32[1, 512, 1]"
  t1175 = ltorch.rsqrt(t1174)  # t1175: "cuda:0 f32[1, 512, 1]"
    # t1175 = prims.rsqrt(t1174)  # t1175: "cuda:0 f32[1, 512, 1]"
  t1177 = ltorch.mul(t1167, t1175)  # t1177: "cuda:0 f32[1, 512, 4096]"
    # t1176 = prims.broadcast_in_dim(t1175, (1, 512, 4096), (0, 1, 2))  # t1176: "cuda:0 f32[1, 512, 4096]"
    # t1177 = prims.mul(t1167, t1176)  # t1177: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1178 = ltorch.to(t1177, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1178: "cuda:0 bf16[1, 512, 4096]"
    # t1178 = prims.convert_element_type(t1177, dtypes.bfloat16)  # t1178: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1188 = ltorch.mul(t1178, t_transformer_h_7_norm_1_weight)  # t1188: "cuda:0 bf16[1, 512, 4096]"
    # t1184 = prims.broadcast_in_dim(t_transformer_h_7_norm_1_weight, (1, 512, 4096), (2,))  # t1184: "cuda:0 bf16[1, 512, 4096]"
    # t1185 = prims.convert_element_type(t1178, dtypes.float32)  # t1185: "cuda:0 f32[1, 512, 4096]"
    # t1186 = prims.convert_element_type(t1184, dtypes.float32)  # t1186: "cuda:0 f32[1, 512, 4096]"
    # t1187 = prims.mul(t1185, t1186)  # t1187: "cuda:0 f32[1, 512, 4096]"
    # t1188 = prims.convert_element_type(t1187, dtypes.bfloat16)  # t1188: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1193 = ltorch.linear(t1188, t_transformer_h_7_attn_attn_weight, None)  # t1193: "cuda:0 bf16[1, 512, 12288]"
    # t1193 = prims.linear(t1188, t_transformer_h_7_attn_attn_weight, None)  # t1193: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1194 = ltorch.view(t1193, 1, 512, 32, 3, 128)  # t1194: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1194 = ltorch.reshape(t1193, (1, 512, 32, 3, 128))  # t1194: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1194 = prims.reshape(t1193, (1, 512, 32, 3, 128))  # t1194: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1195 = ltorch.permute(t1194, 0, 2, 3, 1, 4)  # t1195: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1195 = prims.transpose(t1194, (0, 2, 3, 1, 4))  # t1195: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1196, t1197, t1198) = ltorch.split(t1195, (1, 1, 1), 2)
    # t1196 = prims.slice_prim(t1195, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1196: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1197 = prims.slice_prim(t1195, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1197: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1198 = prims.slice_prim(t1195, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1198: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1199 = ltorch.reshape(t1196, 1, -1, 512, 128)  # t1199: "cuda:0 bf16[1, 32, 512, 128]"
    # t1199 = prims.reshape(t1196, (1, 32, 512, 128))  # t1199: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1200 = ltorch.reshape(t1197, 1, -1, 512, 128)  # t1200: "cuda:0 bf16[1, 32, 512, 128]"
    # t1200 = prims.reshape(t1197, (1, 32, 512, 128))  # t1200: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1201 = ltorch.reshape(t1198, 1, -1, 512, 128)  # t1201: "cuda:0 bf16[1, 32, 512, 128]"
    # t1201 = prims.reshape(t1198, (1, 32, 512, 128))  # t1201: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1202 = ltorch.getitem(t1199, (..., slice(None, 128, None)))  # t1202: "cuda:0 bf16[1, 32, 512, 128]"
    # t1202 = prims.slice_prim(t1199, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1202: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1203 = ltorch.getitem(t1202, (..., slice(None, 64, None)))  # t1203: "cuda:0 bf16[1, 32, 512, 64]"
    # t1203 = prims.slice_prim(t1202, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1203: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1204 = ltorch.getitem(t1202, (..., slice(64, None, None)))  # t1204: "cuda:0 bf16[1, 32, 512, 64]"
    # t1204 = prims.slice_prim(t1202, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1204: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1207 = ltorch.neg(t1204)  # t1207: "cuda:0 bf16[1, 32, 512, 64]"
    # t1205 = prims.convert_element_type(t1204, dtypes.float32)  # t1205: "cuda:0 f32[1, 32, 512, 64]"
    # t1206 = prims.neg(t1205)  # t1206: "cuda:0 f32[1, 32, 512, 64]"
    # t1207 = prims.convert_element_type(t1206, dtypes.bfloat16)  # t1207: "cuda:0 bf16[1, 32, 512, 64]"
  t1208 = ltorch.cat((t1207, t1203), -1)  # t1208: "cuda:0 bf16[1, 32, 512, 128]"
    # t1208 = prims.cat((t1207, t1203), -1)  # t1208: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1211 = ltorch.mul(t1202, cos)  # t1211: "cuda:0 f32[1, 32, 512, 128]"
    # t1209 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1209: "cuda:0 f32[1, 32, 512, 128]"
    # t1210 = prims.convert_element_type(t1202, dtypes.float32)  # t1210: "cuda:0 f32[1, 32, 512, 128]"
    # t1211 = prims.mul(t1210, t1209)  # t1211: "cuda:0 f32[1, 32, 512, 128]"
  t1214 = ltorch.mul(t1208, sin)  # t1214: "cuda:0 f32[1, 32, 512, 128]"
    # t1212 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1212: "cuda:0 f32[1, 32, 512, 128]"
    # t1213 = prims.convert_element_type(t1208, dtypes.float32)  # t1213: "cuda:0 f32[1, 32, 512, 128]"
    # t1214 = prims.mul(t1213, t1212)  # t1214: "cuda:0 f32[1, 32, 512, 128]"
  t1215 = ltorch.add(t1211, t1214, alpha=None)  # t1215: "cuda:0 f32[1, 32, 512, 128]"
    # t1215 = prims.add(t1211, t1214)  # t1215: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1216 = ltorch.to(t1215, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1216: "cuda:0 bf16[1, 32, 512, 128]"
    # t1216 = prims.convert_element_type(t1215, dtypes.bfloat16)  # t1216: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1217 = ltorch.getitem(t1200, (..., slice(None, 128, None)))  # t1217: "cuda:0 bf16[1, 32, 512, 128]"
    # t1217 = prims.slice_prim(t1200, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1217: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1218 = ltorch.getitem(t1217, (..., slice(None, 64, None)))  # t1218: "cuda:0 bf16[1, 32, 512, 64]"
    # t1218 = prims.slice_prim(t1217, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1218: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1219 = ltorch.getitem(t1217, (..., slice(64, None, None)))  # t1219: "cuda:0 bf16[1, 32, 512, 64]"
    # t1219 = prims.slice_prim(t1217, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1219: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1222 = ltorch.neg(t1219)  # t1222: "cuda:0 bf16[1, 32, 512, 64]"
    # t1220 = prims.convert_element_type(t1219, dtypes.float32)  # t1220: "cuda:0 f32[1, 32, 512, 64]"
    # t1221 = prims.neg(t1220)  # t1221: "cuda:0 f32[1, 32, 512, 64]"
    # t1222 = prims.convert_element_type(t1221, dtypes.bfloat16)  # t1222: "cuda:0 bf16[1, 32, 512, 64]"
  t1223 = ltorch.cat((t1222, t1218), -1)  # t1223: "cuda:0 bf16[1, 32, 512, 128]"
    # t1223 = prims.cat((t1222, t1218), -1)  # t1223: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1226 = ltorch.mul(t1217, cos)  # t1226: "cuda:0 f32[1, 32, 512, 128]"
    # t1224 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1224: "cuda:0 f32[1, 32, 512, 128]"
    # t1225 = prims.convert_element_type(t1217, dtypes.float32)  # t1225: "cuda:0 f32[1, 32, 512, 128]"
    # t1226 = prims.mul(t1225, t1224)  # t1226: "cuda:0 f32[1, 32, 512, 128]"
  t1229 = ltorch.mul(t1223, sin)  # t1229: "cuda:0 f32[1, 32, 512, 128]"
    # t1227 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1227: "cuda:0 f32[1, 32, 512, 128]"
    # t1228 = prims.convert_element_type(t1223, dtypes.float32)  # t1228: "cuda:0 f32[1, 32, 512, 128]"
    # t1229 = prims.mul(t1228, t1227)  # t1229: "cuda:0 f32[1, 32, 512, 128]"
  t1230 = ltorch.add(t1226, t1229, alpha=None)  # t1230: "cuda:0 f32[1, 32, 512, 128]"
    # t1230 = prims.add(t1226, t1229)  # t1230: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1231 = ltorch.to(t1230, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1231: "cuda:0 bf16[1, 32, 512, 128]"
    # t1231 = prims.convert_element_type(t1230, dtypes.bfloat16)  # t1231: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1232 = ltorch.getitem(t1199, (..., slice(128, None, None)))  # t1232: "cuda:0 bf16[1, 32, 512, 0]"
    # t1232 = prims.slice_prim(t1199, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1232: "cuda:0 bf16[1, 32, 512, 0]"
  t1233 = ltorch.cat((t1216, t1232), -1)  # t1233: "cuda:0 bf16[1, 32, 512, 128]"
    # t1233 = prims.cat((t1216, t1232), -1)  # t1233: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1234 = ltorch.getitem(t1200, (..., slice(128, None, None)))  # t1234: "cuda:0 bf16[1, 32, 512, 0]"
    # t1234 = prims.slice_prim(t1200, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1234: "cuda:0 bf16[1, 32, 512, 0]"
  t1235 = ltorch.cat((t1231, t1234), -1)  # t1235: "cuda:0 bf16[1, 32, 512, 128]"
    # t1235 = prims.cat((t1231, t1234), -1)  # t1235: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1265 = ltorch.scaled_dot_product_attention(t1233, t1235, t1201, None, 0.0, True, scale=0.08838834764831843)  # t1265: "cuda:0 bf16[1, 32, 512, 128]"
    # t1238 = ltorch.mul(t1233, 0.29730177875068026)  # t1238: "cuda:0 bf16[1, 32, 512, 128]"
      # t1236 = prims.convert_element_type(t1233, dtypes.float32)  # t1236: "cuda:0 f32[1, 32, 512, 128]"
      # t1237 = prims.mul(t1236, 0.29730177875068026)  # t1237: "cuda:0 f32[1, 32, 512, 128]"
      # t1238 = prims.convert_element_type(t1237, dtypes.bfloat16)  # t1238: "cuda:0 bf16[1, 32, 512, 128]"
    # t1239 = ltorch.transpose(t1235, -2, -1)  # t1239: "cuda:0 bf16[1, 32, 128, 512]"
      # t1239 = prims.transpose(t1235, (0, 1, 3, 2))  # t1239: "cuda:0 bf16[1, 32, 128, 512]"
    # t1242 = ltorch.mul(t1239, 0.29730177875068026)  # t1242: "cuda:0 bf16[1, 32, 128, 512]"
      # t1240 = prims.convert_element_type(t1239, dtypes.float32)  # t1240: "cuda:0 f32[1, 32, 128, 512]"
      # t1241 = prims.mul(t1240, 0.29730177875068026)  # t1241: "cuda:0 f32[1, 32, 128, 512]"
      # t1242 = prims.convert_element_type(t1241, dtypes.bfloat16)  # t1242: "cuda:0 bf16[1, 32, 128, 512]"
    # t1243 = ltorch.matmul(t1238, t1242)  # t1243: "cuda:0 bf16[1, 32, 512, 512]"
      # t1243 = prims.matmul(t1238, t1242)  # t1243: "cuda:0 bf16[1, 32, 512, 512]"
    # t1253 = ltorch.tril(t1243, 0, fill_value=-float('inf'))  # t1253: "cuda:0 bf16[1, 32, 512, 512]"
      # t1244 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1244: "cuda:0 i64[512]"
        # t1244 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1244: "cuda:0 i64[512]"
      # t1245 = ltorch.unsqueeze(t1244, -1)  # t1245: "cuda:0 i64[512, 1]"
        # t1245 = prims.broadcast_in_dim(t1244, [512, 1], [0])  # t1245: "cuda:0 i64[512, 1]"
      # t1246 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1246: "cuda:0 i64[512]"
        # t1246 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1246: "cuda:0 i64[512]"
      # t1247 = ltorch.unsqueeze(t1246, -2)  # t1247: "cuda:0 i64[1, 512]"
        # t1247 = prims.broadcast_in_dim(t1246, [1, 512], [1])  # t1247: "cuda:0 i64[1, 512]"
      # t1248 = ltorch.add(t1245, 0, alpha=None)  # t1248: "cuda:0 i64[512, 1]"
        # t1248 = prims.add(t1245, 0)  # t1248: "cuda:0 i64[512, 1]"
      # t1251 = ltorch.ge(t1248, t1247)  # t1251: "cuda:0 b8[512, 512]"
        # t1249 = prims.broadcast_in_dim(t1248, (512, 512), (0, 1))  # t1249: "cuda:0 i64[512, 512]"
        # t1250 = prims.broadcast_in_dim(t1247, (512, 512), (0, 1))  # t1250: "cuda:0 i64[512, 512]"
        # t1251 = prims.ge(t1249, t1250)  # t1251: "cuda:0 b8[512, 512]"
      # t1253 = ltorch.where(t1251, t1243, -float('inf'))  # t1253: "cuda:0 bf16[1, 32, 512, 512]"
        # t1252 = prims.broadcast_in_dim(t1251, (1, 32, 512, 512), (2, 3))  # t1252: "cuda:0 b8[1, 32, 512, 512]"
        # t1253 = prims.where(t1252, t1243, -float('inf'))  # t1253: "cuda:0 bf16[1, 32, 512, 512]"
    # t1264 = ltorch._softmax(t1253, -1, dtype=None)  # t1264: "cuda:0 bf16[1, 32, 512, 512]"
      # t1254 = ltorch.to(t1253, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1254: "cuda:0 f32[1, 32, 512, 512]"
        # t1254 = prims.convert_element_type(t1253, dtypes.float32)  # t1254: "cuda:0 f32[1, 32, 512, 512]"
      # t1256 = ltorch.amax(t1254, -1, True)  # t1256: "cuda:0 f32[1, 32, 512, 1]"
        # t1255 = prims.amax(t1254, (3,))  # t1255: "cuda:0 f32[1, 32, 512]"
        # t1256 = prims.broadcast_in_dim(t1255, [1, 32, 512, 1], [0, 1, 2])  # t1256: "cuda:0 f32[1, 32, 512, 1]"
      # t1258 = ltorch.sub(t1254, t1256, alpha=None)  # t1258: "cuda:0 f32[1, 32, 512, 512]"
        # t1257 = prims.broadcast_in_dim(t1256, (1, 32, 512, 512), (0, 1, 2, 3))  # t1257: "cuda:0 f32[1, 32, 512, 512]"
        # t1258 = prims.sub(t1254, t1257)  # t1258: "cuda:0 f32[1, 32, 512, 512]"
      # t1259 = ltorch.exp(t1258)  # t1259: "cuda:0 f32[1, 32, 512, 512]"
        # t1259 = prims.exp(t1258)  # t1259: "cuda:0 f32[1, 32, 512, 512]"
      # t1261 = ltorch.sum(t1259, -1, True, dtype=None)  # t1261: "cuda:0 f32[1, 32, 512, 1]"
        # t1260 = prims.sum(t1259, (3,))  # t1260: "cuda:0 f32[1, 32, 512]"
        # t1261 = prims.broadcast_in_dim(t1260, [1, 32, 512, 1], [0, 1, 2])  # t1261: "cuda:0 f32[1, 32, 512, 1]"
      # t1263 = ltorch.true_divide(t1259, t1261)  # t1263: "cuda:0 f32[1, 32, 512, 512]"
        # t1262 = prims.broadcast_in_dim(t1261, (1, 32, 512, 512), (0, 1, 2, 3))  # t1262: "cuda:0 f32[1, 32, 512, 512]"
        # t1263 = prims.div(t1259, t1262)  # t1263: "cuda:0 f32[1, 32, 512, 512]"
      # t1264 = ltorch.to(t1263, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1264: "cuda:0 bf16[1, 32, 512, 512]"
        # t1264 = prims.convert_element_type(t1263, dtypes.bfloat16)  # t1264: "cuda:0 bf16[1, 32, 512, 512]"
    # t1265 = ltorch.matmul(t1264, t1201)  # t1265: "cuda:0 bf16[1, 32, 512, 128]"
      # t1265 = prims.matmul(t1264, t1201)  # t1265: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1266 = ltorch.transpose(t1265, 1, 2)  # t1266: "cuda:0 bf16[1, 512, 32, 128]"
    # t1266 = prims.transpose(t1265, (0, 2, 1, 3))  # t1266: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1267 = ltorch.reshape(t1266, 1, 512, 4096)  # t1267: "cuda:0 bf16[1, 512, 4096]"
    # t1267 = prims.reshape(t1266, (1, 512, 4096))  # t1267: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1271 = ltorch.linear(t1267, t_transformer_h_7_attn_proj_weight, None)  # t1271: "cuda:0 bf16[1, 512, 4096]"
    # t1271 = prims.linear(t1267, t_transformer_h_7_attn_proj_weight, None)  # t1271: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1275 = ltorch.add(t1271, t1165, alpha=None)  # t1275: "cuda:0 bf16[1, 512, 4096]"
    # t1272 = prims.convert_element_type(t1271, dtypes.float32)  # t1272: "cuda:0 f32[1, 512, 4096]"
    # t1273 = prims.convert_element_type(t1165, dtypes.float32)  # t1273: "cuda:0 f32[1, 512, 4096]"
    # t1274 = prims.add(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 4096]"
    # t1275 = prims.convert_element_type(t1274, dtypes.bfloat16)  # t1275: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1276 = prims.convert_element_type(t1275, dtypes.float32)  # t1276: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1277 = ltorch.mul(t1276, t1276)  # t1277: "cuda:0 f32[1, 512, 4096]"
    # t1277 = prims.mul(t1276, t1276)  # t1277: "cuda:0 f32[1, 512, 4096]"
  t1281 = ltorch.mean(t1277, -1, True, dtype=None)  # t1281: "cuda:0 f32[1, 512, 1]"
    # t1279 = prims.sum(t1277, (2,))  # t1279: "cuda:0 f32[1, 512]"
    # t1280 = prims.broadcast_in_dim(t1279, [1, 512, 1], [0, 1])  # t1280: "cuda:0 f32[1, 512, 1]"
    # t1281 = ltorch.true_divide(t1280, 4096)  # t1281: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1281 = prims.div(t1280, 4096.0)  # t1281: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1283 = ltorch.add(t1281, 1e-05, alpha=None)  # t1283: "cuda:0 f32[1, 512, 1]"
    # t1283 = prims.add(t1281, 1e-05)  # t1283: "cuda:0 f32[1, 512, 1]"
  t1284 = ltorch.rsqrt(t1283)  # t1284: "cuda:0 f32[1, 512, 1]"
    # t1284 = prims.rsqrt(t1283)  # t1284: "cuda:0 f32[1, 512, 1]"
  t1286 = ltorch.mul(t1276, t1284)  # t1286: "cuda:0 f32[1, 512, 4096]"
    # t1285 = prims.broadcast_in_dim(t1284, (1, 512, 4096), (0, 1, 2))  # t1285: "cuda:0 f32[1, 512, 4096]"
    # t1286 = prims.mul(t1276, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1287 = ltorch.to(t1286, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1287: "cuda:0 bf16[1, 512, 4096]"
    # t1287 = prims.convert_element_type(t1286, dtypes.bfloat16)  # t1287: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1297 = ltorch.mul(t1287, t_transformer_h_7_norm_2_weight)  # t1297: "cuda:0 bf16[1, 512, 4096]"
    # t1293 = prims.broadcast_in_dim(t_transformer_h_7_norm_2_weight, (1, 512, 4096), (2,))  # t1293: "cuda:0 bf16[1, 512, 4096]"
    # t1294 = prims.convert_element_type(t1287, dtypes.float32)  # t1294: "cuda:0 f32[1, 512, 4096]"
    # t1295 = prims.convert_element_type(t1293, dtypes.float32)  # t1295: "cuda:0 f32[1, 512, 4096]"
    # t1296 = prims.mul(t1294, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
    # t1297 = prims.convert_element_type(t1296, dtypes.bfloat16)  # t1297: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1302 = ltorch.linear(t1297, t_transformer_h_7_mlp_fc_1_weight, None)  # t1302: "cuda:0 bf16[1, 512, 11008]"
    # t1302 = prims.linear(t1297, t_transformer_h_7_mlp_fc_1_weight, None)  # t1302: "cuda:0 bf16[1, 512, 11008]"
  t1306 = ltorch.linear(t1297, t_transformer_h_7_mlp_fc_2_weight, None)  # t1306: "cuda:0 bf16[1, 512, 11008]"
    # t1306 = prims.linear(t1297, t_transformer_h_7_mlp_fc_2_weight, None)  # t1306: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1316 = ltorch.silu(t1302, False)  # t1316: "cuda:0 bf16[1, 512, 11008]"
    # t1307 = prims.convert_element_type(t1302, dtypes.float32)  # t1307: "cuda:0 f32[1, 512, 11008]"
    # t1308 = prims.neg(t1307)  # t1308: "cuda:0 f32[1, 512, 11008]"
    # t1309 = prims.exp(t1308)  # t1309: "cuda:0 f32[1, 512, 11008]"
    # t1310 = prims.add(1.0, t1309)  # t1310: "cuda:0 f32[1, 512, 11008]"
    # t1311 = prims.reciprocal(t1310)  # t1311: "cuda:0 f32[1, 512, 11008]"
    # t1312 = prims.convert_element_type(t1311, dtypes.bfloat16)  # t1312: "cuda:0 bf16[1, 512, 11008]"
    # t1313 = prims.convert_element_type(t1302, dtypes.float32)  # t1313: "cuda:0 f32[1, 512, 11008]"
    # t1314 = prims.convert_element_type(t1312, dtypes.float32)  # t1314: "cuda:0 f32[1, 512, 11008]"
    # t1315 = prims.mul(t1313, t1314)  # t1315: "cuda:0 f32[1, 512, 11008]"
    # t1316 = prims.convert_element_type(t1315, dtypes.bfloat16)  # t1316: "cuda:0 bf16[1, 512, 11008]"
  t1320 = ltorch.mul(t1316, t1306)  # t1320: "cuda:0 bf16[1, 512, 11008]"
    # t1317 = prims.convert_element_type(t1316, dtypes.float32)  # t1317: "cuda:0 f32[1, 512, 11008]"
    # t1318 = prims.convert_element_type(t1306, dtypes.float32)  # t1318: "cuda:0 f32[1, 512, 11008]"
    # t1319 = prims.mul(t1317, t1318)  # t1319: "cuda:0 f32[1, 512, 11008]"
    # t1320 = prims.convert_element_type(t1319, dtypes.bfloat16)  # t1320: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1324 = ltorch.linear(t1320, t_transformer_h_7_mlp_proj_weight, None)  # t1324: "cuda:0 bf16[1, 512, 4096]"
    # t1324 = prims.linear(t1320, t_transformer_h_7_mlp_proj_weight, None)  # t1324: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1328 = ltorch.add(t1324, t1275, alpha=None)  # t1328: "cuda:0 bf16[1, 512, 4096]"
    # t1325 = prims.convert_element_type(t1324, dtypes.float32)  # t1325: "cuda:0 f32[1, 512, 4096]"
    # t1326 = prims.convert_element_type(t1275, dtypes.float32)  # t1326: "cuda:0 f32[1, 512, 4096]"
    # t1327 = prims.add(t1325, t1326)  # t1327: "cuda:0 f32[1, 512, 4096]"
    # t1328 = prims.convert_element_type(t1327, dtypes.bfloat16)  # t1328: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1330 = prims.convert_element_type(t1328, dtypes.float32)  # t1330: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1331 = ltorch.mul(t1330, t1330)  # t1331: "cuda:0 f32[1, 512, 4096]"
    # t1331 = prims.mul(t1330, t1330)  # t1331: "cuda:0 f32[1, 512, 4096]"
  t1335 = ltorch.mean(t1331, -1, True, dtype=None)  # t1335: "cuda:0 f32[1, 512, 1]"
    # t1333 = prims.sum(t1331, (2,))  # t1333: "cuda:0 f32[1, 512]"
    # t1334 = prims.broadcast_in_dim(t1333, [1, 512, 1], [0, 1])  # t1334: "cuda:0 f32[1, 512, 1]"
    # t1335 = ltorch.true_divide(t1334, 4096)  # t1335: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1335 = prims.div(t1334, 4096.0)  # t1335: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1337 = ltorch.add(t1335, 1e-05, alpha=None)  # t1337: "cuda:0 f32[1, 512, 1]"
    # t1337 = prims.add(t1335, 1e-05)  # t1337: "cuda:0 f32[1, 512, 1]"
  t1338 = ltorch.rsqrt(t1337)  # t1338: "cuda:0 f32[1, 512, 1]"
    # t1338 = prims.rsqrt(t1337)  # t1338: "cuda:0 f32[1, 512, 1]"
  t1340 = ltorch.mul(t1330, t1338)  # t1340: "cuda:0 f32[1, 512, 4096]"
    # t1339 = prims.broadcast_in_dim(t1338, (1, 512, 4096), (0, 1, 2))  # t1339: "cuda:0 f32[1, 512, 4096]"
    # t1340 = prims.mul(t1330, t1339)  # t1340: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1341 = ltorch.to(t1340, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1341: "cuda:0 bf16[1, 512, 4096]"
    # t1341 = prims.convert_element_type(t1340, dtypes.bfloat16)  # t1341: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1351 = ltorch.mul(t1341, t_transformer_h_8_norm_1_weight)  # t1351: "cuda:0 bf16[1, 512, 4096]"
    # t1347 = prims.broadcast_in_dim(t_transformer_h_8_norm_1_weight, (1, 512, 4096), (2,))  # t1347: "cuda:0 bf16[1, 512, 4096]"
    # t1348 = prims.convert_element_type(t1341, dtypes.float32)  # t1348: "cuda:0 f32[1, 512, 4096]"
    # t1349 = prims.convert_element_type(t1347, dtypes.float32)  # t1349: "cuda:0 f32[1, 512, 4096]"
    # t1350 = prims.mul(t1348, t1349)  # t1350: "cuda:0 f32[1, 512, 4096]"
    # t1351 = prims.convert_element_type(t1350, dtypes.bfloat16)  # t1351: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1356 = ltorch.linear(t1351, t_transformer_h_8_attn_attn_weight, None)  # t1356: "cuda:0 bf16[1, 512, 12288]"
    # t1356 = prims.linear(t1351, t_transformer_h_8_attn_attn_weight, None)  # t1356: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1357 = ltorch.view(t1356, 1, 512, 32, 3, 128)  # t1357: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1357 = ltorch.reshape(t1356, (1, 512, 32, 3, 128))  # t1357: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1357 = prims.reshape(t1356, (1, 512, 32, 3, 128))  # t1357: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1358 = ltorch.permute(t1357, 0, 2, 3, 1, 4)  # t1358: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1358 = prims.transpose(t1357, (0, 2, 3, 1, 4))  # t1358: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1359, t1360, t1361) = ltorch.split(t1358, (1, 1, 1), 2)
    # t1359 = prims.slice_prim(t1358, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1359: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1360 = prims.slice_prim(t1358, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1360: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1361 = prims.slice_prim(t1358, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1361: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1362 = ltorch.reshape(t1359, 1, -1, 512, 128)  # t1362: "cuda:0 bf16[1, 32, 512, 128]"
    # t1362 = prims.reshape(t1359, (1, 32, 512, 128))  # t1362: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1363 = ltorch.reshape(t1360, 1, -1, 512, 128)  # t1363: "cuda:0 bf16[1, 32, 512, 128]"
    # t1363 = prims.reshape(t1360, (1, 32, 512, 128))  # t1363: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1364 = ltorch.reshape(t1361, 1, -1, 512, 128)  # t1364: "cuda:0 bf16[1, 32, 512, 128]"
    # t1364 = prims.reshape(t1361, (1, 32, 512, 128))  # t1364: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1365 = ltorch.getitem(t1362, (..., slice(None, 128, None)))  # t1365: "cuda:0 bf16[1, 32, 512, 128]"
    # t1365 = prims.slice_prim(t1362, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1365: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1366 = ltorch.getitem(t1365, (..., slice(None, 64, None)))  # t1366: "cuda:0 bf16[1, 32, 512, 64]"
    # t1366 = prims.slice_prim(t1365, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1366: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1367 = ltorch.getitem(t1365, (..., slice(64, None, None)))  # t1367: "cuda:0 bf16[1, 32, 512, 64]"
    # t1367 = prims.slice_prim(t1365, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1367: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1370 = ltorch.neg(t1367)  # t1370: "cuda:0 bf16[1, 32, 512, 64]"
    # t1368 = prims.convert_element_type(t1367, dtypes.float32)  # t1368: "cuda:0 f32[1, 32, 512, 64]"
    # t1369 = prims.neg(t1368)  # t1369: "cuda:0 f32[1, 32, 512, 64]"
    # t1370 = prims.convert_element_type(t1369, dtypes.bfloat16)  # t1370: "cuda:0 bf16[1, 32, 512, 64]"
  t1371 = ltorch.cat((t1370, t1366), -1)  # t1371: "cuda:0 bf16[1, 32, 512, 128]"
    # t1371 = prims.cat((t1370, t1366), -1)  # t1371: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1374 = ltorch.mul(t1365, cos)  # t1374: "cuda:0 f32[1, 32, 512, 128]"
    # t1372 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1372: "cuda:0 f32[1, 32, 512, 128]"
    # t1373 = prims.convert_element_type(t1365, dtypes.float32)  # t1373: "cuda:0 f32[1, 32, 512, 128]"
    # t1374 = prims.mul(t1373, t1372)  # t1374: "cuda:0 f32[1, 32, 512, 128]"
  t1377 = ltorch.mul(t1371, sin)  # t1377: "cuda:0 f32[1, 32, 512, 128]"
    # t1375 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1375: "cuda:0 f32[1, 32, 512, 128]"
    # t1376 = prims.convert_element_type(t1371, dtypes.float32)  # t1376: "cuda:0 f32[1, 32, 512, 128]"
    # t1377 = prims.mul(t1376, t1375)  # t1377: "cuda:0 f32[1, 32, 512, 128]"
  t1378 = ltorch.add(t1374, t1377, alpha=None)  # t1378: "cuda:0 f32[1, 32, 512, 128]"
    # t1378 = prims.add(t1374, t1377)  # t1378: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1379 = ltorch.to(t1378, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1379: "cuda:0 bf16[1, 32, 512, 128]"
    # t1379 = prims.convert_element_type(t1378, dtypes.bfloat16)  # t1379: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1380 = ltorch.getitem(t1363, (..., slice(None, 128, None)))  # t1380: "cuda:0 bf16[1, 32, 512, 128]"
    # t1380 = prims.slice_prim(t1363, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1380: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1381 = ltorch.getitem(t1380, (..., slice(None, 64, None)))  # t1381: "cuda:0 bf16[1, 32, 512, 64]"
    # t1381 = prims.slice_prim(t1380, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1381: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1382 = ltorch.getitem(t1380, (..., slice(64, None, None)))  # t1382: "cuda:0 bf16[1, 32, 512, 64]"
    # t1382 = prims.slice_prim(t1380, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1382: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1385 = ltorch.neg(t1382)  # t1385: "cuda:0 bf16[1, 32, 512, 64]"
    # t1383 = prims.convert_element_type(t1382, dtypes.float32)  # t1383: "cuda:0 f32[1, 32, 512, 64]"
    # t1384 = prims.neg(t1383)  # t1384: "cuda:0 f32[1, 32, 512, 64]"
    # t1385 = prims.convert_element_type(t1384, dtypes.bfloat16)  # t1385: "cuda:0 bf16[1, 32, 512, 64]"
  t1386 = ltorch.cat((t1385, t1381), -1)  # t1386: "cuda:0 bf16[1, 32, 512, 128]"
    # t1386 = prims.cat((t1385, t1381), -1)  # t1386: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1389 = ltorch.mul(t1380, cos)  # t1389: "cuda:0 f32[1, 32, 512, 128]"
    # t1387 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1387: "cuda:0 f32[1, 32, 512, 128]"
    # t1388 = prims.convert_element_type(t1380, dtypes.float32)  # t1388: "cuda:0 f32[1, 32, 512, 128]"
    # t1389 = prims.mul(t1388, t1387)  # t1389: "cuda:0 f32[1, 32, 512, 128]"
  t1392 = ltorch.mul(t1386, sin)  # t1392: "cuda:0 f32[1, 32, 512, 128]"
    # t1390 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1390: "cuda:0 f32[1, 32, 512, 128]"
    # t1391 = prims.convert_element_type(t1386, dtypes.float32)  # t1391: "cuda:0 f32[1, 32, 512, 128]"
    # t1392 = prims.mul(t1391, t1390)  # t1392: "cuda:0 f32[1, 32, 512, 128]"
  t1393 = ltorch.add(t1389, t1392, alpha=None)  # t1393: "cuda:0 f32[1, 32, 512, 128]"
    # t1393 = prims.add(t1389, t1392)  # t1393: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1394 = ltorch.to(t1393, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1394: "cuda:0 bf16[1, 32, 512, 128]"
    # t1394 = prims.convert_element_type(t1393, dtypes.bfloat16)  # t1394: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1395 = ltorch.getitem(t1362, (..., slice(128, None, None)))  # t1395: "cuda:0 bf16[1, 32, 512, 0]"
    # t1395 = prims.slice_prim(t1362, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1395: "cuda:0 bf16[1, 32, 512, 0]"
  t1396 = ltorch.cat((t1379, t1395), -1)  # t1396: "cuda:0 bf16[1, 32, 512, 128]"
    # t1396 = prims.cat((t1379, t1395), -1)  # t1396: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1397 = ltorch.getitem(t1363, (..., slice(128, None, None)))  # t1397: "cuda:0 bf16[1, 32, 512, 0]"
    # t1397 = prims.slice_prim(t1363, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1397: "cuda:0 bf16[1, 32, 512, 0]"
  t1398 = ltorch.cat((t1394, t1397), -1)  # t1398: "cuda:0 bf16[1, 32, 512, 128]"
    # t1398 = prims.cat((t1394, t1397), -1)  # t1398: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1428 = ltorch.scaled_dot_product_attention(t1396, t1398, t1364, None, 0.0, True, scale=0.08838834764831843)  # t1428: "cuda:0 bf16[1, 32, 512, 128]"
    # t1401 = ltorch.mul(t1396, 0.29730177875068026)  # t1401: "cuda:0 bf16[1, 32, 512, 128]"
      # t1399 = prims.convert_element_type(t1396, dtypes.float32)  # t1399: "cuda:0 f32[1, 32, 512, 128]"
      # t1400 = prims.mul(t1399, 0.29730177875068026)  # t1400: "cuda:0 f32[1, 32, 512, 128]"
      # t1401 = prims.convert_element_type(t1400, dtypes.bfloat16)  # t1401: "cuda:0 bf16[1, 32, 512, 128]"
    # t1402 = ltorch.transpose(t1398, -2, -1)  # t1402: "cuda:0 bf16[1, 32, 128, 512]"
      # t1402 = prims.transpose(t1398, (0, 1, 3, 2))  # t1402: "cuda:0 bf16[1, 32, 128, 512]"
    # t1405 = ltorch.mul(t1402, 0.29730177875068026)  # t1405: "cuda:0 bf16[1, 32, 128, 512]"
      # t1403 = prims.convert_element_type(t1402, dtypes.float32)  # t1403: "cuda:0 f32[1, 32, 128, 512]"
      # t1404 = prims.mul(t1403, 0.29730177875068026)  # t1404: "cuda:0 f32[1, 32, 128, 512]"
      # t1405 = prims.convert_element_type(t1404, dtypes.bfloat16)  # t1405: "cuda:0 bf16[1, 32, 128, 512]"
    # t1406 = ltorch.matmul(t1401, t1405)  # t1406: "cuda:0 bf16[1, 32, 512, 512]"
      # t1406 = prims.matmul(t1401, t1405)  # t1406: "cuda:0 bf16[1, 32, 512, 512]"
    # t1416 = ltorch.tril(t1406, 0, fill_value=-float('inf'))  # t1416: "cuda:0 bf16[1, 32, 512, 512]"
      # t1407 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1407: "cuda:0 i64[512]"
        # t1407 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1407: "cuda:0 i64[512]"
      # t1408 = ltorch.unsqueeze(t1407, -1)  # t1408: "cuda:0 i64[512, 1]"
        # t1408 = prims.broadcast_in_dim(t1407, [512, 1], [0])  # t1408: "cuda:0 i64[512, 1]"
      # t1409 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1409: "cuda:0 i64[512]"
        # t1409 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1409: "cuda:0 i64[512]"
      # t1410 = ltorch.unsqueeze(t1409, -2)  # t1410: "cuda:0 i64[1, 512]"
        # t1410 = prims.broadcast_in_dim(t1409, [1, 512], [1])  # t1410: "cuda:0 i64[1, 512]"
      # t1411 = ltorch.add(t1408, 0, alpha=None)  # t1411: "cuda:0 i64[512, 1]"
        # t1411 = prims.add(t1408, 0)  # t1411: "cuda:0 i64[512, 1]"
      # t1414 = ltorch.ge(t1411, t1410)  # t1414: "cuda:0 b8[512, 512]"
        # t1412 = prims.broadcast_in_dim(t1411, (512, 512), (0, 1))  # t1412: "cuda:0 i64[512, 512]"
        # t1413 = prims.broadcast_in_dim(t1410, (512, 512), (0, 1))  # t1413: "cuda:0 i64[512, 512]"
        # t1414 = prims.ge(t1412, t1413)  # t1414: "cuda:0 b8[512, 512]"
      # t1416 = ltorch.where(t1414, t1406, -float('inf'))  # t1416: "cuda:0 bf16[1, 32, 512, 512]"
        # t1415 = prims.broadcast_in_dim(t1414, (1, 32, 512, 512), (2, 3))  # t1415: "cuda:0 b8[1, 32, 512, 512]"
        # t1416 = prims.where(t1415, t1406, -float('inf'))  # t1416: "cuda:0 bf16[1, 32, 512, 512]"
    # t1427 = ltorch._softmax(t1416, -1, dtype=None)  # t1427: "cuda:0 bf16[1, 32, 512, 512]"
      # t1417 = ltorch.to(t1416, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1417: "cuda:0 f32[1, 32, 512, 512]"
        # t1417 = prims.convert_element_type(t1416, dtypes.float32)  # t1417: "cuda:0 f32[1, 32, 512, 512]"
      # t1419 = ltorch.amax(t1417, -1, True)  # t1419: "cuda:0 f32[1, 32, 512, 1]"
        # t1418 = prims.amax(t1417, (3,))  # t1418: "cuda:0 f32[1, 32, 512]"
        # t1419 = prims.broadcast_in_dim(t1418, [1, 32, 512, 1], [0, 1, 2])  # t1419: "cuda:0 f32[1, 32, 512, 1]"
      # t1421 = ltorch.sub(t1417, t1419, alpha=None)  # t1421: "cuda:0 f32[1, 32, 512, 512]"
        # t1420 = prims.broadcast_in_dim(t1419, (1, 32, 512, 512), (0, 1, 2, 3))  # t1420: "cuda:0 f32[1, 32, 512, 512]"
        # t1421 = prims.sub(t1417, t1420)  # t1421: "cuda:0 f32[1, 32, 512, 512]"
      # t1422 = ltorch.exp(t1421)  # t1422: "cuda:0 f32[1, 32, 512, 512]"
        # t1422 = prims.exp(t1421)  # t1422: "cuda:0 f32[1, 32, 512, 512]"
      # t1424 = ltorch.sum(t1422, -1, True, dtype=None)  # t1424: "cuda:0 f32[1, 32, 512, 1]"
        # t1423 = prims.sum(t1422, (3,))  # t1423: "cuda:0 f32[1, 32, 512]"
        # t1424 = prims.broadcast_in_dim(t1423, [1, 32, 512, 1], [0, 1, 2])  # t1424: "cuda:0 f32[1, 32, 512, 1]"
      # t1426 = ltorch.true_divide(t1422, t1424)  # t1426: "cuda:0 f32[1, 32, 512, 512]"
        # t1425 = prims.broadcast_in_dim(t1424, (1, 32, 512, 512), (0, 1, 2, 3))  # t1425: "cuda:0 f32[1, 32, 512, 512]"
        # t1426 = prims.div(t1422, t1425)  # t1426: "cuda:0 f32[1, 32, 512, 512]"
      # t1427 = ltorch.to(t1426, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1427: "cuda:0 bf16[1, 32, 512, 512]"
        # t1427 = prims.convert_element_type(t1426, dtypes.bfloat16)  # t1427: "cuda:0 bf16[1, 32, 512, 512]"
    # t1428 = ltorch.matmul(t1427, t1364)  # t1428: "cuda:0 bf16[1, 32, 512, 128]"
      # t1428 = prims.matmul(t1427, t1364)  # t1428: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1429 = ltorch.transpose(t1428, 1, 2)  # t1429: "cuda:0 bf16[1, 512, 32, 128]"
    # t1429 = prims.transpose(t1428, (0, 2, 1, 3))  # t1429: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1430 = ltorch.reshape(t1429, 1, 512, 4096)  # t1430: "cuda:0 bf16[1, 512, 4096]"
    # t1430 = prims.reshape(t1429, (1, 512, 4096))  # t1430: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1434 = ltorch.linear(t1430, t_transformer_h_8_attn_proj_weight, None)  # t1434: "cuda:0 bf16[1, 512, 4096]"
    # t1434 = prims.linear(t1430, t_transformer_h_8_attn_proj_weight, None)  # t1434: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1438 = ltorch.add(t1434, t1328, alpha=None)  # t1438: "cuda:0 bf16[1, 512, 4096]"
    # t1435 = prims.convert_element_type(t1434, dtypes.float32)  # t1435: "cuda:0 f32[1, 512, 4096]"
    # t1436 = prims.convert_element_type(t1328, dtypes.float32)  # t1436: "cuda:0 f32[1, 512, 4096]"
    # t1437 = prims.add(t1435, t1436)  # t1437: "cuda:0 f32[1, 512, 4096]"
    # t1438 = prims.convert_element_type(t1437, dtypes.bfloat16)  # t1438: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1439 = prims.convert_element_type(t1438, dtypes.float32)  # t1439: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1440 = ltorch.mul(t1439, t1439)  # t1440: "cuda:0 f32[1, 512, 4096]"
    # t1440 = prims.mul(t1439, t1439)  # t1440: "cuda:0 f32[1, 512, 4096]"
  t1444 = ltorch.mean(t1440, -1, True, dtype=None)  # t1444: "cuda:0 f32[1, 512, 1]"
    # t1442 = prims.sum(t1440, (2,))  # t1442: "cuda:0 f32[1, 512]"
    # t1443 = prims.broadcast_in_dim(t1442, [1, 512, 1], [0, 1])  # t1443: "cuda:0 f32[1, 512, 1]"
    # t1444 = ltorch.true_divide(t1443, 4096)  # t1444: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1444 = prims.div(t1443, 4096.0)  # t1444: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1446 = ltorch.add(t1444, 1e-05, alpha=None)  # t1446: "cuda:0 f32[1, 512, 1]"
    # t1446 = prims.add(t1444, 1e-05)  # t1446: "cuda:0 f32[1, 512, 1]"
  t1447 = ltorch.rsqrt(t1446)  # t1447: "cuda:0 f32[1, 512, 1]"
    # t1447 = prims.rsqrt(t1446)  # t1447: "cuda:0 f32[1, 512, 1]"
  t1449 = ltorch.mul(t1439, t1447)  # t1449: "cuda:0 f32[1, 512, 4096]"
    # t1448 = prims.broadcast_in_dim(t1447, (1, 512, 4096), (0, 1, 2))  # t1448: "cuda:0 f32[1, 512, 4096]"
    # t1449 = prims.mul(t1439, t1448)  # t1449: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1450 = ltorch.to(t1449, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1450: "cuda:0 bf16[1, 512, 4096]"
    # t1450 = prims.convert_element_type(t1449, dtypes.bfloat16)  # t1450: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1460 = ltorch.mul(t1450, t_transformer_h_8_norm_2_weight)  # t1460: "cuda:0 bf16[1, 512, 4096]"
    # t1456 = prims.broadcast_in_dim(t_transformer_h_8_norm_2_weight, (1, 512, 4096), (2,))  # t1456: "cuda:0 bf16[1, 512, 4096]"
    # t1457 = prims.convert_element_type(t1450, dtypes.float32)  # t1457: "cuda:0 f32[1, 512, 4096]"
    # t1458 = prims.convert_element_type(t1456, dtypes.float32)  # t1458: "cuda:0 f32[1, 512, 4096]"
    # t1459 = prims.mul(t1457, t1458)  # t1459: "cuda:0 f32[1, 512, 4096]"
    # t1460 = prims.convert_element_type(t1459, dtypes.bfloat16)  # t1460: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1465 = ltorch.linear(t1460, t_transformer_h_8_mlp_fc_1_weight, None)  # t1465: "cuda:0 bf16[1, 512, 11008]"
    # t1465 = prims.linear(t1460, t_transformer_h_8_mlp_fc_1_weight, None)  # t1465: "cuda:0 bf16[1, 512, 11008]"
  t1469 = ltorch.linear(t1460, t_transformer_h_8_mlp_fc_2_weight, None)  # t1469: "cuda:0 bf16[1, 512, 11008]"
    # t1469 = prims.linear(t1460, t_transformer_h_8_mlp_fc_2_weight, None)  # t1469: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1479 = ltorch.silu(t1465, False)  # t1479: "cuda:0 bf16[1, 512, 11008]"
    # t1470 = prims.convert_element_type(t1465, dtypes.float32)  # t1470: "cuda:0 f32[1, 512, 11008]"
    # t1471 = prims.neg(t1470)  # t1471: "cuda:0 f32[1, 512, 11008]"
    # t1472 = prims.exp(t1471)  # t1472: "cuda:0 f32[1, 512, 11008]"
    # t1473 = prims.add(1.0, t1472)  # t1473: "cuda:0 f32[1, 512, 11008]"
    # t1474 = prims.reciprocal(t1473)  # t1474: "cuda:0 f32[1, 512, 11008]"
    # t1475 = prims.convert_element_type(t1474, dtypes.bfloat16)  # t1475: "cuda:0 bf16[1, 512, 11008]"
    # t1476 = prims.convert_element_type(t1465, dtypes.float32)  # t1476: "cuda:0 f32[1, 512, 11008]"
    # t1477 = prims.convert_element_type(t1475, dtypes.float32)  # t1477: "cuda:0 f32[1, 512, 11008]"
    # t1478 = prims.mul(t1476, t1477)  # t1478: "cuda:0 f32[1, 512, 11008]"
    # t1479 = prims.convert_element_type(t1478, dtypes.bfloat16)  # t1479: "cuda:0 bf16[1, 512, 11008]"
  t1483 = ltorch.mul(t1479, t1469)  # t1483: "cuda:0 bf16[1, 512, 11008]"
    # t1480 = prims.convert_element_type(t1479, dtypes.float32)  # t1480: "cuda:0 f32[1, 512, 11008]"
    # t1481 = prims.convert_element_type(t1469, dtypes.float32)  # t1481: "cuda:0 f32[1, 512, 11008]"
    # t1482 = prims.mul(t1480, t1481)  # t1482: "cuda:0 f32[1, 512, 11008]"
    # t1483 = prims.convert_element_type(t1482, dtypes.bfloat16)  # t1483: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1487 = ltorch.linear(t1483, t_transformer_h_8_mlp_proj_weight, None)  # t1487: "cuda:0 bf16[1, 512, 4096]"
    # t1487 = prims.linear(t1483, t_transformer_h_8_mlp_proj_weight, None)  # t1487: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1491 = ltorch.add(t1487, t1438, alpha=None)  # t1491: "cuda:0 bf16[1, 512, 4096]"
    # t1488 = prims.convert_element_type(t1487, dtypes.float32)  # t1488: "cuda:0 f32[1, 512, 4096]"
    # t1489 = prims.convert_element_type(t1438, dtypes.float32)  # t1489: "cuda:0 f32[1, 512, 4096]"
    # t1490 = prims.add(t1488, t1489)  # t1490: "cuda:0 f32[1, 512, 4096]"
    # t1491 = prims.convert_element_type(t1490, dtypes.bfloat16)  # t1491: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1493 = prims.convert_element_type(t1491, dtypes.float32)  # t1493: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1494 = ltorch.mul(t1493, t1493)  # t1494: "cuda:0 f32[1, 512, 4096]"
    # t1494 = prims.mul(t1493, t1493)  # t1494: "cuda:0 f32[1, 512, 4096]"
  t1498 = ltorch.mean(t1494, -1, True, dtype=None)  # t1498: "cuda:0 f32[1, 512, 1]"
    # t1496 = prims.sum(t1494, (2,))  # t1496: "cuda:0 f32[1, 512]"
    # t1497 = prims.broadcast_in_dim(t1496, [1, 512, 1], [0, 1])  # t1497: "cuda:0 f32[1, 512, 1]"
    # t1498 = ltorch.true_divide(t1497, 4096)  # t1498: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1498 = prims.div(t1497, 4096.0)  # t1498: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1500 = ltorch.add(t1498, 1e-05, alpha=None)  # t1500: "cuda:0 f32[1, 512, 1]"
    # t1500 = prims.add(t1498, 1e-05)  # t1500: "cuda:0 f32[1, 512, 1]"
  t1501 = ltorch.rsqrt(t1500)  # t1501: "cuda:0 f32[1, 512, 1]"
    # t1501 = prims.rsqrt(t1500)  # t1501: "cuda:0 f32[1, 512, 1]"
  t1503 = ltorch.mul(t1493, t1501)  # t1503: "cuda:0 f32[1, 512, 4096]"
    # t1502 = prims.broadcast_in_dim(t1501, (1, 512, 4096), (0, 1, 2))  # t1502: "cuda:0 f32[1, 512, 4096]"
    # t1503 = prims.mul(t1493, t1502)  # t1503: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1504 = ltorch.to(t1503, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1504: "cuda:0 bf16[1, 512, 4096]"
    # t1504 = prims.convert_element_type(t1503, dtypes.bfloat16)  # t1504: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1514 = ltorch.mul(t1504, t_transformer_h_9_norm_1_weight)  # t1514: "cuda:0 bf16[1, 512, 4096]"
    # t1510 = prims.broadcast_in_dim(t_transformer_h_9_norm_1_weight, (1, 512, 4096), (2,))  # t1510: "cuda:0 bf16[1, 512, 4096]"
    # t1511 = prims.convert_element_type(t1504, dtypes.float32)  # t1511: "cuda:0 f32[1, 512, 4096]"
    # t1512 = prims.convert_element_type(t1510, dtypes.float32)  # t1512: "cuda:0 f32[1, 512, 4096]"
    # t1513 = prims.mul(t1511, t1512)  # t1513: "cuda:0 f32[1, 512, 4096]"
    # t1514 = prims.convert_element_type(t1513, dtypes.bfloat16)  # t1514: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1519 = ltorch.linear(t1514, t_transformer_h_9_attn_attn_weight, None)  # t1519: "cuda:0 bf16[1, 512, 12288]"
    # t1519 = prims.linear(t1514, t_transformer_h_9_attn_attn_weight, None)  # t1519: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1520 = ltorch.view(t1519, 1, 512, 32, 3, 128)  # t1520: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1520 = ltorch.reshape(t1519, (1, 512, 32, 3, 128))  # t1520: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1520 = prims.reshape(t1519, (1, 512, 32, 3, 128))  # t1520: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1521 = ltorch.permute(t1520, 0, 2, 3, 1, 4)  # t1521: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1521 = prims.transpose(t1520, (0, 2, 3, 1, 4))  # t1521: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1522, t1523, t1524) = ltorch.split(t1521, (1, 1, 1), 2)
    # t1522 = prims.slice_prim(t1521, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1522: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1523 = prims.slice_prim(t1521, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1523: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1524 = prims.slice_prim(t1521, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1524: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1525 = ltorch.reshape(t1522, 1, -1, 512, 128)  # t1525: "cuda:0 bf16[1, 32, 512, 128]"
    # t1525 = prims.reshape(t1522, (1, 32, 512, 128))  # t1525: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1526 = ltorch.reshape(t1523, 1, -1, 512, 128)  # t1526: "cuda:0 bf16[1, 32, 512, 128]"
    # t1526 = prims.reshape(t1523, (1, 32, 512, 128))  # t1526: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1527 = ltorch.reshape(t1524, 1, -1, 512, 128)  # t1527: "cuda:0 bf16[1, 32, 512, 128]"
    # t1527 = prims.reshape(t1524, (1, 32, 512, 128))  # t1527: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1528 = ltorch.getitem(t1525, (..., slice(None, 128, None)))  # t1528: "cuda:0 bf16[1, 32, 512, 128]"
    # t1528 = prims.slice_prim(t1525, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1528: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1529 = ltorch.getitem(t1528, (..., slice(None, 64, None)))  # t1529: "cuda:0 bf16[1, 32, 512, 64]"
    # t1529 = prims.slice_prim(t1528, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1529: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1530 = ltorch.getitem(t1528, (..., slice(64, None, None)))  # t1530: "cuda:0 bf16[1, 32, 512, 64]"
    # t1530 = prims.slice_prim(t1528, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1530: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1533 = ltorch.neg(t1530)  # t1533: "cuda:0 bf16[1, 32, 512, 64]"
    # t1531 = prims.convert_element_type(t1530, dtypes.float32)  # t1531: "cuda:0 f32[1, 32, 512, 64]"
    # t1532 = prims.neg(t1531)  # t1532: "cuda:0 f32[1, 32, 512, 64]"
    # t1533 = prims.convert_element_type(t1532, dtypes.bfloat16)  # t1533: "cuda:0 bf16[1, 32, 512, 64]"
  t1534 = ltorch.cat((t1533, t1529), -1)  # t1534: "cuda:0 bf16[1, 32, 512, 128]"
    # t1534 = prims.cat((t1533, t1529), -1)  # t1534: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1537 = ltorch.mul(t1528, cos)  # t1537: "cuda:0 f32[1, 32, 512, 128]"
    # t1535 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1535: "cuda:0 f32[1, 32, 512, 128]"
    # t1536 = prims.convert_element_type(t1528, dtypes.float32)  # t1536: "cuda:0 f32[1, 32, 512, 128]"
    # t1537 = prims.mul(t1536, t1535)  # t1537: "cuda:0 f32[1, 32, 512, 128]"
  t1540 = ltorch.mul(t1534, sin)  # t1540: "cuda:0 f32[1, 32, 512, 128]"
    # t1538 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1538: "cuda:0 f32[1, 32, 512, 128]"
    # t1539 = prims.convert_element_type(t1534, dtypes.float32)  # t1539: "cuda:0 f32[1, 32, 512, 128]"
    # t1540 = prims.mul(t1539, t1538)  # t1540: "cuda:0 f32[1, 32, 512, 128]"
  t1541 = ltorch.add(t1537, t1540, alpha=None)  # t1541: "cuda:0 f32[1, 32, 512, 128]"
    # t1541 = prims.add(t1537, t1540)  # t1541: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1542 = ltorch.to(t1541, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1542: "cuda:0 bf16[1, 32, 512, 128]"
    # t1542 = prims.convert_element_type(t1541, dtypes.bfloat16)  # t1542: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1543 = ltorch.getitem(t1526, (..., slice(None, 128, None)))  # t1543: "cuda:0 bf16[1, 32, 512, 128]"
    # t1543 = prims.slice_prim(t1526, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1543: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1544 = ltorch.getitem(t1543, (..., slice(None, 64, None)))  # t1544: "cuda:0 bf16[1, 32, 512, 64]"
    # t1544 = prims.slice_prim(t1543, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1544: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1545 = ltorch.getitem(t1543, (..., slice(64, None, None)))  # t1545: "cuda:0 bf16[1, 32, 512, 64]"
    # t1545 = prims.slice_prim(t1543, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1545: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1548 = ltorch.neg(t1545)  # t1548: "cuda:0 bf16[1, 32, 512, 64]"
    # t1546 = prims.convert_element_type(t1545, dtypes.float32)  # t1546: "cuda:0 f32[1, 32, 512, 64]"
    # t1547 = prims.neg(t1546)  # t1547: "cuda:0 f32[1, 32, 512, 64]"
    # t1548 = prims.convert_element_type(t1547, dtypes.bfloat16)  # t1548: "cuda:0 bf16[1, 32, 512, 64]"
  t1549 = ltorch.cat((t1548, t1544), -1)  # t1549: "cuda:0 bf16[1, 32, 512, 128]"
    # t1549 = prims.cat((t1548, t1544), -1)  # t1549: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1552 = ltorch.mul(t1543, cos)  # t1552: "cuda:0 f32[1, 32, 512, 128]"
    # t1550 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1550: "cuda:0 f32[1, 32, 512, 128]"
    # t1551 = prims.convert_element_type(t1543, dtypes.float32)  # t1551: "cuda:0 f32[1, 32, 512, 128]"
    # t1552 = prims.mul(t1551, t1550)  # t1552: "cuda:0 f32[1, 32, 512, 128]"
  t1555 = ltorch.mul(t1549, sin)  # t1555: "cuda:0 f32[1, 32, 512, 128]"
    # t1553 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1553: "cuda:0 f32[1, 32, 512, 128]"
    # t1554 = prims.convert_element_type(t1549, dtypes.float32)  # t1554: "cuda:0 f32[1, 32, 512, 128]"
    # t1555 = prims.mul(t1554, t1553)  # t1555: "cuda:0 f32[1, 32, 512, 128]"
  t1556 = ltorch.add(t1552, t1555, alpha=None)  # t1556: "cuda:0 f32[1, 32, 512, 128]"
    # t1556 = prims.add(t1552, t1555)  # t1556: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1557 = ltorch.to(t1556, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1557: "cuda:0 bf16[1, 32, 512, 128]"
    # t1557 = prims.convert_element_type(t1556, dtypes.bfloat16)  # t1557: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1558 = ltorch.getitem(t1525, (..., slice(128, None, None)))  # t1558: "cuda:0 bf16[1, 32, 512, 0]"
    # t1558 = prims.slice_prim(t1525, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1558: "cuda:0 bf16[1, 32, 512, 0]"
  t1559 = ltorch.cat((t1542, t1558), -1)  # t1559: "cuda:0 bf16[1, 32, 512, 128]"
    # t1559 = prims.cat((t1542, t1558), -1)  # t1559: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1560 = ltorch.getitem(t1526, (..., slice(128, None, None)))  # t1560: "cuda:0 bf16[1, 32, 512, 0]"
    # t1560 = prims.slice_prim(t1526, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1560: "cuda:0 bf16[1, 32, 512, 0]"
  t1561 = ltorch.cat((t1557, t1560), -1)  # t1561: "cuda:0 bf16[1, 32, 512, 128]"
    # t1561 = prims.cat((t1557, t1560), -1)  # t1561: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1591 = ltorch.scaled_dot_product_attention(t1559, t1561, t1527, None, 0.0, True, scale=0.08838834764831843)  # t1591: "cuda:0 bf16[1, 32, 512, 128]"
    # t1564 = ltorch.mul(t1559, 0.29730177875068026)  # t1564: "cuda:0 bf16[1, 32, 512, 128]"
      # t1562 = prims.convert_element_type(t1559, dtypes.float32)  # t1562: "cuda:0 f32[1, 32, 512, 128]"
      # t1563 = prims.mul(t1562, 0.29730177875068026)  # t1563: "cuda:0 f32[1, 32, 512, 128]"
      # t1564 = prims.convert_element_type(t1563, dtypes.bfloat16)  # t1564: "cuda:0 bf16[1, 32, 512, 128]"
    # t1565 = ltorch.transpose(t1561, -2, -1)  # t1565: "cuda:0 bf16[1, 32, 128, 512]"
      # t1565 = prims.transpose(t1561, (0, 1, 3, 2))  # t1565: "cuda:0 bf16[1, 32, 128, 512]"
    # t1568 = ltorch.mul(t1565, 0.29730177875068026)  # t1568: "cuda:0 bf16[1, 32, 128, 512]"
      # t1566 = prims.convert_element_type(t1565, dtypes.float32)  # t1566: "cuda:0 f32[1, 32, 128, 512]"
      # t1567 = prims.mul(t1566, 0.29730177875068026)  # t1567: "cuda:0 f32[1, 32, 128, 512]"
      # t1568 = prims.convert_element_type(t1567, dtypes.bfloat16)  # t1568: "cuda:0 bf16[1, 32, 128, 512]"
    # t1569 = ltorch.matmul(t1564, t1568)  # t1569: "cuda:0 bf16[1, 32, 512, 512]"
      # t1569 = prims.matmul(t1564, t1568)  # t1569: "cuda:0 bf16[1, 32, 512, 512]"
    # t1579 = ltorch.tril(t1569, 0, fill_value=-float('inf'))  # t1579: "cuda:0 bf16[1, 32, 512, 512]"
      # t1570 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1570: "cuda:0 i64[512]"
        # t1570 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1570: "cuda:0 i64[512]"
      # t1571 = ltorch.unsqueeze(t1570, -1)  # t1571: "cuda:0 i64[512, 1]"
        # t1571 = prims.broadcast_in_dim(t1570, [512, 1], [0])  # t1571: "cuda:0 i64[512, 1]"
      # t1572 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1572: "cuda:0 i64[512]"
        # t1572 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1572: "cuda:0 i64[512]"
      # t1573 = ltorch.unsqueeze(t1572, -2)  # t1573: "cuda:0 i64[1, 512]"
        # t1573 = prims.broadcast_in_dim(t1572, [1, 512], [1])  # t1573: "cuda:0 i64[1, 512]"
      # t1574 = ltorch.add(t1571, 0, alpha=None)  # t1574: "cuda:0 i64[512, 1]"
        # t1574 = prims.add(t1571, 0)  # t1574: "cuda:0 i64[512, 1]"
      # t1577 = ltorch.ge(t1574, t1573)  # t1577: "cuda:0 b8[512, 512]"
        # t1575 = prims.broadcast_in_dim(t1574, (512, 512), (0, 1))  # t1575: "cuda:0 i64[512, 512]"
        # t1576 = prims.broadcast_in_dim(t1573, (512, 512), (0, 1))  # t1576: "cuda:0 i64[512, 512]"
        # t1577 = prims.ge(t1575, t1576)  # t1577: "cuda:0 b8[512, 512]"
      # t1579 = ltorch.where(t1577, t1569, -float('inf'))  # t1579: "cuda:0 bf16[1, 32, 512, 512]"
        # t1578 = prims.broadcast_in_dim(t1577, (1, 32, 512, 512), (2, 3))  # t1578: "cuda:0 b8[1, 32, 512, 512]"
        # t1579 = prims.where(t1578, t1569, -float('inf'))  # t1579: "cuda:0 bf16[1, 32, 512, 512]"
    # t1590 = ltorch._softmax(t1579, -1, dtype=None)  # t1590: "cuda:0 bf16[1, 32, 512, 512]"
      # t1580 = ltorch.to(t1579, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1580: "cuda:0 f32[1, 32, 512, 512]"
        # t1580 = prims.convert_element_type(t1579, dtypes.float32)  # t1580: "cuda:0 f32[1, 32, 512, 512]"
      # t1582 = ltorch.amax(t1580, -1, True)  # t1582: "cuda:0 f32[1, 32, 512, 1]"
        # t1581 = prims.amax(t1580, (3,))  # t1581: "cuda:0 f32[1, 32, 512]"
        # t1582 = prims.broadcast_in_dim(t1581, [1, 32, 512, 1], [0, 1, 2])  # t1582: "cuda:0 f32[1, 32, 512, 1]"
      # t1584 = ltorch.sub(t1580, t1582, alpha=None)  # t1584: "cuda:0 f32[1, 32, 512, 512]"
        # t1583 = prims.broadcast_in_dim(t1582, (1, 32, 512, 512), (0, 1, 2, 3))  # t1583: "cuda:0 f32[1, 32, 512, 512]"
        # t1584 = prims.sub(t1580, t1583)  # t1584: "cuda:0 f32[1, 32, 512, 512]"
      # t1585 = ltorch.exp(t1584)  # t1585: "cuda:0 f32[1, 32, 512, 512]"
        # t1585 = prims.exp(t1584)  # t1585: "cuda:0 f32[1, 32, 512, 512]"
      # t1587 = ltorch.sum(t1585, -1, True, dtype=None)  # t1587: "cuda:0 f32[1, 32, 512, 1]"
        # t1586 = prims.sum(t1585, (3,))  # t1586: "cuda:0 f32[1, 32, 512]"
        # t1587 = prims.broadcast_in_dim(t1586, [1, 32, 512, 1], [0, 1, 2])  # t1587: "cuda:0 f32[1, 32, 512, 1]"
      # t1589 = ltorch.true_divide(t1585, t1587)  # t1589: "cuda:0 f32[1, 32, 512, 512]"
        # t1588 = prims.broadcast_in_dim(t1587, (1, 32, 512, 512), (0, 1, 2, 3))  # t1588: "cuda:0 f32[1, 32, 512, 512]"
        # t1589 = prims.div(t1585, t1588)  # t1589: "cuda:0 f32[1, 32, 512, 512]"
      # t1590 = ltorch.to(t1589, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1590: "cuda:0 bf16[1, 32, 512, 512]"
        # t1590 = prims.convert_element_type(t1589, dtypes.bfloat16)  # t1590: "cuda:0 bf16[1, 32, 512, 512]"
    # t1591 = ltorch.matmul(t1590, t1527)  # t1591: "cuda:0 bf16[1, 32, 512, 128]"
      # t1591 = prims.matmul(t1590, t1527)  # t1591: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1592 = ltorch.transpose(t1591, 1, 2)  # t1592: "cuda:0 bf16[1, 512, 32, 128]"
    # t1592 = prims.transpose(t1591, (0, 2, 1, 3))  # t1592: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1593 = ltorch.reshape(t1592, 1, 512, 4096)  # t1593: "cuda:0 bf16[1, 512, 4096]"
    # t1593 = prims.reshape(t1592, (1, 512, 4096))  # t1593: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1597 = ltorch.linear(t1593, t_transformer_h_9_attn_proj_weight, None)  # t1597: "cuda:0 bf16[1, 512, 4096]"
    # t1597 = prims.linear(t1593, t_transformer_h_9_attn_proj_weight, None)  # t1597: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1601 = ltorch.add(t1597, t1491, alpha=None)  # t1601: "cuda:0 bf16[1, 512, 4096]"
    # t1598 = prims.convert_element_type(t1597, dtypes.float32)  # t1598: "cuda:0 f32[1, 512, 4096]"
    # t1599 = prims.convert_element_type(t1491, dtypes.float32)  # t1599: "cuda:0 f32[1, 512, 4096]"
    # t1600 = prims.add(t1598, t1599)  # t1600: "cuda:0 f32[1, 512, 4096]"
    # t1601 = prims.convert_element_type(t1600, dtypes.bfloat16)  # t1601: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1602 = prims.convert_element_type(t1601, dtypes.float32)  # t1602: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1603 = ltorch.mul(t1602, t1602)  # t1603: "cuda:0 f32[1, 512, 4096]"
    # t1603 = prims.mul(t1602, t1602)  # t1603: "cuda:0 f32[1, 512, 4096]"
  t1607 = ltorch.mean(t1603, -1, True, dtype=None)  # t1607: "cuda:0 f32[1, 512, 1]"
    # t1605 = prims.sum(t1603, (2,))  # t1605: "cuda:0 f32[1, 512]"
    # t1606 = prims.broadcast_in_dim(t1605, [1, 512, 1], [0, 1])  # t1606: "cuda:0 f32[1, 512, 1]"
    # t1607 = ltorch.true_divide(t1606, 4096)  # t1607: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1607 = prims.div(t1606, 4096.0)  # t1607: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1609 = ltorch.add(t1607, 1e-05, alpha=None)  # t1609: "cuda:0 f32[1, 512, 1]"
    # t1609 = prims.add(t1607, 1e-05)  # t1609: "cuda:0 f32[1, 512, 1]"
  t1610 = ltorch.rsqrt(t1609)  # t1610: "cuda:0 f32[1, 512, 1]"
    # t1610 = prims.rsqrt(t1609)  # t1610: "cuda:0 f32[1, 512, 1]"
  t1612 = ltorch.mul(t1602, t1610)  # t1612: "cuda:0 f32[1, 512, 4096]"
    # t1611 = prims.broadcast_in_dim(t1610, (1, 512, 4096), (0, 1, 2))  # t1611: "cuda:0 f32[1, 512, 4096]"
    # t1612 = prims.mul(t1602, t1611)  # t1612: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1613 = ltorch.to(t1612, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1613: "cuda:0 bf16[1, 512, 4096]"
    # t1613 = prims.convert_element_type(t1612, dtypes.bfloat16)  # t1613: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1623 = ltorch.mul(t1613, t_transformer_h_9_norm_2_weight)  # t1623: "cuda:0 bf16[1, 512, 4096]"
    # t1619 = prims.broadcast_in_dim(t_transformer_h_9_norm_2_weight, (1, 512, 4096), (2,))  # t1619: "cuda:0 bf16[1, 512, 4096]"
    # t1620 = prims.convert_element_type(t1613, dtypes.float32)  # t1620: "cuda:0 f32[1, 512, 4096]"
    # t1621 = prims.convert_element_type(t1619, dtypes.float32)  # t1621: "cuda:0 f32[1, 512, 4096]"
    # t1622 = prims.mul(t1620, t1621)  # t1622: "cuda:0 f32[1, 512, 4096]"
    # t1623 = prims.convert_element_type(t1622, dtypes.bfloat16)  # t1623: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1628 = ltorch.linear(t1623, t_transformer_h_9_mlp_fc_1_weight, None)  # t1628: "cuda:0 bf16[1, 512, 11008]"
    # t1628 = prims.linear(t1623, t_transformer_h_9_mlp_fc_1_weight, None)  # t1628: "cuda:0 bf16[1, 512, 11008]"
  t1632 = ltorch.linear(t1623, t_transformer_h_9_mlp_fc_2_weight, None)  # t1632: "cuda:0 bf16[1, 512, 11008]"
    # t1632 = prims.linear(t1623, t_transformer_h_9_mlp_fc_2_weight, None)  # t1632: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1642 = ltorch.silu(t1628, False)  # t1642: "cuda:0 bf16[1, 512, 11008]"
    # t1633 = prims.convert_element_type(t1628, dtypes.float32)  # t1633: "cuda:0 f32[1, 512, 11008]"
    # t1634 = prims.neg(t1633)  # t1634: "cuda:0 f32[1, 512, 11008]"
    # t1635 = prims.exp(t1634)  # t1635: "cuda:0 f32[1, 512, 11008]"
    # t1636 = prims.add(1.0, t1635)  # t1636: "cuda:0 f32[1, 512, 11008]"
    # t1637 = prims.reciprocal(t1636)  # t1637: "cuda:0 f32[1, 512, 11008]"
    # t1638 = prims.convert_element_type(t1637, dtypes.bfloat16)  # t1638: "cuda:0 bf16[1, 512, 11008]"
    # t1639 = prims.convert_element_type(t1628, dtypes.float32)  # t1639: "cuda:0 f32[1, 512, 11008]"
    # t1640 = prims.convert_element_type(t1638, dtypes.float32)  # t1640: "cuda:0 f32[1, 512, 11008]"
    # t1641 = prims.mul(t1639, t1640)  # t1641: "cuda:0 f32[1, 512, 11008]"
    # t1642 = prims.convert_element_type(t1641, dtypes.bfloat16)  # t1642: "cuda:0 bf16[1, 512, 11008]"
  t1646 = ltorch.mul(t1642, t1632)  # t1646: "cuda:0 bf16[1, 512, 11008]"
    # t1643 = prims.convert_element_type(t1642, dtypes.float32)  # t1643: "cuda:0 f32[1, 512, 11008]"
    # t1644 = prims.convert_element_type(t1632, dtypes.float32)  # t1644: "cuda:0 f32[1, 512, 11008]"
    # t1645 = prims.mul(t1643, t1644)  # t1645: "cuda:0 f32[1, 512, 11008]"
    # t1646 = prims.convert_element_type(t1645, dtypes.bfloat16)  # t1646: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1650 = ltorch.linear(t1646, t_transformer_h_9_mlp_proj_weight, None)  # t1650: "cuda:0 bf16[1, 512, 4096]"
    # t1650 = prims.linear(t1646, t_transformer_h_9_mlp_proj_weight, None)  # t1650: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1654 = ltorch.add(t1650, t1601, alpha=None)  # t1654: "cuda:0 bf16[1, 512, 4096]"
    # t1651 = prims.convert_element_type(t1650, dtypes.float32)  # t1651: "cuda:0 f32[1, 512, 4096]"
    # t1652 = prims.convert_element_type(t1601, dtypes.float32)  # t1652: "cuda:0 f32[1, 512, 4096]"
    # t1653 = prims.add(t1651, t1652)  # t1653: "cuda:0 f32[1, 512, 4096]"
    # t1654 = prims.convert_element_type(t1653, dtypes.bfloat16)  # t1654: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1656 = prims.convert_element_type(t1654, dtypes.float32)  # t1656: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1657 = ltorch.mul(t1656, t1656)  # t1657: "cuda:0 f32[1, 512, 4096]"
    # t1657 = prims.mul(t1656, t1656)  # t1657: "cuda:0 f32[1, 512, 4096]"
  t1661 = ltorch.mean(t1657, -1, True, dtype=None)  # t1661: "cuda:0 f32[1, 512, 1]"
    # t1659 = prims.sum(t1657, (2,))  # t1659: "cuda:0 f32[1, 512]"
    # t1660 = prims.broadcast_in_dim(t1659, [1, 512, 1], [0, 1])  # t1660: "cuda:0 f32[1, 512, 1]"
    # t1661 = ltorch.true_divide(t1660, 4096)  # t1661: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1661 = prims.div(t1660, 4096.0)  # t1661: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1663 = ltorch.add(t1661, 1e-05, alpha=None)  # t1663: "cuda:0 f32[1, 512, 1]"
    # t1663 = prims.add(t1661, 1e-05)  # t1663: "cuda:0 f32[1, 512, 1]"
  t1664 = ltorch.rsqrt(t1663)  # t1664: "cuda:0 f32[1, 512, 1]"
    # t1664 = prims.rsqrt(t1663)  # t1664: "cuda:0 f32[1, 512, 1]"
  t1666 = ltorch.mul(t1656, t1664)  # t1666: "cuda:0 f32[1, 512, 4096]"
    # t1665 = prims.broadcast_in_dim(t1664, (1, 512, 4096), (0, 1, 2))  # t1665: "cuda:0 f32[1, 512, 4096]"
    # t1666 = prims.mul(t1656, t1665)  # t1666: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1667 = ltorch.to(t1666, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1667: "cuda:0 bf16[1, 512, 4096]"
    # t1667 = prims.convert_element_type(t1666, dtypes.bfloat16)  # t1667: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1677 = ltorch.mul(t1667, t_transformer_h_10_norm_1_weight)  # t1677: "cuda:0 bf16[1, 512, 4096]"
    # t1673 = prims.broadcast_in_dim(t_transformer_h_10_norm_1_weight, (1, 512, 4096), (2,))  # t1673: "cuda:0 bf16[1, 512, 4096]"
    # t1674 = prims.convert_element_type(t1667, dtypes.float32)  # t1674: "cuda:0 f32[1, 512, 4096]"
    # t1675 = prims.convert_element_type(t1673, dtypes.float32)  # t1675: "cuda:0 f32[1, 512, 4096]"
    # t1676 = prims.mul(t1674, t1675)  # t1676: "cuda:0 f32[1, 512, 4096]"
    # t1677 = prims.convert_element_type(t1676, dtypes.bfloat16)  # t1677: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1682 = ltorch.linear(t1677, t_transformer_h_10_attn_attn_weight, None)  # t1682: "cuda:0 bf16[1, 512, 12288]"
    # t1682 = prims.linear(t1677, t_transformer_h_10_attn_attn_weight, None)  # t1682: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1683 = ltorch.view(t1682, 1, 512, 32, 3, 128)  # t1683: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1683 = ltorch.reshape(t1682, (1, 512, 32, 3, 128))  # t1683: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1683 = prims.reshape(t1682, (1, 512, 32, 3, 128))  # t1683: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1684 = ltorch.permute(t1683, 0, 2, 3, 1, 4)  # t1684: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1684 = prims.transpose(t1683, (0, 2, 3, 1, 4))  # t1684: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1685, t1686, t1687) = ltorch.split(t1684, (1, 1, 1), 2)
    # t1685 = prims.slice_prim(t1684, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1685: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1686 = prims.slice_prim(t1684, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1686: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1687 = prims.slice_prim(t1684, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1687: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1688 = ltorch.reshape(t1685, 1, -1, 512, 128)  # t1688: "cuda:0 bf16[1, 32, 512, 128]"
    # t1688 = prims.reshape(t1685, (1, 32, 512, 128))  # t1688: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1689 = ltorch.reshape(t1686, 1, -1, 512, 128)  # t1689: "cuda:0 bf16[1, 32, 512, 128]"
    # t1689 = prims.reshape(t1686, (1, 32, 512, 128))  # t1689: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1690 = ltorch.reshape(t1687, 1, -1, 512, 128)  # t1690: "cuda:0 bf16[1, 32, 512, 128]"
    # t1690 = prims.reshape(t1687, (1, 32, 512, 128))  # t1690: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1691 = ltorch.getitem(t1688, (..., slice(None, 128, None)))  # t1691: "cuda:0 bf16[1, 32, 512, 128]"
    # t1691 = prims.slice_prim(t1688, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1691: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1692 = ltorch.getitem(t1691, (..., slice(None, 64, None)))  # t1692: "cuda:0 bf16[1, 32, 512, 64]"
    # t1692 = prims.slice_prim(t1691, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1692: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1693 = ltorch.getitem(t1691, (..., slice(64, None, None)))  # t1693: "cuda:0 bf16[1, 32, 512, 64]"
    # t1693 = prims.slice_prim(t1691, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1693: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1696 = ltorch.neg(t1693)  # t1696: "cuda:0 bf16[1, 32, 512, 64]"
    # t1694 = prims.convert_element_type(t1693, dtypes.float32)  # t1694: "cuda:0 f32[1, 32, 512, 64]"
    # t1695 = prims.neg(t1694)  # t1695: "cuda:0 f32[1, 32, 512, 64]"
    # t1696 = prims.convert_element_type(t1695, dtypes.bfloat16)  # t1696: "cuda:0 bf16[1, 32, 512, 64]"
  t1697 = ltorch.cat((t1696, t1692), -1)  # t1697: "cuda:0 bf16[1, 32, 512, 128]"
    # t1697 = prims.cat((t1696, t1692), -1)  # t1697: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1700 = ltorch.mul(t1691, cos)  # t1700: "cuda:0 f32[1, 32, 512, 128]"
    # t1698 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1698: "cuda:0 f32[1, 32, 512, 128]"
    # t1699 = prims.convert_element_type(t1691, dtypes.float32)  # t1699: "cuda:0 f32[1, 32, 512, 128]"
    # t1700 = prims.mul(t1699, t1698)  # t1700: "cuda:0 f32[1, 32, 512, 128]"
  t1703 = ltorch.mul(t1697, sin)  # t1703: "cuda:0 f32[1, 32, 512, 128]"
    # t1701 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1701: "cuda:0 f32[1, 32, 512, 128]"
    # t1702 = prims.convert_element_type(t1697, dtypes.float32)  # t1702: "cuda:0 f32[1, 32, 512, 128]"
    # t1703 = prims.mul(t1702, t1701)  # t1703: "cuda:0 f32[1, 32, 512, 128]"
  t1704 = ltorch.add(t1700, t1703, alpha=None)  # t1704: "cuda:0 f32[1, 32, 512, 128]"
    # t1704 = prims.add(t1700, t1703)  # t1704: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1705 = ltorch.to(t1704, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1705: "cuda:0 bf16[1, 32, 512, 128]"
    # t1705 = prims.convert_element_type(t1704, dtypes.bfloat16)  # t1705: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1706 = ltorch.getitem(t1689, (..., slice(None, 128, None)))  # t1706: "cuda:0 bf16[1, 32, 512, 128]"
    # t1706 = prims.slice_prim(t1689, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1706: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1707 = ltorch.getitem(t1706, (..., slice(None, 64, None)))  # t1707: "cuda:0 bf16[1, 32, 512, 64]"
    # t1707 = prims.slice_prim(t1706, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1707: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1708 = ltorch.getitem(t1706, (..., slice(64, None, None)))  # t1708: "cuda:0 bf16[1, 32, 512, 64]"
    # t1708 = prims.slice_prim(t1706, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1708: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1711 = ltorch.neg(t1708)  # t1711: "cuda:0 bf16[1, 32, 512, 64]"
    # t1709 = prims.convert_element_type(t1708, dtypes.float32)  # t1709: "cuda:0 f32[1, 32, 512, 64]"
    # t1710 = prims.neg(t1709)  # t1710: "cuda:0 f32[1, 32, 512, 64]"
    # t1711 = prims.convert_element_type(t1710, dtypes.bfloat16)  # t1711: "cuda:0 bf16[1, 32, 512, 64]"
  t1712 = ltorch.cat((t1711, t1707), -1)  # t1712: "cuda:0 bf16[1, 32, 512, 128]"
    # t1712 = prims.cat((t1711, t1707), -1)  # t1712: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1715 = ltorch.mul(t1706, cos)  # t1715: "cuda:0 f32[1, 32, 512, 128]"
    # t1713 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1713: "cuda:0 f32[1, 32, 512, 128]"
    # t1714 = prims.convert_element_type(t1706, dtypes.float32)  # t1714: "cuda:0 f32[1, 32, 512, 128]"
    # t1715 = prims.mul(t1714, t1713)  # t1715: "cuda:0 f32[1, 32, 512, 128]"
  t1718 = ltorch.mul(t1712, sin)  # t1718: "cuda:0 f32[1, 32, 512, 128]"
    # t1716 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1716: "cuda:0 f32[1, 32, 512, 128]"
    # t1717 = prims.convert_element_type(t1712, dtypes.float32)  # t1717: "cuda:0 f32[1, 32, 512, 128]"
    # t1718 = prims.mul(t1717, t1716)  # t1718: "cuda:0 f32[1, 32, 512, 128]"
  t1719 = ltorch.add(t1715, t1718, alpha=None)  # t1719: "cuda:0 f32[1, 32, 512, 128]"
    # t1719 = prims.add(t1715, t1718)  # t1719: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1720 = ltorch.to(t1719, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1720: "cuda:0 bf16[1, 32, 512, 128]"
    # t1720 = prims.convert_element_type(t1719, dtypes.bfloat16)  # t1720: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1721 = ltorch.getitem(t1688, (..., slice(128, None, None)))  # t1721: "cuda:0 bf16[1, 32, 512, 0]"
    # t1721 = prims.slice_prim(t1688, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1721: "cuda:0 bf16[1, 32, 512, 0]"
  t1722 = ltorch.cat((t1705, t1721), -1)  # t1722: "cuda:0 bf16[1, 32, 512, 128]"
    # t1722 = prims.cat((t1705, t1721), -1)  # t1722: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1723 = ltorch.getitem(t1689, (..., slice(128, None, None)))  # t1723: "cuda:0 bf16[1, 32, 512, 0]"
    # t1723 = prims.slice_prim(t1689, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1723: "cuda:0 bf16[1, 32, 512, 0]"
  t1724 = ltorch.cat((t1720, t1723), -1)  # t1724: "cuda:0 bf16[1, 32, 512, 128]"
    # t1724 = prims.cat((t1720, t1723), -1)  # t1724: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1754 = ltorch.scaled_dot_product_attention(t1722, t1724, t1690, None, 0.0, True, scale=0.08838834764831843)  # t1754: "cuda:0 bf16[1, 32, 512, 128]"
    # t1727 = ltorch.mul(t1722, 0.29730177875068026)  # t1727: "cuda:0 bf16[1, 32, 512, 128]"
      # t1725 = prims.convert_element_type(t1722, dtypes.float32)  # t1725: "cuda:0 f32[1, 32, 512, 128]"
      # t1726 = prims.mul(t1725, 0.29730177875068026)  # t1726: "cuda:0 f32[1, 32, 512, 128]"
      # t1727 = prims.convert_element_type(t1726, dtypes.bfloat16)  # t1727: "cuda:0 bf16[1, 32, 512, 128]"
    # t1728 = ltorch.transpose(t1724, -2, -1)  # t1728: "cuda:0 bf16[1, 32, 128, 512]"
      # t1728 = prims.transpose(t1724, (0, 1, 3, 2))  # t1728: "cuda:0 bf16[1, 32, 128, 512]"
    # t1731 = ltorch.mul(t1728, 0.29730177875068026)  # t1731: "cuda:0 bf16[1, 32, 128, 512]"
      # t1729 = prims.convert_element_type(t1728, dtypes.float32)  # t1729: "cuda:0 f32[1, 32, 128, 512]"
      # t1730 = prims.mul(t1729, 0.29730177875068026)  # t1730: "cuda:0 f32[1, 32, 128, 512]"
      # t1731 = prims.convert_element_type(t1730, dtypes.bfloat16)  # t1731: "cuda:0 bf16[1, 32, 128, 512]"
    # t1732 = ltorch.matmul(t1727, t1731)  # t1732: "cuda:0 bf16[1, 32, 512, 512]"
      # t1732 = prims.matmul(t1727, t1731)  # t1732: "cuda:0 bf16[1, 32, 512, 512]"
    # t1742 = ltorch.tril(t1732, 0, fill_value=-float('inf'))  # t1742: "cuda:0 bf16[1, 32, 512, 512]"
      # t1733 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1733: "cuda:0 i64[512]"
        # t1733 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1733: "cuda:0 i64[512]"
      # t1734 = ltorch.unsqueeze(t1733, -1)  # t1734: "cuda:0 i64[512, 1]"
        # t1734 = prims.broadcast_in_dim(t1733, [512, 1], [0])  # t1734: "cuda:0 i64[512, 1]"
      # t1735 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1735: "cuda:0 i64[512]"
        # t1735 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1735: "cuda:0 i64[512]"
      # t1736 = ltorch.unsqueeze(t1735, -2)  # t1736: "cuda:0 i64[1, 512]"
        # t1736 = prims.broadcast_in_dim(t1735, [1, 512], [1])  # t1736: "cuda:0 i64[1, 512]"
      # t1737 = ltorch.add(t1734, 0, alpha=None)  # t1737: "cuda:0 i64[512, 1]"
        # t1737 = prims.add(t1734, 0)  # t1737: "cuda:0 i64[512, 1]"
      # t1740 = ltorch.ge(t1737, t1736)  # t1740: "cuda:0 b8[512, 512]"
        # t1738 = prims.broadcast_in_dim(t1737, (512, 512), (0, 1))  # t1738: "cuda:0 i64[512, 512]"
        # t1739 = prims.broadcast_in_dim(t1736, (512, 512), (0, 1))  # t1739: "cuda:0 i64[512, 512]"
        # t1740 = prims.ge(t1738, t1739)  # t1740: "cuda:0 b8[512, 512]"
      # t1742 = ltorch.where(t1740, t1732, -float('inf'))  # t1742: "cuda:0 bf16[1, 32, 512, 512]"
        # t1741 = prims.broadcast_in_dim(t1740, (1, 32, 512, 512), (2, 3))  # t1741: "cuda:0 b8[1, 32, 512, 512]"
        # t1742 = prims.where(t1741, t1732, -float('inf'))  # t1742: "cuda:0 bf16[1, 32, 512, 512]"
    # t1753 = ltorch._softmax(t1742, -1, dtype=None)  # t1753: "cuda:0 bf16[1, 32, 512, 512]"
      # t1743 = ltorch.to(t1742, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1743: "cuda:0 f32[1, 32, 512, 512]"
        # t1743 = prims.convert_element_type(t1742, dtypes.float32)  # t1743: "cuda:0 f32[1, 32, 512, 512]"
      # t1745 = ltorch.amax(t1743, -1, True)  # t1745: "cuda:0 f32[1, 32, 512, 1]"
        # t1744 = prims.amax(t1743, (3,))  # t1744: "cuda:0 f32[1, 32, 512]"
        # t1745 = prims.broadcast_in_dim(t1744, [1, 32, 512, 1], [0, 1, 2])  # t1745: "cuda:0 f32[1, 32, 512, 1]"
      # t1747 = ltorch.sub(t1743, t1745, alpha=None)  # t1747: "cuda:0 f32[1, 32, 512, 512]"
        # t1746 = prims.broadcast_in_dim(t1745, (1, 32, 512, 512), (0, 1, 2, 3))  # t1746: "cuda:0 f32[1, 32, 512, 512]"
        # t1747 = prims.sub(t1743, t1746)  # t1747: "cuda:0 f32[1, 32, 512, 512]"
      # t1748 = ltorch.exp(t1747)  # t1748: "cuda:0 f32[1, 32, 512, 512]"
        # t1748 = prims.exp(t1747)  # t1748: "cuda:0 f32[1, 32, 512, 512]"
      # t1750 = ltorch.sum(t1748, -1, True, dtype=None)  # t1750: "cuda:0 f32[1, 32, 512, 1]"
        # t1749 = prims.sum(t1748, (3,))  # t1749: "cuda:0 f32[1, 32, 512]"
        # t1750 = prims.broadcast_in_dim(t1749, [1, 32, 512, 1], [0, 1, 2])  # t1750: "cuda:0 f32[1, 32, 512, 1]"
      # t1752 = ltorch.true_divide(t1748, t1750)  # t1752: "cuda:0 f32[1, 32, 512, 512]"
        # t1751 = prims.broadcast_in_dim(t1750, (1, 32, 512, 512), (0, 1, 2, 3))  # t1751: "cuda:0 f32[1, 32, 512, 512]"
        # t1752 = prims.div(t1748, t1751)  # t1752: "cuda:0 f32[1, 32, 512, 512]"
      # t1753 = ltorch.to(t1752, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1753: "cuda:0 bf16[1, 32, 512, 512]"
        # t1753 = prims.convert_element_type(t1752, dtypes.bfloat16)  # t1753: "cuda:0 bf16[1, 32, 512, 512]"
    # t1754 = ltorch.matmul(t1753, t1690)  # t1754: "cuda:0 bf16[1, 32, 512, 128]"
      # t1754 = prims.matmul(t1753, t1690)  # t1754: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1755 = ltorch.transpose(t1754, 1, 2)  # t1755: "cuda:0 bf16[1, 512, 32, 128]"
    # t1755 = prims.transpose(t1754, (0, 2, 1, 3))  # t1755: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1756 = ltorch.reshape(t1755, 1, 512, 4096)  # t1756: "cuda:0 bf16[1, 512, 4096]"
    # t1756 = prims.reshape(t1755, (1, 512, 4096))  # t1756: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1760 = ltorch.linear(t1756, t_transformer_h_10_attn_proj_weight, None)  # t1760: "cuda:0 bf16[1, 512, 4096]"
    # t1760 = prims.linear(t1756, t_transformer_h_10_attn_proj_weight, None)  # t1760: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1764 = ltorch.add(t1760, t1654, alpha=None)  # t1764: "cuda:0 bf16[1, 512, 4096]"
    # t1761 = prims.convert_element_type(t1760, dtypes.float32)  # t1761: "cuda:0 f32[1, 512, 4096]"
    # t1762 = prims.convert_element_type(t1654, dtypes.float32)  # t1762: "cuda:0 f32[1, 512, 4096]"
    # t1763 = prims.add(t1761, t1762)  # t1763: "cuda:0 f32[1, 512, 4096]"
    # t1764 = prims.convert_element_type(t1763, dtypes.bfloat16)  # t1764: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1765 = prims.convert_element_type(t1764, dtypes.float32)  # t1765: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1766 = ltorch.mul(t1765, t1765)  # t1766: "cuda:0 f32[1, 512, 4096]"
    # t1766 = prims.mul(t1765, t1765)  # t1766: "cuda:0 f32[1, 512, 4096]"
  t1770 = ltorch.mean(t1766, -1, True, dtype=None)  # t1770: "cuda:0 f32[1, 512, 1]"
    # t1768 = prims.sum(t1766, (2,))  # t1768: "cuda:0 f32[1, 512]"
    # t1769 = prims.broadcast_in_dim(t1768, [1, 512, 1], [0, 1])  # t1769: "cuda:0 f32[1, 512, 1]"
    # t1770 = ltorch.true_divide(t1769, 4096)  # t1770: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1770 = prims.div(t1769, 4096.0)  # t1770: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1772 = ltorch.add(t1770, 1e-05, alpha=None)  # t1772: "cuda:0 f32[1, 512, 1]"
    # t1772 = prims.add(t1770, 1e-05)  # t1772: "cuda:0 f32[1, 512, 1]"
  t1773 = ltorch.rsqrt(t1772)  # t1773: "cuda:0 f32[1, 512, 1]"
    # t1773 = prims.rsqrt(t1772)  # t1773: "cuda:0 f32[1, 512, 1]"
  t1775 = ltorch.mul(t1765, t1773)  # t1775: "cuda:0 f32[1, 512, 4096]"
    # t1774 = prims.broadcast_in_dim(t1773, (1, 512, 4096), (0, 1, 2))  # t1774: "cuda:0 f32[1, 512, 4096]"
    # t1775 = prims.mul(t1765, t1774)  # t1775: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1776 = ltorch.to(t1775, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1776: "cuda:0 bf16[1, 512, 4096]"
    # t1776 = prims.convert_element_type(t1775, dtypes.bfloat16)  # t1776: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1786 = ltorch.mul(t1776, t_transformer_h_10_norm_2_weight)  # t1786: "cuda:0 bf16[1, 512, 4096]"
    # t1782 = prims.broadcast_in_dim(t_transformer_h_10_norm_2_weight, (1, 512, 4096), (2,))  # t1782: "cuda:0 bf16[1, 512, 4096]"
    # t1783 = prims.convert_element_type(t1776, dtypes.float32)  # t1783: "cuda:0 f32[1, 512, 4096]"
    # t1784 = prims.convert_element_type(t1782, dtypes.float32)  # t1784: "cuda:0 f32[1, 512, 4096]"
    # t1785 = prims.mul(t1783, t1784)  # t1785: "cuda:0 f32[1, 512, 4096]"
    # t1786 = prims.convert_element_type(t1785, dtypes.bfloat16)  # t1786: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1791 = ltorch.linear(t1786, t_transformer_h_10_mlp_fc_1_weight, None)  # t1791: "cuda:0 bf16[1, 512, 11008]"
    # t1791 = prims.linear(t1786, t_transformer_h_10_mlp_fc_1_weight, None)  # t1791: "cuda:0 bf16[1, 512, 11008]"
  t1795 = ltorch.linear(t1786, t_transformer_h_10_mlp_fc_2_weight, None)  # t1795: "cuda:0 bf16[1, 512, 11008]"
    # t1795 = prims.linear(t1786, t_transformer_h_10_mlp_fc_2_weight, None)  # t1795: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1805 = ltorch.silu(t1791, False)  # t1805: "cuda:0 bf16[1, 512, 11008]"
    # t1796 = prims.convert_element_type(t1791, dtypes.float32)  # t1796: "cuda:0 f32[1, 512, 11008]"
    # t1797 = prims.neg(t1796)  # t1797: "cuda:0 f32[1, 512, 11008]"
    # t1798 = prims.exp(t1797)  # t1798: "cuda:0 f32[1, 512, 11008]"
    # t1799 = prims.add(1.0, t1798)  # t1799: "cuda:0 f32[1, 512, 11008]"
    # t1800 = prims.reciprocal(t1799)  # t1800: "cuda:0 f32[1, 512, 11008]"
    # t1801 = prims.convert_element_type(t1800, dtypes.bfloat16)  # t1801: "cuda:0 bf16[1, 512, 11008]"
    # t1802 = prims.convert_element_type(t1791, dtypes.float32)  # t1802: "cuda:0 f32[1, 512, 11008]"
    # t1803 = prims.convert_element_type(t1801, dtypes.float32)  # t1803: "cuda:0 f32[1, 512, 11008]"
    # t1804 = prims.mul(t1802, t1803)  # t1804: "cuda:0 f32[1, 512, 11008]"
    # t1805 = prims.convert_element_type(t1804, dtypes.bfloat16)  # t1805: "cuda:0 bf16[1, 512, 11008]"
  t1809 = ltorch.mul(t1805, t1795)  # t1809: "cuda:0 bf16[1, 512, 11008]"
    # t1806 = prims.convert_element_type(t1805, dtypes.float32)  # t1806: "cuda:0 f32[1, 512, 11008]"
    # t1807 = prims.convert_element_type(t1795, dtypes.float32)  # t1807: "cuda:0 f32[1, 512, 11008]"
    # t1808 = prims.mul(t1806, t1807)  # t1808: "cuda:0 f32[1, 512, 11008]"
    # t1809 = prims.convert_element_type(t1808, dtypes.bfloat16)  # t1809: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1813 = ltorch.linear(t1809, t_transformer_h_10_mlp_proj_weight, None)  # t1813: "cuda:0 bf16[1, 512, 4096]"
    # t1813 = prims.linear(t1809, t_transformer_h_10_mlp_proj_weight, None)  # t1813: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1817 = ltorch.add(t1813, t1764, alpha=None)  # t1817: "cuda:0 bf16[1, 512, 4096]"
    # t1814 = prims.convert_element_type(t1813, dtypes.float32)  # t1814: "cuda:0 f32[1, 512, 4096]"
    # t1815 = prims.convert_element_type(t1764, dtypes.float32)  # t1815: "cuda:0 f32[1, 512, 4096]"
    # t1816 = prims.add(t1814, t1815)  # t1816: "cuda:0 f32[1, 512, 4096]"
    # t1817 = prims.convert_element_type(t1816, dtypes.bfloat16)  # t1817: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1819 = prims.convert_element_type(t1817, dtypes.float32)  # t1819: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1820 = ltorch.mul(t1819, t1819)  # t1820: "cuda:0 f32[1, 512, 4096]"
    # t1820 = prims.mul(t1819, t1819)  # t1820: "cuda:0 f32[1, 512, 4096]"
  t1824 = ltorch.mean(t1820, -1, True, dtype=None)  # t1824: "cuda:0 f32[1, 512, 1]"
    # t1822 = prims.sum(t1820, (2,))  # t1822: "cuda:0 f32[1, 512]"
    # t1823 = prims.broadcast_in_dim(t1822, [1, 512, 1], [0, 1])  # t1823: "cuda:0 f32[1, 512, 1]"
    # t1824 = ltorch.true_divide(t1823, 4096)  # t1824: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1824 = prims.div(t1823, 4096.0)  # t1824: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1826 = ltorch.add(t1824, 1e-05, alpha=None)  # t1826: "cuda:0 f32[1, 512, 1]"
    # t1826 = prims.add(t1824, 1e-05)  # t1826: "cuda:0 f32[1, 512, 1]"
  t1827 = ltorch.rsqrt(t1826)  # t1827: "cuda:0 f32[1, 512, 1]"
    # t1827 = prims.rsqrt(t1826)  # t1827: "cuda:0 f32[1, 512, 1]"
  t1829 = ltorch.mul(t1819, t1827)  # t1829: "cuda:0 f32[1, 512, 4096]"
    # t1828 = prims.broadcast_in_dim(t1827, (1, 512, 4096), (0, 1, 2))  # t1828: "cuda:0 f32[1, 512, 4096]"
    # t1829 = prims.mul(t1819, t1828)  # t1829: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1830 = ltorch.to(t1829, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1830: "cuda:0 bf16[1, 512, 4096]"
    # t1830 = prims.convert_element_type(t1829, dtypes.bfloat16)  # t1830: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1840 = ltorch.mul(t1830, t_transformer_h_11_norm_1_weight)  # t1840: "cuda:0 bf16[1, 512, 4096]"
    # t1836 = prims.broadcast_in_dim(t_transformer_h_11_norm_1_weight, (1, 512, 4096), (2,))  # t1836: "cuda:0 bf16[1, 512, 4096]"
    # t1837 = prims.convert_element_type(t1830, dtypes.float32)  # t1837: "cuda:0 f32[1, 512, 4096]"
    # t1838 = prims.convert_element_type(t1836, dtypes.float32)  # t1838: "cuda:0 f32[1, 512, 4096]"
    # t1839 = prims.mul(t1837, t1838)  # t1839: "cuda:0 f32[1, 512, 4096]"
    # t1840 = prims.convert_element_type(t1839, dtypes.bfloat16)  # t1840: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1845 = ltorch.linear(t1840, t_transformer_h_11_attn_attn_weight, None)  # t1845: "cuda:0 bf16[1, 512, 12288]"
    # t1845 = prims.linear(t1840, t_transformer_h_11_attn_attn_weight, None)  # t1845: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t1846 = ltorch.view(t1845, 1, 512, 32, 3, 128)  # t1846: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t1846 = ltorch.reshape(t1845, (1, 512, 32, 3, 128))  # t1846: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t1846 = prims.reshape(t1845, (1, 512, 32, 3, 128))  # t1846: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t1847 = ltorch.permute(t1846, 0, 2, 3, 1, 4)  # t1847: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t1847 = prims.transpose(t1846, (0, 2, 3, 1, 4))  # t1847: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t1848, t1849, t1850) = ltorch.split(t1847, (1, 1, 1), 2)
    # t1848 = prims.slice_prim(t1847, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1848: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1849 = prims.slice_prim(t1847, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1849: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1850 = prims.slice_prim(t1847, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1850: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t1851 = ltorch.reshape(t1848, 1, -1, 512, 128)  # t1851: "cuda:0 bf16[1, 32, 512, 128]"
    # t1851 = prims.reshape(t1848, (1, 32, 512, 128))  # t1851: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t1852 = ltorch.reshape(t1849, 1, -1, 512, 128)  # t1852: "cuda:0 bf16[1, 32, 512, 128]"
    # t1852 = prims.reshape(t1849, (1, 32, 512, 128))  # t1852: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t1853 = ltorch.reshape(t1850, 1, -1, 512, 128)  # t1853: "cuda:0 bf16[1, 32, 512, 128]"
    # t1853 = prims.reshape(t1850, (1, 32, 512, 128))  # t1853: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t1854 = ltorch.getitem(t1851, (..., slice(None, 128, None)))  # t1854: "cuda:0 bf16[1, 32, 512, 128]"
    # t1854 = prims.slice_prim(t1851, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1854: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1855 = ltorch.getitem(t1854, (..., slice(None, 64, None)))  # t1855: "cuda:0 bf16[1, 32, 512, 64]"
    # t1855 = prims.slice_prim(t1854, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1855: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1856 = ltorch.getitem(t1854, (..., slice(64, None, None)))  # t1856: "cuda:0 bf16[1, 32, 512, 64]"
    # t1856 = prims.slice_prim(t1854, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1856: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1859 = ltorch.neg(t1856)  # t1859: "cuda:0 bf16[1, 32, 512, 64]"
    # t1857 = prims.convert_element_type(t1856, dtypes.float32)  # t1857: "cuda:0 f32[1, 32, 512, 64]"
    # t1858 = prims.neg(t1857)  # t1858: "cuda:0 f32[1, 32, 512, 64]"
    # t1859 = prims.convert_element_type(t1858, dtypes.bfloat16)  # t1859: "cuda:0 bf16[1, 32, 512, 64]"
  t1860 = ltorch.cat((t1859, t1855), -1)  # t1860: "cuda:0 bf16[1, 32, 512, 128]"
    # t1860 = prims.cat((t1859, t1855), -1)  # t1860: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1863 = ltorch.mul(t1854, cos)  # t1863: "cuda:0 f32[1, 32, 512, 128]"
    # t1861 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1861: "cuda:0 f32[1, 32, 512, 128]"
    # t1862 = prims.convert_element_type(t1854, dtypes.float32)  # t1862: "cuda:0 f32[1, 32, 512, 128]"
    # t1863 = prims.mul(t1862, t1861)  # t1863: "cuda:0 f32[1, 32, 512, 128]"
  t1866 = ltorch.mul(t1860, sin)  # t1866: "cuda:0 f32[1, 32, 512, 128]"
    # t1864 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1864: "cuda:0 f32[1, 32, 512, 128]"
    # t1865 = prims.convert_element_type(t1860, dtypes.float32)  # t1865: "cuda:0 f32[1, 32, 512, 128]"
    # t1866 = prims.mul(t1865, t1864)  # t1866: "cuda:0 f32[1, 32, 512, 128]"
  t1867 = ltorch.add(t1863, t1866, alpha=None)  # t1867: "cuda:0 f32[1, 32, 512, 128]"
    # t1867 = prims.add(t1863, t1866)  # t1867: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1868 = ltorch.to(t1867, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1868: "cuda:0 bf16[1, 32, 512, 128]"
    # t1868 = prims.convert_element_type(t1867, dtypes.bfloat16)  # t1868: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t1869 = ltorch.getitem(t1852, (..., slice(None, 128, None)))  # t1869: "cuda:0 bf16[1, 32, 512, 128]"
    # t1869 = prims.slice_prim(t1852, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1869: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t1870 = ltorch.getitem(t1869, (..., slice(None, 64, None)))  # t1870: "cuda:0 bf16[1, 32, 512, 64]"
    # t1870 = prims.slice_prim(t1869, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1870: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t1871 = ltorch.getitem(t1869, (..., slice(64, None, None)))  # t1871: "cuda:0 bf16[1, 32, 512, 64]"
    # t1871 = prims.slice_prim(t1869, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1871: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t1874 = ltorch.neg(t1871)  # t1874: "cuda:0 bf16[1, 32, 512, 64]"
    # t1872 = prims.convert_element_type(t1871, dtypes.float32)  # t1872: "cuda:0 f32[1, 32, 512, 64]"
    # t1873 = prims.neg(t1872)  # t1873: "cuda:0 f32[1, 32, 512, 64]"
    # t1874 = prims.convert_element_type(t1873, dtypes.bfloat16)  # t1874: "cuda:0 bf16[1, 32, 512, 64]"
  t1875 = ltorch.cat((t1874, t1870), -1)  # t1875: "cuda:0 bf16[1, 32, 512, 128]"
    # t1875 = prims.cat((t1874, t1870), -1)  # t1875: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t1878 = ltorch.mul(t1869, cos)  # t1878: "cuda:0 f32[1, 32, 512, 128]"
    # t1876 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t1876: "cuda:0 f32[1, 32, 512, 128]"
    # t1877 = prims.convert_element_type(t1869, dtypes.float32)  # t1877: "cuda:0 f32[1, 32, 512, 128]"
    # t1878 = prims.mul(t1877, t1876)  # t1878: "cuda:0 f32[1, 32, 512, 128]"
  t1881 = ltorch.mul(t1875, sin)  # t1881: "cuda:0 f32[1, 32, 512, 128]"
    # t1879 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t1879: "cuda:0 f32[1, 32, 512, 128]"
    # t1880 = prims.convert_element_type(t1875, dtypes.float32)  # t1880: "cuda:0 f32[1, 32, 512, 128]"
    # t1881 = prims.mul(t1880, t1879)  # t1881: "cuda:0 f32[1, 32, 512, 128]"
  t1882 = ltorch.add(t1878, t1881, alpha=None)  # t1882: "cuda:0 f32[1, 32, 512, 128]"
    # t1882 = prims.add(t1878, t1881)  # t1882: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t1883 = ltorch.to(t1882, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1883: "cuda:0 bf16[1, 32, 512, 128]"
    # t1883 = prims.convert_element_type(t1882, dtypes.bfloat16)  # t1883: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t1884 = ltorch.getitem(t1851, (..., slice(128, None, None)))  # t1884: "cuda:0 bf16[1, 32, 512, 0]"
    # t1884 = prims.slice_prim(t1851, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1884: "cuda:0 bf16[1, 32, 512, 0]"
  t1885 = ltorch.cat((t1868, t1884), -1)  # t1885: "cuda:0 bf16[1, 32, 512, 128]"
    # t1885 = prims.cat((t1868, t1884), -1)  # t1885: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t1886 = ltorch.getitem(t1852, (..., slice(128, None, None)))  # t1886: "cuda:0 bf16[1, 32, 512, 0]"
    # t1886 = prims.slice_prim(t1852, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1886: "cuda:0 bf16[1, 32, 512, 0]"
  t1887 = ltorch.cat((t1883, t1886), -1)  # t1887: "cuda:0 bf16[1, 32, 512, 128]"
    # t1887 = prims.cat((t1883, t1886), -1)  # t1887: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t1917 = ltorch.scaled_dot_product_attention(t1885, t1887, t1853, None, 0.0, True, scale=0.08838834764831843)  # t1917: "cuda:0 bf16[1, 32, 512, 128]"
    # t1890 = ltorch.mul(t1885, 0.29730177875068026)  # t1890: "cuda:0 bf16[1, 32, 512, 128]"
      # t1888 = prims.convert_element_type(t1885, dtypes.float32)  # t1888: "cuda:0 f32[1, 32, 512, 128]"
      # t1889 = prims.mul(t1888, 0.29730177875068026)  # t1889: "cuda:0 f32[1, 32, 512, 128]"
      # t1890 = prims.convert_element_type(t1889, dtypes.bfloat16)  # t1890: "cuda:0 bf16[1, 32, 512, 128]"
    # t1891 = ltorch.transpose(t1887, -2, -1)  # t1891: "cuda:0 bf16[1, 32, 128, 512]"
      # t1891 = prims.transpose(t1887, (0, 1, 3, 2))  # t1891: "cuda:0 bf16[1, 32, 128, 512]"
    # t1894 = ltorch.mul(t1891, 0.29730177875068026)  # t1894: "cuda:0 bf16[1, 32, 128, 512]"
      # t1892 = prims.convert_element_type(t1891, dtypes.float32)  # t1892: "cuda:0 f32[1, 32, 128, 512]"
      # t1893 = prims.mul(t1892, 0.29730177875068026)  # t1893: "cuda:0 f32[1, 32, 128, 512]"
      # t1894 = prims.convert_element_type(t1893, dtypes.bfloat16)  # t1894: "cuda:0 bf16[1, 32, 128, 512]"
    # t1895 = ltorch.matmul(t1890, t1894)  # t1895: "cuda:0 bf16[1, 32, 512, 512]"
      # t1895 = prims.matmul(t1890, t1894)  # t1895: "cuda:0 bf16[1, 32, 512, 512]"
    # t1905 = ltorch.tril(t1895, 0, fill_value=-float('inf'))  # t1905: "cuda:0 bf16[1, 32, 512, 512]"
      # t1896 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1896: "cuda:0 i64[512]"
        # t1896 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1896: "cuda:0 i64[512]"
      # t1897 = ltorch.unsqueeze(t1896, -1)  # t1897: "cuda:0 i64[512, 1]"
        # t1897 = prims.broadcast_in_dim(t1896, [512, 1], [0])  # t1897: "cuda:0 i64[512, 1]"
      # t1898 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t1898: "cuda:0 i64[512]"
        # t1898 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t1898: "cuda:0 i64[512]"
      # t1899 = ltorch.unsqueeze(t1898, -2)  # t1899: "cuda:0 i64[1, 512]"
        # t1899 = prims.broadcast_in_dim(t1898, [1, 512], [1])  # t1899: "cuda:0 i64[1, 512]"
      # t1900 = ltorch.add(t1897, 0, alpha=None)  # t1900: "cuda:0 i64[512, 1]"
        # t1900 = prims.add(t1897, 0)  # t1900: "cuda:0 i64[512, 1]"
      # t1903 = ltorch.ge(t1900, t1899)  # t1903: "cuda:0 b8[512, 512]"
        # t1901 = prims.broadcast_in_dim(t1900, (512, 512), (0, 1))  # t1901: "cuda:0 i64[512, 512]"
        # t1902 = prims.broadcast_in_dim(t1899, (512, 512), (0, 1))  # t1902: "cuda:0 i64[512, 512]"
        # t1903 = prims.ge(t1901, t1902)  # t1903: "cuda:0 b8[512, 512]"
      # t1905 = ltorch.where(t1903, t1895, -float('inf'))  # t1905: "cuda:0 bf16[1, 32, 512, 512]"
        # t1904 = prims.broadcast_in_dim(t1903, (1, 32, 512, 512), (2, 3))  # t1904: "cuda:0 b8[1, 32, 512, 512]"
        # t1905 = prims.where(t1904, t1895, -float('inf'))  # t1905: "cuda:0 bf16[1, 32, 512, 512]"
    # t1916 = ltorch._softmax(t1905, -1, dtype=None)  # t1916: "cuda:0 bf16[1, 32, 512, 512]"
      # t1906 = ltorch.to(t1905, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t1906: "cuda:0 f32[1, 32, 512, 512]"
        # t1906 = prims.convert_element_type(t1905, dtypes.float32)  # t1906: "cuda:0 f32[1, 32, 512, 512]"
      # t1908 = ltorch.amax(t1906, -1, True)  # t1908: "cuda:0 f32[1, 32, 512, 1]"
        # t1907 = prims.amax(t1906, (3,))  # t1907: "cuda:0 f32[1, 32, 512]"
        # t1908 = prims.broadcast_in_dim(t1907, [1, 32, 512, 1], [0, 1, 2])  # t1908: "cuda:0 f32[1, 32, 512, 1]"
      # t1910 = ltorch.sub(t1906, t1908, alpha=None)  # t1910: "cuda:0 f32[1, 32, 512, 512]"
        # t1909 = prims.broadcast_in_dim(t1908, (1, 32, 512, 512), (0, 1, 2, 3))  # t1909: "cuda:0 f32[1, 32, 512, 512]"
        # t1910 = prims.sub(t1906, t1909)  # t1910: "cuda:0 f32[1, 32, 512, 512]"
      # t1911 = ltorch.exp(t1910)  # t1911: "cuda:0 f32[1, 32, 512, 512]"
        # t1911 = prims.exp(t1910)  # t1911: "cuda:0 f32[1, 32, 512, 512]"
      # t1913 = ltorch.sum(t1911, -1, True, dtype=None)  # t1913: "cuda:0 f32[1, 32, 512, 1]"
        # t1912 = prims.sum(t1911, (3,))  # t1912: "cuda:0 f32[1, 32, 512]"
        # t1913 = prims.broadcast_in_dim(t1912, [1, 32, 512, 1], [0, 1, 2])  # t1913: "cuda:0 f32[1, 32, 512, 1]"
      # t1915 = ltorch.true_divide(t1911, t1913)  # t1915: "cuda:0 f32[1, 32, 512, 512]"
        # t1914 = prims.broadcast_in_dim(t1913, (1, 32, 512, 512), (0, 1, 2, 3))  # t1914: "cuda:0 f32[1, 32, 512, 512]"
        # t1915 = prims.div(t1911, t1914)  # t1915: "cuda:0 f32[1, 32, 512, 512]"
      # t1916 = ltorch.to(t1915, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t1916: "cuda:0 bf16[1, 32, 512, 512]"
        # t1916 = prims.convert_element_type(t1915, dtypes.bfloat16)  # t1916: "cuda:0 bf16[1, 32, 512, 512]"
    # t1917 = ltorch.matmul(t1916, t1853)  # t1917: "cuda:0 bf16[1, 32, 512, 128]"
      # t1917 = prims.matmul(t1916, t1853)  # t1917: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t1918 = ltorch.transpose(t1917, 1, 2)  # t1918: "cuda:0 bf16[1, 512, 32, 128]"
    # t1918 = prims.transpose(t1917, (0, 2, 1, 3))  # t1918: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t1919 = ltorch.reshape(t1918, 1, 512, 4096)  # t1919: "cuda:0 bf16[1, 512, 4096]"
    # t1919 = prims.reshape(t1918, (1, 512, 4096))  # t1919: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1923 = ltorch.linear(t1919, t_transformer_h_11_attn_proj_weight, None)  # t1923: "cuda:0 bf16[1, 512, 4096]"
    # t1923 = prims.linear(t1919, t_transformer_h_11_attn_proj_weight, None)  # t1923: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t1927 = ltorch.add(t1923, t1817, alpha=None)  # t1927: "cuda:0 bf16[1, 512, 4096]"
    # t1924 = prims.convert_element_type(t1923, dtypes.float32)  # t1924: "cuda:0 f32[1, 512, 4096]"
    # t1925 = prims.convert_element_type(t1817, dtypes.float32)  # t1925: "cuda:0 f32[1, 512, 4096]"
    # t1926 = prims.add(t1924, t1925)  # t1926: "cuda:0 f32[1, 512, 4096]"
    # t1927 = prims.convert_element_type(t1926, dtypes.bfloat16)  # t1927: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1928 = prims.convert_element_type(t1927, dtypes.float32)  # t1928: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1929 = ltorch.mul(t1928, t1928)  # t1929: "cuda:0 f32[1, 512, 4096]"
    # t1929 = prims.mul(t1928, t1928)  # t1929: "cuda:0 f32[1, 512, 4096]"
  t1933 = ltorch.mean(t1929, -1, True, dtype=None)  # t1933: "cuda:0 f32[1, 512, 1]"
    # t1931 = prims.sum(t1929, (2,))  # t1931: "cuda:0 f32[1, 512]"
    # t1932 = prims.broadcast_in_dim(t1931, [1, 512, 1], [0, 1])  # t1932: "cuda:0 f32[1, 512, 1]"
    # t1933 = ltorch.true_divide(t1932, 4096)  # t1933: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1933 = prims.div(t1932, 4096.0)  # t1933: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1935 = ltorch.add(t1933, 1e-05, alpha=None)  # t1935: "cuda:0 f32[1, 512, 1]"
    # t1935 = prims.add(t1933, 1e-05)  # t1935: "cuda:0 f32[1, 512, 1]"
  t1936 = ltorch.rsqrt(t1935)  # t1936: "cuda:0 f32[1, 512, 1]"
    # t1936 = prims.rsqrt(t1935)  # t1936: "cuda:0 f32[1, 512, 1]"
  t1938 = ltorch.mul(t1928, t1936)  # t1938: "cuda:0 f32[1, 512, 4096]"
    # t1937 = prims.broadcast_in_dim(t1936, (1, 512, 4096), (0, 1, 2))  # t1937: "cuda:0 f32[1, 512, 4096]"
    # t1938 = prims.mul(t1928, t1937)  # t1938: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1939 = ltorch.to(t1938, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1939: "cuda:0 bf16[1, 512, 4096]"
    # t1939 = prims.convert_element_type(t1938, dtypes.bfloat16)  # t1939: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t1949 = ltorch.mul(t1939, t_transformer_h_11_norm_2_weight)  # t1949: "cuda:0 bf16[1, 512, 4096]"
    # t1945 = prims.broadcast_in_dim(t_transformer_h_11_norm_2_weight, (1, 512, 4096), (2,))  # t1945: "cuda:0 bf16[1, 512, 4096]"
    # t1946 = prims.convert_element_type(t1939, dtypes.float32)  # t1946: "cuda:0 f32[1, 512, 4096]"
    # t1947 = prims.convert_element_type(t1945, dtypes.float32)  # t1947: "cuda:0 f32[1, 512, 4096]"
    # t1948 = prims.mul(t1946, t1947)  # t1948: "cuda:0 f32[1, 512, 4096]"
    # t1949 = prims.convert_element_type(t1948, dtypes.bfloat16)  # t1949: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1954 = ltorch.linear(t1949, t_transformer_h_11_mlp_fc_1_weight, None)  # t1954: "cuda:0 bf16[1, 512, 11008]"
    # t1954 = prims.linear(t1949, t_transformer_h_11_mlp_fc_1_weight, None)  # t1954: "cuda:0 bf16[1, 512, 11008]"
  t1958 = ltorch.linear(t1949, t_transformer_h_11_mlp_fc_2_weight, None)  # t1958: "cuda:0 bf16[1, 512, 11008]"
    # t1958 = prims.linear(t1949, t_transformer_h_11_mlp_fc_2_weight, None)  # t1958: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t1968 = ltorch.silu(t1954, False)  # t1968: "cuda:0 bf16[1, 512, 11008]"
    # t1959 = prims.convert_element_type(t1954, dtypes.float32)  # t1959: "cuda:0 f32[1, 512, 11008]"
    # t1960 = prims.neg(t1959)  # t1960: "cuda:0 f32[1, 512, 11008]"
    # t1961 = prims.exp(t1960)  # t1961: "cuda:0 f32[1, 512, 11008]"
    # t1962 = prims.add(1.0, t1961)  # t1962: "cuda:0 f32[1, 512, 11008]"
    # t1963 = prims.reciprocal(t1962)  # t1963: "cuda:0 f32[1, 512, 11008]"
    # t1964 = prims.convert_element_type(t1963, dtypes.bfloat16)  # t1964: "cuda:0 bf16[1, 512, 11008]"
    # t1965 = prims.convert_element_type(t1954, dtypes.float32)  # t1965: "cuda:0 f32[1, 512, 11008]"
    # t1966 = prims.convert_element_type(t1964, dtypes.float32)  # t1966: "cuda:0 f32[1, 512, 11008]"
    # t1967 = prims.mul(t1965, t1966)  # t1967: "cuda:0 f32[1, 512, 11008]"
    # t1968 = prims.convert_element_type(t1967, dtypes.bfloat16)  # t1968: "cuda:0 bf16[1, 512, 11008]"
  t1972 = ltorch.mul(t1968, t1958)  # t1972: "cuda:0 bf16[1, 512, 11008]"
    # t1969 = prims.convert_element_type(t1968, dtypes.float32)  # t1969: "cuda:0 f32[1, 512, 11008]"
    # t1970 = prims.convert_element_type(t1958, dtypes.float32)  # t1970: "cuda:0 f32[1, 512, 11008]"
    # t1971 = prims.mul(t1969, t1970)  # t1971: "cuda:0 f32[1, 512, 11008]"
    # t1972 = prims.convert_element_type(t1971, dtypes.bfloat16)  # t1972: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t1976 = ltorch.linear(t1972, t_transformer_h_11_mlp_proj_weight, None)  # t1976: "cuda:0 bf16[1, 512, 4096]"
    # t1976 = prims.linear(t1972, t_transformer_h_11_mlp_proj_weight, None)  # t1976: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t1980 = ltorch.add(t1976, t1927, alpha=None)  # t1980: "cuda:0 bf16[1, 512, 4096]"
    # t1977 = prims.convert_element_type(t1976, dtypes.float32)  # t1977: "cuda:0 f32[1, 512, 4096]"
    # t1978 = prims.convert_element_type(t1927, dtypes.float32)  # t1978: "cuda:0 f32[1, 512, 4096]"
    # t1979 = prims.add(t1977, t1978)  # t1979: "cuda:0 f32[1, 512, 4096]"
    # t1980 = prims.convert_element_type(t1979, dtypes.bfloat16)  # t1980: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t1982 = prims.convert_element_type(t1980, dtypes.float32)  # t1982: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t1983 = ltorch.mul(t1982, t1982)  # t1983: "cuda:0 f32[1, 512, 4096]"
    # t1983 = prims.mul(t1982, t1982)  # t1983: "cuda:0 f32[1, 512, 4096]"
  t1987 = ltorch.mean(t1983, -1, True, dtype=None)  # t1987: "cuda:0 f32[1, 512, 1]"
    # t1985 = prims.sum(t1983, (2,))  # t1985: "cuda:0 f32[1, 512]"
    # t1986 = prims.broadcast_in_dim(t1985, [1, 512, 1], [0, 1])  # t1986: "cuda:0 f32[1, 512, 1]"
    # t1987 = ltorch.true_divide(t1986, 4096)  # t1987: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t1987 = prims.div(t1986, 4096.0)  # t1987: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t1989 = ltorch.add(t1987, 1e-05, alpha=None)  # t1989: "cuda:0 f32[1, 512, 1]"
    # t1989 = prims.add(t1987, 1e-05)  # t1989: "cuda:0 f32[1, 512, 1]"
  t1990 = ltorch.rsqrt(t1989)  # t1990: "cuda:0 f32[1, 512, 1]"
    # t1990 = prims.rsqrt(t1989)  # t1990: "cuda:0 f32[1, 512, 1]"
  t1992 = ltorch.mul(t1982, t1990)  # t1992: "cuda:0 f32[1, 512, 4096]"
    # t1991 = prims.broadcast_in_dim(t1990, (1, 512, 4096), (0, 1, 2))  # t1991: "cuda:0 f32[1, 512, 4096]"
    # t1992 = prims.mul(t1982, t1991)  # t1992: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t1993 = ltorch.to(t1992, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t1993: "cuda:0 bf16[1, 512, 4096]"
    # t1993 = prims.convert_element_type(t1992, dtypes.bfloat16)  # t1993: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2003 = ltorch.mul(t1993, t_transformer_h_12_norm_1_weight)  # t2003: "cuda:0 bf16[1, 512, 4096]"
    # t1999 = prims.broadcast_in_dim(t_transformer_h_12_norm_1_weight, (1, 512, 4096), (2,))  # t1999: "cuda:0 bf16[1, 512, 4096]"
    # t2000 = prims.convert_element_type(t1993, dtypes.float32)  # t2000: "cuda:0 f32[1, 512, 4096]"
    # t2001 = prims.convert_element_type(t1999, dtypes.float32)  # t2001: "cuda:0 f32[1, 512, 4096]"
    # t2002 = prims.mul(t2000, t2001)  # t2002: "cuda:0 f32[1, 512, 4096]"
    # t2003 = prims.convert_element_type(t2002, dtypes.bfloat16)  # t2003: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2008 = ltorch.linear(t2003, t_transformer_h_12_attn_attn_weight, None)  # t2008: "cuda:0 bf16[1, 512, 12288]"
    # t2008 = prims.linear(t2003, t_transformer_h_12_attn_attn_weight, None)  # t2008: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t2009 = ltorch.view(t2008, 1, 512, 32, 3, 128)  # t2009: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t2009 = ltorch.reshape(t2008, (1, 512, 32, 3, 128))  # t2009: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t2009 = prims.reshape(t2008, (1, 512, 32, 3, 128))  # t2009: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t2010 = ltorch.permute(t2009, 0, 2, 3, 1, 4)  # t2010: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t2010 = prims.transpose(t2009, (0, 2, 3, 1, 4))  # t2010: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t2011, t2012, t2013) = ltorch.split(t2010, (1, 1, 1), 2)
    # t2011 = prims.slice_prim(t2010, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2011: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2012 = prims.slice_prim(t2010, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2012: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2013 = prims.slice_prim(t2010, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2013: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t2014 = ltorch.reshape(t2011, 1, -1, 512, 128)  # t2014: "cuda:0 bf16[1, 32, 512, 128]"
    # t2014 = prims.reshape(t2011, (1, 32, 512, 128))  # t2014: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t2015 = ltorch.reshape(t2012, 1, -1, 512, 128)  # t2015: "cuda:0 bf16[1, 32, 512, 128]"
    # t2015 = prims.reshape(t2012, (1, 32, 512, 128))  # t2015: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t2016 = ltorch.reshape(t2013, 1, -1, 512, 128)  # t2016: "cuda:0 bf16[1, 32, 512, 128]"
    # t2016 = prims.reshape(t2013, (1, 32, 512, 128))  # t2016: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t2017 = ltorch.getitem(t2014, (..., slice(None, 128, None)))  # t2017: "cuda:0 bf16[1, 32, 512, 128]"
    # t2017 = prims.slice_prim(t2014, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2017: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2018 = ltorch.getitem(t2017, (..., slice(None, 64, None)))  # t2018: "cuda:0 bf16[1, 32, 512, 64]"
    # t2018 = prims.slice_prim(t2017, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2018: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2019 = ltorch.getitem(t2017, (..., slice(64, None, None)))  # t2019: "cuda:0 bf16[1, 32, 512, 64]"
    # t2019 = prims.slice_prim(t2017, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2019: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2022 = ltorch.neg(t2019)  # t2022: "cuda:0 bf16[1, 32, 512, 64]"
    # t2020 = prims.convert_element_type(t2019, dtypes.float32)  # t2020: "cuda:0 f32[1, 32, 512, 64]"
    # t2021 = prims.neg(t2020)  # t2021: "cuda:0 f32[1, 32, 512, 64]"
    # t2022 = prims.convert_element_type(t2021, dtypes.bfloat16)  # t2022: "cuda:0 bf16[1, 32, 512, 64]"
  t2023 = ltorch.cat((t2022, t2018), -1)  # t2023: "cuda:0 bf16[1, 32, 512, 128]"
    # t2023 = prims.cat((t2022, t2018), -1)  # t2023: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2026 = ltorch.mul(t2017, cos)  # t2026: "cuda:0 f32[1, 32, 512, 128]"
    # t2024 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2024: "cuda:0 f32[1, 32, 512, 128]"
    # t2025 = prims.convert_element_type(t2017, dtypes.float32)  # t2025: "cuda:0 f32[1, 32, 512, 128]"
    # t2026 = prims.mul(t2025, t2024)  # t2026: "cuda:0 f32[1, 32, 512, 128]"
  t2029 = ltorch.mul(t2023, sin)  # t2029: "cuda:0 f32[1, 32, 512, 128]"
    # t2027 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2027: "cuda:0 f32[1, 32, 512, 128]"
    # t2028 = prims.convert_element_type(t2023, dtypes.float32)  # t2028: "cuda:0 f32[1, 32, 512, 128]"
    # t2029 = prims.mul(t2028, t2027)  # t2029: "cuda:0 f32[1, 32, 512, 128]"
  t2030 = ltorch.add(t2026, t2029, alpha=None)  # t2030: "cuda:0 f32[1, 32, 512, 128]"
    # t2030 = prims.add(t2026, t2029)  # t2030: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2031 = ltorch.to(t2030, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2031: "cuda:0 bf16[1, 32, 512, 128]"
    # t2031 = prims.convert_element_type(t2030, dtypes.bfloat16)  # t2031: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t2032 = ltorch.getitem(t2015, (..., slice(None, 128, None)))  # t2032: "cuda:0 bf16[1, 32, 512, 128]"
    # t2032 = prims.slice_prim(t2015, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2032: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2033 = ltorch.getitem(t2032, (..., slice(None, 64, None)))  # t2033: "cuda:0 bf16[1, 32, 512, 64]"
    # t2033 = prims.slice_prim(t2032, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2033: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2034 = ltorch.getitem(t2032, (..., slice(64, None, None)))  # t2034: "cuda:0 bf16[1, 32, 512, 64]"
    # t2034 = prims.slice_prim(t2032, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2034: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2037 = ltorch.neg(t2034)  # t2037: "cuda:0 bf16[1, 32, 512, 64]"
    # t2035 = prims.convert_element_type(t2034, dtypes.float32)  # t2035: "cuda:0 f32[1, 32, 512, 64]"
    # t2036 = prims.neg(t2035)  # t2036: "cuda:0 f32[1, 32, 512, 64]"
    # t2037 = prims.convert_element_type(t2036, dtypes.bfloat16)  # t2037: "cuda:0 bf16[1, 32, 512, 64]"
  t2038 = ltorch.cat((t2037, t2033), -1)  # t2038: "cuda:0 bf16[1, 32, 512, 128]"
    # t2038 = prims.cat((t2037, t2033), -1)  # t2038: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2041 = ltorch.mul(t2032, cos)  # t2041: "cuda:0 f32[1, 32, 512, 128]"
    # t2039 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2039: "cuda:0 f32[1, 32, 512, 128]"
    # t2040 = prims.convert_element_type(t2032, dtypes.float32)  # t2040: "cuda:0 f32[1, 32, 512, 128]"
    # t2041 = prims.mul(t2040, t2039)  # t2041: "cuda:0 f32[1, 32, 512, 128]"
  t2044 = ltorch.mul(t2038, sin)  # t2044: "cuda:0 f32[1, 32, 512, 128]"
    # t2042 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2042: "cuda:0 f32[1, 32, 512, 128]"
    # t2043 = prims.convert_element_type(t2038, dtypes.float32)  # t2043: "cuda:0 f32[1, 32, 512, 128]"
    # t2044 = prims.mul(t2043, t2042)  # t2044: "cuda:0 f32[1, 32, 512, 128]"
  t2045 = ltorch.add(t2041, t2044, alpha=None)  # t2045: "cuda:0 f32[1, 32, 512, 128]"
    # t2045 = prims.add(t2041, t2044)  # t2045: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2046 = ltorch.to(t2045, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2046: "cuda:0 bf16[1, 32, 512, 128]"
    # t2046 = prims.convert_element_type(t2045, dtypes.bfloat16)  # t2046: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t2047 = ltorch.getitem(t2014, (..., slice(128, None, None)))  # t2047: "cuda:0 bf16[1, 32, 512, 0]"
    # t2047 = prims.slice_prim(t2014, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2047: "cuda:0 bf16[1, 32, 512, 0]"
  t2048 = ltorch.cat((t2031, t2047), -1)  # t2048: "cuda:0 bf16[1, 32, 512, 128]"
    # t2048 = prims.cat((t2031, t2047), -1)  # t2048: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t2049 = ltorch.getitem(t2015, (..., slice(128, None, None)))  # t2049: "cuda:0 bf16[1, 32, 512, 0]"
    # t2049 = prims.slice_prim(t2015, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2049: "cuda:0 bf16[1, 32, 512, 0]"
  t2050 = ltorch.cat((t2046, t2049), -1)  # t2050: "cuda:0 bf16[1, 32, 512, 128]"
    # t2050 = prims.cat((t2046, t2049), -1)  # t2050: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t2080 = ltorch.scaled_dot_product_attention(t2048, t2050, t2016, None, 0.0, True, scale=0.08838834764831843)  # t2080: "cuda:0 bf16[1, 32, 512, 128]"
    # t2053 = ltorch.mul(t2048, 0.29730177875068026)  # t2053: "cuda:0 bf16[1, 32, 512, 128]"
      # t2051 = prims.convert_element_type(t2048, dtypes.float32)  # t2051: "cuda:0 f32[1, 32, 512, 128]"
      # t2052 = prims.mul(t2051, 0.29730177875068026)  # t2052: "cuda:0 f32[1, 32, 512, 128]"
      # t2053 = prims.convert_element_type(t2052, dtypes.bfloat16)  # t2053: "cuda:0 bf16[1, 32, 512, 128]"
    # t2054 = ltorch.transpose(t2050, -2, -1)  # t2054: "cuda:0 bf16[1, 32, 128, 512]"
      # t2054 = prims.transpose(t2050, (0, 1, 3, 2))  # t2054: "cuda:0 bf16[1, 32, 128, 512]"
    # t2057 = ltorch.mul(t2054, 0.29730177875068026)  # t2057: "cuda:0 bf16[1, 32, 128, 512]"
      # t2055 = prims.convert_element_type(t2054, dtypes.float32)  # t2055: "cuda:0 f32[1, 32, 128, 512]"
      # t2056 = prims.mul(t2055, 0.29730177875068026)  # t2056: "cuda:0 f32[1, 32, 128, 512]"
      # t2057 = prims.convert_element_type(t2056, dtypes.bfloat16)  # t2057: "cuda:0 bf16[1, 32, 128, 512]"
    # t2058 = ltorch.matmul(t2053, t2057)  # t2058: "cuda:0 bf16[1, 32, 512, 512]"
      # t2058 = prims.matmul(t2053, t2057)  # t2058: "cuda:0 bf16[1, 32, 512, 512]"
    # t2068 = ltorch.tril(t2058, 0, fill_value=-float('inf'))  # t2068: "cuda:0 bf16[1, 32, 512, 512]"
      # t2059 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2059: "cuda:0 i64[512]"
        # t2059 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2059: "cuda:0 i64[512]"
      # t2060 = ltorch.unsqueeze(t2059, -1)  # t2060: "cuda:0 i64[512, 1]"
        # t2060 = prims.broadcast_in_dim(t2059, [512, 1], [0])  # t2060: "cuda:0 i64[512, 1]"
      # t2061 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2061: "cuda:0 i64[512]"
        # t2061 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2061: "cuda:0 i64[512]"
      # t2062 = ltorch.unsqueeze(t2061, -2)  # t2062: "cuda:0 i64[1, 512]"
        # t2062 = prims.broadcast_in_dim(t2061, [1, 512], [1])  # t2062: "cuda:0 i64[1, 512]"
      # t2063 = ltorch.add(t2060, 0, alpha=None)  # t2063: "cuda:0 i64[512, 1]"
        # t2063 = prims.add(t2060, 0)  # t2063: "cuda:0 i64[512, 1]"
      # t2066 = ltorch.ge(t2063, t2062)  # t2066: "cuda:0 b8[512, 512]"
        # t2064 = prims.broadcast_in_dim(t2063, (512, 512), (0, 1))  # t2064: "cuda:0 i64[512, 512]"
        # t2065 = prims.broadcast_in_dim(t2062, (512, 512), (0, 1))  # t2065: "cuda:0 i64[512, 512]"
        # t2066 = prims.ge(t2064, t2065)  # t2066: "cuda:0 b8[512, 512]"
      # t2068 = ltorch.where(t2066, t2058, -float('inf'))  # t2068: "cuda:0 bf16[1, 32, 512, 512]"
        # t2067 = prims.broadcast_in_dim(t2066, (1, 32, 512, 512), (2, 3))  # t2067: "cuda:0 b8[1, 32, 512, 512]"
        # t2068 = prims.where(t2067, t2058, -float('inf'))  # t2068: "cuda:0 bf16[1, 32, 512, 512]"
    # t2079 = ltorch._softmax(t2068, -1, dtype=None)  # t2079: "cuda:0 bf16[1, 32, 512, 512]"
      # t2069 = ltorch.to(t2068, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t2069: "cuda:0 f32[1, 32, 512, 512]"
        # t2069 = prims.convert_element_type(t2068, dtypes.float32)  # t2069: "cuda:0 f32[1, 32, 512, 512]"
      # t2071 = ltorch.amax(t2069, -1, True)  # t2071: "cuda:0 f32[1, 32, 512, 1]"
        # t2070 = prims.amax(t2069, (3,))  # t2070: "cuda:0 f32[1, 32, 512]"
        # t2071 = prims.broadcast_in_dim(t2070, [1, 32, 512, 1], [0, 1, 2])  # t2071: "cuda:0 f32[1, 32, 512, 1]"
      # t2073 = ltorch.sub(t2069, t2071, alpha=None)  # t2073: "cuda:0 f32[1, 32, 512, 512]"
        # t2072 = prims.broadcast_in_dim(t2071, (1, 32, 512, 512), (0, 1, 2, 3))  # t2072: "cuda:0 f32[1, 32, 512, 512]"
        # t2073 = prims.sub(t2069, t2072)  # t2073: "cuda:0 f32[1, 32, 512, 512]"
      # t2074 = ltorch.exp(t2073)  # t2074: "cuda:0 f32[1, 32, 512, 512]"
        # t2074 = prims.exp(t2073)  # t2074: "cuda:0 f32[1, 32, 512, 512]"
      # t2076 = ltorch.sum(t2074, -1, True, dtype=None)  # t2076: "cuda:0 f32[1, 32, 512, 1]"
        # t2075 = prims.sum(t2074, (3,))  # t2075: "cuda:0 f32[1, 32, 512]"
        # t2076 = prims.broadcast_in_dim(t2075, [1, 32, 512, 1], [0, 1, 2])  # t2076: "cuda:0 f32[1, 32, 512, 1]"
      # t2078 = ltorch.true_divide(t2074, t2076)  # t2078: "cuda:0 f32[1, 32, 512, 512]"
        # t2077 = prims.broadcast_in_dim(t2076, (1, 32, 512, 512), (0, 1, 2, 3))  # t2077: "cuda:0 f32[1, 32, 512, 512]"
        # t2078 = prims.div(t2074, t2077)  # t2078: "cuda:0 f32[1, 32, 512, 512]"
      # t2079 = ltorch.to(t2078, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t2079: "cuda:0 bf16[1, 32, 512, 512]"
        # t2079 = prims.convert_element_type(t2078, dtypes.bfloat16)  # t2079: "cuda:0 bf16[1, 32, 512, 512]"
    # t2080 = ltorch.matmul(t2079, t2016)  # t2080: "cuda:0 bf16[1, 32, 512, 128]"
      # t2080 = prims.matmul(t2079, t2016)  # t2080: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t2081 = ltorch.transpose(t2080, 1, 2)  # t2081: "cuda:0 bf16[1, 512, 32, 128]"
    # t2081 = prims.transpose(t2080, (0, 2, 1, 3))  # t2081: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t2082 = ltorch.reshape(t2081, 1, 512, 4096)  # t2082: "cuda:0 bf16[1, 512, 4096]"
    # t2082 = prims.reshape(t2081, (1, 512, 4096))  # t2082: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2086 = ltorch.linear(t2082, t_transformer_h_12_attn_proj_weight, None)  # t2086: "cuda:0 bf16[1, 512, 4096]"
    # t2086 = prims.linear(t2082, t_transformer_h_12_attn_proj_weight, None)  # t2086: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t2090 = ltorch.add(t2086, t1980, alpha=None)  # t2090: "cuda:0 bf16[1, 512, 4096]"
    # t2087 = prims.convert_element_type(t2086, dtypes.float32)  # t2087: "cuda:0 f32[1, 512, 4096]"
    # t2088 = prims.convert_element_type(t1980, dtypes.float32)  # t2088: "cuda:0 f32[1, 512, 4096]"
    # t2089 = prims.add(t2087, t2088)  # t2089: "cuda:0 f32[1, 512, 4096]"
    # t2090 = prims.convert_element_type(t2089, dtypes.bfloat16)  # t2090: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2091 = prims.convert_element_type(t2090, dtypes.float32)  # t2091: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2092 = ltorch.mul(t2091, t2091)  # t2092: "cuda:0 f32[1, 512, 4096]"
    # t2092 = prims.mul(t2091, t2091)  # t2092: "cuda:0 f32[1, 512, 4096]"
  t2096 = ltorch.mean(t2092, -1, True, dtype=None)  # t2096: "cuda:0 f32[1, 512, 1]"
    # t2094 = prims.sum(t2092, (2,))  # t2094: "cuda:0 f32[1, 512]"
    # t2095 = prims.broadcast_in_dim(t2094, [1, 512, 1], [0, 1])  # t2095: "cuda:0 f32[1, 512, 1]"
    # t2096 = ltorch.true_divide(t2095, 4096)  # t2096: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2096 = prims.div(t2095, 4096.0)  # t2096: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2098 = ltorch.add(t2096, 1e-05, alpha=None)  # t2098: "cuda:0 f32[1, 512, 1]"
    # t2098 = prims.add(t2096, 1e-05)  # t2098: "cuda:0 f32[1, 512, 1]"
  t2099 = ltorch.rsqrt(t2098)  # t2099: "cuda:0 f32[1, 512, 1]"
    # t2099 = prims.rsqrt(t2098)  # t2099: "cuda:0 f32[1, 512, 1]"
  t2101 = ltorch.mul(t2091, t2099)  # t2101: "cuda:0 f32[1, 512, 4096]"
    # t2100 = prims.broadcast_in_dim(t2099, (1, 512, 4096), (0, 1, 2))  # t2100: "cuda:0 f32[1, 512, 4096]"
    # t2101 = prims.mul(t2091, t2100)  # t2101: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2102 = ltorch.to(t2101, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2102: "cuda:0 bf16[1, 512, 4096]"
    # t2102 = prims.convert_element_type(t2101, dtypes.bfloat16)  # t2102: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2112 = ltorch.mul(t2102, t_transformer_h_12_norm_2_weight)  # t2112: "cuda:0 bf16[1, 512, 4096]"
    # t2108 = prims.broadcast_in_dim(t_transformer_h_12_norm_2_weight, (1, 512, 4096), (2,))  # t2108: "cuda:0 bf16[1, 512, 4096]"
    # t2109 = prims.convert_element_type(t2102, dtypes.float32)  # t2109: "cuda:0 f32[1, 512, 4096]"
    # t2110 = prims.convert_element_type(t2108, dtypes.float32)  # t2110: "cuda:0 f32[1, 512, 4096]"
    # t2111 = prims.mul(t2109, t2110)  # t2111: "cuda:0 f32[1, 512, 4096]"
    # t2112 = prims.convert_element_type(t2111, dtypes.bfloat16)  # t2112: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2117 = ltorch.linear(t2112, t_transformer_h_12_mlp_fc_1_weight, None)  # t2117: "cuda:0 bf16[1, 512, 11008]"
    # t2117 = prims.linear(t2112, t_transformer_h_12_mlp_fc_1_weight, None)  # t2117: "cuda:0 bf16[1, 512, 11008]"
  t2121 = ltorch.linear(t2112, t_transformer_h_12_mlp_fc_2_weight, None)  # t2121: "cuda:0 bf16[1, 512, 11008]"
    # t2121 = prims.linear(t2112, t_transformer_h_12_mlp_fc_2_weight, None)  # t2121: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t2131 = ltorch.silu(t2117, False)  # t2131: "cuda:0 bf16[1, 512, 11008]"
    # t2122 = prims.convert_element_type(t2117, dtypes.float32)  # t2122: "cuda:0 f32[1, 512, 11008]"
    # t2123 = prims.neg(t2122)  # t2123: "cuda:0 f32[1, 512, 11008]"
    # t2124 = prims.exp(t2123)  # t2124: "cuda:0 f32[1, 512, 11008]"
    # t2125 = prims.add(1.0, t2124)  # t2125: "cuda:0 f32[1, 512, 11008]"
    # t2126 = prims.reciprocal(t2125)  # t2126: "cuda:0 f32[1, 512, 11008]"
    # t2127 = prims.convert_element_type(t2126, dtypes.bfloat16)  # t2127: "cuda:0 bf16[1, 512, 11008]"
    # t2128 = prims.convert_element_type(t2117, dtypes.float32)  # t2128: "cuda:0 f32[1, 512, 11008]"
    # t2129 = prims.convert_element_type(t2127, dtypes.float32)  # t2129: "cuda:0 f32[1, 512, 11008]"
    # t2130 = prims.mul(t2128, t2129)  # t2130: "cuda:0 f32[1, 512, 11008]"
    # t2131 = prims.convert_element_type(t2130, dtypes.bfloat16)  # t2131: "cuda:0 bf16[1, 512, 11008]"
  t2135 = ltorch.mul(t2131, t2121)  # t2135: "cuda:0 bf16[1, 512, 11008]"
    # t2132 = prims.convert_element_type(t2131, dtypes.float32)  # t2132: "cuda:0 f32[1, 512, 11008]"
    # t2133 = prims.convert_element_type(t2121, dtypes.float32)  # t2133: "cuda:0 f32[1, 512, 11008]"
    # t2134 = prims.mul(t2132, t2133)  # t2134: "cuda:0 f32[1, 512, 11008]"
    # t2135 = prims.convert_element_type(t2134, dtypes.bfloat16)  # t2135: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2139 = ltorch.linear(t2135, t_transformer_h_12_mlp_proj_weight, None)  # t2139: "cuda:0 bf16[1, 512, 4096]"
    # t2139 = prims.linear(t2135, t_transformer_h_12_mlp_proj_weight, None)  # t2139: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t2143 = ltorch.add(t2139, t2090, alpha=None)  # t2143: "cuda:0 bf16[1, 512, 4096]"
    # t2140 = prims.convert_element_type(t2139, dtypes.float32)  # t2140: "cuda:0 f32[1, 512, 4096]"
    # t2141 = prims.convert_element_type(t2090, dtypes.float32)  # t2141: "cuda:0 f32[1, 512, 4096]"
    # t2142 = prims.add(t2140, t2141)  # t2142: "cuda:0 f32[1, 512, 4096]"
    # t2143 = prims.convert_element_type(t2142, dtypes.bfloat16)  # t2143: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2145 = prims.convert_element_type(t2143, dtypes.float32)  # t2145: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2146 = ltorch.mul(t2145, t2145)  # t2146: "cuda:0 f32[1, 512, 4096]"
    # t2146 = prims.mul(t2145, t2145)  # t2146: "cuda:0 f32[1, 512, 4096]"
  t2150 = ltorch.mean(t2146, -1, True, dtype=None)  # t2150: "cuda:0 f32[1, 512, 1]"
    # t2148 = prims.sum(t2146, (2,))  # t2148: "cuda:0 f32[1, 512]"
    # t2149 = prims.broadcast_in_dim(t2148, [1, 512, 1], [0, 1])  # t2149: "cuda:0 f32[1, 512, 1]"
    # t2150 = ltorch.true_divide(t2149, 4096)  # t2150: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2150 = prims.div(t2149, 4096.0)  # t2150: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2152 = ltorch.add(t2150, 1e-05, alpha=None)  # t2152: "cuda:0 f32[1, 512, 1]"
    # t2152 = prims.add(t2150, 1e-05)  # t2152: "cuda:0 f32[1, 512, 1]"
  t2153 = ltorch.rsqrt(t2152)  # t2153: "cuda:0 f32[1, 512, 1]"
    # t2153 = prims.rsqrt(t2152)  # t2153: "cuda:0 f32[1, 512, 1]"
  t2155 = ltorch.mul(t2145, t2153)  # t2155: "cuda:0 f32[1, 512, 4096]"
    # t2154 = prims.broadcast_in_dim(t2153, (1, 512, 4096), (0, 1, 2))  # t2154: "cuda:0 f32[1, 512, 4096]"
    # t2155 = prims.mul(t2145, t2154)  # t2155: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2156 = ltorch.to(t2155, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2156: "cuda:0 bf16[1, 512, 4096]"
    # t2156 = prims.convert_element_type(t2155, dtypes.bfloat16)  # t2156: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2166 = ltorch.mul(t2156, t_transformer_h_13_norm_1_weight)  # t2166: "cuda:0 bf16[1, 512, 4096]"
    # t2162 = prims.broadcast_in_dim(t_transformer_h_13_norm_1_weight, (1, 512, 4096), (2,))  # t2162: "cuda:0 bf16[1, 512, 4096]"
    # t2163 = prims.convert_element_type(t2156, dtypes.float32)  # t2163: "cuda:0 f32[1, 512, 4096]"
    # t2164 = prims.convert_element_type(t2162, dtypes.float32)  # t2164: "cuda:0 f32[1, 512, 4096]"
    # t2165 = prims.mul(t2163, t2164)  # t2165: "cuda:0 f32[1, 512, 4096]"
    # t2166 = prims.convert_element_type(t2165, dtypes.bfloat16)  # t2166: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2171 = ltorch.linear(t2166, t_transformer_h_13_attn_attn_weight, None)  # t2171: "cuda:0 bf16[1, 512, 12288]"
    # t2171 = prims.linear(t2166, t_transformer_h_13_attn_attn_weight, None)  # t2171: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t2172 = ltorch.view(t2171, 1, 512, 32, 3, 128)  # t2172: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t2172 = ltorch.reshape(t2171, (1, 512, 32, 3, 128))  # t2172: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t2172 = prims.reshape(t2171, (1, 512, 32, 3, 128))  # t2172: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t2173 = ltorch.permute(t2172, 0, 2, 3, 1, 4)  # t2173: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t2173 = prims.transpose(t2172, (0, 2, 3, 1, 4))  # t2173: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t2174, t2175, t2176) = ltorch.split(t2173, (1, 1, 1), 2)
    # t2174 = prims.slice_prim(t2173, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2174: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2175 = prims.slice_prim(t2173, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2175: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2176 = prims.slice_prim(t2173, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2176: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t2177 = ltorch.reshape(t2174, 1, -1, 512, 128)  # t2177: "cuda:0 bf16[1, 32, 512, 128]"
    # t2177 = prims.reshape(t2174, (1, 32, 512, 128))  # t2177: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t2178 = ltorch.reshape(t2175, 1, -1, 512, 128)  # t2178: "cuda:0 bf16[1, 32, 512, 128]"
    # t2178 = prims.reshape(t2175, (1, 32, 512, 128))  # t2178: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t2179 = ltorch.reshape(t2176, 1, -1, 512, 128)  # t2179: "cuda:0 bf16[1, 32, 512, 128]"
    # t2179 = prims.reshape(t2176, (1, 32, 512, 128))  # t2179: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t2180 = ltorch.getitem(t2177, (..., slice(None, 128, None)))  # t2180: "cuda:0 bf16[1, 32, 512, 128]"
    # t2180 = prims.slice_prim(t2177, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2180: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2181 = ltorch.getitem(t2180, (..., slice(None, 64, None)))  # t2181: "cuda:0 bf16[1, 32, 512, 64]"
    # t2181 = prims.slice_prim(t2180, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2181: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2182 = ltorch.getitem(t2180, (..., slice(64, None, None)))  # t2182: "cuda:0 bf16[1, 32, 512, 64]"
    # t2182 = prims.slice_prim(t2180, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2182: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2185 = ltorch.neg(t2182)  # t2185: "cuda:0 bf16[1, 32, 512, 64]"
    # t2183 = prims.convert_element_type(t2182, dtypes.float32)  # t2183: "cuda:0 f32[1, 32, 512, 64]"
    # t2184 = prims.neg(t2183)  # t2184: "cuda:0 f32[1, 32, 512, 64]"
    # t2185 = prims.convert_element_type(t2184, dtypes.bfloat16)  # t2185: "cuda:0 bf16[1, 32, 512, 64]"
  t2186 = ltorch.cat((t2185, t2181), -1)  # t2186: "cuda:0 bf16[1, 32, 512, 128]"
    # t2186 = prims.cat((t2185, t2181), -1)  # t2186: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2189 = ltorch.mul(t2180, cos)  # t2189: "cuda:0 f32[1, 32, 512, 128]"
    # t2187 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2187: "cuda:0 f32[1, 32, 512, 128]"
    # t2188 = prims.convert_element_type(t2180, dtypes.float32)  # t2188: "cuda:0 f32[1, 32, 512, 128]"
    # t2189 = prims.mul(t2188, t2187)  # t2189: "cuda:0 f32[1, 32, 512, 128]"
  t2192 = ltorch.mul(t2186, sin)  # t2192: "cuda:0 f32[1, 32, 512, 128]"
    # t2190 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2190: "cuda:0 f32[1, 32, 512, 128]"
    # t2191 = prims.convert_element_type(t2186, dtypes.float32)  # t2191: "cuda:0 f32[1, 32, 512, 128]"
    # t2192 = prims.mul(t2191, t2190)  # t2192: "cuda:0 f32[1, 32, 512, 128]"
  t2193 = ltorch.add(t2189, t2192, alpha=None)  # t2193: "cuda:0 f32[1, 32, 512, 128]"
    # t2193 = prims.add(t2189, t2192)  # t2193: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2194 = ltorch.to(t2193, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2194: "cuda:0 bf16[1, 32, 512, 128]"
    # t2194 = prims.convert_element_type(t2193, dtypes.bfloat16)  # t2194: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t2195 = ltorch.getitem(t2178, (..., slice(None, 128, None)))  # t2195: "cuda:0 bf16[1, 32, 512, 128]"
    # t2195 = prims.slice_prim(t2178, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2195: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2196 = ltorch.getitem(t2195, (..., slice(None, 64, None)))  # t2196: "cuda:0 bf16[1, 32, 512, 64]"
    # t2196 = prims.slice_prim(t2195, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2196: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2197 = ltorch.getitem(t2195, (..., slice(64, None, None)))  # t2197: "cuda:0 bf16[1, 32, 512, 64]"
    # t2197 = prims.slice_prim(t2195, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2197: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2200 = ltorch.neg(t2197)  # t2200: "cuda:0 bf16[1, 32, 512, 64]"
    # t2198 = prims.convert_element_type(t2197, dtypes.float32)  # t2198: "cuda:0 f32[1, 32, 512, 64]"
    # t2199 = prims.neg(t2198)  # t2199: "cuda:0 f32[1, 32, 512, 64]"
    # t2200 = prims.convert_element_type(t2199, dtypes.bfloat16)  # t2200: "cuda:0 bf16[1, 32, 512, 64]"
  t2201 = ltorch.cat((t2200, t2196), -1)  # t2201: "cuda:0 bf16[1, 32, 512, 128]"
    # t2201 = prims.cat((t2200, t2196), -1)  # t2201: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2204 = ltorch.mul(t2195, cos)  # t2204: "cuda:0 f32[1, 32, 512, 128]"
    # t2202 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2202: "cuda:0 f32[1, 32, 512, 128]"
    # t2203 = prims.convert_element_type(t2195, dtypes.float32)  # t2203: "cuda:0 f32[1, 32, 512, 128]"
    # t2204 = prims.mul(t2203, t2202)  # t2204: "cuda:0 f32[1, 32, 512, 128]"
  t2207 = ltorch.mul(t2201, sin)  # t2207: "cuda:0 f32[1, 32, 512, 128]"
    # t2205 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2205: "cuda:0 f32[1, 32, 512, 128]"
    # t2206 = prims.convert_element_type(t2201, dtypes.float32)  # t2206: "cuda:0 f32[1, 32, 512, 128]"
    # t2207 = prims.mul(t2206, t2205)  # t2207: "cuda:0 f32[1, 32, 512, 128]"
  t2208 = ltorch.add(t2204, t2207, alpha=None)  # t2208: "cuda:0 f32[1, 32, 512, 128]"
    # t2208 = prims.add(t2204, t2207)  # t2208: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2209 = ltorch.to(t2208, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2209: "cuda:0 bf16[1, 32, 512, 128]"
    # t2209 = prims.convert_element_type(t2208, dtypes.bfloat16)  # t2209: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t2210 = ltorch.getitem(t2177, (..., slice(128, None, None)))  # t2210: "cuda:0 bf16[1, 32, 512, 0]"
    # t2210 = prims.slice_prim(t2177, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2210: "cuda:0 bf16[1, 32, 512, 0]"
  t2211 = ltorch.cat((t2194, t2210), -1)  # t2211: "cuda:0 bf16[1, 32, 512, 128]"
    # t2211 = prims.cat((t2194, t2210), -1)  # t2211: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t2212 = ltorch.getitem(t2178, (..., slice(128, None, None)))  # t2212: "cuda:0 bf16[1, 32, 512, 0]"
    # t2212 = prims.slice_prim(t2178, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2212: "cuda:0 bf16[1, 32, 512, 0]"
  t2213 = ltorch.cat((t2209, t2212), -1)  # t2213: "cuda:0 bf16[1, 32, 512, 128]"
    # t2213 = prims.cat((t2209, t2212), -1)  # t2213: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t2243 = ltorch.scaled_dot_product_attention(t2211, t2213, t2179, None, 0.0, True, scale=0.08838834764831843)  # t2243: "cuda:0 bf16[1, 32, 512, 128]"
    # t2216 = ltorch.mul(t2211, 0.29730177875068026)  # t2216: "cuda:0 bf16[1, 32, 512, 128]"
      # t2214 = prims.convert_element_type(t2211, dtypes.float32)  # t2214: "cuda:0 f32[1, 32, 512, 128]"
      # t2215 = prims.mul(t2214, 0.29730177875068026)  # t2215: "cuda:0 f32[1, 32, 512, 128]"
      # t2216 = prims.convert_element_type(t2215, dtypes.bfloat16)  # t2216: "cuda:0 bf16[1, 32, 512, 128]"
    # t2217 = ltorch.transpose(t2213, -2, -1)  # t2217: "cuda:0 bf16[1, 32, 128, 512]"
      # t2217 = prims.transpose(t2213, (0, 1, 3, 2))  # t2217: "cuda:0 bf16[1, 32, 128, 512]"
    # t2220 = ltorch.mul(t2217, 0.29730177875068026)  # t2220: "cuda:0 bf16[1, 32, 128, 512]"
      # t2218 = prims.convert_element_type(t2217, dtypes.float32)  # t2218: "cuda:0 f32[1, 32, 128, 512]"
      # t2219 = prims.mul(t2218, 0.29730177875068026)  # t2219: "cuda:0 f32[1, 32, 128, 512]"
      # t2220 = prims.convert_element_type(t2219, dtypes.bfloat16)  # t2220: "cuda:0 bf16[1, 32, 128, 512]"
    # t2221 = ltorch.matmul(t2216, t2220)  # t2221: "cuda:0 bf16[1, 32, 512, 512]"
      # t2221 = prims.matmul(t2216, t2220)  # t2221: "cuda:0 bf16[1, 32, 512, 512]"
    # t2231 = ltorch.tril(t2221, 0, fill_value=-float('inf'))  # t2231: "cuda:0 bf16[1, 32, 512, 512]"
      # t2222 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2222: "cuda:0 i64[512]"
        # t2222 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2222: "cuda:0 i64[512]"
      # t2223 = ltorch.unsqueeze(t2222, -1)  # t2223: "cuda:0 i64[512, 1]"
        # t2223 = prims.broadcast_in_dim(t2222, [512, 1], [0])  # t2223: "cuda:0 i64[512, 1]"
      # t2224 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2224: "cuda:0 i64[512]"
        # t2224 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2224: "cuda:0 i64[512]"
      # t2225 = ltorch.unsqueeze(t2224, -2)  # t2225: "cuda:0 i64[1, 512]"
        # t2225 = prims.broadcast_in_dim(t2224, [1, 512], [1])  # t2225: "cuda:0 i64[1, 512]"
      # t2226 = ltorch.add(t2223, 0, alpha=None)  # t2226: "cuda:0 i64[512, 1]"
        # t2226 = prims.add(t2223, 0)  # t2226: "cuda:0 i64[512, 1]"
      # t2229 = ltorch.ge(t2226, t2225)  # t2229: "cuda:0 b8[512, 512]"
        # t2227 = prims.broadcast_in_dim(t2226, (512, 512), (0, 1))  # t2227: "cuda:0 i64[512, 512]"
        # t2228 = prims.broadcast_in_dim(t2225, (512, 512), (0, 1))  # t2228: "cuda:0 i64[512, 512]"
        # t2229 = prims.ge(t2227, t2228)  # t2229: "cuda:0 b8[512, 512]"
      # t2231 = ltorch.where(t2229, t2221, -float('inf'))  # t2231: "cuda:0 bf16[1, 32, 512, 512]"
        # t2230 = prims.broadcast_in_dim(t2229, (1, 32, 512, 512), (2, 3))  # t2230: "cuda:0 b8[1, 32, 512, 512]"
        # t2231 = prims.where(t2230, t2221, -float('inf'))  # t2231: "cuda:0 bf16[1, 32, 512, 512]"
    # t2242 = ltorch._softmax(t2231, -1, dtype=None)  # t2242: "cuda:0 bf16[1, 32, 512, 512]"
      # t2232 = ltorch.to(t2231, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t2232: "cuda:0 f32[1, 32, 512, 512]"
        # t2232 = prims.convert_element_type(t2231, dtypes.float32)  # t2232: "cuda:0 f32[1, 32, 512, 512]"
      # t2234 = ltorch.amax(t2232, -1, True)  # t2234: "cuda:0 f32[1, 32, 512, 1]"
        # t2233 = prims.amax(t2232, (3,))  # t2233: "cuda:0 f32[1, 32, 512]"
        # t2234 = prims.broadcast_in_dim(t2233, [1, 32, 512, 1], [0, 1, 2])  # t2234: "cuda:0 f32[1, 32, 512, 1]"
      # t2236 = ltorch.sub(t2232, t2234, alpha=None)  # t2236: "cuda:0 f32[1, 32, 512, 512]"
        # t2235 = prims.broadcast_in_dim(t2234, (1, 32, 512, 512), (0, 1, 2, 3))  # t2235: "cuda:0 f32[1, 32, 512, 512]"
        # t2236 = prims.sub(t2232, t2235)  # t2236: "cuda:0 f32[1, 32, 512, 512]"
      # t2237 = ltorch.exp(t2236)  # t2237: "cuda:0 f32[1, 32, 512, 512]"
        # t2237 = prims.exp(t2236)  # t2237: "cuda:0 f32[1, 32, 512, 512]"
      # t2239 = ltorch.sum(t2237, -1, True, dtype=None)  # t2239: "cuda:0 f32[1, 32, 512, 1]"
        # t2238 = prims.sum(t2237, (3,))  # t2238: "cuda:0 f32[1, 32, 512]"
        # t2239 = prims.broadcast_in_dim(t2238, [1, 32, 512, 1], [0, 1, 2])  # t2239: "cuda:0 f32[1, 32, 512, 1]"
      # t2241 = ltorch.true_divide(t2237, t2239)  # t2241: "cuda:0 f32[1, 32, 512, 512]"
        # t2240 = prims.broadcast_in_dim(t2239, (1, 32, 512, 512), (0, 1, 2, 3))  # t2240: "cuda:0 f32[1, 32, 512, 512]"
        # t2241 = prims.div(t2237, t2240)  # t2241: "cuda:0 f32[1, 32, 512, 512]"
      # t2242 = ltorch.to(t2241, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t2242: "cuda:0 bf16[1, 32, 512, 512]"
        # t2242 = prims.convert_element_type(t2241, dtypes.bfloat16)  # t2242: "cuda:0 bf16[1, 32, 512, 512]"
    # t2243 = ltorch.matmul(t2242, t2179)  # t2243: "cuda:0 bf16[1, 32, 512, 128]"
      # t2243 = prims.matmul(t2242, t2179)  # t2243: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t2244 = ltorch.transpose(t2243, 1, 2)  # t2244: "cuda:0 bf16[1, 512, 32, 128]"
    # t2244 = prims.transpose(t2243, (0, 2, 1, 3))  # t2244: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t2245 = ltorch.reshape(t2244, 1, 512, 4096)  # t2245: "cuda:0 bf16[1, 512, 4096]"
    # t2245 = prims.reshape(t2244, (1, 512, 4096))  # t2245: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2249 = ltorch.linear(t2245, t_transformer_h_13_attn_proj_weight, None)  # t2249: "cuda:0 bf16[1, 512, 4096]"
    # t2249 = prims.linear(t2245, t_transformer_h_13_attn_proj_weight, None)  # t2249: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t2253 = ltorch.add(t2249, t2143, alpha=None)  # t2253: "cuda:0 bf16[1, 512, 4096]"
    # t2250 = prims.convert_element_type(t2249, dtypes.float32)  # t2250: "cuda:0 f32[1, 512, 4096]"
    # t2251 = prims.convert_element_type(t2143, dtypes.float32)  # t2251: "cuda:0 f32[1, 512, 4096]"
    # t2252 = prims.add(t2250, t2251)  # t2252: "cuda:0 f32[1, 512, 4096]"
    # t2253 = prims.convert_element_type(t2252, dtypes.bfloat16)  # t2253: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2254 = prims.convert_element_type(t2253, dtypes.float32)  # t2254: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2255 = ltorch.mul(t2254, t2254)  # t2255: "cuda:0 f32[1, 512, 4096]"
    # t2255 = prims.mul(t2254, t2254)  # t2255: "cuda:0 f32[1, 512, 4096]"
  t2259 = ltorch.mean(t2255, -1, True, dtype=None)  # t2259: "cuda:0 f32[1, 512, 1]"
    # t2257 = prims.sum(t2255, (2,))  # t2257: "cuda:0 f32[1, 512]"
    # t2258 = prims.broadcast_in_dim(t2257, [1, 512, 1], [0, 1])  # t2258: "cuda:0 f32[1, 512, 1]"
    # t2259 = ltorch.true_divide(t2258, 4096)  # t2259: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2259 = prims.div(t2258, 4096.0)  # t2259: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2261 = ltorch.add(t2259, 1e-05, alpha=None)  # t2261: "cuda:0 f32[1, 512, 1]"
    # t2261 = prims.add(t2259, 1e-05)  # t2261: "cuda:0 f32[1, 512, 1]"
  t2262 = ltorch.rsqrt(t2261)  # t2262: "cuda:0 f32[1, 512, 1]"
    # t2262 = prims.rsqrt(t2261)  # t2262: "cuda:0 f32[1, 512, 1]"
  t2264 = ltorch.mul(t2254, t2262)  # t2264: "cuda:0 f32[1, 512, 4096]"
    # t2263 = prims.broadcast_in_dim(t2262, (1, 512, 4096), (0, 1, 2))  # t2263: "cuda:0 f32[1, 512, 4096]"
    # t2264 = prims.mul(t2254, t2263)  # t2264: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2265 = ltorch.to(t2264, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2265: "cuda:0 bf16[1, 512, 4096]"
    # t2265 = prims.convert_element_type(t2264, dtypes.bfloat16)  # t2265: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2275 = ltorch.mul(t2265, t_transformer_h_13_norm_2_weight)  # t2275: "cuda:0 bf16[1, 512, 4096]"
    # t2271 = prims.broadcast_in_dim(t_transformer_h_13_norm_2_weight, (1, 512, 4096), (2,))  # t2271: "cuda:0 bf16[1, 512, 4096]"
    # t2272 = prims.convert_element_type(t2265, dtypes.float32)  # t2272: "cuda:0 f32[1, 512, 4096]"
    # t2273 = prims.convert_element_type(t2271, dtypes.float32)  # t2273: "cuda:0 f32[1, 512, 4096]"
    # t2274 = prims.mul(t2272, t2273)  # t2274: "cuda:0 f32[1, 512, 4096]"
    # t2275 = prims.convert_element_type(t2274, dtypes.bfloat16)  # t2275: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2280 = ltorch.linear(t2275, t_transformer_h_13_mlp_fc_1_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
    # t2280 = prims.linear(t2275, t_transformer_h_13_mlp_fc_1_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
  t2284 = ltorch.linear(t2275, t_transformer_h_13_mlp_fc_2_weight, None)  # t2284: "cuda:0 bf16[1, 512, 11008]"
    # t2284 = prims.linear(t2275, t_transformer_h_13_mlp_fc_2_weight, None)  # t2284: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t2294 = ltorch.silu(t2280, False)  # t2294: "cuda:0 bf16[1, 512, 11008]"
    # t2285 = prims.convert_element_type(t2280, dtypes.float32)  # t2285: "cuda:0 f32[1, 512, 11008]"
    # t2286 = prims.neg(t2285)  # t2286: "cuda:0 f32[1, 512, 11008]"
    # t2287 = prims.exp(t2286)  # t2287: "cuda:0 f32[1, 512, 11008]"
    # t2288 = prims.add(1.0, t2287)  # t2288: "cuda:0 f32[1, 512, 11008]"
    # t2289 = prims.reciprocal(t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
    # t2290 = prims.convert_element_type(t2289, dtypes.bfloat16)  # t2290: "cuda:0 bf16[1, 512, 11008]"
    # t2291 = prims.convert_element_type(t2280, dtypes.float32)  # t2291: "cuda:0 f32[1, 512, 11008]"
    # t2292 = prims.convert_element_type(t2290, dtypes.float32)  # t2292: "cuda:0 f32[1, 512, 11008]"
    # t2293 = prims.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
    # t2294 = prims.convert_element_type(t2293, dtypes.bfloat16)  # t2294: "cuda:0 bf16[1, 512, 11008]"
  t2298 = ltorch.mul(t2294, t2284)  # t2298: "cuda:0 bf16[1, 512, 11008]"
    # t2295 = prims.convert_element_type(t2294, dtypes.float32)  # t2295: "cuda:0 f32[1, 512, 11008]"
    # t2296 = prims.convert_element_type(t2284, dtypes.float32)  # t2296: "cuda:0 f32[1, 512, 11008]"
    # t2297 = prims.mul(t2295, t2296)  # t2297: "cuda:0 f32[1, 512, 11008]"
    # t2298 = prims.convert_element_type(t2297, dtypes.bfloat16)  # t2298: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2302 = ltorch.linear(t2298, t_transformer_h_13_mlp_proj_weight, None)  # t2302: "cuda:0 bf16[1, 512, 4096]"
    # t2302 = prims.linear(t2298, t_transformer_h_13_mlp_proj_weight, None)  # t2302: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t2306 = ltorch.add(t2302, t2253, alpha=None)  # t2306: "cuda:0 bf16[1, 512, 4096]"
    # t2303 = prims.convert_element_type(t2302, dtypes.float32)  # t2303: "cuda:0 f32[1, 512, 4096]"
    # t2304 = prims.convert_element_type(t2253, dtypes.float32)  # t2304: "cuda:0 f32[1, 512, 4096]"
    # t2305 = prims.add(t2303, t2304)  # t2305: "cuda:0 f32[1, 512, 4096]"
    # t2306 = prims.convert_element_type(t2305, dtypes.bfloat16)  # t2306: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2308 = prims.convert_element_type(t2306, dtypes.float32)  # t2308: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2309 = ltorch.mul(t2308, t2308)  # t2309: "cuda:0 f32[1, 512, 4096]"
    # t2309 = prims.mul(t2308, t2308)  # t2309: "cuda:0 f32[1, 512, 4096]"
  t2313 = ltorch.mean(t2309, -1, True, dtype=None)  # t2313: "cuda:0 f32[1, 512, 1]"
    # t2311 = prims.sum(t2309, (2,))  # t2311: "cuda:0 f32[1, 512]"
    # t2312 = prims.broadcast_in_dim(t2311, [1, 512, 1], [0, 1])  # t2312: "cuda:0 f32[1, 512, 1]"
    # t2313 = ltorch.true_divide(t2312, 4096)  # t2313: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2313 = prims.div(t2312, 4096.0)  # t2313: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2315 = ltorch.add(t2313, 1e-05, alpha=None)  # t2315: "cuda:0 f32[1, 512, 1]"
    # t2315 = prims.add(t2313, 1e-05)  # t2315: "cuda:0 f32[1, 512, 1]"
  t2316 = ltorch.rsqrt(t2315)  # t2316: "cuda:0 f32[1, 512, 1]"
    # t2316 = prims.rsqrt(t2315)  # t2316: "cuda:0 f32[1, 512, 1]"
  t2318 = ltorch.mul(t2308, t2316)  # t2318: "cuda:0 f32[1, 512, 4096]"
    # t2317 = prims.broadcast_in_dim(t2316, (1, 512, 4096), (0, 1, 2))  # t2317: "cuda:0 f32[1, 512, 4096]"
    # t2318 = prims.mul(t2308, t2317)  # t2318: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2319 = ltorch.to(t2318, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2319: "cuda:0 bf16[1, 512, 4096]"
    # t2319 = prims.convert_element_type(t2318, dtypes.bfloat16)  # t2319: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2329 = ltorch.mul(t2319, t_transformer_h_14_norm_1_weight)  # t2329: "cuda:0 bf16[1, 512, 4096]"
    # t2325 = prims.broadcast_in_dim(t_transformer_h_14_norm_1_weight, (1, 512, 4096), (2,))  # t2325: "cuda:0 bf16[1, 512, 4096]"
    # t2326 = prims.convert_element_type(t2319, dtypes.float32)  # t2326: "cuda:0 f32[1, 512, 4096]"
    # t2327 = prims.convert_element_type(t2325, dtypes.float32)  # t2327: "cuda:0 f32[1, 512, 4096]"
    # t2328 = prims.mul(t2326, t2327)  # t2328: "cuda:0 f32[1, 512, 4096]"
    # t2329 = prims.convert_element_type(t2328, dtypes.bfloat16)  # t2329: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2334 = ltorch.linear(t2329, t_transformer_h_14_attn_attn_weight, None)  # t2334: "cuda:0 bf16[1, 512, 12288]"
    # t2334 = prims.linear(t2329, t_transformer_h_14_attn_attn_weight, None)  # t2334: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t2335 = ltorch.view(t2334, 1, 512, 32, 3, 128)  # t2335: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t2335 = ltorch.reshape(t2334, (1, 512, 32, 3, 128))  # t2335: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t2335 = prims.reshape(t2334, (1, 512, 32, 3, 128))  # t2335: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t2336 = ltorch.permute(t2335, 0, 2, 3, 1, 4)  # t2336: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t2336 = prims.transpose(t2335, (0, 2, 3, 1, 4))  # t2336: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t2337, t2338, t2339) = ltorch.split(t2336, (1, 1, 1), 2)
    # t2337 = prims.slice_prim(t2336, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2337: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2338 = prims.slice_prim(t2336, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2338: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2339 = prims.slice_prim(t2336, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2339: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t2340 = ltorch.reshape(t2337, 1, -1, 512, 128)  # t2340: "cuda:0 bf16[1, 32, 512, 128]"
    # t2340 = prims.reshape(t2337, (1, 32, 512, 128))  # t2340: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t2341 = ltorch.reshape(t2338, 1, -1, 512, 128)  # t2341: "cuda:0 bf16[1, 32, 512, 128]"
    # t2341 = prims.reshape(t2338, (1, 32, 512, 128))  # t2341: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t2342 = ltorch.reshape(t2339, 1, -1, 512, 128)  # t2342: "cuda:0 bf16[1, 32, 512, 128]"
    # t2342 = prims.reshape(t2339, (1, 32, 512, 128))  # t2342: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t2343 = ltorch.getitem(t2340, (..., slice(None, 128, None)))  # t2343: "cuda:0 bf16[1, 32, 512, 128]"
    # t2343 = prims.slice_prim(t2340, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2343: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2344 = ltorch.getitem(t2343, (..., slice(None, 64, None)))  # t2344: "cuda:0 bf16[1, 32, 512, 64]"
    # t2344 = prims.slice_prim(t2343, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2344: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2345 = ltorch.getitem(t2343, (..., slice(64, None, None)))  # t2345: "cuda:0 bf16[1, 32, 512, 64]"
    # t2345 = prims.slice_prim(t2343, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2345: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2348 = ltorch.neg(t2345)  # t2348: "cuda:0 bf16[1, 32, 512, 64]"
    # t2346 = prims.convert_element_type(t2345, dtypes.float32)  # t2346: "cuda:0 f32[1, 32, 512, 64]"
    # t2347 = prims.neg(t2346)  # t2347: "cuda:0 f32[1, 32, 512, 64]"
    # t2348 = prims.convert_element_type(t2347, dtypes.bfloat16)  # t2348: "cuda:0 bf16[1, 32, 512, 64]"
  t2349 = ltorch.cat((t2348, t2344), -1)  # t2349: "cuda:0 bf16[1, 32, 512, 128]"
    # t2349 = prims.cat((t2348, t2344), -1)  # t2349: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2352 = ltorch.mul(t2343, cos)  # t2352: "cuda:0 f32[1, 32, 512, 128]"
    # t2350 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2350: "cuda:0 f32[1, 32, 512, 128]"
    # t2351 = prims.convert_element_type(t2343, dtypes.float32)  # t2351: "cuda:0 f32[1, 32, 512, 128]"
    # t2352 = prims.mul(t2351, t2350)  # t2352: "cuda:0 f32[1, 32, 512, 128]"
  t2355 = ltorch.mul(t2349, sin)  # t2355: "cuda:0 f32[1, 32, 512, 128]"
    # t2353 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2353: "cuda:0 f32[1, 32, 512, 128]"
    # t2354 = prims.convert_element_type(t2349, dtypes.float32)  # t2354: "cuda:0 f32[1, 32, 512, 128]"
    # t2355 = prims.mul(t2354, t2353)  # t2355: "cuda:0 f32[1, 32, 512, 128]"
  t2356 = ltorch.add(t2352, t2355, alpha=None)  # t2356: "cuda:0 f32[1, 32, 512, 128]"
    # t2356 = prims.add(t2352, t2355)  # t2356: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2357 = ltorch.to(t2356, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2357: "cuda:0 bf16[1, 32, 512, 128]"
    # t2357 = prims.convert_element_type(t2356, dtypes.bfloat16)  # t2357: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t2358 = ltorch.getitem(t2341, (..., slice(None, 128, None)))  # t2358: "cuda:0 bf16[1, 32, 512, 128]"
    # t2358 = prims.slice_prim(t2341, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2358: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2359 = ltorch.getitem(t2358, (..., slice(None, 64, None)))  # t2359: "cuda:0 bf16[1, 32, 512, 64]"
    # t2359 = prims.slice_prim(t2358, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2359: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2360 = ltorch.getitem(t2358, (..., slice(64, None, None)))  # t2360: "cuda:0 bf16[1, 32, 512, 64]"
    # t2360 = prims.slice_prim(t2358, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2360: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2363 = ltorch.neg(t2360)  # t2363: "cuda:0 bf16[1, 32, 512, 64]"
    # t2361 = prims.convert_element_type(t2360, dtypes.float32)  # t2361: "cuda:0 f32[1, 32, 512, 64]"
    # t2362 = prims.neg(t2361)  # t2362: "cuda:0 f32[1, 32, 512, 64]"
    # t2363 = prims.convert_element_type(t2362, dtypes.bfloat16)  # t2363: "cuda:0 bf16[1, 32, 512, 64]"
  t2364 = ltorch.cat((t2363, t2359), -1)  # t2364: "cuda:0 bf16[1, 32, 512, 128]"
    # t2364 = prims.cat((t2363, t2359), -1)  # t2364: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2367 = ltorch.mul(t2358, cos)  # t2367: "cuda:0 f32[1, 32, 512, 128]"
    # t2365 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2365: "cuda:0 f32[1, 32, 512, 128]"
    # t2366 = prims.convert_element_type(t2358, dtypes.float32)  # t2366: "cuda:0 f32[1, 32, 512, 128]"
    # t2367 = prims.mul(t2366, t2365)  # t2367: "cuda:0 f32[1, 32, 512, 128]"
  t2370 = ltorch.mul(t2364, sin)  # t2370: "cuda:0 f32[1, 32, 512, 128]"
    # t2368 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2368: "cuda:0 f32[1, 32, 512, 128]"
    # t2369 = prims.convert_element_type(t2364, dtypes.float32)  # t2369: "cuda:0 f32[1, 32, 512, 128]"
    # t2370 = prims.mul(t2369, t2368)  # t2370: "cuda:0 f32[1, 32, 512, 128]"
  t2371 = ltorch.add(t2367, t2370, alpha=None)  # t2371: "cuda:0 f32[1, 32, 512, 128]"
    # t2371 = prims.add(t2367, t2370)  # t2371: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2372 = ltorch.to(t2371, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2372: "cuda:0 bf16[1, 32, 512, 128]"
    # t2372 = prims.convert_element_type(t2371, dtypes.bfloat16)  # t2372: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t2373 = ltorch.getitem(t2340, (..., slice(128, None, None)))  # t2373: "cuda:0 bf16[1, 32, 512, 0]"
    # t2373 = prims.slice_prim(t2340, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2373: "cuda:0 bf16[1, 32, 512, 0]"
  t2374 = ltorch.cat((t2357, t2373), -1)  # t2374: "cuda:0 bf16[1, 32, 512, 128]"
    # t2374 = prims.cat((t2357, t2373), -1)  # t2374: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t2375 = ltorch.getitem(t2341, (..., slice(128, None, None)))  # t2375: "cuda:0 bf16[1, 32, 512, 0]"
    # t2375 = prims.slice_prim(t2341, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2375: "cuda:0 bf16[1, 32, 512, 0]"
  t2376 = ltorch.cat((t2372, t2375), -1)  # t2376: "cuda:0 bf16[1, 32, 512, 128]"
    # t2376 = prims.cat((t2372, t2375), -1)  # t2376: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t2406 = ltorch.scaled_dot_product_attention(t2374, t2376, t2342, None, 0.0, True, scale=0.08838834764831843)  # t2406: "cuda:0 bf16[1, 32, 512, 128]"
    # t2379 = ltorch.mul(t2374, 0.29730177875068026)  # t2379: "cuda:0 bf16[1, 32, 512, 128]"
      # t2377 = prims.convert_element_type(t2374, dtypes.float32)  # t2377: "cuda:0 f32[1, 32, 512, 128]"
      # t2378 = prims.mul(t2377, 0.29730177875068026)  # t2378: "cuda:0 f32[1, 32, 512, 128]"
      # t2379 = prims.convert_element_type(t2378, dtypes.bfloat16)  # t2379: "cuda:0 bf16[1, 32, 512, 128]"
    # t2380 = ltorch.transpose(t2376, -2, -1)  # t2380: "cuda:0 bf16[1, 32, 128, 512]"
      # t2380 = prims.transpose(t2376, (0, 1, 3, 2))  # t2380: "cuda:0 bf16[1, 32, 128, 512]"
    # t2383 = ltorch.mul(t2380, 0.29730177875068026)  # t2383: "cuda:0 bf16[1, 32, 128, 512]"
      # t2381 = prims.convert_element_type(t2380, dtypes.float32)  # t2381: "cuda:0 f32[1, 32, 128, 512]"
      # t2382 = prims.mul(t2381, 0.29730177875068026)  # t2382: "cuda:0 f32[1, 32, 128, 512]"
      # t2383 = prims.convert_element_type(t2382, dtypes.bfloat16)  # t2383: "cuda:0 bf16[1, 32, 128, 512]"
    # t2384 = ltorch.matmul(t2379, t2383)  # t2384: "cuda:0 bf16[1, 32, 512, 512]"
      # t2384 = prims.matmul(t2379, t2383)  # t2384: "cuda:0 bf16[1, 32, 512, 512]"
    # t2394 = ltorch.tril(t2384, 0, fill_value=-float('inf'))  # t2394: "cuda:0 bf16[1, 32, 512, 512]"
      # t2385 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2385: "cuda:0 i64[512]"
        # t2385 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2385: "cuda:0 i64[512]"
      # t2386 = ltorch.unsqueeze(t2385, -1)  # t2386: "cuda:0 i64[512, 1]"
        # t2386 = prims.broadcast_in_dim(t2385, [512, 1], [0])  # t2386: "cuda:0 i64[512, 1]"
      # t2387 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2387: "cuda:0 i64[512]"
        # t2387 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2387: "cuda:0 i64[512]"
      # t2388 = ltorch.unsqueeze(t2387, -2)  # t2388: "cuda:0 i64[1, 512]"
        # t2388 = prims.broadcast_in_dim(t2387, [1, 512], [1])  # t2388: "cuda:0 i64[1, 512]"
      # t2389 = ltorch.add(t2386, 0, alpha=None)  # t2389: "cuda:0 i64[512, 1]"
        # t2389 = prims.add(t2386, 0)  # t2389: "cuda:0 i64[512, 1]"
      # t2392 = ltorch.ge(t2389, t2388)  # t2392: "cuda:0 b8[512, 512]"
        # t2390 = prims.broadcast_in_dim(t2389, (512, 512), (0, 1))  # t2390: "cuda:0 i64[512, 512]"
        # t2391 = prims.broadcast_in_dim(t2388, (512, 512), (0, 1))  # t2391: "cuda:0 i64[512, 512]"
        # t2392 = prims.ge(t2390, t2391)  # t2392: "cuda:0 b8[512, 512]"
      # t2394 = ltorch.where(t2392, t2384, -float('inf'))  # t2394: "cuda:0 bf16[1, 32, 512, 512]"
        # t2393 = prims.broadcast_in_dim(t2392, (1, 32, 512, 512), (2, 3))  # t2393: "cuda:0 b8[1, 32, 512, 512]"
        # t2394 = prims.where(t2393, t2384, -float('inf'))  # t2394: "cuda:0 bf16[1, 32, 512, 512]"
    # t2405 = ltorch._softmax(t2394, -1, dtype=None)  # t2405: "cuda:0 bf16[1, 32, 512, 512]"
      # t2395 = ltorch.to(t2394, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t2395: "cuda:0 f32[1, 32, 512, 512]"
        # t2395 = prims.convert_element_type(t2394, dtypes.float32)  # t2395: "cuda:0 f32[1, 32, 512, 512]"
      # t2397 = ltorch.amax(t2395, -1, True)  # t2397: "cuda:0 f32[1, 32, 512, 1]"
        # t2396 = prims.amax(t2395, (3,))  # t2396: "cuda:0 f32[1, 32, 512]"
        # t2397 = prims.broadcast_in_dim(t2396, [1, 32, 512, 1], [0, 1, 2])  # t2397: "cuda:0 f32[1, 32, 512, 1]"
      # t2399 = ltorch.sub(t2395, t2397, alpha=None)  # t2399: "cuda:0 f32[1, 32, 512, 512]"
        # t2398 = prims.broadcast_in_dim(t2397, (1, 32, 512, 512), (0, 1, 2, 3))  # t2398: "cuda:0 f32[1, 32, 512, 512]"
        # t2399 = prims.sub(t2395, t2398)  # t2399: "cuda:0 f32[1, 32, 512, 512]"
      # t2400 = ltorch.exp(t2399)  # t2400: "cuda:0 f32[1, 32, 512, 512]"
        # t2400 = prims.exp(t2399)  # t2400: "cuda:0 f32[1, 32, 512, 512]"
      # t2402 = ltorch.sum(t2400, -1, True, dtype=None)  # t2402: "cuda:0 f32[1, 32, 512, 1]"
        # t2401 = prims.sum(t2400, (3,))  # t2401: "cuda:0 f32[1, 32, 512]"
        # t2402 = prims.broadcast_in_dim(t2401, [1, 32, 512, 1], [0, 1, 2])  # t2402: "cuda:0 f32[1, 32, 512, 1]"
      # t2404 = ltorch.true_divide(t2400, t2402)  # t2404: "cuda:0 f32[1, 32, 512, 512]"
        # t2403 = prims.broadcast_in_dim(t2402, (1, 32, 512, 512), (0, 1, 2, 3))  # t2403: "cuda:0 f32[1, 32, 512, 512]"
        # t2404 = prims.div(t2400, t2403)  # t2404: "cuda:0 f32[1, 32, 512, 512]"
      # t2405 = ltorch.to(t2404, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t2405: "cuda:0 bf16[1, 32, 512, 512]"
        # t2405 = prims.convert_element_type(t2404, dtypes.bfloat16)  # t2405: "cuda:0 bf16[1, 32, 512, 512]"
    # t2406 = ltorch.matmul(t2405, t2342)  # t2406: "cuda:0 bf16[1, 32, 512, 128]"
      # t2406 = prims.matmul(t2405, t2342)  # t2406: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t2407 = ltorch.transpose(t2406, 1, 2)  # t2407: "cuda:0 bf16[1, 512, 32, 128]"
    # t2407 = prims.transpose(t2406, (0, 2, 1, 3))  # t2407: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t2408 = ltorch.reshape(t2407, 1, 512, 4096)  # t2408: "cuda:0 bf16[1, 512, 4096]"
    # t2408 = prims.reshape(t2407, (1, 512, 4096))  # t2408: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2412 = ltorch.linear(t2408, t_transformer_h_14_attn_proj_weight, None)  # t2412: "cuda:0 bf16[1, 512, 4096]"
    # t2412 = prims.linear(t2408, t_transformer_h_14_attn_proj_weight, None)  # t2412: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t2416 = ltorch.add(t2412, t2306, alpha=None)  # t2416: "cuda:0 bf16[1, 512, 4096]"
    # t2413 = prims.convert_element_type(t2412, dtypes.float32)  # t2413: "cuda:0 f32[1, 512, 4096]"
    # t2414 = prims.convert_element_type(t2306, dtypes.float32)  # t2414: "cuda:0 f32[1, 512, 4096]"
    # t2415 = prims.add(t2413, t2414)  # t2415: "cuda:0 f32[1, 512, 4096]"
    # t2416 = prims.convert_element_type(t2415, dtypes.bfloat16)  # t2416: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2417 = prims.convert_element_type(t2416, dtypes.float32)  # t2417: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2418 = ltorch.mul(t2417, t2417)  # t2418: "cuda:0 f32[1, 512, 4096]"
    # t2418 = prims.mul(t2417, t2417)  # t2418: "cuda:0 f32[1, 512, 4096]"
  t2422 = ltorch.mean(t2418, -1, True, dtype=None)  # t2422: "cuda:0 f32[1, 512, 1]"
    # t2420 = prims.sum(t2418, (2,))  # t2420: "cuda:0 f32[1, 512]"
    # t2421 = prims.broadcast_in_dim(t2420, [1, 512, 1], [0, 1])  # t2421: "cuda:0 f32[1, 512, 1]"
    # t2422 = ltorch.true_divide(t2421, 4096)  # t2422: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2422 = prims.div(t2421, 4096.0)  # t2422: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2424 = ltorch.add(t2422, 1e-05, alpha=None)  # t2424: "cuda:0 f32[1, 512, 1]"
    # t2424 = prims.add(t2422, 1e-05)  # t2424: "cuda:0 f32[1, 512, 1]"
  t2425 = ltorch.rsqrt(t2424)  # t2425: "cuda:0 f32[1, 512, 1]"
    # t2425 = prims.rsqrt(t2424)  # t2425: "cuda:0 f32[1, 512, 1]"
  t2427 = ltorch.mul(t2417, t2425)  # t2427: "cuda:0 f32[1, 512, 4096]"
    # t2426 = prims.broadcast_in_dim(t2425, (1, 512, 4096), (0, 1, 2))  # t2426: "cuda:0 f32[1, 512, 4096]"
    # t2427 = prims.mul(t2417, t2426)  # t2427: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2428 = ltorch.to(t2427, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2428: "cuda:0 bf16[1, 512, 4096]"
    # t2428 = prims.convert_element_type(t2427, dtypes.bfloat16)  # t2428: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2438 = ltorch.mul(t2428, t_transformer_h_14_norm_2_weight)  # t2438: "cuda:0 bf16[1, 512, 4096]"
    # t2434 = prims.broadcast_in_dim(t_transformer_h_14_norm_2_weight, (1, 512, 4096), (2,))  # t2434: "cuda:0 bf16[1, 512, 4096]"
    # t2435 = prims.convert_element_type(t2428, dtypes.float32)  # t2435: "cuda:0 f32[1, 512, 4096]"
    # t2436 = prims.convert_element_type(t2434, dtypes.float32)  # t2436: "cuda:0 f32[1, 512, 4096]"
    # t2437 = prims.mul(t2435, t2436)  # t2437: "cuda:0 f32[1, 512, 4096]"
    # t2438 = prims.convert_element_type(t2437, dtypes.bfloat16)  # t2438: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2443 = ltorch.linear(t2438, t_transformer_h_14_mlp_fc_1_weight, None)  # t2443: "cuda:0 bf16[1, 512, 11008]"
    # t2443 = prims.linear(t2438, t_transformer_h_14_mlp_fc_1_weight, None)  # t2443: "cuda:0 bf16[1, 512, 11008]"
  t2447 = ltorch.linear(t2438, t_transformer_h_14_mlp_fc_2_weight, None)  # t2447: "cuda:0 bf16[1, 512, 11008]"
    # t2447 = prims.linear(t2438, t_transformer_h_14_mlp_fc_2_weight, None)  # t2447: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t2457 = ltorch.silu(t2443, False)  # t2457: "cuda:0 bf16[1, 512, 11008]"
    # t2448 = prims.convert_element_type(t2443, dtypes.float32)  # t2448: "cuda:0 f32[1, 512, 11008]"
    # t2449 = prims.neg(t2448)  # t2449: "cuda:0 f32[1, 512, 11008]"
    # t2450 = prims.exp(t2449)  # t2450: "cuda:0 f32[1, 512, 11008]"
    # t2451 = prims.add(1.0, t2450)  # t2451: "cuda:0 f32[1, 512, 11008]"
    # t2452 = prims.reciprocal(t2451)  # t2452: "cuda:0 f32[1, 512, 11008]"
    # t2453 = prims.convert_element_type(t2452, dtypes.bfloat16)  # t2453: "cuda:0 bf16[1, 512, 11008]"
    # t2454 = prims.convert_element_type(t2443, dtypes.float32)  # t2454: "cuda:0 f32[1, 512, 11008]"
    # t2455 = prims.convert_element_type(t2453, dtypes.float32)  # t2455: "cuda:0 f32[1, 512, 11008]"
    # t2456 = prims.mul(t2454, t2455)  # t2456: "cuda:0 f32[1, 512, 11008]"
    # t2457 = prims.convert_element_type(t2456, dtypes.bfloat16)  # t2457: "cuda:0 bf16[1, 512, 11008]"
  t2461 = ltorch.mul(t2457, t2447)  # t2461: "cuda:0 bf16[1, 512, 11008]"
    # t2458 = prims.convert_element_type(t2457, dtypes.float32)  # t2458: "cuda:0 f32[1, 512, 11008]"
    # t2459 = prims.convert_element_type(t2447, dtypes.float32)  # t2459: "cuda:0 f32[1, 512, 11008]"
    # t2460 = prims.mul(t2458, t2459)  # t2460: "cuda:0 f32[1, 512, 11008]"
    # t2461 = prims.convert_element_type(t2460, dtypes.bfloat16)  # t2461: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2465 = ltorch.linear(t2461, t_transformer_h_14_mlp_proj_weight, None)  # t2465: "cuda:0 bf16[1, 512, 4096]"
    # t2465 = prims.linear(t2461, t_transformer_h_14_mlp_proj_weight, None)  # t2465: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t2469 = ltorch.add(t2465, t2416, alpha=None)  # t2469: "cuda:0 bf16[1, 512, 4096]"
    # t2466 = prims.convert_element_type(t2465, dtypes.float32)  # t2466: "cuda:0 f32[1, 512, 4096]"
    # t2467 = prims.convert_element_type(t2416, dtypes.float32)  # t2467: "cuda:0 f32[1, 512, 4096]"
    # t2468 = prims.add(t2466, t2467)  # t2468: "cuda:0 f32[1, 512, 4096]"
    # t2469 = prims.convert_element_type(t2468, dtypes.bfloat16)  # t2469: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2471 = prims.convert_element_type(t2469, dtypes.float32)  # t2471: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2472 = ltorch.mul(t2471, t2471)  # t2472: "cuda:0 f32[1, 512, 4096]"
    # t2472 = prims.mul(t2471, t2471)  # t2472: "cuda:0 f32[1, 512, 4096]"
  t2476 = ltorch.mean(t2472, -1, True, dtype=None)  # t2476: "cuda:0 f32[1, 512, 1]"
    # t2474 = prims.sum(t2472, (2,))  # t2474: "cuda:0 f32[1, 512]"
    # t2475 = prims.broadcast_in_dim(t2474, [1, 512, 1], [0, 1])  # t2475: "cuda:0 f32[1, 512, 1]"
    # t2476 = ltorch.true_divide(t2475, 4096)  # t2476: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2476 = prims.div(t2475, 4096.0)  # t2476: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2478 = ltorch.add(t2476, 1e-05, alpha=None)  # t2478: "cuda:0 f32[1, 512, 1]"
    # t2478 = prims.add(t2476, 1e-05)  # t2478: "cuda:0 f32[1, 512, 1]"
  t2479 = ltorch.rsqrt(t2478)  # t2479: "cuda:0 f32[1, 512, 1]"
    # t2479 = prims.rsqrt(t2478)  # t2479: "cuda:0 f32[1, 512, 1]"
  t2481 = ltorch.mul(t2471, t2479)  # t2481: "cuda:0 f32[1, 512, 4096]"
    # t2480 = prims.broadcast_in_dim(t2479, (1, 512, 4096), (0, 1, 2))  # t2480: "cuda:0 f32[1, 512, 4096]"
    # t2481 = prims.mul(t2471, t2480)  # t2481: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2482 = ltorch.to(t2481, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2482: "cuda:0 bf16[1, 512, 4096]"
    # t2482 = prims.convert_element_type(t2481, dtypes.bfloat16)  # t2482: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2492 = ltorch.mul(t2482, t_transformer_h_15_norm_1_weight)  # t2492: "cuda:0 bf16[1, 512, 4096]"
    # t2488 = prims.broadcast_in_dim(t_transformer_h_15_norm_1_weight, (1, 512, 4096), (2,))  # t2488: "cuda:0 bf16[1, 512, 4096]"
    # t2489 = prims.convert_element_type(t2482, dtypes.float32)  # t2489: "cuda:0 f32[1, 512, 4096]"
    # t2490 = prims.convert_element_type(t2488, dtypes.float32)  # t2490: "cuda:0 f32[1, 512, 4096]"
    # t2491 = prims.mul(t2489, t2490)  # t2491: "cuda:0 f32[1, 512, 4096]"
    # t2492 = prims.convert_element_type(t2491, dtypes.bfloat16)  # t2492: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2497 = ltorch.linear(t2492, t_transformer_h_15_attn_attn_weight, None)  # t2497: "cuda:0 bf16[1, 512, 12288]"
    # t2497 = prims.linear(t2492, t_transformer_h_15_attn_attn_weight, None)  # t2497: "cuda:0 bf16[1, 512, 12288]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:220: 	        qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
  t2498 = ltorch.view(t2497, 1, 512, 32, 3, 128)  # t2498: "cuda:0 bf16[1, 512, 32, 3, 128]"
    # t2498 = ltorch.reshape(t2497, (1, 512, 32, 3, 128))  # t2498: "cuda:0 bf16[1, 512, 32, 3, 128]"
      # t2498 = prims.reshape(t2497, (1, 512, 32, 3, 128))  # t2498: "cuda:0 bf16[1, 512, 32, 3, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:221: 	        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
  t2499 = ltorch.permute(t2498, 0, 2, 3, 1, 4)  # t2499: "cuda:0 bf16[1, 32, 3, 512, 128]"
    # t2499 = prims.transpose(t2498, (0, 2, 3, 1, 4))  # t2499: "cuda:0 bf16[1, 32, 3, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:224: 	        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
  (t2500, t2501, t2502) = ltorch.split(t2499, (1, 1, 1), 2)
    # t2500 = prims.slice_prim(t2499, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2500: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2501 = prims.slice_prim(t2499, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2501: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2502 = prims.slice_prim(t2499, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2502: "cuda:0 bf16[1, 32, 1, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:233: 	        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
  t2503 = ltorch.reshape(t2500, 1, -1, 512, 128)  # t2503: "cuda:0 bf16[1, 32, 512, 128]"
    # t2503 = prims.reshape(t2500, (1, 32, 512, 128))  # t2503: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:234: 	        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
  t2504 = ltorch.reshape(t2501, 1, -1, 512, 128)  # t2504: "cuda:0 bf16[1, 32, 512, 128]"
    # t2504 = prims.reshape(t2501, (1, 32, 512, 128))  # t2504: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:235: 	        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
  t2505 = ltorch.reshape(t2502, 1, -1, 512, 128)  # t2505: "cuda:0 bf16[1, 32, 512, 128]"
    # t2505 = prims.reshape(t2502, (1, 32, 512, 128))  # t2505: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:237: 	        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
  t2506 = ltorch.getitem(t2503, (..., slice(None, 128, None)))  # t2506: "cuda:0 bf16[1, 32, 512, 128]"
    # t2506 = prims.slice_prim(t2503, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2506: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2507 = ltorch.getitem(t2506, (..., slice(None, 64, None)))  # t2507: "cuda:0 bf16[1, 32, 512, 64]"
    # t2507 = prims.slice_prim(t2506, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2507: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2508 = ltorch.getitem(t2506, (..., slice(64, None, None)))  # t2508: "cuda:0 bf16[1, 32, 512, 64]"
    # t2508 = prims.slice_prim(t2506, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2508: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2511 = ltorch.neg(t2508)  # t2511: "cuda:0 bf16[1, 32, 512, 64]"
    # t2509 = prims.convert_element_type(t2508, dtypes.float32)  # t2509: "cuda:0 f32[1, 32, 512, 64]"
    # t2510 = prims.neg(t2509)  # t2510: "cuda:0 f32[1, 32, 512, 64]"
    # t2511 = prims.convert_element_type(t2510, dtypes.bfloat16)  # t2511: "cuda:0 bf16[1, 32, 512, 64]"
  t2512 = ltorch.cat((t2511, t2507), -1)  # t2512: "cuda:0 bf16[1, 32, 512, 128]"
    # t2512 = prims.cat((t2511, t2507), -1)  # t2512: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2515 = ltorch.mul(t2506, cos)  # t2515: "cuda:0 f32[1, 32, 512, 128]"
    # t2513 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2513: "cuda:0 f32[1, 32, 512, 128]"
    # t2514 = prims.convert_element_type(t2506, dtypes.float32)  # t2514: "cuda:0 f32[1, 32, 512, 128]"
    # t2515 = prims.mul(t2514, t2513)  # t2515: "cuda:0 f32[1, 32, 512, 128]"
  t2518 = ltorch.mul(t2512, sin)  # t2518: "cuda:0 f32[1, 32, 512, 128]"
    # t2516 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2516: "cuda:0 f32[1, 32, 512, 128]"
    # t2517 = prims.convert_element_type(t2512, dtypes.float32)  # t2517: "cuda:0 f32[1, 32, 512, 128]"
    # t2518 = prims.mul(t2517, t2516)  # t2518: "cuda:0 f32[1, 32, 512, 128]"
  t2519 = ltorch.add(t2515, t2518, alpha=None)  # t2519: "cuda:0 f32[1, 32, 512, 128]"
    # t2519 = prims.add(t2515, t2518)  # t2519: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2520 = ltorch.to(t2519, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2520: "cuda:0 bf16[1, 32, 512, 128]"
    # t2520 = prims.convert_element_type(t2519, dtypes.bfloat16)  # t2520: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:238: 	        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
  t2521 = ltorch.getitem(t2504, (..., slice(None, 128, None)))  # t2521: "cuda:0 bf16[1, 32, 512, 128]"
    # t2521 = prims.slice_prim(t2504, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2521: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:375: 	    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
  t2522 = ltorch.getitem(t2521, (..., slice(None, 64, None)))  # t2522: "cuda:0 bf16[1, 32, 512, 64]"
    # t2522 = prims.slice_prim(t2521, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2522: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:376: 	    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
  t2523 = ltorch.getitem(t2521, (..., slice(64, None, None)))  # t2523: "cuda:0 bf16[1, 32, 512, 64]"
    # t2523 = prims.slice_prim(t2521, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2523: "cuda:0 bf16[1, 32, 512, 64]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:377: 	    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
  t2526 = ltorch.neg(t2523)  # t2526: "cuda:0 bf16[1, 32, 512, 64]"
    # t2524 = prims.convert_element_type(t2523, dtypes.float32)  # t2524: "cuda:0 f32[1, 32, 512, 64]"
    # t2525 = prims.neg(t2524)  # t2525: "cuda:0 f32[1, 32, 512, 64]"
    # t2526 = prims.convert_element_type(t2525, dtypes.bfloat16)  # t2526: "cuda:0 bf16[1, 32, 512, 64]"
  t2527 = ltorch.cat((t2526, t2522), -1)  # t2527: "cuda:0 bf16[1, 32, 512, 128]"
    # t2527 = prims.cat((t2526, t2522), -1)  # t2527: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:378: 	    roped = (x * cos) + (rotated * sin)
  t2530 = ltorch.mul(t2521, cos)  # t2530: "cuda:0 f32[1, 32, 512, 128]"
    # t2528 = prims.broadcast_in_dim(cos, (1, 32, 512, 128), (2, 3))  # t2528: "cuda:0 f32[1, 32, 512, 128]"
    # t2529 = prims.convert_element_type(t2521, dtypes.float32)  # t2529: "cuda:0 f32[1, 32, 512, 128]"
    # t2530 = prims.mul(t2529, t2528)  # t2530: "cuda:0 f32[1, 32, 512, 128]"
  t2533 = ltorch.mul(t2527, sin)  # t2533: "cuda:0 f32[1, 32, 512, 128]"
    # t2531 = prims.broadcast_in_dim(sin, (1, 32, 512, 128), (2, 3))  # t2531: "cuda:0 f32[1, 32, 512, 128]"
    # t2532 = prims.convert_element_type(t2527, dtypes.float32)  # t2532: "cuda:0 f32[1, 32, 512, 128]"
    # t2533 = prims.mul(t2532, t2531)  # t2533: "cuda:0 f32[1, 32, 512, 128]"
  t2534 = ltorch.add(t2530, t2533, alpha=None)  # t2534: "cuda:0 f32[1, 32, 512, 128]"
    # t2534 = prims.add(t2530, t2533)  # t2534: "cuda:0 f32[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:379: 	    return roped.to(dtype=x.dtype)
  t2535 = ltorch.to(t2534, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2535: "cuda:0 bf16[1, 32, 512, 128]"
    # t2535 = prims.convert_element_type(t2534, dtypes.bfloat16)  # t2535: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:239: 	        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
  t2536 = ltorch.getitem(t2503, (..., slice(128, None, None)))  # t2536: "cuda:0 bf16[1, 32, 512, 0]"
    # t2536 = prims.slice_prim(t2503, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2536: "cuda:0 bf16[1, 32, 512, 0]"
  t2537 = ltorch.cat((t2520, t2536), -1)  # t2537: "cuda:0 bf16[1, 32, 512, 128]"
    # t2537 = prims.cat((t2520, t2536), -1)  # t2537: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:240: 	        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
  t2538 = ltorch.getitem(t2504, (..., slice(128, None, None)))  # t2538: "cuda:0 bf16[1, 32, 512, 0]"
    # t2538 = prims.slice_prim(t2504, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2538: "cuda:0 bf16[1, 32, 512, 0]"
  t2539 = ltorch.cat((t2535, t2538), -1)  # t2539: "cuda:0 bf16[1, 32, 512, 128]"
    # t2539 = prims.cat((t2535, t2538), -1)  # t2539: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:258: 	        y = torch.nn.functional.scaled_dot_product_attention(
  t2569 = ltorch.scaled_dot_product_attention(t2537, t2539, t2505, None, 0.0, True, scale=0.08838834764831843)  # t2569: "cuda:0 bf16[1, 32, 512, 128]"
    # t2542 = ltorch.mul(t2537, 0.29730177875068026)  # t2542: "cuda:0 bf16[1, 32, 512, 128]"
      # t2540 = prims.convert_element_type(t2537, dtypes.float32)  # t2540: "cuda:0 f32[1, 32, 512, 128]"
      # t2541 = prims.mul(t2540, 0.29730177875068026)  # t2541: "cuda:0 f32[1, 32, 512, 128]"
      # t2542 = prims.convert_element_type(t2541, dtypes.bfloat16)  # t2542: "cuda:0 bf16[1, 32, 512, 128]"
    # t2543 = ltorch.transpose(t2539, -2, -1)  # t2543: "cuda:0 bf16[1, 32, 128, 512]"
      # t2543 = prims.transpose(t2539, (0, 1, 3, 2))  # t2543: "cuda:0 bf16[1, 32, 128, 512]"
    # t2546 = ltorch.mul(t2543, 0.29730177875068026)  # t2546: "cuda:0 bf16[1, 32, 128, 512]"
      # t2544 = prims.convert_element_type(t2543, dtypes.float32)  # t2544: "cuda:0 f32[1, 32, 128, 512]"
      # t2545 = prims.mul(t2544, 0.29730177875068026)  # t2545: "cuda:0 f32[1, 32, 128, 512]"
      # t2546 = prims.convert_element_type(t2545, dtypes.bfloat16)  # t2546: "cuda:0 bf16[1, 32, 128, 512]"
    # t2547 = ltorch.matmul(t2542, t2546)  # t2547: "cuda:0 bf16[1, 32, 512, 512]"
      # t2547 = prims.matmul(t2542, t2546)  # t2547: "cuda:0 bf16[1, 32, 512, 512]"
    # t2557 = ltorch.tril(t2547, 0, fill_value=-float('inf'))  # t2557: "cuda:0 bf16[1, 32, 512, 512]"
      # t2548 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2548: "cuda:0 i64[512]"
        # t2548 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2548: "cuda:0 i64[512]"
      # t2549 = ltorch.unsqueeze(t2548, -1)  # t2549: "cuda:0 i64[512, 1]"
        # t2549 = prims.broadcast_in_dim(t2548, [512, 1], [0])  # t2549: "cuda:0 i64[512, 1]"
      # t2550 = ltorch.arange(512, None, 1, device=devices.Device("cuda:0"), dtype=None)  # t2550: "cuda:0 i64[512]"
        # t2550 = prims.iota(512, start=0, step=1, device=devices.Device("cuda:0"), dtype=dtypes.int64)  # t2550: "cuda:0 i64[512]"
      # t2551 = ltorch.unsqueeze(t2550, -2)  # t2551: "cuda:0 i64[1, 512]"
        # t2551 = prims.broadcast_in_dim(t2550, [1, 512], [1])  # t2551: "cuda:0 i64[1, 512]"
      # t2552 = ltorch.add(t2549, 0, alpha=None)  # t2552: "cuda:0 i64[512, 1]"
        # t2552 = prims.add(t2549, 0)  # t2552: "cuda:0 i64[512, 1]"
      # t2555 = ltorch.ge(t2552, t2551)  # t2555: "cuda:0 b8[512, 512]"
        # t2553 = prims.broadcast_in_dim(t2552, (512, 512), (0, 1))  # t2553: "cuda:0 i64[512, 512]"
        # t2554 = prims.broadcast_in_dim(t2551, (512, 512), (0, 1))  # t2554: "cuda:0 i64[512, 512]"
        # t2555 = prims.ge(t2553, t2554)  # t2555: "cuda:0 b8[512, 512]"
      # t2557 = ltorch.where(t2555, t2547, -float('inf'))  # t2557: "cuda:0 bf16[1, 32, 512, 512]"
        # t2556 = prims.broadcast_in_dim(t2555, (1, 32, 512, 512), (2, 3))  # t2556: "cuda:0 b8[1, 32, 512, 512]"
        # t2557 = prims.where(t2556, t2547, -float('inf'))  # t2557: "cuda:0 bf16[1, 32, 512, 512]"
    # t2568 = ltorch._softmax(t2557, -1, dtype=None)  # t2568: "cuda:0 bf16[1, 32, 512, 512]"
      # t2558 = ltorch.to(t2557, dtypes.float32, None, device=None, dtype=None, copy=False, memory_format=None)  # t2558: "cuda:0 f32[1, 32, 512, 512]"
        # t2558 = prims.convert_element_type(t2557, dtypes.float32)  # t2558: "cuda:0 f32[1, 32, 512, 512]"
      # t2560 = ltorch.amax(t2558, -1, True)  # t2560: "cuda:0 f32[1, 32, 512, 1]"
        # t2559 = prims.amax(t2558, (3,))  # t2559: "cuda:0 f32[1, 32, 512]"
        # t2560 = prims.broadcast_in_dim(t2559, [1, 32, 512, 1], [0, 1, 2])  # t2560: "cuda:0 f32[1, 32, 512, 1]"
      # t2562 = ltorch.sub(t2558, t2560, alpha=None)  # t2562: "cuda:0 f32[1, 32, 512, 512]"
        # t2561 = prims.broadcast_in_dim(t2560, (1, 32, 512, 512), (0, 1, 2, 3))  # t2561: "cuda:0 f32[1, 32, 512, 512]"
        # t2562 = prims.sub(t2558, t2561)  # t2562: "cuda:0 f32[1, 32, 512, 512]"
      # t2563 = ltorch.exp(t2562)  # t2563: "cuda:0 f32[1, 32, 512, 512]"
        # t2563 = prims.exp(t2562)  # t2563: "cuda:0 f32[1, 32, 512, 512]"
      # t2565 = ltorch.sum(t2563, -1, True, dtype=None)  # t2565: "cuda:0 f32[1, 32, 512, 1]"
        # t2564 = prims.sum(t2563, (3,))  # t2564: "cuda:0 f32[1, 32, 512]"
        # t2565 = prims.broadcast_in_dim(t2564, [1, 32, 512, 1], [0, 1, 2])  # t2565: "cuda:0 f32[1, 32, 512, 1]"
      # t2567 = ltorch.true_divide(t2563, t2565)  # t2567: "cuda:0 f32[1, 32, 512, 512]"
        # t2566 = prims.broadcast_in_dim(t2565, (1, 32, 512, 512), (0, 1, 2, 3))  # t2566: "cuda:0 f32[1, 32, 512, 512]"
        # t2567 = prims.div(t2563, t2566)  # t2567: "cuda:0 f32[1, 32, 512, 512]"
      # t2568 = ltorch.to(t2567, dtypes.bfloat16, None, device=None, dtype=None, copy=False, memory_format=None)  # t2568: "cuda:0 bf16[1, 32, 512, 512]"
        # t2568 = prims.convert_element_type(t2567, dtypes.bfloat16)  # t2568: "cuda:0 bf16[1, 32, 512, 512]"
    # t2569 = ltorch.matmul(t2568, t2505)  # t2569: "cuda:0 bf16[1, 32, 512, 128]"
      # t2569 = prims.matmul(t2568, t2505)  # t2569: "cuda:0 bf16[1, 32, 512, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:261: 	        return y.transpose(1, 2)
  t2570 = ltorch.transpose(t2569, 1, 2)  # t2570: "cuda:0 bf16[1, 512, 32, 128]"
    # t2570 = prims.transpose(t2569, (0, 2, 1, 3))  # t2570: "cuda:0 bf16[1, 512, 32, 128]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:249: 	        y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
  t2571 = ltorch.reshape(t2570, 1, 512, 4096)  # t2571: "cuda:0 bf16[1, 512, 4096]"
    # t2571 = prims.reshape(t2570, (1, 512, 4096))  # t2571: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2575 = ltorch.linear(t2571, t_transformer_h_15_attn_proj_weight, None)  # t2575: "cuda:0 bf16[1, 512, 4096]"
    # t2575 = prims.linear(t2571, t_transformer_h_15_attn_proj_weight, None)  # t2575: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:186: 	            x = attention_output + x
  t2579 = ltorch.add(t2575, t2469, alpha=None)  # t2579: "cuda:0 bf16[1, 512, 4096]"
    # t2576 = prims.convert_element_type(t2575, dtypes.float32)  # t2576: "cuda:0 f32[1, 512, 4096]"
    # t2577 = prims.convert_element_type(t2469, dtypes.float32)  # t2577: "cuda:0 f32[1, 512, 4096]"
    # t2578 = prims.add(t2576, t2577)  # t2578: "cuda:0 f32[1, 512, 4096]"
    # t2579 = prims.convert_element_type(t2578, dtypes.bfloat16)  # t2579: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2580 = prims.convert_element_type(t2579, dtypes.float32)  # t2580: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2581 = ltorch.mul(t2580, t2580)  # t2581: "cuda:0 f32[1, 512, 4096]"
    # t2581 = prims.mul(t2580, t2580)  # t2581: "cuda:0 f32[1, 512, 4096]"
  t2585 = ltorch.mean(t2581, -1, True, dtype=None)  # t2585: "cuda:0 f32[1, 512, 1]"
    # t2583 = prims.sum(t2581, (2,))  # t2583: "cuda:0 f32[1, 512]"
    # t2584 = prims.broadcast_in_dim(t2583, [1, 512, 1], [0, 1])  # t2584: "cuda:0 f32[1, 512, 1]"
    # t2585 = ltorch.true_divide(t2584, 4096)  # t2585: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2585 = prims.div(t2584, 4096.0)  # t2585: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2587 = ltorch.add(t2585, 1e-05, alpha=None)  # t2587: "cuda:0 f32[1, 512, 1]"
    # t2587 = prims.add(t2585, 1e-05)  # t2587: "cuda:0 f32[1, 512, 1]"
  t2588 = ltorch.rsqrt(t2587)  # t2588: "cuda:0 f32[1, 512, 1]"
    # t2588 = prims.rsqrt(t2587)  # t2588: "cuda:0 f32[1, 512, 1]"
  t2590 = ltorch.mul(t2580, t2588)  # t2590: "cuda:0 f32[1, 512, 4096]"
    # t2589 = prims.broadcast_in_dim(t2588, (1, 512, 4096), (0, 1, 2))  # t2589: "cuda:0 f32[1, 512, 4096]"
    # t2590 = prims.mul(t2580, t2589)  # t2590: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2591 = ltorch.to(t2590, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2591: "cuda:0 bf16[1, 512, 4096]"
    # t2591 = prims.convert_element_type(t2590, dtypes.bfloat16)  # t2591: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2601 = ltorch.mul(t2591, t_transformer_h_15_norm_2_weight)  # t2601: "cuda:0 bf16[1, 512, 4096]"
    # t2597 = prims.broadcast_in_dim(t_transformer_h_15_norm_2_weight, (1, 512, 4096), (2,))  # t2597: "cuda:0 bf16[1, 512, 4096]"
    # t2598 = prims.convert_element_type(t2591, dtypes.float32)  # t2598: "cuda:0 f32[1, 512, 4096]"
    # t2599 = prims.convert_element_type(t2597, dtypes.float32)  # t2599: "cuda:0 f32[1, 512, 4096]"
    # t2600 = prims.mul(t2598, t2599)  # t2600: "cuda:0 f32[1, 512, 4096]"
    # t2601 = prims.convert_element_type(t2600, dtypes.bfloat16)  # t2601: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2606 = ltorch.linear(t2601, t_transformer_h_15_mlp_fc_1_weight, None)  # t2606: "cuda:0 bf16[1, 512, 11008]"
    # t2606 = prims.linear(t2601, t_transformer_h_15_mlp_fc_1_weight, None)  # t2606: "cuda:0 bf16[1, 512, 11008]"
  t2610 = ltorch.linear(t2601, t_transformer_h_15_mlp_fc_2_weight, None)  # t2610: "cuda:0 bf16[1, 512, 11008]"
    # t2610 = prims.linear(t2601, t_transformer_h_15_mlp_fc_2_weight, None)  # t2610: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:313: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  t2620 = ltorch.silu(t2606, False)  # t2620: "cuda:0 bf16[1, 512, 11008]"
    # t2611 = prims.convert_element_type(t2606, dtypes.float32)  # t2611: "cuda:0 f32[1, 512, 11008]"
    # t2612 = prims.neg(t2611)  # t2612: "cuda:0 f32[1, 512, 11008]"
    # t2613 = prims.exp(t2612)  # t2613: "cuda:0 f32[1, 512, 11008]"
    # t2614 = prims.add(1.0, t2613)  # t2614: "cuda:0 f32[1, 512, 11008]"
    # t2615 = prims.reciprocal(t2614)  # t2615: "cuda:0 f32[1, 512, 11008]"
    # t2616 = prims.convert_element_type(t2615, dtypes.bfloat16)  # t2616: "cuda:0 bf16[1, 512, 11008]"
    # t2617 = prims.convert_element_type(t2606, dtypes.float32)  # t2617: "cuda:0 f32[1, 512, 11008]"
    # t2618 = prims.convert_element_type(t2616, dtypes.float32)  # t2618: "cuda:0 f32[1, 512, 11008]"
    # t2619 = prims.mul(t2617, t2618)  # t2619: "cuda:0 f32[1, 512, 11008]"
    # t2620 = prims.convert_element_type(t2619, dtypes.bfloat16)  # t2620: "cuda:0 bf16[1, 512, 11008]"
  t2624 = ltorch.mul(t2620, t2610)  # t2624: "cuda:0 bf16[1, 512, 11008]"
    # t2621 = prims.convert_element_type(t2620, dtypes.float32)  # t2621: "cuda:0 f32[1, 512, 11008]"
    # t2622 = prims.convert_element_type(t2610, dtypes.float32)  # t2622: "cuda:0 f32[1, 512, 11008]"
    # t2623 = prims.mul(t2621, t2622)  # t2623: "cuda:0 f32[1, 512, 11008]"
    # t2624 = prims.convert_element_type(t2623, dtypes.bfloat16)  # t2624: "cuda:0 bf16[1, 512, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2628 = ltorch.linear(t2624, t_transformer_h_15_mlp_proj_weight, None)  # t2628: "cuda:0 bf16[1, 512, 4096]"
    # t2628 = prims.linear(t2624, t_transformer_h_15_mlp_proj_weight, None)  # t2628: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:187: 	            x = self.mlp(self.norm_2(x)) + x
  t2632 = ltorch.add(t2628, t2579, alpha=None)  # t2632: "cuda:0 bf16[1, 512, 4096]"
    # t2629 = prims.convert_element_type(t2628, dtypes.float32)  # t2629: "cuda:0 f32[1, 512, 4096]"
    # t2630 = prims.convert_element_type(t2579, dtypes.float32)  # t2630: "cuda:0 f32[1, 512, 4096]"
    # t2631 = prims.add(t2629, t2630)  # t2631: "cuda:0 f32[1, 512, 4096]"
    # t2632 = prims.convert_element_type(t2631, dtypes.bfloat16)  # t2632: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:429: 	        x = x.float()
  t2633 = prims.convert_element_type(t2632, dtypes.float32)  # t2633: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:431: 	        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
  t2634 = ltorch.mul(t2633, t2633)  # t2634: "cuda:0 f32[1, 512, 4096]"
    # t2634 = prims.mul(t2633, t2633)  # t2634: "cuda:0 f32[1, 512, 4096]"
  t2638 = ltorch.mean(t2634, -1, True, dtype=None)  # t2638: "cuda:0 f32[1, 512, 1]"
    # t2636 = prims.sum(t2634, (2,))  # t2636: "cuda:0 f32[1, 512]"
    # t2637 = prims.broadcast_in_dim(t2636, [1, 512, 1], [0, 1])  # t2637: "cuda:0 f32[1, 512, 1]"
    # t2638 = ltorch.true_divide(t2637, 4096)  # t2638: "cuda:0 f32[1, 512, 1]"
      # _ = prims.convert_element_type(4096, float)
      # t2638 = prims.div(t2637, 4096.0)  # t2638: "cuda:0 f32[1, 512, 1]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:432: 	        x_normed = x * torch.rsqrt(norm_x + self.eps)
  t2640 = ltorch.add(t2638, 1e-05, alpha=None)  # t2640: "cuda:0 f32[1, 512, 1]"
    # t2640 = prims.add(t2638, 1e-05)  # t2640: "cuda:0 f32[1, 512, 1]"
  t2641 = ltorch.rsqrt(t2640)  # t2641: "cuda:0 f32[1, 512, 1]"
    # t2641 = prims.rsqrt(t2640)  # t2641: "cuda:0 f32[1, 512, 1]"
  t2643 = ltorch.mul(t2633, t2641)  # t2643: "cuda:0 f32[1, 512, 4096]"
    # t2642 = prims.broadcast_in_dim(t2641, (1, 512, 4096), (0, 1, 2))  # t2642: "cuda:0 f32[1, 512, 4096]"
    # t2643 = prims.mul(t2633, t2642)  # t2643: "cuda:0 f32[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:433: 	        x_normed = x_normed.to(dtype=dtype)
  t2644 = ltorch.to(t2643, None, None, device=None, dtype=dtypes.bfloat16, copy=False, memory_format=None)  # t2644: "cuda:0 bf16[1, 512, 4096]"
    # t2644 = prims.convert_element_type(t2643, dtypes.bfloat16)  # t2644: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/litgpt/model.py:438: 	        return x_normed * self.weight
  t2654 = ltorch.mul(t2644, t_transformer_ln_f_weight)  # t2654: "cuda:0 bf16[1, 512, 4096]"
    # t2650 = prims.broadcast_in_dim(t_transformer_ln_f_weight, (1, 512, 4096), (2,))  # t2650: "cuda:0 bf16[1, 512, 4096]"
    # t2651 = prims.convert_element_type(t2644, dtypes.float32)  # t2651: "cuda:0 f32[1, 512, 4096]"
    # t2652 = prims.convert_element_type(t2650, dtypes.float32)  # t2652: "cuda:0 f32[1, 512, 4096]"
    # t2653 = prims.mul(t2651, t2652)  # t2653: "cuda:0 f32[1, 512, 4096]"
    # t2654 = prims.convert_element_type(t2653, dtypes.bfloat16)  # t2654: "cuda:0 bf16[1, 512, 4096]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t2658 = ltorch.linear(t2654, t_lm_head_weight, None)  # t2658: "cuda:0 bf16[1, 512, 32000]"
    # t2658 = prims.linear(t2654, t_lm_head_weight, None)  # t2658: "cuda:0 bf16[1, 512, 32000]"
  return t2658
============================================ END: primal_trace sort_data_parallel_syncs
============================================ START: primal_trace forward_and_backward_from_trace
# Constructed by Augmented forward pass
import thunder
import thunder.core.dtypes as dtypes
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight):
  # idx: "cuda:0 i64[1, 512]"
  # tos1: "cuda:0 f32[4096, 128]"
  # t_lm_head_weight: "cuda:0 bf16[32000, 4096]"
  # t_sin: "cuda:0 f32[4096, 128]"
  # t_transformer_h_0_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_0_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_0_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_0_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_0_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_1_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_1_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_1_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_2_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_2_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_2_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_3_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_3_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_3_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_4_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_4_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_4_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_5_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_5_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_5_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_6_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_6_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_6_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_7_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_7_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_7_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_8_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_8_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_8_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_9_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_9_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_9_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_10_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_10_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_10_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_11_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_11_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_11_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_12_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_12_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_12_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_13_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_13_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_13_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_14_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_14_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_14_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_15_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_15_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_15_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_ln_f_weight: "cuda:0 bf16[4096]"
  # t_transformer_wte_weight: "cuda:0 bf16[32000, 4096]"
  t0 = prims.slice_prim(tos1, [0, 0], [512, 128], [1, 1])  # t0: "cuda:0 f32[512, 128]"
  t1 = prims.slice_prim(t_sin, [0, 0], [512, 128], [1, 1])  # t1: "cuda:0 f32[512, 128]"
  t4 = ltorch.embedding(idx, t_transformer_wte_weight, None, None, 2.0, False, False)  # t4: "cuda:0 bf16[1, 512, 4096]"
    # t2 = ltorch.reshape(idx, [512])  # t2: "cuda:0 i64[512]"
      # t2 = prims.reshape(idx, (512,))  # t2: "cuda:0 i64[512]"
    # t3 = prims.take(t_transformer_wte_weight, t2, 0)  # t3: "cuda:0 bf16[512, 4096]"
    # t4 = ltorch.reshape(t3, [1, 512, 4096])  # t4: "cuda:0 bf16[1, 512, 4096]"
      # t4 = prims.reshape(t3, (1, 512, 4096))  # t4: "cuda:0 bf16[1, 512, 4096]"
  t5 = prims.convert_element_type(t4, dtypes.float32)  # t5: "cuda:0 f32[1, 512, 4096]"
  t6 = ltorch.mul(t5, t5)  # t6: "cuda:0 f32[1, 512, 4096]"
    # t6 = prims.mul(t5, t5)  # t6: "cuda:0 f32[1, 512, 4096]"
  t7 = prims.sum(t6, (2,))  # t7: "cuda:0 f32[1, 512]"
  t8 = prims.broadcast_in_dim(t7, [1, 512, 1], [0, 1])  # t8: "cuda:0 f32[1, 512, 1]"
  t9 = ltorch.true_divide(t8, 4096.0)  # t9: "cuda:0 f32[1, 512, 1]"
    # t9 = prims.div(t8, 4096.0)  # t9: "cuda:0 f32[1, 512, 1]"
  t10 = ltorch.add(t9, 1e-05, alpha=None)  # t10: "cuda:0 f32[1, 512, 1]"
    # t10 = prims.add(t9, 1e-05)  # t10: "cuda:0 f32[1, 512, 1]"
  t11 = prims.rsqrt(t10)  # t11: "cuda:0 f32[1, 512, 1]"
  t12 = prims.broadcast_in_dim(t11, (1, 512, 4096), (0, 1, 2))  # t12: "cuda:0 f32[1, 512, 4096]"
  t13 = ltorch.mul(t5, t12)  # t13: "cuda:0 f32[1, 512, 4096]"
    # t13 = prims.mul(t5, t12)  # t13: "cuda:0 f32[1, 512, 4096]"
  t14 = prims.convert_element_type(t13, dtypes.bfloat16)  # t14: "cuda:0 bf16[1, 512, 4096]"
  t15 = prims.broadcast_in_dim(t_transformer_h_0_norm_1_weight, (1, 512, 4096), (2,))  # t15: "cuda:0 bf16[1, 512, 4096]"
  t16 = prims.convert_element_type(t14, dtypes.float32)  # t16: "cuda:0 f32[1, 512, 4096]"
  t17 = prims.convert_element_type(t15, dtypes.float32)  # t17: "cuda:0 f32[1, 512, 4096]"
  t18 = ltorch.mul(t16, t17)  # t18: "cuda:0 f32[1, 512, 4096]"
    # t18 = prims.mul(t16, t17)  # t18: "cuda:0 f32[1, 512, 4096]"
  t19 = prims.convert_element_type(t18, dtypes.bfloat16)  # t19: "cuda:0 bf16[1, 512, 4096]"
  t20 = prims.linear(t19, t_transformer_h_0_attn_attn_weight, None)  # t20: "cuda:0 bf16[1, 512, 12288]"
  t21 = prims.reshape(t20, (1, 512, 32, 3, 128))  # t21: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t22 = prims.transpose(t21, (0, 2, 3, 1, 4))  # t22: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t23, t24, t25) = ltorch.split(t22, (1, 1, 1), 2)
    # t23 = prims.slice_prim(t22, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t23: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t24 = prims.slice_prim(t22, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t24: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t25 = prims.slice_prim(t22, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t25: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t26 = prims.reshape(t23, (1, 32, 512, 128))  # t26: "cuda:0 bf16[1, 32, 512, 128]"
  t32 = prims.reshape(t24, (1, 32, 512, 128))  # t32: "cuda:0 bf16[1, 32, 512, 128]"
  t38 = prims.reshape(t25, (1, 32, 512, 128))  # t38: "cuda:0 bf16[1, 32, 512, 128]"
  t39 = prims.slice_prim(t26, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t39: "cuda:0 bf16[1, 32, 512, 128]"
  t40 = prims.slice_prim(t39, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t40: "cuda:0 bf16[1, 32, 512, 64]"
  t41 = prims.slice_prim(t39, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t41: "cuda:0 bf16[1, 32, 512, 64]"
  t42 = prims.convert_element_type(t41, dtypes.float32)  # t42: "cuda:0 f32[1, 32, 512, 64]"
  t43 = prims.neg(t42)  # t43: "cuda:0 f32[1, 32, 512, 64]"
  t44 = prims.convert_element_type(t43, dtypes.bfloat16)  # t44: "cuda:0 bf16[1, 32, 512, 64]"
  t45 = prims.cat((t44, t40), -1)  # t45: "cuda:0 bf16[1, 32, 512, 128]"
  t46 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t46: "cuda:0 f32[1, 32, 512, 128]"
  t47 = prims.convert_element_type(t39, dtypes.float32)  # t47: "cuda:0 f32[1, 32, 512, 128]"
  t48 = ltorch.mul(t47, t46)  # t48: "cuda:0 f32[1, 32, 512, 128]"
    # t48 = prims.mul(t47, t46)  # t48: "cuda:0 f32[1, 32, 512, 128]"
  t49 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t49: "cuda:0 f32[1, 32, 512, 128]"
  t50 = prims.convert_element_type(t45, dtypes.float32)  # t50: "cuda:0 f32[1, 32, 512, 128]"
  t51 = ltorch.mul(t50, t49)  # t51: "cuda:0 f32[1, 32, 512, 128]"
    # t51 = prims.mul(t50, t49)  # t51: "cuda:0 f32[1, 32, 512, 128]"
  t52 = ltorch.add(t48, t51, alpha=None)  # t52: "cuda:0 f32[1, 32, 512, 128]"
    # t52 = prims.add(t48, t51)  # t52: "cuda:0 f32[1, 32, 512, 128]"
  t53 = prims.convert_element_type(t52, dtypes.bfloat16)  # t53: "cuda:0 bf16[1, 32, 512, 128]"
  t54 = prims.slice_prim(t32, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t54: "cuda:0 bf16[1, 32, 512, 128]"
  t55 = prims.slice_prim(t54, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t55: "cuda:0 bf16[1, 32, 512, 64]"
  t56 = prims.slice_prim(t54, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t56: "cuda:0 bf16[1, 32, 512, 64]"
  t57 = prims.convert_element_type(t56, dtypes.float32)  # t57: "cuda:0 f32[1, 32, 512, 64]"
  t58 = prims.neg(t57)  # t58: "cuda:0 f32[1, 32, 512, 64]"
  t59 = prims.convert_element_type(t58, dtypes.bfloat16)  # t59: "cuda:0 bf16[1, 32, 512, 64]"
  t61 = prims.cat((t59, t55), -1)  # t61: "cuda:0 bf16[1, 32, 512, 128]"
  t62 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t62: "cuda:0 f32[1, 32, 512, 128]"
  t63 = prims.convert_element_type(t54, dtypes.float32)  # t63: "cuda:0 f32[1, 32, 512, 128]"
  t64 = ltorch.mul(t63, t62)  # t64: "cuda:0 f32[1, 32, 512, 128]"
    # t64 = prims.mul(t63, t62)  # t64: "cuda:0 f32[1, 32, 512, 128]"
  t65 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t65: "cuda:0 f32[1, 32, 512, 128]"
  t66 = prims.convert_element_type(t61, dtypes.float32)  # t66: "cuda:0 f32[1, 32, 512, 128]"
  t67 = ltorch.mul(t66, t65)  # t67: "cuda:0 f32[1, 32, 512, 128]"
    # t67 = prims.mul(t66, t65)  # t67: "cuda:0 f32[1, 32, 512, 128]"
  t68 = ltorch.add(t64, t67, alpha=None)  # t68: "cuda:0 f32[1, 32, 512, 128]"
    # t68 = prims.add(t64, t67)  # t68: "cuda:0 f32[1, 32, 512, 128]"
  t69 = prims.convert_element_type(t68, dtypes.bfloat16)  # t69: "cuda:0 bf16[1, 32, 512, 128]"
  t70 = prims.slice_prim(t26, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t70: "cuda:0 bf16[1, 32, 512, 0]"
  t71 = prims.cat((t53, t70), -1)  # t71: "cuda:0 bf16[1, 32, 512, 128]"
  t72 = prims.slice_prim(t32, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t72: "cuda:0 bf16[1, 32, 512, 0]"
  t74 = prims.cat((t69, t72), -1)  # t74: "cuda:0 bf16[1, 32, 512, 128]"
  (t75, t76, t77, t78) = cudnn_sdpa_fwd(t71, t74, t38, None, 0.0, True, scale=0.08838834764831843)
  t79 = prims.transpose(t75, (0, 2, 1, 3))  # t79: "cuda:0 bf16[1, 512, 32, 128]"
  t80 = prims.reshape(t79, (1, 512, 4096))  # t80: "cuda:0 bf16[1, 512, 4096]"
  t81 = prims.linear(t80, t_transformer_h_0_attn_proj_weight, None)  # t81: "cuda:0 bf16[1, 512, 4096]"
  t82 = prims.convert_element_type(t81, dtypes.float32)  # t82: "cuda:0 f32[1, 512, 4096]"
  t83 = prims.convert_element_type(t4, dtypes.float32)  # t83: "cuda:0 f32[1, 512, 4096]"
  t84 = ltorch.add(t82, t83, alpha=None)  # t84: "cuda:0 f32[1, 512, 4096]"
    # t84 = prims.add(t82, t83)  # t84: "cuda:0 f32[1, 512, 4096]"
  t85 = prims.convert_element_type(t84, dtypes.bfloat16)  # t85: "cuda:0 bf16[1, 512, 4096]"
  t86 = prims.convert_element_type(t85, dtypes.float32)  # t86: "cuda:0 f32[1, 512, 4096]"
  t87 = ltorch.mul(t86, t86)  # t87: "cuda:0 f32[1, 512, 4096]"
    # t87 = prims.mul(t86, t86)  # t87: "cuda:0 f32[1, 512, 4096]"
  t89 = prims.sum(t87, (2,))  # t89: "cuda:0 f32[1, 512]"
  t90 = prims.broadcast_in_dim(t89, [1, 512, 1], [0, 1])  # t90: "cuda:0 f32[1, 512, 1]"
  t92 = ltorch.true_divide(t90, 4096.0)  # t92: "cuda:0 f32[1, 512, 1]"
    # t92 = prims.div(t90, 4096.0)  # t92: "cuda:0 f32[1, 512, 1]"
  t94 = ltorch.add(t92, 1e-05, alpha=None)  # t94: "cuda:0 f32[1, 512, 1]"
    # t94 = prims.add(t92, 1e-05)  # t94: "cuda:0 f32[1, 512, 1]"
  t95 = prims.rsqrt(t94)  # t95: "cuda:0 f32[1, 512, 1]"
  t96 = prims.broadcast_in_dim(t95, (1, 512, 4096), (0, 1, 2))  # t96: "cuda:0 f32[1, 512, 4096]"
  t97 = ltorch.mul(t86, t96)  # t97: "cuda:0 f32[1, 512, 4096]"
    # t97 = prims.mul(t86, t96)  # t97: "cuda:0 f32[1, 512, 4096]"
  t98 = prims.convert_element_type(t97, dtypes.bfloat16)  # t98: "cuda:0 bf16[1, 512, 4096]"
  t99 = prims.broadcast_in_dim(t_transformer_h_0_norm_2_weight, (1, 512, 4096), (2,))  # t99: "cuda:0 bf16[1, 512, 4096]"
  t100 = prims.convert_element_type(t98, dtypes.float32)  # t100: "cuda:0 f32[1, 512, 4096]"
  t101 = prims.convert_element_type(t99, dtypes.float32)  # t101: "cuda:0 f32[1, 512, 4096]"
  t102 = ltorch.mul(t100, t101)  # t102: "cuda:0 f32[1, 512, 4096]"
    # t102 = prims.mul(t100, t101)  # t102: "cuda:0 f32[1, 512, 4096]"
  t103 = prims.convert_element_type(t102, dtypes.bfloat16)  # t103: "cuda:0 bf16[1, 512, 4096]"
  t104 = prims.linear(t103, t_transformer_h_0_mlp_fc_1_weight, None)  # t104: "cuda:0 bf16[1, 512, 11008]"
  t105 = prims.linear(t103, t_transformer_h_0_mlp_fc_2_weight, None)  # t105: "cuda:0 bf16[1, 512, 11008]"
  t106 = prims.convert_element_type(t104, dtypes.float32)  # t106: "cuda:0 f32[1, 512, 11008]"
  t107 = prims.neg(t106)  # t107: "cuda:0 f32[1, 512, 11008]"
  t108 = prims.exp(t107)  # t108: "cuda:0 f32[1, 512, 11008]"
  t109 = ltorch.add(1.0, t108, alpha=None)  # t109: "cuda:0 f32[1, 512, 11008]"
    # t109 = prims.add(1.0, t108)  # t109: "cuda:0 f32[1, 512, 11008]"
  t110 = prims.reciprocal(t109)  # t110: "cuda:0 f32[1, 512, 11008]"
  t111 = prims.convert_element_type(t110, dtypes.bfloat16)  # t111: "cuda:0 bf16[1, 512, 11008]"
  t112 = prims.convert_element_type(t104, dtypes.float32)  # t112: "cuda:0 f32[1, 512, 11008]"
  t113 = prims.convert_element_type(t111, dtypes.float32)  # t113: "cuda:0 f32[1, 512, 11008]"
  t114 = ltorch.mul(t112, t113)  # t114: "cuda:0 f32[1, 512, 11008]"
    # t114 = prims.mul(t112, t113)  # t114: "cuda:0 f32[1, 512, 11008]"
  t115 = prims.convert_element_type(t114, dtypes.bfloat16)  # t115: "cuda:0 bf16[1, 512, 11008]"
  t116 = prims.convert_element_type(t115, dtypes.float32)  # t116: "cuda:0 f32[1, 512, 11008]"
  t117 = prims.convert_element_type(t105, dtypes.float32)  # t117: "cuda:0 f32[1, 512, 11008]"
  t118 = ltorch.mul(t116, t117)  # t118: "cuda:0 f32[1, 512, 11008]"
    # t118 = prims.mul(t116, t117)  # t118: "cuda:0 f32[1, 512, 11008]"
  t119 = prims.convert_element_type(t118, dtypes.bfloat16)  # t119: "cuda:0 bf16[1, 512, 11008]"
  t120 = prims.linear(t119, t_transformer_h_0_mlp_proj_weight, None)  # t120: "cuda:0 bf16[1, 512, 4096]"
  t121 = prims.convert_element_type(t120, dtypes.float32)  # t121: "cuda:0 f32[1, 512, 4096]"
  t122 = prims.convert_element_type(t85, dtypes.float32)  # t122: "cuda:0 f32[1, 512, 4096]"
  t123 = ltorch.add(t121, t122, alpha=None)  # t123: "cuda:0 f32[1, 512, 4096]"
    # t123 = prims.add(t121, t122)  # t123: "cuda:0 f32[1, 512, 4096]"
  t124 = prims.convert_element_type(t123, dtypes.bfloat16)  # t124: "cuda:0 bf16[1, 512, 4096]"
  t125 = prims.convert_element_type(t124, dtypes.float32)  # t125: "cuda:0 f32[1, 512, 4096]"
  t126 = ltorch.mul(t125, t125)  # t126: "cuda:0 f32[1, 512, 4096]"
    # t126 = prims.mul(t125, t125)  # t126: "cuda:0 f32[1, 512, 4096]"
  t128 = prims.sum(t126, (2,))  # t128: "cuda:0 f32[1, 512]"
  t129 = prims.broadcast_in_dim(t128, [1, 512, 1], [0, 1])  # t129: "cuda:0 f32[1, 512, 1]"
  t131 = ltorch.true_divide(t129, 4096.0)  # t131: "cuda:0 f32[1, 512, 1]"
    # t131 = prims.div(t129, 4096.0)  # t131: "cuda:0 f32[1, 512, 1]"
  t133 = ltorch.add(t131, 1e-05, alpha=None)  # t133: "cuda:0 f32[1, 512, 1]"
    # t133 = prims.add(t131, 1e-05)  # t133: "cuda:0 f32[1, 512, 1]"
  t134 = prims.rsqrt(t133)  # t134: "cuda:0 f32[1, 512, 1]"
  t135 = prims.broadcast_in_dim(t134, (1, 512, 4096), (0, 1, 2))  # t135: "cuda:0 f32[1, 512, 4096]"
  t136 = ltorch.mul(t125, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
    # t136 = prims.mul(t125, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
  t137 = prims.convert_element_type(t136, dtypes.bfloat16)  # t137: "cuda:0 bf16[1, 512, 4096]"
  t138 = prims.broadcast_in_dim(t_transformer_h_1_norm_1_weight, (1, 512, 4096), (2,))  # t138: "cuda:0 bf16[1, 512, 4096]"
  t139 = prims.convert_element_type(t137, dtypes.float32)  # t139: "cuda:0 f32[1, 512, 4096]"
  t140 = prims.convert_element_type(t138, dtypes.float32)  # t140: "cuda:0 f32[1, 512, 4096]"
  t141 = ltorch.mul(t139, t140)  # t141: "cuda:0 f32[1, 512, 4096]"
    # t141 = prims.mul(t139, t140)  # t141: "cuda:0 f32[1, 512, 4096]"
  t142 = prims.convert_element_type(t141, dtypes.bfloat16)  # t142: "cuda:0 bf16[1, 512, 4096]"
  t143 = prims.linear(t142, t_transformer_h_1_attn_attn_weight, None)  # t143: "cuda:0 bf16[1, 512, 12288]"
  t149 = prims.reshape(t143, (1, 512, 32, 3, 128))  # t149: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t155 = prims.transpose(t149, (0, 2, 3, 1, 4))  # t155: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t156, t157, t158) = ltorch.split(t155, (1, 1, 1), 2)
    # t156 = prims.slice_prim(t155, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t156: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t157 = prims.slice_prim(t155, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t157: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t158 = prims.slice_prim(t155, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t158: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t164 = prims.reshape(t156, (1, 32, 512, 128))  # t164: "cuda:0 bf16[1, 32, 512, 128]"
  t170 = prims.reshape(t157, (1, 32, 512, 128))  # t170: "cuda:0 bf16[1, 32, 512, 128]"
  t176 = prims.reshape(t158, (1, 32, 512, 128))  # t176: "cuda:0 bf16[1, 32, 512, 128]"
  t177 = prims.slice_prim(t164, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t177: "cuda:0 bf16[1, 32, 512, 128]"
  t178 = prims.slice_prim(t177, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t178: "cuda:0 bf16[1, 32, 512, 64]"
  t179 = prims.slice_prim(t177, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t179: "cuda:0 bf16[1, 32, 512, 64]"
  t180 = prims.convert_element_type(t179, dtypes.float32)  # t180: "cuda:0 f32[1, 32, 512, 64]"
  t181 = prims.neg(t180)  # t181: "cuda:0 f32[1, 32, 512, 64]"
  t182 = prims.convert_element_type(t181, dtypes.bfloat16)  # t182: "cuda:0 bf16[1, 32, 512, 64]"
  t184 = prims.cat((t182, t178), -1)  # t184: "cuda:0 bf16[1, 32, 512, 128]"
  t185 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t185: "cuda:0 f32[1, 32, 512, 128]"
  t186 = prims.convert_element_type(t177, dtypes.float32)  # t186: "cuda:0 f32[1, 32, 512, 128]"
  t187 = ltorch.mul(t186, t185)  # t187: "cuda:0 f32[1, 32, 512, 128]"
    # t187 = prims.mul(t186, t185)  # t187: "cuda:0 f32[1, 32, 512, 128]"
  t188 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t188: "cuda:0 f32[1, 32, 512, 128]"
  t189 = prims.convert_element_type(t184, dtypes.float32)  # t189: "cuda:0 f32[1, 32, 512, 128]"
  t190 = ltorch.mul(t189, t188)  # t190: "cuda:0 f32[1, 32, 512, 128]"
    # t190 = prims.mul(t189, t188)  # t190: "cuda:0 f32[1, 32, 512, 128]"
  t191 = ltorch.add(t187, t190, alpha=None)  # t191: "cuda:0 f32[1, 32, 512, 128]"
    # t191 = prims.add(t187, t190)  # t191: "cuda:0 f32[1, 32, 512, 128]"
  t192 = prims.convert_element_type(t191, dtypes.bfloat16)  # t192: "cuda:0 bf16[1, 32, 512, 128]"
  t193 = prims.slice_prim(t170, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t193: "cuda:0 bf16[1, 32, 512, 128]"
  t194 = prims.slice_prim(t193, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t194: "cuda:0 bf16[1, 32, 512, 64]"
  t195 = prims.slice_prim(t193, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t195: "cuda:0 bf16[1, 32, 512, 64]"
  t196 = prims.convert_element_type(t195, dtypes.float32)  # t196: "cuda:0 f32[1, 32, 512, 64]"
  t197 = prims.neg(t196)  # t197: "cuda:0 f32[1, 32, 512, 64]"
  t198 = prims.convert_element_type(t197, dtypes.bfloat16)  # t198: "cuda:0 bf16[1, 32, 512, 64]"
  t200 = prims.cat((t198, t194), -1)  # t200: "cuda:0 bf16[1, 32, 512, 128]"
  t201 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t201: "cuda:0 f32[1, 32, 512, 128]"
  t202 = prims.convert_element_type(t193, dtypes.float32)  # t202: "cuda:0 f32[1, 32, 512, 128]"
  t203 = ltorch.mul(t202, t201)  # t203: "cuda:0 f32[1, 32, 512, 128]"
    # t203 = prims.mul(t202, t201)  # t203: "cuda:0 f32[1, 32, 512, 128]"
  t204 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t204: "cuda:0 f32[1, 32, 512, 128]"
  t205 = prims.convert_element_type(t200, dtypes.float32)  # t205: "cuda:0 f32[1, 32, 512, 128]"
  t206 = ltorch.mul(t205, t204)  # t206: "cuda:0 f32[1, 32, 512, 128]"
    # t206 = prims.mul(t205, t204)  # t206: "cuda:0 f32[1, 32, 512, 128]"
  t207 = ltorch.add(t203, t206, alpha=None)  # t207: "cuda:0 f32[1, 32, 512, 128]"
    # t207 = prims.add(t203, t206)  # t207: "cuda:0 f32[1, 32, 512, 128]"
  t208 = prims.convert_element_type(t207, dtypes.bfloat16)  # t208: "cuda:0 bf16[1, 32, 512, 128]"
  t209 = prims.slice_prim(t164, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t209: "cuda:0 bf16[1, 32, 512, 0]"
  t211 = prims.cat((t192, t209), -1)  # t211: "cuda:0 bf16[1, 32, 512, 128]"
  t212 = prims.slice_prim(t170, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t212: "cuda:0 bf16[1, 32, 512, 0]"
  t214 = prims.cat((t208, t212), -1)  # t214: "cuda:0 bf16[1, 32, 512, 128]"
  (t215, t216, t217, t218) = cudnn_sdpa_fwd(t211, t214, t176, None, 0.0, True, scale=0.08838834764831843)
  t221 = prims.transpose(t215, (0, 2, 1, 3))  # t221: "cuda:0 bf16[1, 512, 32, 128]"
  t225 = prims.reshape(t221, (1, 512, 4096))  # t225: "cuda:0 bf16[1, 512, 4096]"
  t226 = prims.linear(t225, t_transformer_h_1_attn_proj_weight, None)  # t226: "cuda:0 bf16[1, 512, 4096]"
  t227 = prims.convert_element_type(t226, dtypes.float32)  # t227: "cuda:0 f32[1, 512, 4096]"
  t228 = prims.convert_element_type(t124, dtypes.float32)  # t228: "cuda:0 f32[1, 512, 4096]"
  t229 = ltorch.add(t227, t228, alpha=None)  # t229: "cuda:0 f32[1, 512, 4096]"
    # t229 = prims.add(t227, t228)  # t229: "cuda:0 f32[1, 512, 4096]"
  t230 = prims.convert_element_type(t229, dtypes.bfloat16)  # t230: "cuda:0 bf16[1, 512, 4096]"
  t231 = prims.convert_element_type(t230, dtypes.float32)  # t231: "cuda:0 f32[1, 512, 4096]"
  t232 = ltorch.mul(t231, t231)  # t232: "cuda:0 f32[1, 512, 4096]"
    # t232 = prims.mul(t231, t231)  # t232: "cuda:0 f32[1, 512, 4096]"
  t234 = prims.sum(t232, (2,))  # t234: "cuda:0 f32[1, 512]"
  t235 = prims.broadcast_in_dim(t234, [1, 512, 1], [0, 1])  # t235: "cuda:0 f32[1, 512, 1]"
  t237 = ltorch.true_divide(t235, 4096.0)  # t237: "cuda:0 f32[1, 512, 1]"
    # t237 = prims.div(t235, 4096.0)  # t237: "cuda:0 f32[1, 512, 1]"
  t239 = ltorch.add(t237, 1e-05, alpha=None)  # t239: "cuda:0 f32[1, 512, 1]"
    # t239 = prims.add(t237, 1e-05)  # t239: "cuda:0 f32[1, 512, 1]"
  t240 = prims.rsqrt(t239)  # t240: "cuda:0 f32[1, 512, 1]"
  t241 = prims.broadcast_in_dim(t240, (1, 512, 4096), (0, 1, 2))  # t241: "cuda:0 f32[1, 512, 4096]"
  t242 = ltorch.mul(t231, t241)  # t242: "cuda:0 f32[1, 512, 4096]"
    # t242 = prims.mul(t231, t241)  # t242: "cuda:0 f32[1, 512, 4096]"
  t243 = prims.convert_element_type(t242, dtypes.bfloat16)  # t243: "cuda:0 bf16[1, 512, 4096]"
  t244 = prims.broadcast_in_dim(t_transformer_h_1_norm_2_weight, (1, 512, 4096), (2,))  # t244: "cuda:0 bf16[1, 512, 4096]"
  t245 = prims.convert_element_type(t243, dtypes.float32)  # t245: "cuda:0 f32[1, 512, 4096]"
  t246 = prims.convert_element_type(t244, dtypes.float32)  # t246: "cuda:0 f32[1, 512, 4096]"
  t247 = ltorch.mul(t245, t246)  # t247: "cuda:0 f32[1, 512, 4096]"
    # t247 = prims.mul(t245, t246)  # t247: "cuda:0 f32[1, 512, 4096]"
  t248 = prims.convert_element_type(t247, dtypes.bfloat16)  # t248: "cuda:0 bf16[1, 512, 4096]"
  t249 = prims.linear(t248, t_transformer_h_1_mlp_fc_1_weight, None)  # t249: "cuda:0 bf16[1, 512, 11008]"
  t250 = prims.linear(t248, t_transformer_h_1_mlp_fc_2_weight, None)  # t250: "cuda:0 bf16[1, 512, 11008]"
  t251 = prims.convert_element_type(t249, dtypes.float32)  # t251: "cuda:0 f32[1, 512, 11008]"
  t252 = prims.neg(t251)  # t252: "cuda:0 f32[1, 512, 11008]"
  t253 = prims.exp(t252)  # t253: "cuda:0 f32[1, 512, 11008]"
  t254 = ltorch.add(1.0, t253, alpha=None)  # t254: "cuda:0 f32[1, 512, 11008]"
    # t254 = prims.add(1.0, t253)  # t254: "cuda:0 f32[1, 512, 11008]"
  t255 = prims.reciprocal(t254)  # t255: "cuda:0 f32[1, 512, 11008]"
  t256 = prims.convert_element_type(t255, dtypes.bfloat16)  # t256: "cuda:0 bf16[1, 512, 11008]"
  t257 = prims.convert_element_type(t249, dtypes.float32)  # t257: "cuda:0 f32[1, 512, 11008]"
  t258 = prims.convert_element_type(t256, dtypes.float32)  # t258: "cuda:0 f32[1, 512, 11008]"
  t259 = ltorch.mul(t257, t258)  # t259: "cuda:0 f32[1, 512, 11008]"
    # t259 = prims.mul(t257, t258)  # t259: "cuda:0 f32[1, 512, 11008]"
  t260 = prims.convert_element_type(t259, dtypes.bfloat16)  # t260: "cuda:0 bf16[1, 512, 11008]"
  t261 = prims.convert_element_type(t260, dtypes.float32)  # t261: "cuda:0 f32[1, 512, 11008]"
  t262 = prims.convert_element_type(t250, dtypes.float32)  # t262: "cuda:0 f32[1, 512, 11008]"
  t263 = ltorch.mul(t261, t262)  # t263: "cuda:0 f32[1, 512, 11008]"
    # t263 = prims.mul(t261, t262)  # t263: "cuda:0 f32[1, 512, 11008]"
  t264 = prims.convert_element_type(t263, dtypes.bfloat16)  # t264: "cuda:0 bf16[1, 512, 11008]"
  t265 = prims.linear(t264, t_transformer_h_1_mlp_proj_weight, None)  # t265: "cuda:0 bf16[1, 512, 4096]"
  t266 = prims.convert_element_type(t265, dtypes.float32)  # t266: "cuda:0 f32[1, 512, 4096]"
  t267 = prims.convert_element_type(t230, dtypes.float32)  # t267: "cuda:0 f32[1, 512, 4096]"
  t268 = ltorch.add(t266, t267, alpha=None)  # t268: "cuda:0 f32[1, 512, 4096]"
    # t268 = prims.add(t266, t267)  # t268: "cuda:0 f32[1, 512, 4096]"
  t269 = prims.convert_element_type(t268, dtypes.bfloat16)  # t269: "cuda:0 bf16[1, 512, 4096]"
  t270 = prims.convert_element_type(t269, dtypes.float32)  # t270: "cuda:0 f32[1, 512, 4096]"
  t271 = ltorch.mul(t270, t270)  # t271: "cuda:0 f32[1, 512, 4096]"
    # t271 = prims.mul(t270, t270)  # t271: "cuda:0 f32[1, 512, 4096]"
  t273 = prims.sum(t271, (2,))  # t273: "cuda:0 f32[1, 512]"
  t274 = prims.broadcast_in_dim(t273, [1, 512, 1], [0, 1])  # t274: "cuda:0 f32[1, 512, 1]"
  t276 = ltorch.true_divide(t274, 4096.0)  # t276: "cuda:0 f32[1, 512, 1]"
    # t276 = prims.div(t274, 4096.0)  # t276: "cuda:0 f32[1, 512, 1]"
  t278 = ltorch.add(t276, 1e-05, alpha=None)  # t278: "cuda:0 f32[1, 512, 1]"
    # t278 = prims.add(t276, 1e-05)  # t278: "cuda:0 f32[1, 512, 1]"
  t279 = prims.rsqrt(t278)  # t279: "cuda:0 f32[1, 512, 1]"
  t280 = prims.broadcast_in_dim(t279, (1, 512, 4096), (0, 1, 2))  # t280: "cuda:0 f32[1, 512, 4096]"
  t281 = ltorch.mul(t270, t280)  # t281: "cuda:0 f32[1, 512, 4096]"
    # t281 = prims.mul(t270, t280)  # t281: "cuda:0 f32[1, 512, 4096]"
  t282 = prims.convert_element_type(t281, dtypes.bfloat16)  # t282: "cuda:0 bf16[1, 512, 4096]"
  t283 = prims.broadcast_in_dim(t_transformer_h_2_norm_1_weight, (1, 512, 4096), (2,))  # t283: "cuda:0 bf16[1, 512, 4096]"
  t284 = prims.convert_element_type(t282, dtypes.float32)  # t284: "cuda:0 f32[1, 512, 4096]"
  t285 = prims.convert_element_type(t283, dtypes.float32)  # t285: "cuda:0 f32[1, 512, 4096]"
  t286 = ltorch.mul(t284, t285)  # t286: "cuda:0 f32[1, 512, 4096]"
    # t286 = prims.mul(t284, t285)  # t286: "cuda:0 f32[1, 512, 4096]"
  t287 = prims.convert_element_type(t286, dtypes.bfloat16)  # t287: "cuda:0 bf16[1, 512, 4096]"
  t288 = prims.linear(t287, t_transformer_h_2_attn_attn_weight, None)  # t288: "cuda:0 bf16[1, 512, 12288]"
  t294 = prims.reshape(t288, (1, 512, 32, 3, 128))  # t294: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t300 = prims.transpose(t294, (0, 2, 3, 1, 4))  # t300: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t301, t302, t303) = ltorch.split(t300, (1, 1, 1), 2)
    # t301 = prims.slice_prim(t300, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t301: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t302 = prims.slice_prim(t300, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t302: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t303 = prims.slice_prim(t300, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t303: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t309 = prims.reshape(t301, (1, 32, 512, 128))  # t309: "cuda:0 bf16[1, 32, 512, 128]"
  t315 = prims.reshape(t302, (1, 32, 512, 128))  # t315: "cuda:0 bf16[1, 32, 512, 128]"
  t321 = prims.reshape(t303, (1, 32, 512, 128))  # t321: "cuda:0 bf16[1, 32, 512, 128]"
  t322 = prims.slice_prim(t309, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t322: "cuda:0 bf16[1, 32, 512, 128]"
  t323 = prims.slice_prim(t322, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t323: "cuda:0 bf16[1, 32, 512, 64]"
  t324 = prims.slice_prim(t322, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t324: "cuda:0 bf16[1, 32, 512, 64]"
  t325 = prims.convert_element_type(t324, dtypes.float32)  # t325: "cuda:0 f32[1, 32, 512, 64]"
  t326 = prims.neg(t325)  # t326: "cuda:0 f32[1, 32, 512, 64]"
  t327 = prims.convert_element_type(t326, dtypes.bfloat16)  # t327: "cuda:0 bf16[1, 32, 512, 64]"
  t329 = prims.cat((t327, t323), -1)  # t329: "cuda:0 bf16[1, 32, 512, 128]"
  t330 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t330: "cuda:0 f32[1, 32, 512, 128]"
  t331 = prims.convert_element_type(t322, dtypes.float32)  # t331: "cuda:0 f32[1, 32, 512, 128]"
  t332 = ltorch.mul(t331, t330)  # t332: "cuda:0 f32[1, 32, 512, 128]"
    # t332 = prims.mul(t331, t330)  # t332: "cuda:0 f32[1, 32, 512, 128]"
  t333 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t333: "cuda:0 f32[1, 32, 512, 128]"
  t334 = prims.convert_element_type(t329, dtypes.float32)  # t334: "cuda:0 f32[1, 32, 512, 128]"
  t335 = ltorch.mul(t334, t333)  # t335: "cuda:0 f32[1, 32, 512, 128]"
    # t335 = prims.mul(t334, t333)  # t335: "cuda:0 f32[1, 32, 512, 128]"
  t336 = ltorch.add(t332, t335, alpha=None)  # t336: "cuda:0 f32[1, 32, 512, 128]"
    # t336 = prims.add(t332, t335)  # t336: "cuda:0 f32[1, 32, 512, 128]"
  t337 = prims.convert_element_type(t336, dtypes.bfloat16)  # t337: "cuda:0 bf16[1, 32, 512, 128]"
  t338 = prims.slice_prim(t315, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t338: "cuda:0 bf16[1, 32, 512, 128]"
  t339 = prims.slice_prim(t338, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t339: "cuda:0 bf16[1, 32, 512, 64]"
  t340 = prims.slice_prim(t338, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t340: "cuda:0 bf16[1, 32, 512, 64]"
  t341 = prims.convert_element_type(t340, dtypes.float32)  # t341: "cuda:0 f32[1, 32, 512, 64]"
  t342 = prims.neg(t341)  # t342: "cuda:0 f32[1, 32, 512, 64]"
  t343 = prims.convert_element_type(t342, dtypes.bfloat16)  # t343: "cuda:0 bf16[1, 32, 512, 64]"
  t345 = prims.cat((t343, t339), -1)  # t345: "cuda:0 bf16[1, 32, 512, 128]"
  t346 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t346: "cuda:0 f32[1, 32, 512, 128]"
  t347 = prims.convert_element_type(t338, dtypes.float32)  # t347: "cuda:0 f32[1, 32, 512, 128]"
  t348 = ltorch.mul(t347, t346)  # t348: "cuda:0 f32[1, 32, 512, 128]"
    # t348 = prims.mul(t347, t346)  # t348: "cuda:0 f32[1, 32, 512, 128]"
  t349 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t349: "cuda:0 f32[1, 32, 512, 128]"
  t350 = prims.convert_element_type(t345, dtypes.float32)  # t350: "cuda:0 f32[1, 32, 512, 128]"
  t351 = ltorch.mul(t350, t349)  # t351: "cuda:0 f32[1, 32, 512, 128]"
    # t351 = prims.mul(t350, t349)  # t351: "cuda:0 f32[1, 32, 512, 128]"
  t352 = ltorch.add(t348, t351, alpha=None)  # t352: "cuda:0 f32[1, 32, 512, 128]"
    # t352 = prims.add(t348, t351)  # t352: "cuda:0 f32[1, 32, 512, 128]"
  t353 = prims.convert_element_type(t352, dtypes.bfloat16)  # t353: "cuda:0 bf16[1, 32, 512, 128]"
  t354 = prims.slice_prim(t309, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t354: "cuda:0 bf16[1, 32, 512, 0]"
  t356 = prims.cat((t337, t354), -1)  # t356: "cuda:0 bf16[1, 32, 512, 128]"
  t357 = prims.slice_prim(t315, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t357: "cuda:0 bf16[1, 32, 512, 0]"
  t359 = prims.cat((t353, t357), -1)  # t359: "cuda:0 bf16[1, 32, 512, 128]"
  (t360, t361, t362, t363) = cudnn_sdpa_fwd(t356, t359, t321, None, 0.0, True, scale=0.08838834764831843)
  t366 = prims.transpose(t360, (0, 2, 1, 3))  # t366: "cuda:0 bf16[1, 512, 32, 128]"
  t370 = prims.reshape(t366, (1, 512, 4096))  # t370: "cuda:0 bf16[1, 512, 4096]"
  t371 = prims.linear(t370, t_transformer_h_2_attn_proj_weight, None)  # t371: "cuda:0 bf16[1, 512, 4096]"
  t372 = prims.convert_element_type(t371, dtypes.float32)  # t372: "cuda:0 f32[1, 512, 4096]"
  t373 = prims.convert_element_type(t269, dtypes.float32)  # t373: "cuda:0 f32[1, 512, 4096]"
  t374 = ltorch.add(t372, t373, alpha=None)  # t374: "cuda:0 f32[1, 512, 4096]"
    # t374 = prims.add(t372, t373)  # t374: "cuda:0 f32[1, 512, 4096]"
  t375 = prims.convert_element_type(t374, dtypes.bfloat16)  # t375: "cuda:0 bf16[1, 512, 4096]"
  t376 = prims.convert_element_type(t375, dtypes.float32)  # t376: "cuda:0 f32[1, 512, 4096]"
  t377 = ltorch.mul(t376, t376)  # t377: "cuda:0 f32[1, 512, 4096]"
    # t377 = prims.mul(t376, t376)  # t377: "cuda:0 f32[1, 512, 4096]"
  t379 = prims.sum(t377, (2,))  # t379: "cuda:0 f32[1, 512]"
  t380 = prims.broadcast_in_dim(t379, [1, 512, 1], [0, 1])  # t380: "cuda:0 f32[1, 512, 1]"
  t382 = ltorch.true_divide(t380, 4096.0)  # t382: "cuda:0 f32[1, 512, 1]"
    # t382 = prims.div(t380, 4096.0)  # t382: "cuda:0 f32[1, 512, 1]"
  t384 = ltorch.add(t382, 1e-05, alpha=None)  # t384: "cuda:0 f32[1, 512, 1]"
    # t384 = prims.add(t382, 1e-05)  # t384: "cuda:0 f32[1, 512, 1]"
  t385 = prims.rsqrt(t384)  # t385: "cuda:0 f32[1, 512, 1]"
  t386 = prims.broadcast_in_dim(t385, (1, 512, 4096), (0, 1, 2))  # t386: "cuda:0 f32[1, 512, 4096]"
  t387 = ltorch.mul(t376, t386)  # t387: "cuda:0 f32[1, 512, 4096]"
    # t387 = prims.mul(t376, t386)  # t387: "cuda:0 f32[1, 512, 4096]"
  t388 = prims.convert_element_type(t387, dtypes.bfloat16)  # t388: "cuda:0 bf16[1, 512, 4096]"
  t389 = prims.broadcast_in_dim(t_transformer_h_2_norm_2_weight, (1, 512, 4096), (2,))  # t389: "cuda:0 bf16[1, 512, 4096]"
  t390 = prims.convert_element_type(t388, dtypes.float32)  # t390: "cuda:0 f32[1, 512, 4096]"
  t391 = prims.convert_element_type(t389, dtypes.float32)  # t391: "cuda:0 f32[1, 512, 4096]"
  t392 = ltorch.mul(t390, t391)  # t392: "cuda:0 f32[1, 512, 4096]"
    # t392 = prims.mul(t390, t391)  # t392: "cuda:0 f32[1, 512, 4096]"
  t393 = prims.convert_element_type(t392, dtypes.bfloat16)  # t393: "cuda:0 bf16[1, 512, 4096]"
  t394 = prims.linear(t393, t_transformer_h_2_mlp_fc_1_weight, None)  # t394: "cuda:0 bf16[1, 512, 11008]"
  t395 = prims.linear(t393, t_transformer_h_2_mlp_fc_2_weight, None)  # t395: "cuda:0 bf16[1, 512, 11008]"
  t396 = prims.convert_element_type(t394, dtypes.float32)  # t396: "cuda:0 f32[1, 512, 11008]"
  t397 = prims.neg(t396)  # t397: "cuda:0 f32[1, 512, 11008]"
  t398 = prims.exp(t397)  # t398: "cuda:0 f32[1, 512, 11008]"
  t399 = ltorch.add(1.0, t398, alpha=None)  # t399: "cuda:0 f32[1, 512, 11008]"
    # t399 = prims.add(1.0, t398)  # t399: "cuda:0 f32[1, 512, 11008]"
  t400 = prims.reciprocal(t399)  # t400: "cuda:0 f32[1, 512, 11008]"
  t401 = prims.convert_element_type(t400, dtypes.bfloat16)  # t401: "cuda:0 bf16[1, 512, 11008]"
  t402 = prims.convert_element_type(t394, dtypes.float32)  # t402: "cuda:0 f32[1, 512, 11008]"
  t403 = prims.convert_element_type(t401, dtypes.float32)  # t403: "cuda:0 f32[1, 512, 11008]"
  t404 = ltorch.mul(t402, t403)  # t404: "cuda:0 f32[1, 512, 11008]"
    # t404 = prims.mul(t402, t403)  # t404: "cuda:0 f32[1, 512, 11008]"
  t405 = prims.convert_element_type(t404, dtypes.bfloat16)  # t405: "cuda:0 bf16[1, 512, 11008]"
  t406 = prims.convert_element_type(t405, dtypes.float32)  # t406: "cuda:0 f32[1, 512, 11008]"
  t407 = prims.convert_element_type(t395, dtypes.float32)  # t407: "cuda:0 f32[1, 512, 11008]"
  t408 = ltorch.mul(t406, t407)  # t408: "cuda:0 f32[1, 512, 11008]"
    # t408 = prims.mul(t406, t407)  # t408: "cuda:0 f32[1, 512, 11008]"
  t409 = prims.convert_element_type(t408, dtypes.bfloat16)  # t409: "cuda:0 bf16[1, 512, 11008]"
  t410 = prims.linear(t409, t_transformer_h_2_mlp_proj_weight, None)  # t410: "cuda:0 bf16[1, 512, 4096]"
  t411 = prims.convert_element_type(t410, dtypes.float32)  # t411: "cuda:0 f32[1, 512, 4096]"
  t412 = prims.convert_element_type(t375, dtypes.float32)  # t412: "cuda:0 f32[1, 512, 4096]"
  t413 = ltorch.add(t411, t412, alpha=None)  # t413: "cuda:0 f32[1, 512, 4096]"
    # t413 = prims.add(t411, t412)  # t413: "cuda:0 f32[1, 512, 4096]"
  t414 = prims.convert_element_type(t413, dtypes.bfloat16)  # t414: "cuda:0 bf16[1, 512, 4096]"
  t415 = prims.convert_element_type(t414, dtypes.float32)  # t415: "cuda:0 f32[1, 512, 4096]"
  t416 = ltorch.mul(t415, t415)  # t416: "cuda:0 f32[1, 512, 4096]"
    # t416 = prims.mul(t415, t415)  # t416: "cuda:0 f32[1, 512, 4096]"
  t418 = prims.sum(t416, (2,))  # t418: "cuda:0 f32[1, 512]"
  t419 = prims.broadcast_in_dim(t418, [1, 512, 1], [0, 1])  # t419: "cuda:0 f32[1, 512, 1]"
  t421 = ltorch.true_divide(t419, 4096.0)  # t421: "cuda:0 f32[1, 512, 1]"
    # t421 = prims.div(t419, 4096.0)  # t421: "cuda:0 f32[1, 512, 1]"
  t423 = ltorch.add(t421, 1e-05, alpha=None)  # t423: "cuda:0 f32[1, 512, 1]"
    # t423 = prims.add(t421, 1e-05)  # t423: "cuda:0 f32[1, 512, 1]"
  t424 = prims.rsqrt(t423)  # t424: "cuda:0 f32[1, 512, 1]"
  t425 = prims.broadcast_in_dim(t424, (1, 512, 4096), (0, 1, 2))  # t425: "cuda:0 f32[1, 512, 4096]"
  t426 = ltorch.mul(t415, t425)  # t426: "cuda:0 f32[1, 512, 4096]"
    # t426 = prims.mul(t415, t425)  # t426: "cuda:0 f32[1, 512, 4096]"
  t427 = prims.convert_element_type(t426, dtypes.bfloat16)  # t427: "cuda:0 bf16[1, 512, 4096]"
  t428 = prims.broadcast_in_dim(t_transformer_h_3_norm_1_weight, (1, 512, 4096), (2,))  # t428: "cuda:0 bf16[1, 512, 4096]"
  t429 = prims.convert_element_type(t427, dtypes.float32)  # t429: "cuda:0 f32[1, 512, 4096]"
  t430 = prims.convert_element_type(t428, dtypes.float32)  # t430: "cuda:0 f32[1, 512, 4096]"
  t431 = ltorch.mul(t429, t430)  # t431: "cuda:0 f32[1, 512, 4096]"
    # t431 = prims.mul(t429, t430)  # t431: "cuda:0 f32[1, 512, 4096]"
  t432 = prims.convert_element_type(t431, dtypes.bfloat16)  # t432: "cuda:0 bf16[1, 512, 4096]"
  t433 = prims.linear(t432, t_transformer_h_3_attn_attn_weight, None)  # t433: "cuda:0 bf16[1, 512, 12288]"
  t439 = prims.reshape(t433, (1, 512, 32, 3, 128))  # t439: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t445 = prims.transpose(t439, (0, 2, 3, 1, 4))  # t445: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t446, t447, t448) = ltorch.split(t445, (1, 1, 1), 2)
    # t446 = prims.slice_prim(t445, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t446: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t447 = prims.slice_prim(t445, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t447: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t448 = prims.slice_prim(t445, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t448: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t454 = prims.reshape(t446, (1, 32, 512, 128))  # t454: "cuda:0 bf16[1, 32, 512, 128]"
  t460 = prims.reshape(t447, (1, 32, 512, 128))  # t460: "cuda:0 bf16[1, 32, 512, 128]"
  t466 = prims.reshape(t448, (1, 32, 512, 128))  # t466: "cuda:0 bf16[1, 32, 512, 128]"
  t467 = prims.slice_prim(t454, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t467: "cuda:0 bf16[1, 32, 512, 128]"
  t468 = prims.slice_prim(t467, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t468: "cuda:0 bf16[1, 32, 512, 64]"
  t469 = prims.slice_prim(t467, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t469: "cuda:0 bf16[1, 32, 512, 64]"
  t470 = prims.convert_element_type(t469, dtypes.float32)  # t470: "cuda:0 f32[1, 32, 512, 64]"
  t471 = prims.neg(t470)  # t471: "cuda:0 f32[1, 32, 512, 64]"
  t472 = prims.convert_element_type(t471, dtypes.bfloat16)  # t472: "cuda:0 bf16[1, 32, 512, 64]"
  t474 = prims.cat((t472, t468), -1)  # t474: "cuda:0 bf16[1, 32, 512, 128]"
  t475 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t475: "cuda:0 f32[1, 32, 512, 128]"
  t476 = prims.convert_element_type(t467, dtypes.float32)  # t476: "cuda:0 f32[1, 32, 512, 128]"
  t477 = ltorch.mul(t476, t475)  # t477: "cuda:0 f32[1, 32, 512, 128]"
    # t477 = prims.mul(t476, t475)  # t477: "cuda:0 f32[1, 32, 512, 128]"
  t478 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t478: "cuda:0 f32[1, 32, 512, 128]"
  t479 = prims.convert_element_type(t474, dtypes.float32)  # t479: "cuda:0 f32[1, 32, 512, 128]"
  t480 = ltorch.mul(t479, t478)  # t480: "cuda:0 f32[1, 32, 512, 128]"
    # t480 = prims.mul(t479, t478)  # t480: "cuda:0 f32[1, 32, 512, 128]"
  t481 = ltorch.add(t477, t480, alpha=None)  # t481: "cuda:0 f32[1, 32, 512, 128]"
    # t481 = prims.add(t477, t480)  # t481: "cuda:0 f32[1, 32, 512, 128]"
  t482 = prims.convert_element_type(t481, dtypes.bfloat16)  # t482: "cuda:0 bf16[1, 32, 512, 128]"
  t483 = prims.slice_prim(t460, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t483: "cuda:0 bf16[1, 32, 512, 128]"
  t484 = prims.slice_prim(t483, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t484: "cuda:0 bf16[1, 32, 512, 64]"
  t485 = prims.slice_prim(t483, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t485: "cuda:0 bf16[1, 32, 512, 64]"
  t486 = prims.convert_element_type(t485, dtypes.float32)  # t486: "cuda:0 f32[1, 32, 512, 64]"
  t487 = prims.neg(t486)  # t487: "cuda:0 f32[1, 32, 512, 64]"
  t488 = prims.convert_element_type(t487, dtypes.bfloat16)  # t488: "cuda:0 bf16[1, 32, 512, 64]"
  t490 = prims.cat((t488, t484), -1)  # t490: "cuda:0 bf16[1, 32, 512, 128]"
  t491 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t491: "cuda:0 f32[1, 32, 512, 128]"
  t492 = prims.convert_element_type(t483, dtypes.float32)  # t492: "cuda:0 f32[1, 32, 512, 128]"
  t493 = ltorch.mul(t492, t491)  # t493: "cuda:0 f32[1, 32, 512, 128]"
    # t493 = prims.mul(t492, t491)  # t493: "cuda:0 f32[1, 32, 512, 128]"
  t494 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t494: "cuda:0 f32[1, 32, 512, 128]"
  t495 = prims.convert_element_type(t490, dtypes.float32)  # t495: "cuda:0 f32[1, 32, 512, 128]"
  t496 = ltorch.mul(t495, t494)  # t496: "cuda:0 f32[1, 32, 512, 128]"
    # t496 = prims.mul(t495, t494)  # t496: "cuda:0 f32[1, 32, 512, 128]"
  t497 = ltorch.add(t493, t496, alpha=None)  # t497: "cuda:0 f32[1, 32, 512, 128]"
    # t497 = prims.add(t493, t496)  # t497: "cuda:0 f32[1, 32, 512, 128]"
  t498 = prims.convert_element_type(t497, dtypes.bfloat16)  # t498: "cuda:0 bf16[1, 32, 512, 128]"
  t499 = prims.slice_prim(t454, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t499: "cuda:0 bf16[1, 32, 512, 0]"
  t501 = prims.cat((t482, t499), -1)  # t501: "cuda:0 bf16[1, 32, 512, 128]"
  t502 = prims.slice_prim(t460, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t502: "cuda:0 bf16[1, 32, 512, 0]"
  t504 = prims.cat((t498, t502), -1)  # t504: "cuda:0 bf16[1, 32, 512, 128]"
  (t505, t506, t507, t508) = cudnn_sdpa_fwd(t501, t504, t466, None, 0.0, True, scale=0.08838834764831843)
  t511 = prims.transpose(t505, (0, 2, 1, 3))  # t511: "cuda:0 bf16[1, 512, 32, 128]"
  t515 = prims.reshape(t511, (1, 512, 4096))  # t515: "cuda:0 bf16[1, 512, 4096]"
  t516 = prims.linear(t515, t_transformer_h_3_attn_proj_weight, None)  # t516: "cuda:0 bf16[1, 512, 4096]"
  t517 = prims.convert_element_type(t516, dtypes.float32)  # t517: "cuda:0 f32[1, 512, 4096]"
  t518 = prims.convert_element_type(t414, dtypes.float32)  # t518: "cuda:0 f32[1, 512, 4096]"
  t519 = ltorch.add(t517, t518, alpha=None)  # t519: "cuda:0 f32[1, 512, 4096]"
    # t519 = prims.add(t517, t518)  # t519: "cuda:0 f32[1, 512, 4096]"
  t520 = prims.convert_element_type(t519, dtypes.bfloat16)  # t520: "cuda:0 bf16[1, 512, 4096]"
  t521 = prims.convert_element_type(t520, dtypes.float32)  # t521: "cuda:0 f32[1, 512, 4096]"
  t522 = ltorch.mul(t521, t521)  # t522: "cuda:0 f32[1, 512, 4096]"
    # t522 = prims.mul(t521, t521)  # t522: "cuda:0 f32[1, 512, 4096]"
  t524 = prims.sum(t522, (2,))  # t524: "cuda:0 f32[1, 512]"
  t525 = prims.broadcast_in_dim(t524, [1, 512, 1], [0, 1])  # t525: "cuda:0 f32[1, 512, 1]"
  t527 = ltorch.true_divide(t525, 4096.0)  # t527: "cuda:0 f32[1, 512, 1]"
    # t527 = prims.div(t525, 4096.0)  # t527: "cuda:0 f32[1, 512, 1]"
  t529 = ltorch.add(t527, 1e-05, alpha=None)  # t529: "cuda:0 f32[1, 512, 1]"
    # t529 = prims.add(t527, 1e-05)  # t529: "cuda:0 f32[1, 512, 1]"
  t530 = prims.rsqrt(t529)  # t530: "cuda:0 f32[1, 512, 1]"
  t531 = prims.broadcast_in_dim(t530, (1, 512, 4096), (0, 1, 2))  # t531: "cuda:0 f32[1, 512, 4096]"
  t532 = ltorch.mul(t521, t531)  # t532: "cuda:0 f32[1, 512, 4096]"
    # t532 = prims.mul(t521, t531)  # t532: "cuda:0 f32[1, 512, 4096]"
  t533 = prims.convert_element_type(t532, dtypes.bfloat16)  # t533: "cuda:0 bf16[1, 512, 4096]"
  t534 = prims.broadcast_in_dim(t_transformer_h_3_norm_2_weight, (1, 512, 4096), (2,))  # t534: "cuda:0 bf16[1, 512, 4096]"
  t535 = prims.convert_element_type(t533, dtypes.float32)  # t535: "cuda:0 f32[1, 512, 4096]"
  t536 = prims.convert_element_type(t534, dtypes.float32)  # t536: "cuda:0 f32[1, 512, 4096]"
  t537 = ltorch.mul(t535, t536)  # t537: "cuda:0 f32[1, 512, 4096]"
    # t537 = prims.mul(t535, t536)  # t537: "cuda:0 f32[1, 512, 4096]"
  t538 = prims.convert_element_type(t537, dtypes.bfloat16)  # t538: "cuda:0 bf16[1, 512, 4096]"
  t539 = prims.linear(t538, t_transformer_h_3_mlp_fc_1_weight, None)  # t539: "cuda:0 bf16[1, 512, 11008]"
  t540 = prims.linear(t538, t_transformer_h_3_mlp_fc_2_weight, None)  # t540: "cuda:0 bf16[1, 512, 11008]"
  t541 = prims.convert_element_type(t539, dtypes.float32)  # t541: "cuda:0 f32[1, 512, 11008]"
  t542 = prims.neg(t541)  # t542: "cuda:0 f32[1, 512, 11008]"
  t543 = prims.exp(t542)  # t543: "cuda:0 f32[1, 512, 11008]"
  t544 = ltorch.add(1.0, t543, alpha=None)  # t544: "cuda:0 f32[1, 512, 11008]"
    # t544 = prims.add(1.0, t543)  # t544: "cuda:0 f32[1, 512, 11008]"
  t545 = prims.reciprocal(t544)  # t545: "cuda:0 f32[1, 512, 11008]"
  t546 = prims.convert_element_type(t545, dtypes.bfloat16)  # t546: "cuda:0 bf16[1, 512, 11008]"
  t547 = prims.convert_element_type(t539, dtypes.float32)  # t547: "cuda:0 f32[1, 512, 11008]"
  t548 = prims.convert_element_type(t546, dtypes.float32)  # t548: "cuda:0 f32[1, 512, 11008]"
  t549 = ltorch.mul(t547, t548)  # t549: "cuda:0 f32[1, 512, 11008]"
    # t549 = prims.mul(t547, t548)  # t549: "cuda:0 f32[1, 512, 11008]"
  t550 = prims.convert_element_type(t549, dtypes.bfloat16)  # t550: "cuda:0 bf16[1, 512, 11008]"
  t551 = prims.convert_element_type(t550, dtypes.float32)  # t551: "cuda:0 f32[1, 512, 11008]"
  t552 = prims.convert_element_type(t540, dtypes.float32)  # t552: "cuda:0 f32[1, 512, 11008]"
  t553 = ltorch.mul(t551, t552)  # t553: "cuda:0 f32[1, 512, 11008]"
    # t553 = prims.mul(t551, t552)  # t553: "cuda:0 f32[1, 512, 11008]"
  t554 = prims.convert_element_type(t553, dtypes.bfloat16)  # t554: "cuda:0 bf16[1, 512, 11008]"
  t555 = prims.linear(t554, t_transformer_h_3_mlp_proj_weight, None)  # t555: "cuda:0 bf16[1, 512, 4096]"
  t556 = prims.convert_element_type(t555, dtypes.float32)  # t556: "cuda:0 f32[1, 512, 4096]"
  t557 = prims.convert_element_type(t520, dtypes.float32)  # t557: "cuda:0 f32[1, 512, 4096]"
  t558 = ltorch.add(t556, t557, alpha=None)  # t558: "cuda:0 f32[1, 512, 4096]"
    # t558 = prims.add(t556, t557)  # t558: "cuda:0 f32[1, 512, 4096]"
  t559 = prims.convert_element_type(t558, dtypes.bfloat16)  # t559: "cuda:0 bf16[1, 512, 4096]"
  t560 = prims.convert_element_type(t559, dtypes.float32)  # t560: "cuda:0 f32[1, 512, 4096]"
  t561 = ltorch.mul(t560, t560)  # t561: "cuda:0 f32[1, 512, 4096]"
    # t561 = prims.mul(t560, t560)  # t561: "cuda:0 f32[1, 512, 4096]"
  t563 = prims.sum(t561, (2,))  # t563: "cuda:0 f32[1, 512]"
  t564 = prims.broadcast_in_dim(t563, [1, 512, 1], [0, 1])  # t564: "cuda:0 f32[1, 512, 1]"
  t566 = ltorch.true_divide(t564, 4096.0)  # t566: "cuda:0 f32[1, 512, 1]"
    # t566 = prims.div(t564, 4096.0)  # t566: "cuda:0 f32[1, 512, 1]"
  t568 = ltorch.add(t566, 1e-05, alpha=None)  # t568: "cuda:0 f32[1, 512, 1]"
    # t568 = prims.add(t566, 1e-05)  # t568: "cuda:0 f32[1, 512, 1]"
  t569 = prims.rsqrt(t568)  # t569: "cuda:0 f32[1, 512, 1]"
  t570 = prims.broadcast_in_dim(t569, (1, 512, 4096), (0, 1, 2))  # t570: "cuda:0 f32[1, 512, 4096]"
  t571 = ltorch.mul(t560, t570)  # t571: "cuda:0 f32[1, 512, 4096]"
    # t571 = prims.mul(t560, t570)  # t571: "cuda:0 f32[1, 512, 4096]"
  t572 = prims.convert_element_type(t571, dtypes.bfloat16)  # t572: "cuda:0 bf16[1, 512, 4096]"
  t573 = prims.broadcast_in_dim(t_transformer_h_4_norm_1_weight, (1, 512, 4096), (2,))  # t573: "cuda:0 bf16[1, 512, 4096]"
  t574 = prims.convert_element_type(t572, dtypes.float32)  # t574: "cuda:0 f32[1, 512, 4096]"
  t575 = prims.convert_element_type(t573, dtypes.float32)  # t575: "cuda:0 f32[1, 512, 4096]"
  t576 = ltorch.mul(t574, t575)  # t576: "cuda:0 f32[1, 512, 4096]"
    # t576 = prims.mul(t574, t575)  # t576: "cuda:0 f32[1, 512, 4096]"
  t577 = prims.convert_element_type(t576, dtypes.bfloat16)  # t577: "cuda:0 bf16[1, 512, 4096]"
  t578 = prims.linear(t577, t_transformer_h_4_attn_attn_weight, None)  # t578: "cuda:0 bf16[1, 512, 12288]"
  t584 = prims.reshape(t578, (1, 512, 32, 3, 128))  # t584: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t590 = prims.transpose(t584, (0, 2, 3, 1, 4))  # t590: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t591, t592, t593) = ltorch.split(t590, (1, 1, 1), 2)
    # t591 = prims.slice_prim(t590, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t591: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t592 = prims.slice_prim(t590, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t592: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t593 = prims.slice_prim(t590, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t593: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t599 = prims.reshape(t591, (1, 32, 512, 128))  # t599: "cuda:0 bf16[1, 32, 512, 128]"
  t605 = prims.reshape(t592, (1, 32, 512, 128))  # t605: "cuda:0 bf16[1, 32, 512, 128]"
  t611 = prims.reshape(t593, (1, 32, 512, 128))  # t611: "cuda:0 bf16[1, 32, 512, 128]"
  t612 = prims.slice_prim(t599, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t612: "cuda:0 bf16[1, 32, 512, 128]"
  t613 = prims.slice_prim(t612, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t613: "cuda:0 bf16[1, 32, 512, 64]"
  t614 = prims.slice_prim(t612, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t614: "cuda:0 bf16[1, 32, 512, 64]"
  t615 = prims.convert_element_type(t614, dtypes.float32)  # t615: "cuda:0 f32[1, 32, 512, 64]"
  t616 = prims.neg(t615)  # t616: "cuda:0 f32[1, 32, 512, 64]"
  t617 = prims.convert_element_type(t616, dtypes.bfloat16)  # t617: "cuda:0 bf16[1, 32, 512, 64]"
  t619 = prims.cat((t617, t613), -1)  # t619: "cuda:0 bf16[1, 32, 512, 128]"
  t620 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t620: "cuda:0 f32[1, 32, 512, 128]"
  t621 = prims.convert_element_type(t612, dtypes.float32)  # t621: "cuda:0 f32[1, 32, 512, 128]"
  t622 = ltorch.mul(t621, t620)  # t622: "cuda:0 f32[1, 32, 512, 128]"
    # t622 = prims.mul(t621, t620)  # t622: "cuda:0 f32[1, 32, 512, 128]"
  t623 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t623: "cuda:0 f32[1, 32, 512, 128]"
  t624 = prims.convert_element_type(t619, dtypes.float32)  # t624: "cuda:0 f32[1, 32, 512, 128]"
  t625 = ltorch.mul(t624, t623)  # t625: "cuda:0 f32[1, 32, 512, 128]"
    # t625 = prims.mul(t624, t623)  # t625: "cuda:0 f32[1, 32, 512, 128]"
  t626 = ltorch.add(t622, t625, alpha=None)  # t626: "cuda:0 f32[1, 32, 512, 128]"
    # t626 = prims.add(t622, t625)  # t626: "cuda:0 f32[1, 32, 512, 128]"
  t627 = prims.convert_element_type(t626, dtypes.bfloat16)  # t627: "cuda:0 bf16[1, 32, 512, 128]"
  t628 = prims.slice_prim(t605, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t628: "cuda:0 bf16[1, 32, 512, 128]"
  t629 = prims.slice_prim(t628, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t629: "cuda:0 bf16[1, 32, 512, 64]"
  t630 = prims.slice_prim(t628, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t630: "cuda:0 bf16[1, 32, 512, 64]"
  t631 = prims.convert_element_type(t630, dtypes.float32)  # t631: "cuda:0 f32[1, 32, 512, 64]"
  t632 = prims.neg(t631)  # t632: "cuda:0 f32[1, 32, 512, 64]"
  t633 = prims.convert_element_type(t632, dtypes.bfloat16)  # t633: "cuda:0 bf16[1, 32, 512, 64]"
  t635 = prims.cat((t633, t629), -1)  # t635: "cuda:0 bf16[1, 32, 512, 128]"
  t636 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t636: "cuda:0 f32[1, 32, 512, 128]"
  t637 = prims.convert_element_type(t628, dtypes.float32)  # t637: "cuda:0 f32[1, 32, 512, 128]"
  t638 = ltorch.mul(t637, t636)  # t638: "cuda:0 f32[1, 32, 512, 128]"
    # t638 = prims.mul(t637, t636)  # t638: "cuda:0 f32[1, 32, 512, 128]"
  t639 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t639: "cuda:0 f32[1, 32, 512, 128]"
  t640 = prims.convert_element_type(t635, dtypes.float32)  # t640: "cuda:0 f32[1, 32, 512, 128]"
  t641 = ltorch.mul(t640, t639)  # t641: "cuda:0 f32[1, 32, 512, 128]"
    # t641 = prims.mul(t640, t639)  # t641: "cuda:0 f32[1, 32, 512, 128]"
  t642 = ltorch.add(t638, t641, alpha=None)  # t642: "cuda:0 f32[1, 32, 512, 128]"
    # t642 = prims.add(t638, t641)  # t642: "cuda:0 f32[1, 32, 512, 128]"
  t643 = prims.convert_element_type(t642, dtypes.bfloat16)  # t643: "cuda:0 bf16[1, 32, 512, 128]"
  t644 = prims.slice_prim(t599, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t644: "cuda:0 bf16[1, 32, 512, 0]"
  t646 = prims.cat((t627, t644), -1)  # t646: "cuda:0 bf16[1, 32, 512, 128]"
  t647 = prims.slice_prim(t605, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t647: "cuda:0 bf16[1, 32, 512, 0]"
  t649 = prims.cat((t643, t647), -1)  # t649: "cuda:0 bf16[1, 32, 512, 128]"
  (t650, t651, t652, t653) = cudnn_sdpa_fwd(t646, t649, t611, None, 0.0, True, scale=0.08838834764831843)
  t656 = prims.transpose(t650, (0, 2, 1, 3))  # t656: "cuda:0 bf16[1, 512, 32, 128]"
  t660 = prims.reshape(t656, (1, 512, 4096))  # t660: "cuda:0 bf16[1, 512, 4096]"
  t661 = prims.linear(t660, t_transformer_h_4_attn_proj_weight, None)  # t661: "cuda:0 bf16[1, 512, 4096]"
  t662 = prims.convert_element_type(t661, dtypes.float32)  # t662: "cuda:0 f32[1, 512, 4096]"
  t663 = prims.convert_element_type(t559, dtypes.float32)  # t663: "cuda:0 f32[1, 512, 4096]"
  t664 = ltorch.add(t662, t663, alpha=None)  # t664: "cuda:0 f32[1, 512, 4096]"
    # t664 = prims.add(t662, t663)  # t664: "cuda:0 f32[1, 512, 4096]"
  t665 = prims.convert_element_type(t664, dtypes.bfloat16)  # t665: "cuda:0 bf16[1, 512, 4096]"
  t666 = prims.convert_element_type(t665, dtypes.float32)  # t666: "cuda:0 f32[1, 512, 4096]"
  t667 = ltorch.mul(t666, t666)  # t667: "cuda:0 f32[1, 512, 4096]"
    # t667 = prims.mul(t666, t666)  # t667: "cuda:0 f32[1, 512, 4096]"
  t669 = prims.sum(t667, (2,))  # t669: "cuda:0 f32[1, 512]"
  t670 = prims.broadcast_in_dim(t669, [1, 512, 1], [0, 1])  # t670: "cuda:0 f32[1, 512, 1]"
  t672 = ltorch.true_divide(t670, 4096.0)  # t672: "cuda:0 f32[1, 512, 1]"
    # t672 = prims.div(t670, 4096.0)  # t672: "cuda:0 f32[1, 512, 1]"
  t674 = ltorch.add(t672, 1e-05, alpha=None)  # t674: "cuda:0 f32[1, 512, 1]"
    # t674 = prims.add(t672, 1e-05)  # t674: "cuda:0 f32[1, 512, 1]"
  t675 = prims.rsqrt(t674)  # t675: "cuda:0 f32[1, 512, 1]"
  t676 = prims.broadcast_in_dim(t675, (1, 512, 4096), (0, 1, 2))  # t676: "cuda:0 f32[1, 512, 4096]"
  t677 = ltorch.mul(t666, t676)  # t677: "cuda:0 f32[1, 512, 4096]"
    # t677 = prims.mul(t666, t676)  # t677: "cuda:0 f32[1, 512, 4096]"
  t678 = prims.convert_element_type(t677, dtypes.bfloat16)  # t678: "cuda:0 bf16[1, 512, 4096]"
  t679 = prims.broadcast_in_dim(t_transformer_h_4_norm_2_weight, (1, 512, 4096), (2,))  # t679: "cuda:0 bf16[1, 512, 4096]"
  t680 = prims.convert_element_type(t678, dtypes.float32)  # t680: "cuda:0 f32[1, 512, 4096]"
  t681 = prims.convert_element_type(t679, dtypes.float32)  # t681: "cuda:0 f32[1, 512, 4096]"
  t682 = ltorch.mul(t680, t681)  # t682: "cuda:0 f32[1, 512, 4096]"
    # t682 = prims.mul(t680, t681)  # t682: "cuda:0 f32[1, 512, 4096]"
  t683 = prims.convert_element_type(t682, dtypes.bfloat16)  # t683: "cuda:0 bf16[1, 512, 4096]"
  t684 = prims.linear(t683, t_transformer_h_4_mlp_fc_1_weight, None)  # t684: "cuda:0 bf16[1, 512, 11008]"
  t685 = prims.linear(t683, t_transformer_h_4_mlp_fc_2_weight, None)  # t685: "cuda:0 bf16[1, 512, 11008]"
  t686 = prims.convert_element_type(t684, dtypes.float32)  # t686: "cuda:0 f32[1, 512, 11008]"
  t687 = prims.neg(t686)  # t687: "cuda:0 f32[1, 512, 11008]"
  t688 = prims.exp(t687)  # t688: "cuda:0 f32[1, 512, 11008]"
  t689 = ltorch.add(1.0, t688, alpha=None)  # t689: "cuda:0 f32[1, 512, 11008]"
    # t689 = prims.add(1.0, t688)  # t689: "cuda:0 f32[1, 512, 11008]"
  t690 = prims.reciprocal(t689)  # t690: "cuda:0 f32[1, 512, 11008]"
  t691 = prims.convert_element_type(t690, dtypes.bfloat16)  # t691: "cuda:0 bf16[1, 512, 11008]"
  t692 = prims.convert_element_type(t684, dtypes.float32)  # t692: "cuda:0 f32[1, 512, 11008]"
  t693 = prims.convert_element_type(t691, dtypes.float32)  # t693: "cuda:0 f32[1, 512, 11008]"
  t694 = ltorch.mul(t692, t693)  # t694: "cuda:0 f32[1, 512, 11008]"
    # t694 = prims.mul(t692, t693)  # t694: "cuda:0 f32[1, 512, 11008]"
  t695 = prims.convert_element_type(t694, dtypes.bfloat16)  # t695: "cuda:0 bf16[1, 512, 11008]"
  t696 = prims.convert_element_type(t695, dtypes.float32)  # t696: "cuda:0 f32[1, 512, 11008]"
  t697 = prims.convert_element_type(t685, dtypes.float32)  # t697: "cuda:0 f32[1, 512, 11008]"
  t698 = ltorch.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 11008]"
    # t698 = prims.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 11008]"
  t699 = prims.convert_element_type(t698, dtypes.bfloat16)  # t699: "cuda:0 bf16[1, 512, 11008]"
  t700 = prims.linear(t699, t_transformer_h_4_mlp_proj_weight, None)  # t700: "cuda:0 bf16[1, 512, 4096]"
  t701 = prims.convert_element_type(t700, dtypes.float32)  # t701: "cuda:0 f32[1, 512, 4096]"
  t702 = prims.convert_element_type(t665, dtypes.float32)  # t702: "cuda:0 f32[1, 512, 4096]"
  t703 = ltorch.add(t701, t702, alpha=None)  # t703: "cuda:0 f32[1, 512, 4096]"
    # t703 = prims.add(t701, t702)  # t703: "cuda:0 f32[1, 512, 4096]"
  t704 = prims.convert_element_type(t703, dtypes.bfloat16)  # t704: "cuda:0 bf16[1, 512, 4096]"
  t705 = prims.convert_element_type(t704, dtypes.float32)  # t705: "cuda:0 f32[1, 512, 4096]"
  t706 = ltorch.mul(t705, t705)  # t706: "cuda:0 f32[1, 512, 4096]"
    # t706 = prims.mul(t705, t705)  # t706: "cuda:0 f32[1, 512, 4096]"
  t708 = prims.sum(t706, (2,))  # t708: "cuda:0 f32[1, 512]"
  t709 = prims.broadcast_in_dim(t708, [1, 512, 1], [0, 1])  # t709: "cuda:0 f32[1, 512, 1]"
  t711 = ltorch.true_divide(t709, 4096.0)  # t711: "cuda:0 f32[1, 512, 1]"
    # t711 = prims.div(t709, 4096.0)  # t711: "cuda:0 f32[1, 512, 1]"
  t713 = ltorch.add(t711, 1e-05, alpha=None)  # t713: "cuda:0 f32[1, 512, 1]"
    # t713 = prims.add(t711, 1e-05)  # t713: "cuda:0 f32[1, 512, 1]"
  t714 = prims.rsqrt(t713)  # t714: "cuda:0 f32[1, 512, 1]"
  t715 = prims.broadcast_in_dim(t714, (1, 512, 4096), (0, 1, 2))  # t715: "cuda:0 f32[1, 512, 4096]"
  t716 = ltorch.mul(t705, t715)  # t716: "cuda:0 f32[1, 512, 4096]"
    # t716 = prims.mul(t705, t715)  # t716: "cuda:0 f32[1, 512, 4096]"
  t717 = prims.convert_element_type(t716, dtypes.bfloat16)  # t717: "cuda:0 bf16[1, 512, 4096]"
  t718 = prims.broadcast_in_dim(t_transformer_h_5_norm_1_weight, (1, 512, 4096), (2,))  # t718: "cuda:0 bf16[1, 512, 4096]"
  t719 = prims.convert_element_type(t717, dtypes.float32)  # t719: "cuda:0 f32[1, 512, 4096]"
  t720 = prims.convert_element_type(t718, dtypes.float32)  # t720: "cuda:0 f32[1, 512, 4096]"
  t721 = ltorch.mul(t719, t720)  # t721: "cuda:0 f32[1, 512, 4096]"
    # t721 = prims.mul(t719, t720)  # t721: "cuda:0 f32[1, 512, 4096]"
  t722 = prims.convert_element_type(t721, dtypes.bfloat16)  # t722: "cuda:0 bf16[1, 512, 4096]"
  t723 = prims.linear(t722, t_transformer_h_5_attn_attn_weight, None)  # t723: "cuda:0 bf16[1, 512, 12288]"
  t729 = prims.reshape(t723, (1, 512, 32, 3, 128))  # t729: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t735 = prims.transpose(t729, (0, 2, 3, 1, 4))  # t735: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t736, t737, t738) = ltorch.split(t735, (1, 1, 1), 2)
    # t736 = prims.slice_prim(t735, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t736: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t737 = prims.slice_prim(t735, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t737: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t738 = prims.slice_prim(t735, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t738: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t744 = prims.reshape(t736, (1, 32, 512, 128))  # t744: "cuda:0 bf16[1, 32, 512, 128]"
  t750 = prims.reshape(t737, (1, 32, 512, 128))  # t750: "cuda:0 bf16[1, 32, 512, 128]"
  t756 = prims.reshape(t738, (1, 32, 512, 128))  # t756: "cuda:0 bf16[1, 32, 512, 128]"
  t757 = prims.slice_prim(t744, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t757: "cuda:0 bf16[1, 32, 512, 128]"
  t758 = prims.slice_prim(t757, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t758: "cuda:0 bf16[1, 32, 512, 64]"
  t759 = prims.slice_prim(t757, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t759: "cuda:0 bf16[1, 32, 512, 64]"
  t760 = prims.convert_element_type(t759, dtypes.float32)  # t760: "cuda:0 f32[1, 32, 512, 64]"
  t761 = prims.neg(t760)  # t761: "cuda:0 f32[1, 32, 512, 64]"
  t762 = prims.convert_element_type(t761, dtypes.bfloat16)  # t762: "cuda:0 bf16[1, 32, 512, 64]"
  t764 = prims.cat((t762, t758), -1)  # t764: "cuda:0 bf16[1, 32, 512, 128]"
  t765 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t765: "cuda:0 f32[1, 32, 512, 128]"
  t766 = prims.convert_element_type(t757, dtypes.float32)  # t766: "cuda:0 f32[1, 32, 512, 128]"
  t767 = ltorch.mul(t766, t765)  # t767: "cuda:0 f32[1, 32, 512, 128]"
    # t767 = prims.mul(t766, t765)  # t767: "cuda:0 f32[1, 32, 512, 128]"
  t768 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t768: "cuda:0 f32[1, 32, 512, 128]"
  t769 = prims.convert_element_type(t764, dtypes.float32)  # t769: "cuda:0 f32[1, 32, 512, 128]"
  t770 = ltorch.mul(t769, t768)  # t770: "cuda:0 f32[1, 32, 512, 128]"
    # t770 = prims.mul(t769, t768)  # t770: "cuda:0 f32[1, 32, 512, 128]"
  t771 = ltorch.add(t767, t770, alpha=None)  # t771: "cuda:0 f32[1, 32, 512, 128]"
    # t771 = prims.add(t767, t770)  # t771: "cuda:0 f32[1, 32, 512, 128]"
  t772 = prims.convert_element_type(t771, dtypes.bfloat16)  # t772: "cuda:0 bf16[1, 32, 512, 128]"
  t773 = prims.slice_prim(t750, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t773: "cuda:0 bf16[1, 32, 512, 128]"
  t774 = prims.slice_prim(t773, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t774: "cuda:0 bf16[1, 32, 512, 64]"
  t775 = prims.slice_prim(t773, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t775: "cuda:0 bf16[1, 32, 512, 64]"
  t776 = prims.convert_element_type(t775, dtypes.float32)  # t776: "cuda:0 f32[1, 32, 512, 64]"
  t777 = prims.neg(t776)  # t777: "cuda:0 f32[1, 32, 512, 64]"
  t778 = prims.convert_element_type(t777, dtypes.bfloat16)  # t778: "cuda:0 bf16[1, 32, 512, 64]"
  t780 = prims.cat((t778, t774), -1)  # t780: "cuda:0 bf16[1, 32, 512, 128]"
  t781 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t781: "cuda:0 f32[1, 32, 512, 128]"
  t782 = prims.convert_element_type(t773, dtypes.float32)  # t782: "cuda:0 f32[1, 32, 512, 128]"
  t783 = ltorch.mul(t782, t781)  # t783: "cuda:0 f32[1, 32, 512, 128]"
    # t783 = prims.mul(t782, t781)  # t783: "cuda:0 f32[1, 32, 512, 128]"
  t784 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t784: "cuda:0 f32[1, 32, 512, 128]"
  t785 = prims.convert_element_type(t780, dtypes.float32)  # t785: "cuda:0 f32[1, 32, 512, 128]"
  t786 = ltorch.mul(t785, t784)  # t786: "cuda:0 f32[1, 32, 512, 128]"
    # t786 = prims.mul(t785, t784)  # t786: "cuda:0 f32[1, 32, 512, 128]"
  t787 = ltorch.add(t783, t786, alpha=None)  # t787: "cuda:0 f32[1, 32, 512, 128]"
    # t787 = prims.add(t783, t786)  # t787: "cuda:0 f32[1, 32, 512, 128]"
  t788 = prims.convert_element_type(t787, dtypes.bfloat16)  # t788: "cuda:0 bf16[1, 32, 512, 128]"
  t789 = prims.slice_prim(t744, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t789: "cuda:0 bf16[1, 32, 512, 0]"
  t791 = prims.cat((t772, t789), -1)  # t791: "cuda:0 bf16[1, 32, 512, 128]"
  t792 = prims.slice_prim(t750, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t792: "cuda:0 bf16[1, 32, 512, 0]"
  t794 = prims.cat((t788, t792), -1)  # t794: "cuda:0 bf16[1, 32, 512, 128]"
  (t795, t796, t797, t798) = cudnn_sdpa_fwd(t791, t794, t756, None, 0.0, True, scale=0.08838834764831843)
  t801 = prims.transpose(t795, (0, 2, 1, 3))  # t801: "cuda:0 bf16[1, 512, 32, 128]"
  t805 = prims.reshape(t801, (1, 512, 4096))  # t805: "cuda:0 bf16[1, 512, 4096]"
  t806 = prims.linear(t805, t_transformer_h_5_attn_proj_weight, None)  # t806: "cuda:0 bf16[1, 512, 4096]"
  t807 = prims.convert_element_type(t806, dtypes.float32)  # t807: "cuda:0 f32[1, 512, 4096]"
  t808 = prims.convert_element_type(t704, dtypes.float32)  # t808: "cuda:0 f32[1, 512, 4096]"
  t809 = ltorch.add(t807, t808, alpha=None)  # t809: "cuda:0 f32[1, 512, 4096]"
    # t809 = prims.add(t807, t808)  # t809: "cuda:0 f32[1, 512, 4096]"
  t810 = prims.convert_element_type(t809, dtypes.bfloat16)  # t810: "cuda:0 bf16[1, 512, 4096]"
  t811 = prims.convert_element_type(t810, dtypes.float32)  # t811: "cuda:0 f32[1, 512, 4096]"
  t812 = ltorch.mul(t811, t811)  # t812: "cuda:0 f32[1, 512, 4096]"
    # t812 = prims.mul(t811, t811)  # t812: "cuda:0 f32[1, 512, 4096]"
  t814 = prims.sum(t812, (2,))  # t814: "cuda:0 f32[1, 512]"
  t815 = prims.broadcast_in_dim(t814, [1, 512, 1], [0, 1])  # t815: "cuda:0 f32[1, 512, 1]"
  t817 = ltorch.true_divide(t815, 4096.0)  # t817: "cuda:0 f32[1, 512, 1]"
    # t817 = prims.div(t815, 4096.0)  # t817: "cuda:0 f32[1, 512, 1]"
  t819 = ltorch.add(t817, 1e-05, alpha=None)  # t819: "cuda:0 f32[1, 512, 1]"
    # t819 = prims.add(t817, 1e-05)  # t819: "cuda:0 f32[1, 512, 1]"
  t820 = prims.rsqrt(t819)  # t820: "cuda:0 f32[1, 512, 1]"
  t821 = prims.broadcast_in_dim(t820, (1, 512, 4096), (0, 1, 2))  # t821: "cuda:0 f32[1, 512, 4096]"
  t822 = ltorch.mul(t811, t821)  # t822: "cuda:0 f32[1, 512, 4096]"
    # t822 = prims.mul(t811, t821)  # t822: "cuda:0 f32[1, 512, 4096]"
  t823 = prims.convert_element_type(t822, dtypes.bfloat16)  # t823: "cuda:0 bf16[1, 512, 4096]"
  t824 = prims.broadcast_in_dim(t_transformer_h_5_norm_2_weight, (1, 512, 4096), (2,))  # t824: "cuda:0 bf16[1, 512, 4096]"
  t825 = prims.convert_element_type(t823, dtypes.float32)  # t825: "cuda:0 f32[1, 512, 4096]"
  t826 = prims.convert_element_type(t824, dtypes.float32)  # t826: "cuda:0 f32[1, 512, 4096]"
  t827 = ltorch.mul(t825, t826)  # t827: "cuda:0 f32[1, 512, 4096]"
    # t827 = prims.mul(t825, t826)  # t827: "cuda:0 f32[1, 512, 4096]"
  t828 = prims.convert_element_type(t827, dtypes.bfloat16)  # t828: "cuda:0 bf16[1, 512, 4096]"
  t829 = prims.linear(t828, t_transformer_h_5_mlp_fc_1_weight, None)  # t829: "cuda:0 bf16[1, 512, 11008]"
  t830 = prims.linear(t828, t_transformer_h_5_mlp_fc_2_weight, None)  # t830: "cuda:0 bf16[1, 512, 11008]"
  t831 = prims.convert_element_type(t829, dtypes.float32)  # t831: "cuda:0 f32[1, 512, 11008]"
  t832 = prims.neg(t831)  # t832: "cuda:0 f32[1, 512, 11008]"
  t833 = prims.exp(t832)  # t833: "cuda:0 f32[1, 512, 11008]"
  t834 = ltorch.add(1.0, t833, alpha=None)  # t834: "cuda:0 f32[1, 512, 11008]"
    # t834 = prims.add(1.0, t833)  # t834: "cuda:0 f32[1, 512, 11008]"
  t835 = prims.reciprocal(t834)  # t835: "cuda:0 f32[1, 512, 11008]"
  t836 = prims.convert_element_type(t835, dtypes.bfloat16)  # t836: "cuda:0 bf16[1, 512, 11008]"
  t837 = prims.convert_element_type(t829, dtypes.float32)  # t837: "cuda:0 f32[1, 512, 11008]"
  t838 = prims.convert_element_type(t836, dtypes.float32)  # t838: "cuda:0 f32[1, 512, 11008]"
  t839 = ltorch.mul(t837, t838)  # t839: "cuda:0 f32[1, 512, 11008]"
    # t839 = prims.mul(t837, t838)  # t839: "cuda:0 f32[1, 512, 11008]"
  t840 = prims.convert_element_type(t839, dtypes.bfloat16)  # t840: "cuda:0 bf16[1, 512, 11008]"
  t841 = prims.convert_element_type(t840, dtypes.float32)  # t841: "cuda:0 f32[1, 512, 11008]"
  t842 = prims.convert_element_type(t830, dtypes.float32)  # t842: "cuda:0 f32[1, 512, 11008]"
  t843 = ltorch.mul(t841, t842)  # t843: "cuda:0 f32[1, 512, 11008]"
    # t843 = prims.mul(t841, t842)  # t843: "cuda:0 f32[1, 512, 11008]"
  t844 = prims.convert_element_type(t843, dtypes.bfloat16)  # t844: "cuda:0 bf16[1, 512, 11008]"
  t845 = prims.linear(t844, t_transformer_h_5_mlp_proj_weight, None)  # t845: "cuda:0 bf16[1, 512, 4096]"
  t846 = prims.convert_element_type(t845, dtypes.float32)  # t846: "cuda:0 f32[1, 512, 4096]"
  t847 = prims.convert_element_type(t810, dtypes.float32)  # t847: "cuda:0 f32[1, 512, 4096]"
  t848 = ltorch.add(t846, t847, alpha=None)  # t848: "cuda:0 f32[1, 512, 4096]"
    # t848 = prims.add(t846, t847)  # t848: "cuda:0 f32[1, 512, 4096]"
  t849 = prims.convert_element_type(t848, dtypes.bfloat16)  # t849: "cuda:0 bf16[1, 512, 4096]"
  t850 = prims.convert_element_type(t849, dtypes.float32)  # t850: "cuda:0 f32[1, 512, 4096]"
  t851 = ltorch.mul(t850, t850)  # t851: "cuda:0 f32[1, 512, 4096]"
    # t851 = prims.mul(t850, t850)  # t851: "cuda:0 f32[1, 512, 4096]"
  t853 = prims.sum(t851, (2,))  # t853: "cuda:0 f32[1, 512]"
  t854 = prims.broadcast_in_dim(t853, [1, 512, 1], [0, 1])  # t854: "cuda:0 f32[1, 512, 1]"
  t856 = ltorch.true_divide(t854, 4096.0)  # t856: "cuda:0 f32[1, 512, 1]"
    # t856 = prims.div(t854, 4096.0)  # t856: "cuda:0 f32[1, 512, 1]"
  t858 = ltorch.add(t856, 1e-05, alpha=None)  # t858: "cuda:0 f32[1, 512, 1]"
    # t858 = prims.add(t856, 1e-05)  # t858: "cuda:0 f32[1, 512, 1]"
  t859 = prims.rsqrt(t858)  # t859: "cuda:0 f32[1, 512, 1]"
  t860 = prims.broadcast_in_dim(t859, (1, 512, 4096), (0, 1, 2))  # t860: "cuda:0 f32[1, 512, 4096]"
  t861 = ltorch.mul(t850, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
    # t861 = prims.mul(t850, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
  t862 = prims.convert_element_type(t861, dtypes.bfloat16)  # t862: "cuda:0 bf16[1, 512, 4096]"
  t863 = prims.broadcast_in_dim(t_transformer_h_6_norm_1_weight, (1, 512, 4096), (2,))  # t863: "cuda:0 bf16[1, 512, 4096]"
  t864 = prims.convert_element_type(t862, dtypes.float32)  # t864: "cuda:0 f32[1, 512, 4096]"
  t865 = prims.convert_element_type(t863, dtypes.float32)  # t865: "cuda:0 f32[1, 512, 4096]"
  t866 = ltorch.mul(t864, t865)  # t866: "cuda:0 f32[1, 512, 4096]"
    # t866 = prims.mul(t864, t865)  # t866: "cuda:0 f32[1, 512, 4096]"
  t867 = prims.convert_element_type(t866, dtypes.bfloat16)  # t867: "cuda:0 bf16[1, 512, 4096]"
  t868 = prims.linear(t867, t_transformer_h_6_attn_attn_weight, None)  # t868: "cuda:0 bf16[1, 512, 12288]"
  t874 = prims.reshape(t868, (1, 512, 32, 3, 128))  # t874: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t880 = prims.transpose(t874, (0, 2, 3, 1, 4))  # t880: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t881, t882, t883) = ltorch.split(t880, (1, 1, 1), 2)
    # t881 = prims.slice_prim(t880, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t881: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t882 = prims.slice_prim(t880, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t882: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t883 = prims.slice_prim(t880, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t883: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t889 = prims.reshape(t881, (1, 32, 512, 128))  # t889: "cuda:0 bf16[1, 32, 512, 128]"
  t895 = prims.reshape(t882, (1, 32, 512, 128))  # t895: "cuda:0 bf16[1, 32, 512, 128]"
  t901 = prims.reshape(t883, (1, 32, 512, 128))  # t901: "cuda:0 bf16[1, 32, 512, 128]"
  t902 = prims.slice_prim(t889, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t902: "cuda:0 bf16[1, 32, 512, 128]"
  t903 = prims.slice_prim(t902, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t903: "cuda:0 bf16[1, 32, 512, 64]"
  t904 = prims.slice_prim(t902, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t904: "cuda:0 bf16[1, 32, 512, 64]"
  t905 = prims.convert_element_type(t904, dtypes.float32)  # t905: "cuda:0 f32[1, 32, 512, 64]"
  t906 = prims.neg(t905)  # t906: "cuda:0 f32[1, 32, 512, 64]"
  t907 = prims.convert_element_type(t906, dtypes.bfloat16)  # t907: "cuda:0 bf16[1, 32, 512, 64]"
  t909 = prims.cat((t907, t903), -1)  # t909: "cuda:0 bf16[1, 32, 512, 128]"
  t910 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t910: "cuda:0 f32[1, 32, 512, 128]"
  t911 = prims.convert_element_type(t902, dtypes.float32)  # t911: "cuda:0 f32[1, 32, 512, 128]"
  t912 = ltorch.mul(t911, t910)  # t912: "cuda:0 f32[1, 32, 512, 128]"
    # t912 = prims.mul(t911, t910)  # t912: "cuda:0 f32[1, 32, 512, 128]"
  t913 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t913: "cuda:0 f32[1, 32, 512, 128]"
  t914 = prims.convert_element_type(t909, dtypes.float32)  # t914: "cuda:0 f32[1, 32, 512, 128]"
  t915 = ltorch.mul(t914, t913)  # t915: "cuda:0 f32[1, 32, 512, 128]"
    # t915 = prims.mul(t914, t913)  # t915: "cuda:0 f32[1, 32, 512, 128]"
  t916 = ltorch.add(t912, t915, alpha=None)  # t916: "cuda:0 f32[1, 32, 512, 128]"
    # t916 = prims.add(t912, t915)  # t916: "cuda:0 f32[1, 32, 512, 128]"
  t917 = prims.convert_element_type(t916, dtypes.bfloat16)  # t917: "cuda:0 bf16[1, 32, 512, 128]"
  t918 = prims.slice_prim(t895, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t918: "cuda:0 bf16[1, 32, 512, 128]"
  t919 = prims.slice_prim(t918, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t919: "cuda:0 bf16[1, 32, 512, 64]"
  t920 = prims.slice_prim(t918, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t920: "cuda:0 bf16[1, 32, 512, 64]"
  t921 = prims.convert_element_type(t920, dtypes.float32)  # t921: "cuda:0 f32[1, 32, 512, 64]"
  t922 = prims.neg(t921)  # t922: "cuda:0 f32[1, 32, 512, 64]"
  t923 = prims.convert_element_type(t922, dtypes.bfloat16)  # t923: "cuda:0 bf16[1, 32, 512, 64]"
  t925 = prims.cat((t923, t919), -1)  # t925: "cuda:0 bf16[1, 32, 512, 128]"
  t926 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t926: "cuda:0 f32[1, 32, 512, 128]"
  t927 = prims.convert_element_type(t918, dtypes.float32)  # t927: "cuda:0 f32[1, 32, 512, 128]"
  t928 = ltorch.mul(t927, t926)  # t928: "cuda:0 f32[1, 32, 512, 128]"
    # t928 = prims.mul(t927, t926)  # t928: "cuda:0 f32[1, 32, 512, 128]"
  t929 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t929: "cuda:0 f32[1, 32, 512, 128]"
  t930 = prims.convert_element_type(t925, dtypes.float32)  # t930: "cuda:0 f32[1, 32, 512, 128]"
  t931 = ltorch.mul(t930, t929)  # t931: "cuda:0 f32[1, 32, 512, 128]"
    # t931 = prims.mul(t930, t929)  # t931: "cuda:0 f32[1, 32, 512, 128]"
  t932 = ltorch.add(t928, t931, alpha=None)  # t932: "cuda:0 f32[1, 32, 512, 128]"
    # t932 = prims.add(t928, t931)  # t932: "cuda:0 f32[1, 32, 512, 128]"
  t933 = prims.convert_element_type(t932, dtypes.bfloat16)  # t933: "cuda:0 bf16[1, 32, 512, 128]"
  t934 = prims.slice_prim(t889, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t934: "cuda:0 bf16[1, 32, 512, 0]"
  t936 = prims.cat((t917, t934), -1)  # t936: "cuda:0 bf16[1, 32, 512, 128]"
  t937 = prims.slice_prim(t895, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t937: "cuda:0 bf16[1, 32, 512, 0]"
  t939 = prims.cat((t933, t937), -1)  # t939: "cuda:0 bf16[1, 32, 512, 128]"
  (t940, t941, t942, t943) = cudnn_sdpa_fwd(t936, t939, t901, None, 0.0, True, scale=0.08838834764831843)
  t946 = prims.transpose(t940, (0, 2, 1, 3))  # t946: "cuda:0 bf16[1, 512, 32, 128]"
  t950 = prims.reshape(t946, (1, 512, 4096))  # t950: "cuda:0 bf16[1, 512, 4096]"
  t951 = prims.linear(t950, t_transformer_h_6_attn_proj_weight, None)  # t951: "cuda:0 bf16[1, 512, 4096]"
  t952 = prims.convert_element_type(t951, dtypes.float32)  # t952: "cuda:0 f32[1, 512, 4096]"
  t953 = prims.convert_element_type(t849, dtypes.float32)  # t953: "cuda:0 f32[1, 512, 4096]"
  t954 = ltorch.add(t952, t953, alpha=None)  # t954: "cuda:0 f32[1, 512, 4096]"
    # t954 = prims.add(t952, t953)  # t954: "cuda:0 f32[1, 512, 4096]"
  t955 = prims.convert_element_type(t954, dtypes.bfloat16)  # t955: "cuda:0 bf16[1, 512, 4096]"
  t956 = prims.convert_element_type(t955, dtypes.float32)  # t956: "cuda:0 f32[1, 512, 4096]"
  t957 = ltorch.mul(t956, t956)  # t957: "cuda:0 f32[1, 512, 4096]"
    # t957 = prims.mul(t956, t956)  # t957: "cuda:0 f32[1, 512, 4096]"
  t959 = prims.sum(t957, (2,))  # t959: "cuda:0 f32[1, 512]"
  t960 = prims.broadcast_in_dim(t959, [1, 512, 1], [0, 1])  # t960: "cuda:0 f32[1, 512, 1]"
  t962 = ltorch.true_divide(t960, 4096.0)  # t962: "cuda:0 f32[1, 512, 1]"
    # t962 = prims.div(t960, 4096.0)  # t962: "cuda:0 f32[1, 512, 1]"
  t964 = ltorch.add(t962, 1e-05, alpha=None)  # t964: "cuda:0 f32[1, 512, 1]"
    # t964 = prims.add(t962, 1e-05)  # t964: "cuda:0 f32[1, 512, 1]"
  t965 = prims.rsqrt(t964)  # t965: "cuda:0 f32[1, 512, 1]"
  t966 = prims.broadcast_in_dim(t965, (1, 512, 4096), (0, 1, 2))  # t966: "cuda:0 f32[1, 512, 4096]"
  t967 = ltorch.mul(t956, t966)  # t967: "cuda:0 f32[1, 512, 4096]"
    # t967 = prims.mul(t956, t966)  # t967: "cuda:0 f32[1, 512, 4096]"
  t968 = prims.convert_element_type(t967, dtypes.bfloat16)  # t968: "cuda:0 bf16[1, 512, 4096]"
  t969 = prims.broadcast_in_dim(t_transformer_h_6_norm_2_weight, (1, 512, 4096), (2,))  # t969: "cuda:0 bf16[1, 512, 4096]"
  t970 = prims.convert_element_type(t968, dtypes.float32)  # t970: "cuda:0 f32[1, 512, 4096]"
  t971 = prims.convert_element_type(t969, dtypes.float32)  # t971: "cuda:0 f32[1, 512, 4096]"
  t972 = ltorch.mul(t970, t971)  # t972: "cuda:0 f32[1, 512, 4096]"
    # t972 = prims.mul(t970, t971)  # t972: "cuda:0 f32[1, 512, 4096]"
  t973 = prims.convert_element_type(t972, dtypes.bfloat16)  # t973: "cuda:0 bf16[1, 512, 4096]"
  t974 = prims.linear(t973, t_transformer_h_6_mlp_fc_1_weight, None)  # t974: "cuda:0 bf16[1, 512, 11008]"
  t975 = prims.linear(t973, t_transformer_h_6_mlp_fc_2_weight, None)  # t975: "cuda:0 bf16[1, 512, 11008]"
  t976 = prims.convert_element_type(t974, dtypes.float32)  # t976: "cuda:0 f32[1, 512, 11008]"
  t977 = prims.neg(t976)  # t977: "cuda:0 f32[1, 512, 11008]"
  t978 = prims.exp(t977)  # t978: "cuda:0 f32[1, 512, 11008]"
  t979 = ltorch.add(1.0, t978, alpha=None)  # t979: "cuda:0 f32[1, 512, 11008]"
    # t979 = prims.add(1.0, t978)  # t979: "cuda:0 f32[1, 512, 11008]"
  t980 = prims.reciprocal(t979)  # t980: "cuda:0 f32[1, 512, 11008]"
  t981 = prims.convert_element_type(t980, dtypes.bfloat16)  # t981: "cuda:0 bf16[1, 512, 11008]"
  t982 = prims.convert_element_type(t974, dtypes.float32)  # t982: "cuda:0 f32[1, 512, 11008]"
  t983 = prims.convert_element_type(t981, dtypes.float32)  # t983: "cuda:0 f32[1, 512, 11008]"
  t984 = ltorch.mul(t982, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
    # t984 = prims.mul(t982, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
  t985 = prims.convert_element_type(t984, dtypes.bfloat16)  # t985: "cuda:0 bf16[1, 512, 11008]"
  t986 = prims.convert_element_type(t985, dtypes.float32)  # t986: "cuda:0 f32[1, 512, 11008]"
  t987 = prims.convert_element_type(t975, dtypes.float32)  # t987: "cuda:0 f32[1, 512, 11008]"
  t988 = ltorch.mul(t986, t987)  # t988: "cuda:0 f32[1, 512, 11008]"
    # t988 = prims.mul(t986, t987)  # t988: "cuda:0 f32[1, 512, 11008]"
  t989 = prims.convert_element_type(t988, dtypes.bfloat16)  # t989: "cuda:0 bf16[1, 512, 11008]"
  t990 = prims.linear(t989, t_transformer_h_6_mlp_proj_weight, None)  # t990: "cuda:0 bf16[1, 512, 4096]"
  t991 = prims.convert_element_type(t990, dtypes.float32)  # t991: "cuda:0 f32[1, 512, 4096]"
  t992 = prims.convert_element_type(t955, dtypes.float32)  # t992: "cuda:0 f32[1, 512, 4096]"
  t993 = ltorch.add(t991, t992, alpha=None)  # t993: "cuda:0 f32[1, 512, 4096]"
    # t993 = prims.add(t991, t992)  # t993: "cuda:0 f32[1, 512, 4096]"
  t994 = prims.convert_element_type(t993, dtypes.bfloat16)  # t994: "cuda:0 bf16[1, 512, 4096]"
  t995 = prims.convert_element_type(t994, dtypes.float32)  # t995: "cuda:0 f32[1, 512, 4096]"
  t996 = ltorch.mul(t995, t995)  # t996: "cuda:0 f32[1, 512, 4096]"
    # t996 = prims.mul(t995, t995)  # t996: "cuda:0 f32[1, 512, 4096]"
  t998 = prims.sum(t996, (2,))  # t998: "cuda:0 f32[1, 512]"
  t999 = prims.broadcast_in_dim(t998, [1, 512, 1], [0, 1])  # t999: "cuda:0 f32[1, 512, 1]"
  t1001 = ltorch.true_divide(t999, 4096.0)  # t1001: "cuda:0 f32[1, 512, 1]"
    # t1001 = prims.div(t999, 4096.0)  # t1001: "cuda:0 f32[1, 512, 1]"
  t1003 = ltorch.add(t1001, 1e-05, alpha=None)  # t1003: "cuda:0 f32[1, 512, 1]"
    # t1003 = prims.add(t1001, 1e-05)  # t1003: "cuda:0 f32[1, 512, 1]"
  t1004 = prims.rsqrt(t1003)  # t1004: "cuda:0 f32[1, 512, 1]"
  t1005 = prims.broadcast_in_dim(t1004, (1, 512, 4096), (0, 1, 2))  # t1005: "cuda:0 f32[1, 512, 4096]"
  t1006 = ltorch.mul(t995, t1005)  # t1006: "cuda:0 f32[1, 512, 4096]"
    # t1006 = prims.mul(t995, t1005)  # t1006: "cuda:0 f32[1, 512, 4096]"
  t1007 = prims.convert_element_type(t1006, dtypes.bfloat16)  # t1007: "cuda:0 bf16[1, 512, 4096]"
  t1008 = prims.broadcast_in_dim(t_transformer_h_7_norm_1_weight, (1, 512, 4096), (2,))  # t1008: "cuda:0 bf16[1, 512, 4096]"
  t1009 = prims.convert_element_type(t1007, dtypes.float32)  # t1009: "cuda:0 f32[1, 512, 4096]"
  t1010 = prims.convert_element_type(t1008, dtypes.float32)  # t1010: "cuda:0 f32[1, 512, 4096]"
  t1011 = ltorch.mul(t1009, t1010)  # t1011: "cuda:0 f32[1, 512, 4096]"
    # t1011 = prims.mul(t1009, t1010)  # t1011: "cuda:0 f32[1, 512, 4096]"
  t1012 = prims.convert_element_type(t1011, dtypes.bfloat16)  # t1012: "cuda:0 bf16[1, 512, 4096]"
  t1013 = prims.linear(t1012, t_transformer_h_7_attn_attn_weight, None)  # t1013: "cuda:0 bf16[1, 512, 12288]"
  t1019 = prims.reshape(t1013, (1, 512, 32, 3, 128))  # t1019: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1025 = prims.transpose(t1019, (0, 2, 3, 1, 4))  # t1025: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1026, t1027, t1028) = ltorch.split(t1025, (1, 1, 1), 2)
    # t1026 = prims.slice_prim(t1025, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1026: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1027 = prims.slice_prim(t1025, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1027: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1028 = prims.slice_prim(t1025, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1028: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1034 = prims.reshape(t1026, (1, 32, 512, 128))  # t1034: "cuda:0 bf16[1, 32, 512, 128]"
  t1040 = prims.reshape(t1027, (1, 32, 512, 128))  # t1040: "cuda:0 bf16[1, 32, 512, 128]"
  t1046 = prims.reshape(t1028, (1, 32, 512, 128))  # t1046: "cuda:0 bf16[1, 32, 512, 128]"
  t1047 = prims.slice_prim(t1034, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1047: "cuda:0 bf16[1, 32, 512, 128]"
  t1048 = prims.slice_prim(t1047, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1048: "cuda:0 bf16[1, 32, 512, 64]"
  t1049 = prims.slice_prim(t1047, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1049: "cuda:0 bf16[1, 32, 512, 64]"
  t1050 = prims.convert_element_type(t1049, dtypes.float32)  # t1050: "cuda:0 f32[1, 32, 512, 64]"
  t1051 = prims.neg(t1050)  # t1051: "cuda:0 f32[1, 32, 512, 64]"
  t1052 = prims.convert_element_type(t1051, dtypes.bfloat16)  # t1052: "cuda:0 bf16[1, 32, 512, 64]"
  t1054 = prims.cat((t1052, t1048), -1)  # t1054: "cuda:0 bf16[1, 32, 512, 128]"
  t1055 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1055: "cuda:0 f32[1, 32, 512, 128]"
  t1056 = prims.convert_element_type(t1047, dtypes.float32)  # t1056: "cuda:0 f32[1, 32, 512, 128]"
  t1057 = ltorch.mul(t1056, t1055)  # t1057: "cuda:0 f32[1, 32, 512, 128]"
    # t1057 = prims.mul(t1056, t1055)  # t1057: "cuda:0 f32[1, 32, 512, 128]"
  t1058 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1058: "cuda:0 f32[1, 32, 512, 128]"
  t1059 = prims.convert_element_type(t1054, dtypes.float32)  # t1059: "cuda:0 f32[1, 32, 512, 128]"
  t1060 = ltorch.mul(t1059, t1058)  # t1060: "cuda:0 f32[1, 32, 512, 128]"
    # t1060 = prims.mul(t1059, t1058)  # t1060: "cuda:0 f32[1, 32, 512, 128]"
  t1061 = ltorch.add(t1057, t1060, alpha=None)  # t1061: "cuda:0 f32[1, 32, 512, 128]"
    # t1061 = prims.add(t1057, t1060)  # t1061: "cuda:0 f32[1, 32, 512, 128]"
  t1062 = prims.convert_element_type(t1061, dtypes.bfloat16)  # t1062: "cuda:0 bf16[1, 32, 512, 128]"
  t1063 = prims.slice_prim(t1040, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1063: "cuda:0 bf16[1, 32, 512, 128]"
  t1064 = prims.slice_prim(t1063, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1064: "cuda:0 bf16[1, 32, 512, 64]"
  t1065 = prims.slice_prim(t1063, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1065: "cuda:0 bf16[1, 32, 512, 64]"
  t1066 = prims.convert_element_type(t1065, dtypes.float32)  # t1066: "cuda:0 f32[1, 32, 512, 64]"
  t1067 = prims.neg(t1066)  # t1067: "cuda:0 f32[1, 32, 512, 64]"
  t1068 = prims.convert_element_type(t1067, dtypes.bfloat16)  # t1068: "cuda:0 bf16[1, 32, 512, 64]"
  t1070 = prims.cat((t1068, t1064), -1)  # t1070: "cuda:0 bf16[1, 32, 512, 128]"
  t1071 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1071: "cuda:0 f32[1, 32, 512, 128]"
  t1072 = prims.convert_element_type(t1063, dtypes.float32)  # t1072: "cuda:0 f32[1, 32, 512, 128]"
  t1073 = ltorch.mul(t1072, t1071)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
    # t1073 = prims.mul(t1072, t1071)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
  t1074 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1074: "cuda:0 f32[1, 32, 512, 128]"
  t1075 = prims.convert_element_type(t1070, dtypes.float32)  # t1075: "cuda:0 f32[1, 32, 512, 128]"
  t1076 = ltorch.mul(t1075, t1074)  # t1076: "cuda:0 f32[1, 32, 512, 128]"
    # t1076 = prims.mul(t1075, t1074)  # t1076: "cuda:0 f32[1, 32, 512, 128]"
  t1077 = ltorch.add(t1073, t1076, alpha=None)  # t1077: "cuda:0 f32[1, 32, 512, 128]"
    # t1077 = prims.add(t1073, t1076)  # t1077: "cuda:0 f32[1, 32, 512, 128]"
  t1078 = prims.convert_element_type(t1077, dtypes.bfloat16)  # t1078: "cuda:0 bf16[1, 32, 512, 128]"
  t1079 = prims.slice_prim(t1034, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1079: "cuda:0 bf16[1, 32, 512, 0]"
  t1081 = prims.cat((t1062, t1079), -1)  # t1081: "cuda:0 bf16[1, 32, 512, 128]"
  t1082 = prims.slice_prim(t1040, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1082: "cuda:0 bf16[1, 32, 512, 0]"
  t1084 = prims.cat((t1078, t1082), -1)  # t1084: "cuda:0 bf16[1, 32, 512, 128]"
  (t1085, t1086, t1087, t1088) = cudnn_sdpa_fwd(t1081, t1084, t1046, None, 0.0, True, scale=0.08838834764831843)
  t1091 = prims.transpose(t1085, (0, 2, 1, 3))  # t1091: "cuda:0 bf16[1, 512, 32, 128]"
  t1095 = prims.reshape(t1091, (1, 512, 4096))  # t1095: "cuda:0 bf16[1, 512, 4096]"
  t1096 = prims.linear(t1095, t_transformer_h_7_attn_proj_weight, None)  # t1096: "cuda:0 bf16[1, 512, 4096]"
  t1097 = prims.convert_element_type(t1096, dtypes.float32)  # t1097: "cuda:0 f32[1, 512, 4096]"
  t1098 = prims.convert_element_type(t994, dtypes.float32)  # t1098: "cuda:0 f32[1, 512, 4096]"
  t1099 = ltorch.add(t1097, t1098, alpha=None)  # t1099: "cuda:0 f32[1, 512, 4096]"
    # t1099 = prims.add(t1097, t1098)  # t1099: "cuda:0 f32[1, 512, 4096]"
  t1100 = prims.convert_element_type(t1099, dtypes.bfloat16)  # t1100: "cuda:0 bf16[1, 512, 4096]"
  t1101 = prims.convert_element_type(t1100, dtypes.float32)  # t1101: "cuda:0 f32[1, 512, 4096]"
  t1102 = ltorch.mul(t1101, t1101)  # t1102: "cuda:0 f32[1, 512, 4096]"
    # t1102 = prims.mul(t1101, t1101)  # t1102: "cuda:0 f32[1, 512, 4096]"
  t1104 = prims.sum(t1102, (2,))  # t1104: "cuda:0 f32[1, 512]"
  t1105 = prims.broadcast_in_dim(t1104, [1, 512, 1], [0, 1])  # t1105: "cuda:0 f32[1, 512, 1]"
  t1107 = ltorch.true_divide(t1105, 4096.0)  # t1107: "cuda:0 f32[1, 512, 1]"
    # t1107 = prims.div(t1105, 4096.0)  # t1107: "cuda:0 f32[1, 512, 1]"
  t1109 = ltorch.add(t1107, 1e-05, alpha=None)  # t1109: "cuda:0 f32[1, 512, 1]"
    # t1109 = prims.add(t1107, 1e-05)  # t1109: "cuda:0 f32[1, 512, 1]"
  t1110 = prims.rsqrt(t1109)  # t1110: "cuda:0 f32[1, 512, 1]"
  t1111 = prims.broadcast_in_dim(t1110, (1, 512, 4096), (0, 1, 2))  # t1111: "cuda:0 f32[1, 512, 4096]"
  t1112 = ltorch.mul(t1101, t1111)  # t1112: "cuda:0 f32[1, 512, 4096]"
    # t1112 = prims.mul(t1101, t1111)  # t1112: "cuda:0 f32[1, 512, 4096]"
  t1113 = prims.convert_element_type(t1112, dtypes.bfloat16)  # t1113: "cuda:0 bf16[1, 512, 4096]"
  t1114 = prims.broadcast_in_dim(t_transformer_h_7_norm_2_weight, (1, 512, 4096), (2,))  # t1114: "cuda:0 bf16[1, 512, 4096]"
  t1115 = prims.convert_element_type(t1113, dtypes.float32)  # t1115: "cuda:0 f32[1, 512, 4096]"
  t1116 = prims.convert_element_type(t1114, dtypes.float32)  # t1116: "cuda:0 f32[1, 512, 4096]"
  t1117 = ltorch.mul(t1115, t1116)  # t1117: "cuda:0 f32[1, 512, 4096]"
    # t1117 = prims.mul(t1115, t1116)  # t1117: "cuda:0 f32[1, 512, 4096]"
  t1118 = prims.convert_element_type(t1117, dtypes.bfloat16)  # t1118: "cuda:0 bf16[1, 512, 4096]"
  t1119 = prims.linear(t1118, t_transformer_h_7_mlp_fc_1_weight, None)  # t1119: "cuda:0 bf16[1, 512, 11008]"
  t1120 = prims.linear(t1118, t_transformer_h_7_mlp_fc_2_weight, None)  # t1120: "cuda:0 bf16[1, 512, 11008]"
  t1121 = prims.convert_element_type(t1119, dtypes.float32)  # t1121: "cuda:0 f32[1, 512, 11008]"
  t1122 = prims.neg(t1121)  # t1122: "cuda:0 f32[1, 512, 11008]"
  t1123 = prims.exp(t1122)  # t1123: "cuda:0 f32[1, 512, 11008]"
  t1124 = ltorch.add(1.0, t1123, alpha=None)  # t1124: "cuda:0 f32[1, 512, 11008]"
    # t1124 = prims.add(1.0, t1123)  # t1124: "cuda:0 f32[1, 512, 11008]"
  t1125 = prims.reciprocal(t1124)  # t1125: "cuda:0 f32[1, 512, 11008]"
  t1126 = prims.convert_element_type(t1125, dtypes.bfloat16)  # t1126: "cuda:0 bf16[1, 512, 11008]"
  t1127 = prims.convert_element_type(t1119, dtypes.float32)  # t1127: "cuda:0 f32[1, 512, 11008]"
  t1128 = prims.convert_element_type(t1126, dtypes.float32)  # t1128: "cuda:0 f32[1, 512, 11008]"
  t1129 = ltorch.mul(t1127, t1128)  # t1129: "cuda:0 f32[1, 512, 11008]"
    # t1129 = prims.mul(t1127, t1128)  # t1129: "cuda:0 f32[1, 512, 11008]"
  t1130 = prims.convert_element_type(t1129, dtypes.bfloat16)  # t1130: "cuda:0 bf16[1, 512, 11008]"
  t1131 = prims.convert_element_type(t1130, dtypes.float32)  # t1131: "cuda:0 f32[1, 512, 11008]"
  t1132 = prims.convert_element_type(t1120, dtypes.float32)  # t1132: "cuda:0 f32[1, 512, 11008]"
  t1133 = ltorch.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 11008]"
    # t1133 = prims.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 11008]"
  t1134 = prims.convert_element_type(t1133, dtypes.bfloat16)  # t1134: "cuda:0 bf16[1, 512, 11008]"
  t1135 = prims.linear(t1134, t_transformer_h_7_mlp_proj_weight, None)  # t1135: "cuda:0 bf16[1, 512, 4096]"
  t1136 = prims.convert_element_type(t1135, dtypes.float32)  # t1136: "cuda:0 f32[1, 512, 4096]"
  t1137 = prims.convert_element_type(t1100, dtypes.float32)  # t1137: "cuda:0 f32[1, 512, 4096]"
  t1138 = ltorch.add(t1136, t1137, alpha=None)  # t1138: "cuda:0 f32[1, 512, 4096]"
    # t1138 = prims.add(t1136, t1137)  # t1138: "cuda:0 f32[1, 512, 4096]"
  t1139 = prims.convert_element_type(t1138, dtypes.bfloat16)  # t1139: "cuda:0 bf16[1, 512, 4096]"
  t1140 = prims.convert_element_type(t1139, dtypes.float32)  # t1140: "cuda:0 f32[1, 512, 4096]"
  t1141 = ltorch.mul(t1140, t1140)  # t1141: "cuda:0 f32[1, 512, 4096]"
    # t1141 = prims.mul(t1140, t1140)  # t1141: "cuda:0 f32[1, 512, 4096]"
  t1143 = prims.sum(t1141, (2,))  # t1143: "cuda:0 f32[1, 512]"
  t1144 = prims.broadcast_in_dim(t1143, [1, 512, 1], [0, 1])  # t1144: "cuda:0 f32[1, 512, 1]"
  t1146 = ltorch.true_divide(t1144, 4096.0)  # t1146: "cuda:0 f32[1, 512, 1]"
    # t1146 = prims.div(t1144, 4096.0)  # t1146: "cuda:0 f32[1, 512, 1]"
  t1148 = ltorch.add(t1146, 1e-05, alpha=None)  # t1148: "cuda:0 f32[1, 512, 1]"
    # t1148 = prims.add(t1146, 1e-05)  # t1148: "cuda:0 f32[1, 512, 1]"
  t1149 = prims.rsqrt(t1148)  # t1149: "cuda:0 f32[1, 512, 1]"
  t1150 = prims.broadcast_in_dim(t1149, (1, 512, 4096), (0, 1, 2))  # t1150: "cuda:0 f32[1, 512, 4096]"
  t1151 = ltorch.mul(t1140, t1150)  # t1151: "cuda:0 f32[1, 512, 4096]"
    # t1151 = prims.mul(t1140, t1150)  # t1151: "cuda:0 f32[1, 512, 4096]"
  t1152 = prims.convert_element_type(t1151, dtypes.bfloat16)  # t1152: "cuda:0 bf16[1, 512, 4096]"
  t1153 = prims.broadcast_in_dim(t_transformer_h_8_norm_1_weight, (1, 512, 4096), (2,))  # t1153: "cuda:0 bf16[1, 512, 4096]"
  t1154 = prims.convert_element_type(t1152, dtypes.float32)  # t1154: "cuda:0 f32[1, 512, 4096]"
  t1155 = prims.convert_element_type(t1153, dtypes.float32)  # t1155: "cuda:0 f32[1, 512, 4096]"
  t1156 = ltorch.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 4096]"
    # t1156 = prims.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 4096]"
  t1157 = prims.convert_element_type(t1156, dtypes.bfloat16)  # t1157: "cuda:0 bf16[1, 512, 4096]"
  t1158 = prims.linear(t1157, t_transformer_h_8_attn_attn_weight, None)  # t1158: "cuda:0 bf16[1, 512, 12288]"
  t1164 = prims.reshape(t1158, (1, 512, 32, 3, 128))  # t1164: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1170 = prims.transpose(t1164, (0, 2, 3, 1, 4))  # t1170: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1171, t1172, t1173) = ltorch.split(t1170, (1, 1, 1), 2)
    # t1171 = prims.slice_prim(t1170, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1171: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1172 = prims.slice_prim(t1170, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1172: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1173 = prims.slice_prim(t1170, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1173: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1179 = prims.reshape(t1171, (1, 32, 512, 128))  # t1179: "cuda:0 bf16[1, 32, 512, 128]"
  t1185 = prims.reshape(t1172, (1, 32, 512, 128))  # t1185: "cuda:0 bf16[1, 32, 512, 128]"
  t1191 = prims.reshape(t1173, (1, 32, 512, 128))  # t1191: "cuda:0 bf16[1, 32, 512, 128]"
  t1192 = prims.slice_prim(t1179, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1192: "cuda:0 bf16[1, 32, 512, 128]"
  t1193 = prims.slice_prim(t1192, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1193: "cuda:0 bf16[1, 32, 512, 64]"
  t1194 = prims.slice_prim(t1192, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1194: "cuda:0 bf16[1, 32, 512, 64]"
  t1195 = prims.convert_element_type(t1194, dtypes.float32)  # t1195: "cuda:0 f32[1, 32, 512, 64]"
  t1196 = prims.neg(t1195)  # t1196: "cuda:0 f32[1, 32, 512, 64]"
  t1197 = prims.convert_element_type(t1196, dtypes.bfloat16)  # t1197: "cuda:0 bf16[1, 32, 512, 64]"
  t1199 = prims.cat((t1197, t1193), -1)  # t1199: "cuda:0 bf16[1, 32, 512, 128]"
  t1200 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1200: "cuda:0 f32[1, 32, 512, 128]"
  t1201 = prims.convert_element_type(t1192, dtypes.float32)  # t1201: "cuda:0 f32[1, 32, 512, 128]"
  t1202 = ltorch.mul(t1201, t1200)  # t1202: "cuda:0 f32[1, 32, 512, 128]"
    # t1202 = prims.mul(t1201, t1200)  # t1202: "cuda:0 f32[1, 32, 512, 128]"
  t1203 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1203: "cuda:0 f32[1, 32, 512, 128]"
  t1204 = prims.convert_element_type(t1199, dtypes.float32)  # t1204: "cuda:0 f32[1, 32, 512, 128]"
  t1205 = ltorch.mul(t1204, t1203)  # t1205: "cuda:0 f32[1, 32, 512, 128]"
    # t1205 = prims.mul(t1204, t1203)  # t1205: "cuda:0 f32[1, 32, 512, 128]"
  t1206 = ltorch.add(t1202, t1205, alpha=None)  # t1206: "cuda:0 f32[1, 32, 512, 128]"
    # t1206 = prims.add(t1202, t1205)  # t1206: "cuda:0 f32[1, 32, 512, 128]"
  t1207 = prims.convert_element_type(t1206, dtypes.bfloat16)  # t1207: "cuda:0 bf16[1, 32, 512, 128]"
  t1208 = prims.slice_prim(t1185, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1208: "cuda:0 bf16[1, 32, 512, 128]"
  t1209 = prims.slice_prim(t1208, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1209: "cuda:0 bf16[1, 32, 512, 64]"
  t1210 = prims.slice_prim(t1208, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1210: "cuda:0 bf16[1, 32, 512, 64]"
  t1211 = prims.convert_element_type(t1210, dtypes.float32)  # t1211: "cuda:0 f32[1, 32, 512, 64]"
  t1212 = prims.neg(t1211)  # t1212: "cuda:0 f32[1, 32, 512, 64]"
  t1213 = prims.convert_element_type(t1212, dtypes.bfloat16)  # t1213: "cuda:0 bf16[1, 32, 512, 64]"
  t1215 = prims.cat((t1213, t1209), -1)  # t1215: "cuda:0 bf16[1, 32, 512, 128]"
  t1216 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1216: "cuda:0 f32[1, 32, 512, 128]"
  t1217 = prims.convert_element_type(t1208, dtypes.float32)  # t1217: "cuda:0 f32[1, 32, 512, 128]"
  t1218 = ltorch.mul(t1217, t1216)  # t1218: "cuda:0 f32[1, 32, 512, 128]"
    # t1218 = prims.mul(t1217, t1216)  # t1218: "cuda:0 f32[1, 32, 512, 128]"
  t1219 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1219: "cuda:0 f32[1, 32, 512, 128]"
  t1220 = prims.convert_element_type(t1215, dtypes.float32)  # t1220: "cuda:0 f32[1, 32, 512, 128]"
  t1221 = ltorch.mul(t1220, t1219)  # t1221: "cuda:0 f32[1, 32, 512, 128]"
    # t1221 = prims.mul(t1220, t1219)  # t1221: "cuda:0 f32[1, 32, 512, 128]"
  t1222 = ltorch.add(t1218, t1221, alpha=None)  # t1222: "cuda:0 f32[1, 32, 512, 128]"
    # t1222 = prims.add(t1218, t1221)  # t1222: "cuda:0 f32[1, 32, 512, 128]"
  t1223 = prims.convert_element_type(t1222, dtypes.bfloat16)  # t1223: "cuda:0 bf16[1, 32, 512, 128]"
  t1224 = prims.slice_prim(t1179, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1224: "cuda:0 bf16[1, 32, 512, 0]"
  t1226 = prims.cat((t1207, t1224), -1)  # t1226: "cuda:0 bf16[1, 32, 512, 128]"
  t1227 = prims.slice_prim(t1185, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1227: "cuda:0 bf16[1, 32, 512, 0]"
  t1229 = prims.cat((t1223, t1227), -1)  # t1229: "cuda:0 bf16[1, 32, 512, 128]"
  (t1230, t1231, t1232, t1233) = cudnn_sdpa_fwd(t1226, t1229, t1191, None, 0.0, True, scale=0.08838834764831843)
  t1236 = prims.transpose(t1230, (0, 2, 1, 3))  # t1236: "cuda:0 bf16[1, 512, 32, 128]"
  t1240 = prims.reshape(t1236, (1, 512, 4096))  # t1240: "cuda:0 bf16[1, 512, 4096]"
  t1241 = prims.linear(t1240, t_transformer_h_8_attn_proj_weight, None)  # t1241: "cuda:0 bf16[1, 512, 4096]"
  t1242 = prims.convert_element_type(t1241, dtypes.float32)  # t1242: "cuda:0 f32[1, 512, 4096]"
  t1243 = prims.convert_element_type(t1139, dtypes.float32)  # t1243: "cuda:0 f32[1, 512, 4096]"
  t1244 = ltorch.add(t1242, t1243, alpha=None)  # t1244: "cuda:0 f32[1, 512, 4096]"
    # t1244 = prims.add(t1242, t1243)  # t1244: "cuda:0 f32[1, 512, 4096]"
  t1245 = prims.convert_element_type(t1244, dtypes.bfloat16)  # t1245: "cuda:0 bf16[1, 512, 4096]"
  t1246 = prims.convert_element_type(t1245, dtypes.float32)  # t1246: "cuda:0 f32[1, 512, 4096]"
  t1247 = ltorch.mul(t1246, t1246)  # t1247: "cuda:0 f32[1, 512, 4096]"
    # t1247 = prims.mul(t1246, t1246)  # t1247: "cuda:0 f32[1, 512, 4096]"
  t1249 = prims.sum(t1247, (2,))  # t1249: "cuda:0 f32[1, 512]"
  t1250 = prims.broadcast_in_dim(t1249, [1, 512, 1], [0, 1])  # t1250: "cuda:0 f32[1, 512, 1]"
  t1252 = ltorch.true_divide(t1250, 4096.0)  # t1252: "cuda:0 f32[1, 512, 1]"
    # t1252 = prims.div(t1250, 4096.0)  # t1252: "cuda:0 f32[1, 512, 1]"
  t1254 = ltorch.add(t1252, 1e-05, alpha=None)  # t1254: "cuda:0 f32[1, 512, 1]"
    # t1254 = prims.add(t1252, 1e-05)  # t1254: "cuda:0 f32[1, 512, 1]"
  t1255 = prims.rsqrt(t1254)  # t1255: "cuda:0 f32[1, 512, 1]"
  t1256 = prims.broadcast_in_dim(t1255, (1, 512, 4096), (0, 1, 2))  # t1256: "cuda:0 f32[1, 512, 4096]"
  t1257 = ltorch.mul(t1246, t1256)  # t1257: "cuda:0 f32[1, 512, 4096]"
    # t1257 = prims.mul(t1246, t1256)  # t1257: "cuda:0 f32[1, 512, 4096]"
  t1258 = prims.convert_element_type(t1257, dtypes.bfloat16)  # t1258: "cuda:0 bf16[1, 512, 4096]"
  t1259 = prims.broadcast_in_dim(t_transformer_h_8_norm_2_weight, (1, 512, 4096), (2,))  # t1259: "cuda:0 bf16[1, 512, 4096]"
  t1260 = prims.convert_element_type(t1258, dtypes.float32)  # t1260: "cuda:0 f32[1, 512, 4096]"
  t1261 = prims.convert_element_type(t1259, dtypes.float32)  # t1261: "cuda:0 f32[1, 512, 4096]"
  t1262 = ltorch.mul(t1260, t1261)  # t1262: "cuda:0 f32[1, 512, 4096]"
    # t1262 = prims.mul(t1260, t1261)  # t1262: "cuda:0 f32[1, 512, 4096]"
  t1263 = prims.convert_element_type(t1262, dtypes.bfloat16)  # t1263: "cuda:0 bf16[1, 512, 4096]"
  t1264 = prims.linear(t1263, t_transformer_h_8_mlp_fc_1_weight, None)  # t1264: "cuda:0 bf16[1, 512, 11008]"
  t1265 = prims.linear(t1263, t_transformer_h_8_mlp_fc_2_weight, None)  # t1265: "cuda:0 bf16[1, 512, 11008]"
  t1266 = prims.convert_element_type(t1264, dtypes.float32)  # t1266: "cuda:0 f32[1, 512, 11008]"
  t1267 = prims.neg(t1266)  # t1267: "cuda:0 f32[1, 512, 11008]"
  t1268 = prims.exp(t1267)  # t1268: "cuda:0 f32[1, 512, 11008]"
  t1269 = ltorch.add(1.0, t1268, alpha=None)  # t1269: "cuda:0 f32[1, 512, 11008]"
    # t1269 = prims.add(1.0, t1268)  # t1269: "cuda:0 f32[1, 512, 11008]"
  t1270 = prims.reciprocal(t1269)  # t1270: "cuda:0 f32[1, 512, 11008]"
  t1271 = prims.convert_element_type(t1270, dtypes.bfloat16)  # t1271: "cuda:0 bf16[1, 512, 11008]"
  t1272 = prims.convert_element_type(t1264, dtypes.float32)  # t1272: "cuda:0 f32[1, 512, 11008]"
  t1273 = prims.convert_element_type(t1271, dtypes.float32)  # t1273: "cuda:0 f32[1, 512, 11008]"
  t1274 = ltorch.mul(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 11008]"
    # t1274 = prims.mul(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 11008]"
  t1275 = prims.convert_element_type(t1274, dtypes.bfloat16)  # t1275: "cuda:0 bf16[1, 512, 11008]"
  t1276 = prims.convert_element_type(t1275, dtypes.float32)  # t1276: "cuda:0 f32[1, 512, 11008]"
  t1277 = prims.convert_element_type(t1265, dtypes.float32)  # t1277: "cuda:0 f32[1, 512, 11008]"
  t1278 = ltorch.mul(t1276, t1277)  # t1278: "cuda:0 f32[1, 512, 11008]"
    # t1278 = prims.mul(t1276, t1277)  # t1278: "cuda:0 f32[1, 512, 11008]"
  t1279 = prims.convert_element_type(t1278, dtypes.bfloat16)  # t1279: "cuda:0 bf16[1, 512, 11008]"
  t1280 = prims.linear(t1279, t_transformer_h_8_mlp_proj_weight, None)  # t1280: "cuda:0 bf16[1, 512, 4096]"
  t1281 = prims.convert_element_type(t1280, dtypes.float32)  # t1281: "cuda:0 f32[1, 512, 4096]"
  t1282 = prims.convert_element_type(t1245, dtypes.float32)  # t1282: "cuda:0 f32[1, 512, 4096]"
  t1283 = ltorch.add(t1281, t1282, alpha=None)  # t1283: "cuda:0 f32[1, 512, 4096]"
    # t1283 = prims.add(t1281, t1282)  # t1283: "cuda:0 f32[1, 512, 4096]"
  t1284 = prims.convert_element_type(t1283, dtypes.bfloat16)  # t1284: "cuda:0 bf16[1, 512, 4096]"
  t1285 = prims.convert_element_type(t1284, dtypes.float32)  # t1285: "cuda:0 f32[1, 512, 4096]"
  t1286 = ltorch.mul(t1285, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"
    # t1286 = prims.mul(t1285, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"
  t1288 = prims.sum(t1286, (2,))  # t1288: "cuda:0 f32[1, 512]"
  t1289 = prims.broadcast_in_dim(t1288, [1, 512, 1], [0, 1])  # t1289: "cuda:0 f32[1, 512, 1]"
  t1291 = ltorch.true_divide(t1289, 4096.0)  # t1291: "cuda:0 f32[1, 512, 1]"
    # t1291 = prims.div(t1289, 4096.0)  # t1291: "cuda:0 f32[1, 512, 1]"
  t1293 = ltorch.add(t1291, 1e-05, alpha=None)  # t1293: "cuda:0 f32[1, 512, 1]"
    # t1293 = prims.add(t1291, 1e-05)  # t1293: "cuda:0 f32[1, 512, 1]"
  t1294 = prims.rsqrt(t1293)  # t1294: "cuda:0 f32[1, 512, 1]"
  t1295 = prims.broadcast_in_dim(t1294, (1, 512, 4096), (0, 1, 2))  # t1295: "cuda:0 f32[1, 512, 4096]"
  t1296 = ltorch.mul(t1285, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
    # t1296 = prims.mul(t1285, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
  t1297 = prims.convert_element_type(t1296, dtypes.bfloat16)  # t1297: "cuda:0 bf16[1, 512, 4096]"
  t1298 = prims.broadcast_in_dim(t_transformer_h_9_norm_1_weight, (1, 512, 4096), (2,))  # t1298: "cuda:0 bf16[1, 512, 4096]"
  t1299 = prims.convert_element_type(t1297, dtypes.float32)  # t1299: "cuda:0 f32[1, 512, 4096]"
  t1300 = prims.convert_element_type(t1298, dtypes.float32)  # t1300: "cuda:0 f32[1, 512, 4096]"
  t1301 = ltorch.mul(t1299, t1300)  # t1301: "cuda:0 f32[1, 512, 4096]"
    # t1301 = prims.mul(t1299, t1300)  # t1301: "cuda:0 f32[1, 512, 4096]"
  t1302 = prims.convert_element_type(t1301, dtypes.bfloat16)  # t1302: "cuda:0 bf16[1, 512, 4096]"
  t1303 = prims.linear(t1302, t_transformer_h_9_attn_attn_weight, None)  # t1303: "cuda:0 bf16[1, 512, 12288]"
  t1309 = prims.reshape(t1303, (1, 512, 32, 3, 128))  # t1309: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1315 = prims.transpose(t1309, (0, 2, 3, 1, 4))  # t1315: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1316, t1317, t1318) = ltorch.split(t1315, (1, 1, 1), 2)
    # t1316 = prims.slice_prim(t1315, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1316: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1317 = prims.slice_prim(t1315, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1317: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1318 = prims.slice_prim(t1315, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1318: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1324 = prims.reshape(t1316, (1, 32, 512, 128))  # t1324: "cuda:0 bf16[1, 32, 512, 128]"
  t1330 = prims.reshape(t1317, (1, 32, 512, 128))  # t1330: "cuda:0 bf16[1, 32, 512, 128]"
  t1336 = prims.reshape(t1318, (1, 32, 512, 128))  # t1336: "cuda:0 bf16[1, 32, 512, 128]"
  t1337 = prims.slice_prim(t1324, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1337: "cuda:0 bf16[1, 32, 512, 128]"
  t1338 = prims.slice_prim(t1337, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1338: "cuda:0 bf16[1, 32, 512, 64]"
  t1339 = prims.slice_prim(t1337, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1339: "cuda:0 bf16[1, 32, 512, 64]"
  t1340 = prims.convert_element_type(t1339, dtypes.float32)  # t1340: "cuda:0 f32[1, 32, 512, 64]"
  t1341 = prims.neg(t1340)  # t1341: "cuda:0 f32[1, 32, 512, 64]"
  t1342 = prims.convert_element_type(t1341, dtypes.bfloat16)  # t1342: "cuda:0 bf16[1, 32, 512, 64]"
  t1344 = prims.cat((t1342, t1338), -1)  # t1344: "cuda:0 bf16[1, 32, 512, 128]"
  t1345 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1345: "cuda:0 f32[1, 32, 512, 128]"
  t1346 = prims.convert_element_type(t1337, dtypes.float32)  # t1346: "cuda:0 f32[1, 32, 512, 128]"
  t1347 = ltorch.mul(t1346, t1345)  # t1347: "cuda:0 f32[1, 32, 512, 128]"
    # t1347 = prims.mul(t1346, t1345)  # t1347: "cuda:0 f32[1, 32, 512, 128]"
  t1348 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1348: "cuda:0 f32[1, 32, 512, 128]"
  t1349 = prims.convert_element_type(t1344, dtypes.float32)  # t1349: "cuda:0 f32[1, 32, 512, 128]"
  t1350 = ltorch.mul(t1349, t1348)  # t1350: "cuda:0 f32[1, 32, 512, 128]"
    # t1350 = prims.mul(t1349, t1348)  # t1350: "cuda:0 f32[1, 32, 512, 128]"
  t1351 = ltorch.add(t1347, t1350, alpha=None)  # t1351: "cuda:0 f32[1, 32, 512, 128]"
    # t1351 = prims.add(t1347, t1350)  # t1351: "cuda:0 f32[1, 32, 512, 128]"
  t1352 = prims.convert_element_type(t1351, dtypes.bfloat16)  # t1352: "cuda:0 bf16[1, 32, 512, 128]"
  t1353 = prims.slice_prim(t1330, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1353: "cuda:0 bf16[1, 32, 512, 128]"
  t1354 = prims.slice_prim(t1353, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1354: "cuda:0 bf16[1, 32, 512, 64]"
  t1355 = prims.slice_prim(t1353, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1355: "cuda:0 bf16[1, 32, 512, 64]"
  t1356 = prims.convert_element_type(t1355, dtypes.float32)  # t1356: "cuda:0 f32[1, 32, 512, 64]"
  t1357 = prims.neg(t1356)  # t1357: "cuda:0 f32[1, 32, 512, 64]"
  t1358 = prims.convert_element_type(t1357, dtypes.bfloat16)  # t1358: "cuda:0 bf16[1, 32, 512, 64]"
  t1360 = prims.cat((t1358, t1354), -1)  # t1360: "cuda:0 bf16[1, 32, 512, 128]"
  t1361 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1361: "cuda:0 f32[1, 32, 512, 128]"
  t1362 = prims.convert_element_type(t1353, dtypes.float32)  # t1362: "cuda:0 f32[1, 32, 512, 128]"
  t1363 = ltorch.mul(t1362, t1361)  # t1363: "cuda:0 f32[1, 32, 512, 128]"
    # t1363 = prims.mul(t1362, t1361)  # t1363: "cuda:0 f32[1, 32, 512, 128]"
  t1364 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1364: "cuda:0 f32[1, 32, 512, 128]"
  t1365 = prims.convert_element_type(t1360, dtypes.float32)  # t1365: "cuda:0 f32[1, 32, 512, 128]"
  t1366 = ltorch.mul(t1365, t1364)  # t1366: "cuda:0 f32[1, 32, 512, 128]"
    # t1366 = prims.mul(t1365, t1364)  # t1366: "cuda:0 f32[1, 32, 512, 128]"
  t1367 = ltorch.add(t1363, t1366, alpha=None)  # t1367: "cuda:0 f32[1, 32, 512, 128]"
    # t1367 = prims.add(t1363, t1366)  # t1367: "cuda:0 f32[1, 32, 512, 128]"
  t1368 = prims.convert_element_type(t1367, dtypes.bfloat16)  # t1368: "cuda:0 bf16[1, 32, 512, 128]"
  t1369 = prims.slice_prim(t1324, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1369: "cuda:0 bf16[1, 32, 512, 0]"
  t1371 = prims.cat((t1352, t1369), -1)  # t1371: "cuda:0 bf16[1, 32, 512, 128]"
  t1372 = prims.slice_prim(t1330, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1372: "cuda:0 bf16[1, 32, 512, 0]"
  t1374 = prims.cat((t1368, t1372), -1)  # t1374: "cuda:0 bf16[1, 32, 512, 128]"
  (t1375, t1376, t1377, t1378) = cudnn_sdpa_fwd(t1371, t1374, t1336, None, 0.0, True, scale=0.08838834764831843)
  t1381 = prims.transpose(t1375, (0, 2, 1, 3))  # t1381: "cuda:0 bf16[1, 512, 32, 128]"
  t1385 = prims.reshape(t1381, (1, 512, 4096))  # t1385: "cuda:0 bf16[1, 512, 4096]"
  t1386 = prims.linear(t1385, t_transformer_h_9_attn_proj_weight, None)  # t1386: "cuda:0 bf16[1, 512, 4096]"
  t1387 = prims.convert_element_type(t1386, dtypes.float32)  # t1387: "cuda:0 f32[1, 512, 4096]"
  t1388 = prims.convert_element_type(t1284, dtypes.float32)  # t1388: "cuda:0 f32[1, 512, 4096]"
  t1389 = ltorch.add(t1387, t1388, alpha=None)  # t1389: "cuda:0 f32[1, 512, 4096]"
    # t1389 = prims.add(t1387, t1388)  # t1389: "cuda:0 f32[1, 512, 4096]"
  t1390 = prims.convert_element_type(t1389, dtypes.bfloat16)  # t1390: "cuda:0 bf16[1, 512, 4096]"
  t1391 = prims.convert_element_type(t1390, dtypes.float32)  # t1391: "cuda:0 f32[1, 512, 4096]"
  t1392 = ltorch.mul(t1391, t1391)  # t1392: "cuda:0 f32[1, 512, 4096]"
    # t1392 = prims.mul(t1391, t1391)  # t1392: "cuda:0 f32[1, 512, 4096]"
  t1394 = prims.sum(t1392, (2,))  # t1394: "cuda:0 f32[1, 512]"
  t1395 = prims.broadcast_in_dim(t1394, [1, 512, 1], [0, 1])  # t1395: "cuda:0 f32[1, 512, 1]"
  t1397 = ltorch.true_divide(t1395, 4096.0)  # t1397: "cuda:0 f32[1, 512, 1]"
    # t1397 = prims.div(t1395, 4096.0)  # t1397: "cuda:0 f32[1, 512, 1]"
  t1399 = ltorch.add(t1397, 1e-05, alpha=None)  # t1399: "cuda:0 f32[1, 512, 1]"
    # t1399 = prims.add(t1397, 1e-05)  # t1399: "cuda:0 f32[1, 512, 1]"
  t1400 = prims.rsqrt(t1399)  # t1400: "cuda:0 f32[1, 512, 1]"
  t1401 = prims.broadcast_in_dim(t1400, (1, 512, 4096), (0, 1, 2))  # t1401: "cuda:0 f32[1, 512, 4096]"
  t1402 = ltorch.mul(t1391, t1401)  # t1402: "cuda:0 f32[1, 512, 4096]"
    # t1402 = prims.mul(t1391, t1401)  # t1402: "cuda:0 f32[1, 512, 4096]"
  t1403 = prims.convert_element_type(t1402, dtypes.bfloat16)  # t1403: "cuda:0 bf16[1, 512, 4096]"
  t1404 = prims.broadcast_in_dim(t_transformer_h_9_norm_2_weight, (1, 512, 4096), (2,))  # t1404: "cuda:0 bf16[1, 512, 4096]"
  t1405 = prims.convert_element_type(t1403, dtypes.float32)  # t1405: "cuda:0 f32[1, 512, 4096]"
  t1406 = prims.convert_element_type(t1404, dtypes.float32)  # t1406: "cuda:0 f32[1, 512, 4096]"
  t1407 = ltorch.mul(t1405, t1406)  # t1407: "cuda:0 f32[1, 512, 4096]"
    # t1407 = prims.mul(t1405, t1406)  # t1407: "cuda:0 f32[1, 512, 4096]"
  t1408 = prims.convert_element_type(t1407, dtypes.bfloat16)  # t1408: "cuda:0 bf16[1, 512, 4096]"
  t1409 = prims.linear(t1408, t_transformer_h_9_mlp_fc_1_weight, None)  # t1409: "cuda:0 bf16[1, 512, 11008]"
  t1410 = prims.linear(t1408, t_transformer_h_9_mlp_fc_2_weight, None)  # t1410: "cuda:0 bf16[1, 512, 11008]"
  t1411 = prims.convert_element_type(t1409, dtypes.float32)  # t1411: "cuda:0 f32[1, 512, 11008]"
  t1412 = prims.neg(t1411)  # t1412: "cuda:0 f32[1, 512, 11008]"
  t1413 = prims.exp(t1412)  # t1413: "cuda:0 f32[1, 512, 11008]"
  t1414 = ltorch.add(1.0, t1413, alpha=None)  # t1414: "cuda:0 f32[1, 512, 11008]"
    # t1414 = prims.add(1.0, t1413)  # t1414: "cuda:0 f32[1, 512, 11008]"
  t1415 = prims.reciprocal(t1414)  # t1415: "cuda:0 f32[1, 512, 11008]"
  t1416 = prims.convert_element_type(t1415, dtypes.bfloat16)  # t1416: "cuda:0 bf16[1, 512, 11008]"
  t1417 = prims.convert_element_type(t1409, dtypes.float32)  # t1417: "cuda:0 f32[1, 512, 11008]"
  t1418 = prims.convert_element_type(t1416, dtypes.float32)  # t1418: "cuda:0 f32[1, 512, 11008]"
  t1419 = ltorch.mul(t1417, t1418)  # t1419: "cuda:0 f32[1, 512, 11008]"
    # t1419 = prims.mul(t1417, t1418)  # t1419: "cuda:0 f32[1, 512, 11008]"
  t1420 = prims.convert_element_type(t1419, dtypes.bfloat16)  # t1420: "cuda:0 bf16[1, 512, 11008]"
  t1421 = prims.convert_element_type(t1420, dtypes.float32)  # t1421: "cuda:0 f32[1, 512, 11008]"
  t1422 = prims.convert_element_type(t1410, dtypes.float32)  # t1422: "cuda:0 f32[1, 512, 11008]"
  t1423 = ltorch.mul(t1421, t1422)  # t1423: "cuda:0 f32[1, 512, 11008]"
    # t1423 = prims.mul(t1421, t1422)  # t1423: "cuda:0 f32[1, 512, 11008]"
  t1424 = prims.convert_element_type(t1423, dtypes.bfloat16)  # t1424: "cuda:0 bf16[1, 512, 11008]"
  t1425 = prims.linear(t1424, t_transformer_h_9_mlp_proj_weight, None)  # t1425: "cuda:0 bf16[1, 512, 4096]"
  t1426 = prims.convert_element_type(t1425, dtypes.float32)  # t1426: "cuda:0 f32[1, 512, 4096]"
  t1427 = prims.convert_element_type(t1390, dtypes.float32)  # t1427: "cuda:0 f32[1, 512, 4096]"
  t1428 = ltorch.add(t1426, t1427, alpha=None)  # t1428: "cuda:0 f32[1, 512, 4096]"
    # t1428 = prims.add(t1426, t1427)  # t1428: "cuda:0 f32[1, 512, 4096]"
  t1429 = prims.convert_element_type(t1428, dtypes.bfloat16)  # t1429: "cuda:0 bf16[1, 512, 4096]"
  t1430 = prims.convert_element_type(t1429, dtypes.float32)  # t1430: "cuda:0 f32[1, 512, 4096]"
  t1431 = ltorch.mul(t1430, t1430)  # t1431: "cuda:0 f32[1, 512, 4096]"
    # t1431 = prims.mul(t1430, t1430)  # t1431: "cuda:0 f32[1, 512, 4096]"
  t1433 = prims.sum(t1431, (2,))  # t1433: "cuda:0 f32[1, 512]"
  t1434 = prims.broadcast_in_dim(t1433, [1, 512, 1], [0, 1])  # t1434: "cuda:0 f32[1, 512, 1]"
  t1436 = ltorch.true_divide(t1434, 4096.0)  # t1436: "cuda:0 f32[1, 512, 1]"
    # t1436 = prims.div(t1434, 4096.0)  # t1436: "cuda:0 f32[1, 512, 1]"
  t1438 = ltorch.add(t1436, 1e-05, alpha=None)  # t1438: "cuda:0 f32[1, 512, 1]"
    # t1438 = prims.add(t1436, 1e-05)  # t1438: "cuda:0 f32[1, 512, 1]"
  t1439 = prims.rsqrt(t1438)  # t1439: "cuda:0 f32[1, 512, 1]"
  t1440 = prims.broadcast_in_dim(t1439, (1, 512, 4096), (0, 1, 2))  # t1440: "cuda:0 f32[1, 512, 4096]"
  t1441 = ltorch.mul(t1430, t1440)  # t1441: "cuda:0 f32[1, 512, 4096]"
    # t1441 = prims.mul(t1430, t1440)  # t1441: "cuda:0 f32[1, 512, 4096]"
  t1442 = prims.convert_element_type(t1441, dtypes.bfloat16)  # t1442: "cuda:0 bf16[1, 512, 4096]"
  t1443 = prims.broadcast_in_dim(t_transformer_h_10_norm_1_weight, (1, 512, 4096), (2,))  # t1443: "cuda:0 bf16[1, 512, 4096]"
  t1444 = prims.convert_element_type(t1442, dtypes.float32)  # t1444: "cuda:0 f32[1, 512, 4096]"
  t1445 = prims.convert_element_type(t1443, dtypes.float32)  # t1445: "cuda:0 f32[1, 512, 4096]"
  t1446 = ltorch.mul(t1444, t1445)  # t1446: "cuda:0 f32[1, 512, 4096]"
    # t1446 = prims.mul(t1444, t1445)  # t1446: "cuda:0 f32[1, 512, 4096]"
  t1447 = prims.convert_element_type(t1446, dtypes.bfloat16)  # t1447: "cuda:0 bf16[1, 512, 4096]"
  t1448 = prims.linear(t1447, t_transformer_h_10_attn_attn_weight, None)  # t1448: "cuda:0 bf16[1, 512, 12288]"
  t1454 = prims.reshape(t1448, (1, 512, 32, 3, 128))  # t1454: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1460 = prims.transpose(t1454, (0, 2, 3, 1, 4))  # t1460: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1461, t1462, t1463) = ltorch.split(t1460, (1, 1, 1), 2)
    # t1461 = prims.slice_prim(t1460, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1461: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1462 = prims.slice_prim(t1460, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1462: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1463 = prims.slice_prim(t1460, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1463: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1469 = prims.reshape(t1461, (1, 32, 512, 128))  # t1469: "cuda:0 bf16[1, 32, 512, 128]"
  t1475 = prims.reshape(t1462, (1, 32, 512, 128))  # t1475: "cuda:0 bf16[1, 32, 512, 128]"
  t1481 = prims.reshape(t1463, (1, 32, 512, 128))  # t1481: "cuda:0 bf16[1, 32, 512, 128]"
  t1482 = prims.slice_prim(t1469, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1482: "cuda:0 bf16[1, 32, 512, 128]"
  t1483 = prims.slice_prim(t1482, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1483: "cuda:0 bf16[1, 32, 512, 64]"
  t1484 = prims.slice_prim(t1482, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1484: "cuda:0 bf16[1, 32, 512, 64]"
  t1485 = prims.convert_element_type(t1484, dtypes.float32)  # t1485: "cuda:0 f32[1, 32, 512, 64]"
  t1486 = prims.neg(t1485)  # t1486: "cuda:0 f32[1, 32, 512, 64]"
  t1487 = prims.convert_element_type(t1486, dtypes.bfloat16)  # t1487: "cuda:0 bf16[1, 32, 512, 64]"
  t1489 = prims.cat((t1487, t1483), -1)  # t1489: "cuda:0 bf16[1, 32, 512, 128]"
  t1490 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1490: "cuda:0 f32[1, 32, 512, 128]"
  t1491 = prims.convert_element_type(t1482, dtypes.float32)  # t1491: "cuda:0 f32[1, 32, 512, 128]"
  t1492 = ltorch.mul(t1491, t1490)  # t1492: "cuda:0 f32[1, 32, 512, 128]"
    # t1492 = prims.mul(t1491, t1490)  # t1492: "cuda:0 f32[1, 32, 512, 128]"
  t1493 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1493: "cuda:0 f32[1, 32, 512, 128]"
  t1494 = prims.convert_element_type(t1489, dtypes.float32)  # t1494: "cuda:0 f32[1, 32, 512, 128]"
  t1495 = ltorch.mul(t1494, t1493)  # t1495: "cuda:0 f32[1, 32, 512, 128]"
    # t1495 = prims.mul(t1494, t1493)  # t1495: "cuda:0 f32[1, 32, 512, 128]"
  t1496 = ltorch.add(t1492, t1495, alpha=None)  # t1496: "cuda:0 f32[1, 32, 512, 128]"
    # t1496 = prims.add(t1492, t1495)  # t1496: "cuda:0 f32[1, 32, 512, 128]"
  t1497 = prims.convert_element_type(t1496, dtypes.bfloat16)  # t1497: "cuda:0 bf16[1, 32, 512, 128]"
  t1498 = prims.slice_prim(t1475, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1498: "cuda:0 bf16[1, 32, 512, 128]"
  t1499 = prims.slice_prim(t1498, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1499: "cuda:0 bf16[1, 32, 512, 64]"
  t1500 = prims.slice_prim(t1498, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1500: "cuda:0 bf16[1, 32, 512, 64]"
  t1501 = prims.convert_element_type(t1500, dtypes.float32)  # t1501: "cuda:0 f32[1, 32, 512, 64]"
  t1502 = prims.neg(t1501)  # t1502: "cuda:0 f32[1, 32, 512, 64]"
  t1503 = prims.convert_element_type(t1502, dtypes.bfloat16)  # t1503: "cuda:0 bf16[1, 32, 512, 64]"
  t1505 = prims.cat((t1503, t1499), -1)  # t1505: "cuda:0 bf16[1, 32, 512, 128]"
  t1506 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1506: "cuda:0 f32[1, 32, 512, 128]"
  t1507 = prims.convert_element_type(t1498, dtypes.float32)  # t1507: "cuda:0 f32[1, 32, 512, 128]"
  t1508 = ltorch.mul(t1507, t1506)  # t1508: "cuda:0 f32[1, 32, 512, 128]"
    # t1508 = prims.mul(t1507, t1506)  # t1508: "cuda:0 f32[1, 32, 512, 128]"
  t1509 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1509: "cuda:0 f32[1, 32, 512, 128]"
  t1510 = prims.convert_element_type(t1505, dtypes.float32)  # t1510: "cuda:0 f32[1, 32, 512, 128]"
  t1511 = ltorch.mul(t1510, t1509)  # t1511: "cuda:0 f32[1, 32, 512, 128]"
    # t1511 = prims.mul(t1510, t1509)  # t1511: "cuda:0 f32[1, 32, 512, 128]"
  t1512 = ltorch.add(t1508, t1511, alpha=None)  # t1512: "cuda:0 f32[1, 32, 512, 128]"
    # t1512 = prims.add(t1508, t1511)  # t1512: "cuda:0 f32[1, 32, 512, 128]"
  t1513 = prims.convert_element_type(t1512, dtypes.bfloat16)  # t1513: "cuda:0 bf16[1, 32, 512, 128]"
  t1514 = prims.slice_prim(t1469, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1514: "cuda:0 bf16[1, 32, 512, 0]"
  t1516 = prims.cat((t1497, t1514), -1)  # t1516: "cuda:0 bf16[1, 32, 512, 128]"
  t1517 = prims.slice_prim(t1475, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1517: "cuda:0 bf16[1, 32, 512, 0]"
  t1519 = prims.cat((t1513, t1517), -1)  # t1519: "cuda:0 bf16[1, 32, 512, 128]"
  (t1520, t1521, t1522, t1523) = cudnn_sdpa_fwd(t1516, t1519, t1481, None, 0.0, True, scale=0.08838834764831843)
  t1526 = prims.transpose(t1520, (0, 2, 1, 3))  # t1526: "cuda:0 bf16[1, 512, 32, 128]"
  t1530 = prims.reshape(t1526, (1, 512, 4096))  # t1530: "cuda:0 bf16[1, 512, 4096]"
  t1531 = prims.linear(t1530, t_transformer_h_10_attn_proj_weight, None)  # t1531: "cuda:0 bf16[1, 512, 4096]"
  t1532 = prims.convert_element_type(t1531, dtypes.float32)  # t1532: "cuda:0 f32[1, 512, 4096]"
  t1533 = prims.convert_element_type(t1429, dtypes.float32)  # t1533: "cuda:0 f32[1, 512, 4096]"
  t1534 = ltorch.add(t1532, t1533, alpha=None)  # t1534: "cuda:0 f32[1, 512, 4096]"
    # t1534 = prims.add(t1532, t1533)  # t1534: "cuda:0 f32[1, 512, 4096]"
  t1535 = prims.convert_element_type(t1534, dtypes.bfloat16)  # t1535: "cuda:0 bf16[1, 512, 4096]"
  t1536 = prims.convert_element_type(t1535, dtypes.float32)  # t1536: "cuda:0 f32[1, 512, 4096]"
  t1537 = ltorch.mul(t1536, t1536)  # t1537: "cuda:0 f32[1, 512, 4096]"
    # t1537 = prims.mul(t1536, t1536)  # t1537: "cuda:0 f32[1, 512, 4096]"
  t1539 = prims.sum(t1537, (2,))  # t1539: "cuda:0 f32[1, 512]"
  t1540 = prims.broadcast_in_dim(t1539, [1, 512, 1], [0, 1])  # t1540: "cuda:0 f32[1, 512, 1]"
  t1542 = ltorch.true_divide(t1540, 4096.0)  # t1542: "cuda:0 f32[1, 512, 1]"
    # t1542 = prims.div(t1540, 4096.0)  # t1542: "cuda:0 f32[1, 512, 1]"
  t1544 = ltorch.add(t1542, 1e-05, alpha=None)  # t1544: "cuda:0 f32[1, 512, 1]"
    # t1544 = prims.add(t1542, 1e-05)  # t1544: "cuda:0 f32[1, 512, 1]"
  t1545 = prims.rsqrt(t1544)  # t1545: "cuda:0 f32[1, 512, 1]"
  t1546 = prims.broadcast_in_dim(t1545, (1, 512, 4096), (0, 1, 2))  # t1546: "cuda:0 f32[1, 512, 4096]"
  t1547 = ltorch.mul(t1536, t1546)  # t1547: "cuda:0 f32[1, 512, 4096]"
    # t1547 = prims.mul(t1536, t1546)  # t1547: "cuda:0 f32[1, 512, 4096]"
  t1548 = prims.convert_element_type(t1547, dtypes.bfloat16)  # t1548: "cuda:0 bf16[1, 512, 4096]"
  t1549 = prims.broadcast_in_dim(t_transformer_h_10_norm_2_weight, (1, 512, 4096), (2,))  # t1549: "cuda:0 bf16[1, 512, 4096]"
  t1550 = prims.convert_element_type(t1548, dtypes.float32)  # t1550: "cuda:0 f32[1, 512, 4096]"
  t1551 = prims.convert_element_type(t1549, dtypes.float32)  # t1551: "cuda:0 f32[1, 512, 4096]"
  t1552 = ltorch.mul(t1550, t1551)  # t1552: "cuda:0 f32[1, 512, 4096]"
    # t1552 = prims.mul(t1550, t1551)  # t1552: "cuda:0 f32[1, 512, 4096]"
  t1553 = prims.convert_element_type(t1552, dtypes.bfloat16)  # t1553: "cuda:0 bf16[1, 512, 4096]"
  t1554 = prims.linear(t1553, t_transformer_h_10_mlp_fc_1_weight, None)  # t1554: "cuda:0 bf16[1, 512, 11008]"
  t1555 = prims.linear(t1553, t_transformer_h_10_mlp_fc_2_weight, None)  # t1555: "cuda:0 bf16[1, 512, 11008]"
  t1556 = prims.convert_element_type(t1554, dtypes.float32)  # t1556: "cuda:0 f32[1, 512, 11008]"
  t1557 = prims.neg(t1556)  # t1557: "cuda:0 f32[1, 512, 11008]"
  t1558 = prims.exp(t1557)  # t1558: "cuda:0 f32[1, 512, 11008]"
  t1559 = ltorch.add(1.0, t1558, alpha=None)  # t1559: "cuda:0 f32[1, 512, 11008]"
    # t1559 = prims.add(1.0, t1558)  # t1559: "cuda:0 f32[1, 512, 11008]"
  t1560 = prims.reciprocal(t1559)  # t1560: "cuda:0 f32[1, 512, 11008]"
  t1561 = prims.convert_element_type(t1560, dtypes.bfloat16)  # t1561: "cuda:0 bf16[1, 512, 11008]"
  t1562 = prims.convert_element_type(t1554, dtypes.float32)  # t1562: "cuda:0 f32[1, 512, 11008]"
  t1563 = prims.convert_element_type(t1561, dtypes.float32)  # t1563: "cuda:0 f32[1, 512, 11008]"
  t1564 = ltorch.mul(t1562, t1563)  # t1564: "cuda:0 f32[1, 512, 11008]"
    # t1564 = prims.mul(t1562, t1563)  # t1564: "cuda:0 f32[1, 512, 11008]"
  t1565 = prims.convert_element_type(t1564, dtypes.bfloat16)  # t1565: "cuda:0 bf16[1, 512, 11008]"
  t1566 = prims.convert_element_type(t1565, dtypes.float32)  # t1566: "cuda:0 f32[1, 512, 11008]"
  t1567 = prims.convert_element_type(t1555, dtypes.float32)  # t1567: "cuda:0 f32[1, 512, 11008]"
  t1568 = ltorch.mul(t1566, t1567)  # t1568: "cuda:0 f32[1, 512, 11008]"
    # t1568 = prims.mul(t1566, t1567)  # t1568: "cuda:0 f32[1, 512, 11008]"
  t1569 = prims.convert_element_type(t1568, dtypes.bfloat16)  # t1569: "cuda:0 bf16[1, 512, 11008]"
  t1570 = prims.linear(t1569, t_transformer_h_10_mlp_proj_weight, None)  # t1570: "cuda:0 bf16[1, 512, 4096]"
  t1571 = prims.convert_element_type(t1570, dtypes.float32)  # t1571: "cuda:0 f32[1, 512, 4096]"
  t1572 = prims.convert_element_type(t1535, dtypes.float32)  # t1572: "cuda:0 f32[1, 512, 4096]"
  t1573 = ltorch.add(t1571, t1572, alpha=None)  # t1573: "cuda:0 f32[1, 512, 4096]"
    # t1573 = prims.add(t1571, t1572)  # t1573: "cuda:0 f32[1, 512, 4096]"
  t1574 = prims.convert_element_type(t1573, dtypes.bfloat16)  # t1574: "cuda:0 bf16[1, 512, 4096]"
  t1575 = prims.convert_element_type(t1574, dtypes.float32)  # t1575: "cuda:0 f32[1, 512, 4096]"
  t1576 = ltorch.mul(t1575, t1575)  # t1576: "cuda:0 f32[1, 512, 4096]"
    # t1576 = prims.mul(t1575, t1575)  # t1576: "cuda:0 f32[1, 512, 4096]"
  t1578 = prims.sum(t1576, (2,))  # t1578: "cuda:0 f32[1, 512]"
  t1579 = prims.broadcast_in_dim(t1578, [1, 512, 1], [0, 1])  # t1579: "cuda:0 f32[1, 512, 1]"
  t1581 = ltorch.true_divide(t1579, 4096.0)  # t1581: "cuda:0 f32[1, 512, 1]"
    # t1581 = prims.div(t1579, 4096.0)  # t1581: "cuda:0 f32[1, 512, 1]"
  t1583 = ltorch.add(t1581, 1e-05, alpha=None)  # t1583: "cuda:0 f32[1, 512, 1]"
    # t1583 = prims.add(t1581, 1e-05)  # t1583: "cuda:0 f32[1, 512, 1]"
  t1584 = prims.rsqrt(t1583)  # t1584: "cuda:0 f32[1, 512, 1]"
  t1585 = prims.broadcast_in_dim(t1584, (1, 512, 4096), (0, 1, 2))  # t1585: "cuda:0 f32[1, 512, 4096]"
  t1586 = ltorch.mul(t1575, t1585)  # t1586: "cuda:0 f32[1, 512, 4096]"
    # t1586 = prims.mul(t1575, t1585)  # t1586: "cuda:0 f32[1, 512, 4096]"
  t1587 = prims.convert_element_type(t1586, dtypes.bfloat16)  # t1587: "cuda:0 bf16[1, 512, 4096]"
  t1588 = prims.broadcast_in_dim(t_transformer_h_11_norm_1_weight, (1, 512, 4096), (2,))  # t1588: "cuda:0 bf16[1, 512, 4096]"
  t1589 = prims.convert_element_type(t1587, dtypes.float32)  # t1589: "cuda:0 f32[1, 512, 4096]"
  t1590 = prims.convert_element_type(t1588, dtypes.float32)  # t1590: "cuda:0 f32[1, 512, 4096]"
  t1591 = ltorch.mul(t1589, t1590)  # t1591: "cuda:0 f32[1, 512, 4096]"
    # t1591 = prims.mul(t1589, t1590)  # t1591: "cuda:0 f32[1, 512, 4096]"
  t1592 = prims.convert_element_type(t1591, dtypes.bfloat16)  # t1592: "cuda:0 bf16[1, 512, 4096]"
  t1593 = prims.linear(t1592, t_transformer_h_11_attn_attn_weight, None)  # t1593: "cuda:0 bf16[1, 512, 12288]"
  t1599 = prims.reshape(t1593, (1, 512, 32, 3, 128))  # t1599: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1605 = prims.transpose(t1599, (0, 2, 3, 1, 4))  # t1605: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1606, t1607, t1608) = ltorch.split(t1605, (1, 1, 1), 2)
    # t1606 = prims.slice_prim(t1605, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1606: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1607 = prims.slice_prim(t1605, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1607: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1608 = prims.slice_prim(t1605, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1608: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1614 = prims.reshape(t1606, (1, 32, 512, 128))  # t1614: "cuda:0 bf16[1, 32, 512, 128]"
  t1620 = prims.reshape(t1607, (1, 32, 512, 128))  # t1620: "cuda:0 bf16[1, 32, 512, 128]"
  t1626 = prims.reshape(t1608, (1, 32, 512, 128))  # t1626: "cuda:0 bf16[1, 32, 512, 128]"
  t1627 = prims.slice_prim(t1614, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1627: "cuda:0 bf16[1, 32, 512, 128]"
  t1628 = prims.slice_prim(t1627, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1628: "cuda:0 bf16[1, 32, 512, 64]"
  t1629 = prims.slice_prim(t1627, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1629: "cuda:0 bf16[1, 32, 512, 64]"
  t1630 = prims.convert_element_type(t1629, dtypes.float32)  # t1630: "cuda:0 f32[1, 32, 512, 64]"
  t1631 = prims.neg(t1630)  # t1631: "cuda:0 f32[1, 32, 512, 64]"
  t1632 = prims.convert_element_type(t1631, dtypes.bfloat16)  # t1632: "cuda:0 bf16[1, 32, 512, 64]"
  t1634 = prims.cat((t1632, t1628), -1)  # t1634: "cuda:0 bf16[1, 32, 512, 128]"
  t1635 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1635: "cuda:0 f32[1, 32, 512, 128]"
  t1636 = prims.convert_element_type(t1627, dtypes.float32)  # t1636: "cuda:0 f32[1, 32, 512, 128]"
  t1637 = ltorch.mul(t1636, t1635)  # t1637: "cuda:0 f32[1, 32, 512, 128]"
    # t1637 = prims.mul(t1636, t1635)  # t1637: "cuda:0 f32[1, 32, 512, 128]"
  t1638 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1638: "cuda:0 f32[1, 32, 512, 128]"
  t1639 = prims.convert_element_type(t1634, dtypes.float32)  # t1639: "cuda:0 f32[1, 32, 512, 128]"
  t1640 = ltorch.mul(t1639, t1638)  # t1640: "cuda:0 f32[1, 32, 512, 128]"
    # t1640 = prims.mul(t1639, t1638)  # t1640: "cuda:0 f32[1, 32, 512, 128]"
  t1641 = ltorch.add(t1637, t1640, alpha=None)  # t1641: "cuda:0 f32[1, 32, 512, 128]"
    # t1641 = prims.add(t1637, t1640)  # t1641: "cuda:0 f32[1, 32, 512, 128]"
  t1642 = prims.convert_element_type(t1641, dtypes.bfloat16)  # t1642: "cuda:0 bf16[1, 32, 512, 128]"
  t1643 = prims.slice_prim(t1620, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1643: "cuda:0 bf16[1, 32, 512, 128]"
  t1644 = prims.slice_prim(t1643, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1644: "cuda:0 bf16[1, 32, 512, 64]"
  t1645 = prims.slice_prim(t1643, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1645: "cuda:0 bf16[1, 32, 512, 64]"
  t1646 = prims.convert_element_type(t1645, dtypes.float32)  # t1646: "cuda:0 f32[1, 32, 512, 64]"
  t1647 = prims.neg(t1646)  # t1647: "cuda:0 f32[1, 32, 512, 64]"
  t1648 = prims.convert_element_type(t1647, dtypes.bfloat16)  # t1648: "cuda:0 bf16[1, 32, 512, 64]"
  t1650 = prims.cat((t1648, t1644), -1)  # t1650: "cuda:0 bf16[1, 32, 512, 128]"
  t1651 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1651: "cuda:0 f32[1, 32, 512, 128]"
  t1652 = prims.convert_element_type(t1643, dtypes.float32)  # t1652: "cuda:0 f32[1, 32, 512, 128]"
  t1653 = ltorch.mul(t1652, t1651)  # t1653: "cuda:0 f32[1, 32, 512, 128]"
    # t1653 = prims.mul(t1652, t1651)  # t1653: "cuda:0 f32[1, 32, 512, 128]"
  t1654 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1654: "cuda:0 f32[1, 32, 512, 128]"
  t1655 = prims.convert_element_type(t1650, dtypes.float32)  # t1655: "cuda:0 f32[1, 32, 512, 128]"
  t1656 = ltorch.mul(t1655, t1654)  # t1656: "cuda:0 f32[1, 32, 512, 128]"
    # t1656 = prims.mul(t1655, t1654)  # t1656: "cuda:0 f32[1, 32, 512, 128]"
  t1657 = ltorch.add(t1653, t1656, alpha=None)  # t1657: "cuda:0 f32[1, 32, 512, 128]"
    # t1657 = prims.add(t1653, t1656)  # t1657: "cuda:0 f32[1, 32, 512, 128]"
  t1658 = prims.convert_element_type(t1657, dtypes.bfloat16)  # t1658: "cuda:0 bf16[1, 32, 512, 128]"
  t1659 = prims.slice_prim(t1614, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1659: "cuda:0 bf16[1, 32, 512, 0]"
  t1661 = prims.cat((t1642, t1659), -1)  # t1661: "cuda:0 bf16[1, 32, 512, 128]"
  t1662 = prims.slice_prim(t1620, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1662: "cuda:0 bf16[1, 32, 512, 0]"
  t1664 = prims.cat((t1658, t1662), -1)  # t1664: "cuda:0 bf16[1, 32, 512, 128]"
  (t1665, t1666, t1667, t1668) = cudnn_sdpa_fwd(t1661, t1664, t1626, None, 0.0, True, scale=0.08838834764831843)
  t1671 = prims.transpose(t1665, (0, 2, 1, 3))  # t1671: "cuda:0 bf16[1, 512, 32, 128]"
  t1675 = prims.reshape(t1671, (1, 512, 4096))  # t1675: "cuda:0 bf16[1, 512, 4096]"
  t1676 = prims.linear(t1675, t_transformer_h_11_attn_proj_weight, None)  # t1676: "cuda:0 bf16[1, 512, 4096]"
  t1677 = prims.convert_element_type(t1676, dtypes.float32)  # t1677: "cuda:0 f32[1, 512, 4096]"
  t1678 = prims.convert_element_type(t1574, dtypes.float32)  # t1678: "cuda:0 f32[1, 512, 4096]"
  t1679 = ltorch.add(t1677, t1678, alpha=None)  # t1679: "cuda:0 f32[1, 512, 4096]"
    # t1679 = prims.add(t1677, t1678)  # t1679: "cuda:0 f32[1, 512, 4096]"
  t1680 = prims.convert_element_type(t1679, dtypes.bfloat16)  # t1680: "cuda:0 bf16[1, 512, 4096]"
  t1681 = prims.convert_element_type(t1680, dtypes.float32)  # t1681: "cuda:0 f32[1, 512, 4096]"
  t1682 = ltorch.mul(t1681, t1681)  # t1682: "cuda:0 f32[1, 512, 4096]"
    # t1682 = prims.mul(t1681, t1681)  # t1682: "cuda:0 f32[1, 512, 4096]"
  t1684 = prims.sum(t1682, (2,))  # t1684: "cuda:0 f32[1, 512]"
  t1685 = prims.broadcast_in_dim(t1684, [1, 512, 1], [0, 1])  # t1685: "cuda:0 f32[1, 512, 1]"
  t1687 = ltorch.true_divide(t1685, 4096.0)  # t1687: "cuda:0 f32[1, 512, 1]"
    # t1687 = prims.div(t1685, 4096.0)  # t1687: "cuda:0 f32[1, 512, 1]"
  t1689 = ltorch.add(t1687, 1e-05, alpha=None)  # t1689: "cuda:0 f32[1, 512, 1]"
    # t1689 = prims.add(t1687, 1e-05)  # t1689: "cuda:0 f32[1, 512, 1]"
  t1690 = prims.rsqrt(t1689)  # t1690: "cuda:0 f32[1, 512, 1]"
  t1691 = prims.broadcast_in_dim(t1690, (1, 512, 4096), (0, 1, 2))  # t1691: "cuda:0 f32[1, 512, 4096]"
  t1692 = ltorch.mul(t1681, t1691)  # t1692: "cuda:0 f32[1, 512, 4096]"
    # t1692 = prims.mul(t1681, t1691)  # t1692: "cuda:0 f32[1, 512, 4096]"
  t1693 = prims.convert_element_type(t1692, dtypes.bfloat16)  # t1693: "cuda:0 bf16[1, 512, 4096]"
  t1694 = prims.broadcast_in_dim(t_transformer_h_11_norm_2_weight, (1, 512, 4096), (2,))  # t1694: "cuda:0 bf16[1, 512, 4096]"
  t1695 = prims.convert_element_type(t1693, dtypes.float32)  # t1695: "cuda:0 f32[1, 512, 4096]"
  t1696 = prims.convert_element_type(t1694, dtypes.float32)  # t1696: "cuda:0 f32[1, 512, 4096]"
  t1697 = ltorch.mul(t1695, t1696)  # t1697: "cuda:0 f32[1, 512, 4096]"
    # t1697 = prims.mul(t1695, t1696)  # t1697: "cuda:0 f32[1, 512, 4096]"
  t1698 = prims.convert_element_type(t1697, dtypes.bfloat16)  # t1698: "cuda:0 bf16[1, 512, 4096]"
  t1699 = prims.linear(t1698, t_transformer_h_11_mlp_fc_1_weight, None)  # t1699: "cuda:0 bf16[1, 512, 11008]"
  t1700 = prims.linear(t1698, t_transformer_h_11_mlp_fc_2_weight, None)  # t1700: "cuda:0 bf16[1, 512, 11008]"
  t1701 = prims.convert_element_type(t1699, dtypes.float32)  # t1701: "cuda:0 f32[1, 512, 11008]"
  t1702 = prims.neg(t1701)  # t1702: "cuda:0 f32[1, 512, 11008]"
  t1703 = prims.exp(t1702)  # t1703: "cuda:0 f32[1, 512, 11008]"
  t1704 = ltorch.add(1.0, t1703, alpha=None)  # t1704: "cuda:0 f32[1, 512, 11008]"
    # t1704 = prims.add(1.0, t1703)  # t1704: "cuda:0 f32[1, 512, 11008]"
  t1705 = prims.reciprocal(t1704)  # t1705: "cuda:0 f32[1, 512, 11008]"
  t1706 = prims.convert_element_type(t1705, dtypes.bfloat16)  # t1706: "cuda:0 bf16[1, 512, 11008]"
  t1707 = prims.convert_element_type(t1699, dtypes.float32)  # t1707: "cuda:0 f32[1, 512, 11008]"
  t1708 = prims.convert_element_type(t1706, dtypes.float32)  # t1708: "cuda:0 f32[1, 512, 11008]"
  t1709 = ltorch.mul(t1707, t1708)  # t1709: "cuda:0 f32[1, 512, 11008]"
    # t1709 = prims.mul(t1707, t1708)  # t1709: "cuda:0 f32[1, 512, 11008]"
  t1710 = prims.convert_element_type(t1709, dtypes.bfloat16)  # t1710: "cuda:0 bf16[1, 512, 11008]"
  t1711 = prims.convert_element_type(t1710, dtypes.float32)  # t1711: "cuda:0 f32[1, 512, 11008]"
  t1712 = prims.convert_element_type(t1700, dtypes.float32)  # t1712: "cuda:0 f32[1, 512, 11008]"
  t1713 = ltorch.mul(t1711, t1712)  # t1713: "cuda:0 f32[1, 512, 11008]"
    # t1713 = prims.mul(t1711, t1712)  # t1713: "cuda:0 f32[1, 512, 11008]"
  t1714 = prims.convert_element_type(t1713, dtypes.bfloat16)  # t1714: "cuda:0 bf16[1, 512, 11008]"
  t1715 = prims.linear(t1714, t_transformer_h_11_mlp_proj_weight, None)  # t1715: "cuda:0 bf16[1, 512, 4096]"
  t1716 = prims.convert_element_type(t1715, dtypes.float32)  # t1716: "cuda:0 f32[1, 512, 4096]"
  t1717 = prims.convert_element_type(t1680, dtypes.float32)  # t1717: "cuda:0 f32[1, 512, 4096]"
  t1718 = ltorch.add(t1716, t1717, alpha=None)  # t1718: "cuda:0 f32[1, 512, 4096]"
    # t1718 = prims.add(t1716, t1717)  # t1718: "cuda:0 f32[1, 512, 4096]"
  t1719 = prims.convert_element_type(t1718, dtypes.bfloat16)  # t1719: "cuda:0 bf16[1, 512, 4096]"
  t1720 = prims.convert_element_type(t1719, dtypes.float32)  # t1720: "cuda:0 f32[1, 512, 4096]"
  t1721 = ltorch.mul(t1720, t1720)  # t1721: "cuda:0 f32[1, 512, 4096]"
    # t1721 = prims.mul(t1720, t1720)  # t1721: "cuda:0 f32[1, 512, 4096]"
  t1723 = prims.sum(t1721, (2,))  # t1723: "cuda:0 f32[1, 512]"
  t1724 = prims.broadcast_in_dim(t1723, [1, 512, 1], [0, 1])  # t1724: "cuda:0 f32[1, 512, 1]"
  t1726 = ltorch.true_divide(t1724, 4096.0)  # t1726: "cuda:0 f32[1, 512, 1]"
    # t1726 = prims.div(t1724, 4096.0)  # t1726: "cuda:0 f32[1, 512, 1]"
  t1728 = ltorch.add(t1726, 1e-05, alpha=None)  # t1728: "cuda:0 f32[1, 512, 1]"
    # t1728 = prims.add(t1726, 1e-05)  # t1728: "cuda:0 f32[1, 512, 1]"
  t1729 = prims.rsqrt(t1728)  # t1729: "cuda:0 f32[1, 512, 1]"
  t1730 = prims.broadcast_in_dim(t1729, (1, 512, 4096), (0, 1, 2))  # t1730: "cuda:0 f32[1, 512, 4096]"
  t1731 = ltorch.mul(t1720, t1730)  # t1731: "cuda:0 f32[1, 512, 4096]"
    # t1731 = prims.mul(t1720, t1730)  # t1731: "cuda:0 f32[1, 512, 4096]"
  t1732 = prims.convert_element_type(t1731, dtypes.bfloat16)  # t1732: "cuda:0 bf16[1, 512, 4096]"
  t1733 = prims.broadcast_in_dim(t_transformer_h_12_norm_1_weight, (1, 512, 4096), (2,))  # t1733: "cuda:0 bf16[1, 512, 4096]"
  t1734 = prims.convert_element_type(t1732, dtypes.float32)  # t1734: "cuda:0 f32[1, 512, 4096]"
  t1735 = prims.convert_element_type(t1733, dtypes.float32)  # t1735: "cuda:0 f32[1, 512, 4096]"
  t1736 = ltorch.mul(t1734, t1735)  # t1736: "cuda:0 f32[1, 512, 4096]"
    # t1736 = prims.mul(t1734, t1735)  # t1736: "cuda:0 f32[1, 512, 4096]"
  t1737 = prims.convert_element_type(t1736, dtypes.bfloat16)  # t1737: "cuda:0 bf16[1, 512, 4096]"
  t1738 = prims.linear(t1737, t_transformer_h_12_attn_attn_weight, None)  # t1738: "cuda:0 bf16[1, 512, 12288]"
  t1744 = prims.reshape(t1738, (1, 512, 32, 3, 128))  # t1744: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1750 = prims.transpose(t1744, (0, 2, 3, 1, 4))  # t1750: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1751, t1752, t1753) = ltorch.split(t1750, (1, 1, 1), 2)
    # t1751 = prims.slice_prim(t1750, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1751: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1752 = prims.slice_prim(t1750, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1752: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1753 = prims.slice_prim(t1750, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1753: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1759 = prims.reshape(t1751, (1, 32, 512, 128))  # t1759: "cuda:0 bf16[1, 32, 512, 128]"
  t1765 = prims.reshape(t1752, (1, 32, 512, 128))  # t1765: "cuda:0 bf16[1, 32, 512, 128]"
  t1771 = prims.reshape(t1753, (1, 32, 512, 128))  # t1771: "cuda:0 bf16[1, 32, 512, 128]"
  t1772 = prims.slice_prim(t1759, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1772: "cuda:0 bf16[1, 32, 512, 128]"
  t1773 = prims.slice_prim(t1772, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1773: "cuda:0 bf16[1, 32, 512, 64]"
  t1774 = prims.slice_prim(t1772, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1774: "cuda:0 bf16[1, 32, 512, 64]"
  t1775 = prims.convert_element_type(t1774, dtypes.float32)  # t1775: "cuda:0 f32[1, 32, 512, 64]"
  t1776 = prims.neg(t1775)  # t1776: "cuda:0 f32[1, 32, 512, 64]"
  t1777 = prims.convert_element_type(t1776, dtypes.bfloat16)  # t1777: "cuda:0 bf16[1, 32, 512, 64]"
  t1779 = prims.cat((t1777, t1773), -1)  # t1779: "cuda:0 bf16[1, 32, 512, 128]"
  t1780 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1780: "cuda:0 f32[1, 32, 512, 128]"
  t1781 = prims.convert_element_type(t1772, dtypes.float32)  # t1781: "cuda:0 f32[1, 32, 512, 128]"
  t1782 = ltorch.mul(t1781, t1780)  # t1782: "cuda:0 f32[1, 32, 512, 128]"
    # t1782 = prims.mul(t1781, t1780)  # t1782: "cuda:0 f32[1, 32, 512, 128]"
  t1783 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1783: "cuda:0 f32[1, 32, 512, 128]"
  t1784 = prims.convert_element_type(t1779, dtypes.float32)  # t1784: "cuda:0 f32[1, 32, 512, 128]"
  t1785 = ltorch.mul(t1784, t1783)  # t1785: "cuda:0 f32[1, 32, 512, 128]"
    # t1785 = prims.mul(t1784, t1783)  # t1785: "cuda:0 f32[1, 32, 512, 128]"
  t1786 = ltorch.add(t1782, t1785, alpha=None)  # t1786: "cuda:0 f32[1, 32, 512, 128]"
    # t1786 = prims.add(t1782, t1785)  # t1786: "cuda:0 f32[1, 32, 512, 128]"
  t1787 = prims.convert_element_type(t1786, dtypes.bfloat16)  # t1787: "cuda:0 bf16[1, 32, 512, 128]"
  t1788 = prims.slice_prim(t1765, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1788: "cuda:0 bf16[1, 32, 512, 128]"
  t1789 = prims.slice_prim(t1788, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1789: "cuda:0 bf16[1, 32, 512, 64]"
  t1790 = prims.slice_prim(t1788, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1790: "cuda:0 bf16[1, 32, 512, 64]"
  t1791 = prims.convert_element_type(t1790, dtypes.float32)  # t1791: "cuda:0 f32[1, 32, 512, 64]"
  t1792 = prims.neg(t1791)  # t1792: "cuda:0 f32[1, 32, 512, 64]"
  t1793 = prims.convert_element_type(t1792, dtypes.bfloat16)  # t1793: "cuda:0 bf16[1, 32, 512, 64]"
  t1795 = prims.cat((t1793, t1789), -1)  # t1795: "cuda:0 bf16[1, 32, 512, 128]"
  t1796 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1796: "cuda:0 f32[1, 32, 512, 128]"
  t1797 = prims.convert_element_type(t1788, dtypes.float32)  # t1797: "cuda:0 f32[1, 32, 512, 128]"
  t1798 = ltorch.mul(t1797, t1796)  # t1798: "cuda:0 f32[1, 32, 512, 128]"
    # t1798 = prims.mul(t1797, t1796)  # t1798: "cuda:0 f32[1, 32, 512, 128]"
  t1799 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1799: "cuda:0 f32[1, 32, 512, 128]"
  t1800 = prims.convert_element_type(t1795, dtypes.float32)  # t1800: "cuda:0 f32[1, 32, 512, 128]"
  t1801 = ltorch.mul(t1800, t1799)  # t1801: "cuda:0 f32[1, 32, 512, 128]"
    # t1801 = prims.mul(t1800, t1799)  # t1801: "cuda:0 f32[1, 32, 512, 128]"
  t1802 = ltorch.add(t1798, t1801, alpha=None)  # t1802: "cuda:0 f32[1, 32, 512, 128]"
    # t1802 = prims.add(t1798, t1801)  # t1802: "cuda:0 f32[1, 32, 512, 128]"
  t1803 = prims.convert_element_type(t1802, dtypes.bfloat16)  # t1803: "cuda:0 bf16[1, 32, 512, 128]"
  t1804 = prims.slice_prim(t1759, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1804: "cuda:0 bf16[1, 32, 512, 0]"
  t1806 = prims.cat((t1787, t1804), -1)  # t1806: "cuda:0 bf16[1, 32, 512, 128]"
  t1807 = prims.slice_prim(t1765, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1807: "cuda:0 bf16[1, 32, 512, 0]"
  t1809 = prims.cat((t1803, t1807), -1)  # t1809: "cuda:0 bf16[1, 32, 512, 128]"
  (t1810, t1811, t1812, t1813) = cudnn_sdpa_fwd(t1806, t1809, t1771, None, 0.0, True, scale=0.08838834764831843)
  t1816 = prims.transpose(t1810, (0, 2, 1, 3))  # t1816: "cuda:0 bf16[1, 512, 32, 128]"
  t1820 = prims.reshape(t1816, (1, 512, 4096))  # t1820: "cuda:0 bf16[1, 512, 4096]"
  t1821 = prims.linear(t1820, t_transformer_h_12_attn_proj_weight, None)  # t1821: "cuda:0 bf16[1, 512, 4096]"
  t1822 = prims.convert_element_type(t1821, dtypes.float32)  # t1822: "cuda:0 f32[1, 512, 4096]"
  t1823 = prims.convert_element_type(t1719, dtypes.float32)  # t1823: "cuda:0 f32[1, 512, 4096]"
  t1824 = ltorch.add(t1822, t1823, alpha=None)  # t1824: "cuda:0 f32[1, 512, 4096]"
    # t1824 = prims.add(t1822, t1823)  # t1824: "cuda:0 f32[1, 512, 4096]"
  t1825 = prims.convert_element_type(t1824, dtypes.bfloat16)  # t1825: "cuda:0 bf16[1, 512, 4096]"
  t1826 = prims.convert_element_type(t1825, dtypes.float32)  # t1826: "cuda:0 f32[1, 512, 4096]"
  t1827 = ltorch.mul(t1826, t1826)  # t1827: "cuda:0 f32[1, 512, 4096]"
    # t1827 = prims.mul(t1826, t1826)  # t1827: "cuda:0 f32[1, 512, 4096]"
  t1829 = prims.sum(t1827, (2,))  # t1829: "cuda:0 f32[1, 512]"
  t1830 = prims.broadcast_in_dim(t1829, [1, 512, 1], [0, 1])  # t1830: "cuda:0 f32[1, 512, 1]"
  t1832 = ltorch.true_divide(t1830, 4096.0)  # t1832: "cuda:0 f32[1, 512, 1]"
    # t1832 = prims.div(t1830, 4096.0)  # t1832: "cuda:0 f32[1, 512, 1]"
  t1834 = ltorch.add(t1832, 1e-05, alpha=None)  # t1834: "cuda:0 f32[1, 512, 1]"
    # t1834 = prims.add(t1832, 1e-05)  # t1834: "cuda:0 f32[1, 512, 1]"
  t1835 = prims.rsqrt(t1834)  # t1835: "cuda:0 f32[1, 512, 1]"
  t1836 = prims.broadcast_in_dim(t1835, (1, 512, 4096), (0, 1, 2))  # t1836: "cuda:0 f32[1, 512, 4096]"
  t1837 = ltorch.mul(t1826, t1836)  # t1837: "cuda:0 f32[1, 512, 4096]"
    # t1837 = prims.mul(t1826, t1836)  # t1837: "cuda:0 f32[1, 512, 4096]"
  t1838 = prims.convert_element_type(t1837, dtypes.bfloat16)  # t1838: "cuda:0 bf16[1, 512, 4096]"
  t1839 = prims.broadcast_in_dim(t_transformer_h_12_norm_2_weight, (1, 512, 4096), (2,))  # t1839: "cuda:0 bf16[1, 512, 4096]"
  t1840 = prims.convert_element_type(t1838, dtypes.float32)  # t1840: "cuda:0 f32[1, 512, 4096]"
  t1841 = prims.convert_element_type(t1839, dtypes.float32)  # t1841: "cuda:0 f32[1, 512, 4096]"
  t1842 = ltorch.mul(t1840, t1841)  # t1842: "cuda:0 f32[1, 512, 4096]"
    # t1842 = prims.mul(t1840, t1841)  # t1842: "cuda:0 f32[1, 512, 4096]"
  t1843 = prims.convert_element_type(t1842, dtypes.bfloat16)  # t1843: "cuda:0 bf16[1, 512, 4096]"
  t1844 = prims.linear(t1843, t_transformer_h_12_mlp_fc_1_weight, None)  # t1844: "cuda:0 bf16[1, 512, 11008]"
  t1845 = prims.linear(t1843, t_transformer_h_12_mlp_fc_2_weight, None)  # t1845: "cuda:0 bf16[1, 512, 11008]"
  t1846 = prims.convert_element_type(t1844, dtypes.float32)  # t1846: "cuda:0 f32[1, 512, 11008]"
  t1847 = prims.neg(t1846)  # t1847: "cuda:0 f32[1, 512, 11008]"
  t1848 = prims.exp(t1847)  # t1848: "cuda:0 f32[1, 512, 11008]"
  t1849 = ltorch.add(1.0, t1848, alpha=None)  # t1849: "cuda:0 f32[1, 512, 11008]"
    # t1849 = prims.add(1.0, t1848)  # t1849: "cuda:0 f32[1, 512, 11008]"
  t1850 = prims.reciprocal(t1849)  # t1850: "cuda:0 f32[1, 512, 11008]"
  t1851 = prims.convert_element_type(t1850, dtypes.bfloat16)  # t1851: "cuda:0 bf16[1, 512, 11008]"
  t1852 = prims.convert_element_type(t1844, dtypes.float32)  # t1852: "cuda:0 f32[1, 512, 11008]"
  t1853 = prims.convert_element_type(t1851, dtypes.float32)  # t1853: "cuda:0 f32[1, 512, 11008]"
  t1854 = ltorch.mul(t1852, t1853)  # t1854: "cuda:0 f32[1, 512, 11008]"
    # t1854 = prims.mul(t1852, t1853)  # t1854: "cuda:0 f32[1, 512, 11008]"
  t1855 = prims.convert_element_type(t1854, dtypes.bfloat16)  # t1855: "cuda:0 bf16[1, 512, 11008]"
  t1856 = prims.convert_element_type(t1855, dtypes.float32)  # t1856: "cuda:0 f32[1, 512, 11008]"
  t1857 = prims.convert_element_type(t1845, dtypes.float32)  # t1857: "cuda:0 f32[1, 512, 11008]"
  t1858 = ltorch.mul(t1856, t1857)  # t1858: "cuda:0 f32[1, 512, 11008]"
    # t1858 = prims.mul(t1856, t1857)  # t1858: "cuda:0 f32[1, 512, 11008]"
  t1859 = prims.convert_element_type(t1858, dtypes.bfloat16)  # t1859: "cuda:0 bf16[1, 512, 11008]"
  t1860 = prims.linear(t1859, t_transformer_h_12_mlp_proj_weight, None)  # t1860: "cuda:0 bf16[1, 512, 4096]"
  t1861 = prims.convert_element_type(t1860, dtypes.float32)  # t1861: "cuda:0 f32[1, 512, 4096]"
  t1862 = prims.convert_element_type(t1825, dtypes.float32)  # t1862: "cuda:0 f32[1, 512, 4096]"
  t1863 = ltorch.add(t1861, t1862, alpha=None)  # t1863: "cuda:0 f32[1, 512, 4096]"
    # t1863 = prims.add(t1861, t1862)  # t1863: "cuda:0 f32[1, 512, 4096]"
  t1864 = prims.convert_element_type(t1863, dtypes.bfloat16)  # t1864: "cuda:0 bf16[1, 512, 4096]"
  t1865 = prims.convert_element_type(t1864, dtypes.float32)  # t1865: "cuda:0 f32[1, 512, 4096]"
  t1866 = ltorch.mul(t1865, t1865)  # t1866: "cuda:0 f32[1, 512, 4096]"
    # t1866 = prims.mul(t1865, t1865)  # t1866: "cuda:0 f32[1, 512, 4096]"
  t1868 = prims.sum(t1866, (2,))  # t1868: "cuda:0 f32[1, 512]"
  t1869 = prims.broadcast_in_dim(t1868, [1, 512, 1], [0, 1])  # t1869: "cuda:0 f32[1, 512, 1]"
  t1871 = ltorch.true_divide(t1869, 4096.0)  # t1871: "cuda:0 f32[1, 512, 1]"
    # t1871 = prims.div(t1869, 4096.0)  # t1871: "cuda:0 f32[1, 512, 1]"
  t1873 = ltorch.add(t1871, 1e-05, alpha=None)  # t1873: "cuda:0 f32[1, 512, 1]"
    # t1873 = prims.add(t1871, 1e-05)  # t1873: "cuda:0 f32[1, 512, 1]"
  t1874 = prims.rsqrt(t1873)  # t1874: "cuda:0 f32[1, 512, 1]"
  t1875 = prims.broadcast_in_dim(t1874, (1, 512, 4096), (0, 1, 2))  # t1875: "cuda:0 f32[1, 512, 4096]"
  t1876 = ltorch.mul(t1865, t1875)  # t1876: "cuda:0 f32[1, 512, 4096]"
    # t1876 = prims.mul(t1865, t1875)  # t1876: "cuda:0 f32[1, 512, 4096]"
  t1877 = prims.convert_element_type(t1876, dtypes.bfloat16)  # t1877: "cuda:0 bf16[1, 512, 4096]"
  t1878 = prims.broadcast_in_dim(t_transformer_h_13_norm_1_weight, (1, 512, 4096), (2,))  # t1878: "cuda:0 bf16[1, 512, 4096]"
  t1879 = prims.convert_element_type(t1877, dtypes.float32)  # t1879: "cuda:0 f32[1, 512, 4096]"
  t1880 = prims.convert_element_type(t1878, dtypes.float32)  # t1880: "cuda:0 f32[1, 512, 4096]"
  t1881 = ltorch.mul(t1879, t1880)  # t1881: "cuda:0 f32[1, 512, 4096]"
    # t1881 = prims.mul(t1879, t1880)  # t1881: "cuda:0 f32[1, 512, 4096]"
  t1882 = prims.convert_element_type(t1881, dtypes.bfloat16)  # t1882: "cuda:0 bf16[1, 512, 4096]"
  t1883 = prims.linear(t1882, t_transformer_h_13_attn_attn_weight, None)  # t1883: "cuda:0 bf16[1, 512, 12288]"
  t1889 = prims.reshape(t1883, (1, 512, 32, 3, 128))  # t1889: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1895 = prims.transpose(t1889, (0, 2, 3, 1, 4))  # t1895: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1896, t1897, t1898) = ltorch.split(t1895, (1, 1, 1), 2)
    # t1896 = prims.slice_prim(t1895, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1896: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1897 = prims.slice_prim(t1895, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1897: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1898 = prims.slice_prim(t1895, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1898: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1904 = prims.reshape(t1896, (1, 32, 512, 128))  # t1904: "cuda:0 bf16[1, 32, 512, 128]"
  t1910 = prims.reshape(t1897, (1, 32, 512, 128))  # t1910: "cuda:0 bf16[1, 32, 512, 128]"
  t1916 = prims.reshape(t1898, (1, 32, 512, 128))  # t1916: "cuda:0 bf16[1, 32, 512, 128]"
  t1917 = prims.slice_prim(t1904, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1917: "cuda:0 bf16[1, 32, 512, 128]"
  t1918 = prims.slice_prim(t1917, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1918: "cuda:0 bf16[1, 32, 512, 64]"
  t1919 = prims.slice_prim(t1917, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1919: "cuda:0 bf16[1, 32, 512, 64]"
  t1920 = prims.convert_element_type(t1919, dtypes.float32)  # t1920: "cuda:0 f32[1, 32, 512, 64]"
  t1921 = prims.neg(t1920)  # t1921: "cuda:0 f32[1, 32, 512, 64]"
  t1922 = prims.convert_element_type(t1921, dtypes.bfloat16)  # t1922: "cuda:0 bf16[1, 32, 512, 64]"
  t1924 = prims.cat((t1922, t1918), -1)  # t1924: "cuda:0 bf16[1, 32, 512, 128]"
  t1925 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1925: "cuda:0 f32[1, 32, 512, 128]"
  t1926 = prims.convert_element_type(t1917, dtypes.float32)  # t1926: "cuda:0 f32[1, 32, 512, 128]"
  t1927 = ltorch.mul(t1926, t1925)  # t1927: "cuda:0 f32[1, 32, 512, 128]"
    # t1927 = prims.mul(t1926, t1925)  # t1927: "cuda:0 f32[1, 32, 512, 128]"
  t1928 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1928: "cuda:0 f32[1, 32, 512, 128]"
  t1929 = prims.convert_element_type(t1924, dtypes.float32)  # t1929: "cuda:0 f32[1, 32, 512, 128]"
  t1930 = ltorch.mul(t1929, t1928)  # t1930: "cuda:0 f32[1, 32, 512, 128]"
    # t1930 = prims.mul(t1929, t1928)  # t1930: "cuda:0 f32[1, 32, 512, 128]"
  t1931 = ltorch.add(t1927, t1930, alpha=None)  # t1931: "cuda:0 f32[1, 32, 512, 128]"
    # t1931 = prims.add(t1927, t1930)  # t1931: "cuda:0 f32[1, 32, 512, 128]"
  t1932 = prims.convert_element_type(t1931, dtypes.bfloat16)  # t1932: "cuda:0 bf16[1, 32, 512, 128]"
  t1933 = prims.slice_prim(t1910, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1933: "cuda:0 bf16[1, 32, 512, 128]"
  t1934 = prims.slice_prim(t1933, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1934: "cuda:0 bf16[1, 32, 512, 64]"
  t1935 = prims.slice_prim(t1933, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1935: "cuda:0 bf16[1, 32, 512, 64]"
  t1936 = prims.convert_element_type(t1935, dtypes.float32)  # t1936: "cuda:0 f32[1, 32, 512, 64]"
  t1937 = prims.neg(t1936)  # t1937: "cuda:0 f32[1, 32, 512, 64]"
  t1938 = prims.convert_element_type(t1937, dtypes.bfloat16)  # t1938: "cuda:0 bf16[1, 32, 512, 64]"
  t1940 = prims.cat((t1938, t1934), -1)  # t1940: "cuda:0 bf16[1, 32, 512, 128]"
  t1941 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1941: "cuda:0 f32[1, 32, 512, 128]"
  t1942 = prims.convert_element_type(t1933, dtypes.float32)  # t1942: "cuda:0 f32[1, 32, 512, 128]"
  t1943 = ltorch.mul(t1942, t1941)  # t1943: "cuda:0 f32[1, 32, 512, 128]"
    # t1943 = prims.mul(t1942, t1941)  # t1943: "cuda:0 f32[1, 32, 512, 128]"
  t1944 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1944: "cuda:0 f32[1, 32, 512, 128]"
  t1945 = prims.convert_element_type(t1940, dtypes.float32)  # t1945: "cuda:0 f32[1, 32, 512, 128]"
  t1946 = ltorch.mul(t1945, t1944)  # t1946: "cuda:0 f32[1, 32, 512, 128]"
    # t1946 = prims.mul(t1945, t1944)  # t1946: "cuda:0 f32[1, 32, 512, 128]"
  t1947 = ltorch.add(t1943, t1946, alpha=None)  # t1947: "cuda:0 f32[1, 32, 512, 128]"
    # t1947 = prims.add(t1943, t1946)  # t1947: "cuda:0 f32[1, 32, 512, 128]"
  t1948 = prims.convert_element_type(t1947, dtypes.bfloat16)  # t1948: "cuda:0 bf16[1, 32, 512, 128]"
  t1949 = prims.slice_prim(t1904, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1949: "cuda:0 bf16[1, 32, 512, 0]"
  t1951 = prims.cat((t1932, t1949), -1)  # t1951: "cuda:0 bf16[1, 32, 512, 128]"
  t1952 = prims.slice_prim(t1910, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1952: "cuda:0 bf16[1, 32, 512, 0]"
  t1954 = prims.cat((t1948, t1952), -1)  # t1954: "cuda:0 bf16[1, 32, 512, 128]"
  (t1955, t1956, t1957, t1958) = cudnn_sdpa_fwd(t1951, t1954, t1916, None, 0.0, True, scale=0.08838834764831843)
  t1961 = prims.transpose(t1955, (0, 2, 1, 3))  # t1961: "cuda:0 bf16[1, 512, 32, 128]"
  t1965 = prims.reshape(t1961, (1, 512, 4096))  # t1965: "cuda:0 bf16[1, 512, 4096]"
  t1966 = prims.linear(t1965, t_transformer_h_13_attn_proj_weight, None)  # t1966: "cuda:0 bf16[1, 512, 4096]"
  t1967 = prims.convert_element_type(t1966, dtypes.float32)  # t1967: "cuda:0 f32[1, 512, 4096]"
  t1968 = prims.convert_element_type(t1864, dtypes.float32)  # t1968: "cuda:0 f32[1, 512, 4096]"
  t1969 = ltorch.add(t1967, t1968, alpha=None)  # t1969: "cuda:0 f32[1, 512, 4096]"
    # t1969 = prims.add(t1967, t1968)  # t1969: "cuda:0 f32[1, 512, 4096]"
  t1970 = prims.convert_element_type(t1969, dtypes.bfloat16)  # t1970: "cuda:0 bf16[1, 512, 4096]"
  t1971 = prims.convert_element_type(t1970, dtypes.float32)  # t1971: "cuda:0 f32[1, 512, 4096]"
  t1972 = ltorch.mul(t1971, t1971)  # t1972: "cuda:0 f32[1, 512, 4096]"
    # t1972 = prims.mul(t1971, t1971)  # t1972: "cuda:0 f32[1, 512, 4096]"
  t1974 = prims.sum(t1972, (2,))  # t1974: "cuda:0 f32[1, 512]"
  t1975 = prims.broadcast_in_dim(t1974, [1, 512, 1], [0, 1])  # t1975: "cuda:0 f32[1, 512, 1]"
  t1977 = ltorch.true_divide(t1975, 4096.0)  # t1977: "cuda:0 f32[1, 512, 1]"
    # t1977 = prims.div(t1975, 4096.0)  # t1977: "cuda:0 f32[1, 512, 1]"
  t1979 = ltorch.add(t1977, 1e-05, alpha=None)  # t1979: "cuda:0 f32[1, 512, 1]"
    # t1979 = prims.add(t1977, 1e-05)  # t1979: "cuda:0 f32[1, 512, 1]"
  t1980 = prims.rsqrt(t1979)  # t1980: "cuda:0 f32[1, 512, 1]"
  t1981 = prims.broadcast_in_dim(t1980, (1, 512, 4096), (0, 1, 2))  # t1981: "cuda:0 f32[1, 512, 4096]"
  t1982 = ltorch.mul(t1971, t1981)  # t1982: "cuda:0 f32[1, 512, 4096]"
    # t1982 = prims.mul(t1971, t1981)  # t1982: "cuda:0 f32[1, 512, 4096]"
  t1983 = prims.convert_element_type(t1982, dtypes.bfloat16)  # t1983: "cuda:0 bf16[1, 512, 4096]"
  t1984 = prims.broadcast_in_dim(t_transformer_h_13_norm_2_weight, (1, 512, 4096), (2,))  # t1984: "cuda:0 bf16[1, 512, 4096]"
  t1985 = prims.convert_element_type(t1983, dtypes.float32)  # t1985: "cuda:0 f32[1, 512, 4096]"
  t1986 = prims.convert_element_type(t1984, dtypes.float32)  # t1986: "cuda:0 f32[1, 512, 4096]"
  t1987 = ltorch.mul(t1985, t1986)  # t1987: "cuda:0 f32[1, 512, 4096]"
    # t1987 = prims.mul(t1985, t1986)  # t1987: "cuda:0 f32[1, 512, 4096]"
  t1988 = prims.convert_element_type(t1987, dtypes.bfloat16)  # t1988: "cuda:0 bf16[1, 512, 4096]"
  t1989 = prims.linear(t1988, t_transformer_h_13_mlp_fc_1_weight, None)  # t1989: "cuda:0 bf16[1, 512, 11008]"
  t1990 = prims.linear(t1988, t_transformer_h_13_mlp_fc_2_weight, None)  # t1990: "cuda:0 bf16[1, 512, 11008]"
  t1991 = prims.convert_element_type(t1989, dtypes.float32)  # t1991: "cuda:0 f32[1, 512, 11008]"
  t1992 = prims.neg(t1991)  # t1992: "cuda:0 f32[1, 512, 11008]"
  t1993 = prims.exp(t1992)  # t1993: "cuda:0 f32[1, 512, 11008]"
  t1994 = ltorch.add(1.0, t1993, alpha=None)  # t1994: "cuda:0 f32[1, 512, 11008]"
    # t1994 = prims.add(1.0, t1993)  # t1994: "cuda:0 f32[1, 512, 11008]"
  t1995 = prims.reciprocal(t1994)  # t1995: "cuda:0 f32[1, 512, 11008]"
  t1996 = prims.convert_element_type(t1995, dtypes.bfloat16)  # t1996: "cuda:0 bf16[1, 512, 11008]"
  t1997 = prims.convert_element_type(t1989, dtypes.float32)  # t1997: "cuda:0 f32[1, 512, 11008]"
  t1998 = prims.convert_element_type(t1996, dtypes.float32)  # t1998: "cuda:0 f32[1, 512, 11008]"
  t1999 = ltorch.mul(t1997, t1998)  # t1999: "cuda:0 f32[1, 512, 11008]"
    # t1999 = prims.mul(t1997, t1998)  # t1999: "cuda:0 f32[1, 512, 11008]"
  t2000 = prims.convert_element_type(t1999, dtypes.bfloat16)  # t2000: "cuda:0 bf16[1, 512, 11008]"
  t2001 = prims.convert_element_type(t2000, dtypes.float32)  # t2001: "cuda:0 f32[1, 512, 11008]"
  t2002 = prims.convert_element_type(t1990, dtypes.float32)  # t2002: "cuda:0 f32[1, 512, 11008]"
  t2003 = ltorch.mul(t2001, t2002)  # t2003: "cuda:0 f32[1, 512, 11008]"
    # t2003 = prims.mul(t2001, t2002)  # t2003: "cuda:0 f32[1, 512, 11008]"
  t2004 = prims.convert_element_type(t2003, dtypes.bfloat16)  # t2004: "cuda:0 bf16[1, 512, 11008]"
  t2005 = prims.linear(t2004, t_transformer_h_13_mlp_proj_weight, None)  # t2005: "cuda:0 bf16[1, 512, 4096]"
  t2006 = prims.convert_element_type(t2005, dtypes.float32)  # t2006: "cuda:0 f32[1, 512, 4096]"
  t2007 = prims.convert_element_type(t1970, dtypes.float32)  # t2007: "cuda:0 f32[1, 512, 4096]"
  t2008 = ltorch.add(t2006, t2007, alpha=None)  # t2008: "cuda:0 f32[1, 512, 4096]"
    # t2008 = prims.add(t2006, t2007)  # t2008: "cuda:0 f32[1, 512, 4096]"
  t2009 = prims.convert_element_type(t2008, dtypes.bfloat16)  # t2009: "cuda:0 bf16[1, 512, 4096]"
  t2010 = prims.convert_element_type(t2009, dtypes.float32)  # t2010: "cuda:0 f32[1, 512, 4096]"
  t2011 = ltorch.mul(t2010, t2010)  # t2011: "cuda:0 f32[1, 512, 4096]"
    # t2011 = prims.mul(t2010, t2010)  # t2011: "cuda:0 f32[1, 512, 4096]"
  t2013 = prims.sum(t2011, (2,))  # t2013: "cuda:0 f32[1, 512]"
  t2014 = prims.broadcast_in_dim(t2013, [1, 512, 1], [0, 1])  # t2014: "cuda:0 f32[1, 512, 1]"
  t2016 = ltorch.true_divide(t2014, 4096.0)  # t2016: "cuda:0 f32[1, 512, 1]"
    # t2016 = prims.div(t2014, 4096.0)  # t2016: "cuda:0 f32[1, 512, 1]"
  t2018 = ltorch.add(t2016, 1e-05, alpha=None)  # t2018: "cuda:0 f32[1, 512, 1]"
    # t2018 = prims.add(t2016, 1e-05)  # t2018: "cuda:0 f32[1, 512, 1]"
  t2019 = prims.rsqrt(t2018)  # t2019: "cuda:0 f32[1, 512, 1]"
  t2020 = prims.broadcast_in_dim(t2019, (1, 512, 4096), (0, 1, 2))  # t2020: "cuda:0 f32[1, 512, 4096]"
  t2021 = ltorch.mul(t2010, t2020)  # t2021: "cuda:0 f32[1, 512, 4096]"
    # t2021 = prims.mul(t2010, t2020)  # t2021: "cuda:0 f32[1, 512, 4096]"
  t2022 = prims.convert_element_type(t2021, dtypes.bfloat16)  # t2022: "cuda:0 bf16[1, 512, 4096]"
  t2023 = prims.broadcast_in_dim(t_transformer_h_14_norm_1_weight, (1, 512, 4096), (2,))  # t2023: "cuda:0 bf16[1, 512, 4096]"
  t2024 = prims.convert_element_type(t2022, dtypes.float32)  # t2024: "cuda:0 f32[1, 512, 4096]"
  t2025 = prims.convert_element_type(t2023, dtypes.float32)  # t2025: "cuda:0 f32[1, 512, 4096]"
  t2026 = ltorch.mul(t2024, t2025)  # t2026: "cuda:0 f32[1, 512, 4096]"
    # t2026 = prims.mul(t2024, t2025)  # t2026: "cuda:0 f32[1, 512, 4096]"
  t2027 = prims.convert_element_type(t2026, dtypes.bfloat16)  # t2027: "cuda:0 bf16[1, 512, 4096]"
  t2028 = prims.linear(t2027, t_transformer_h_14_attn_attn_weight, None)  # t2028: "cuda:0 bf16[1, 512, 12288]"
  t2034 = prims.reshape(t2028, (1, 512, 32, 3, 128))  # t2034: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t2040 = prims.transpose(t2034, (0, 2, 3, 1, 4))  # t2040: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t2041, t2042, t2043) = ltorch.split(t2040, (1, 1, 1), 2)
    # t2041 = prims.slice_prim(t2040, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2041: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2042 = prims.slice_prim(t2040, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2042: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2043 = prims.slice_prim(t2040, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2043: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t2049 = prims.reshape(t2041, (1, 32, 512, 128))  # t2049: "cuda:0 bf16[1, 32, 512, 128]"
  t2055 = prims.reshape(t2042, (1, 32, 512, 128))  # t2055: "cuda:0 bf16[1, 32, 512, 128]"
  t2061 = prims.reshape(t2043, (1, 32, 512, 128))  # t2061: "cuda:0 bf16[1, 32, 512, 128]"
  t2062 = prims.slice_prim(t2049, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2062: "cuda:0 bf16[1, 32, 512, 128]"
  t2063 = prims.slice_prim(t2062, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2063: "cuda:0 bf16[1, 32, 512, 64]"
  t2064 = prims.slice_prim(t2062, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2064: "cuda:0 bf16[1, 32, 512, 64]"
  t2065 = prims.convert_element_type(t2064, dtypes.float32)  # t2065: "cuda:0 f32[1, 32, 512, 64]"
  t2066 = prims.neg(t2065)  # t2066: "cuda:0 f32[1, 32, 512, 64]"
  t2067 = prims.convert_element_type(t2066, dtypes.bfloat16)  # t2067: "cuda:0 bf16[1, 32, 512, 64]"
  t2069 = prims.cat((t2067, t2063), -1)  # t2069: "cuda:0 bf16[1, 32, 512, 128]"
  t2070 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2070: "cuda:0 f32[1, 32, 512, 128]"
  t2071 = prims.convert_element_type(t2062, dtypes.float32)  # t2071: "cuda:0 f32[1, 32, 512, 128]"
  t2072 = ltorch.mul(t2071, t2070)  # t2072: "cuda:0 f32[1, 32, 512, 128]"
    # t2072 = prims.mul(t2071, t2070)  # t2072: "cuda:0 f32[1, 32, 512, 128]"
  t2073 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2073: "cuda:0 f32[1, 32, 512, 128]"
  t2074 = prims.convert_element_type(t2069, dtypes.float32)  # t2074: "cuda:0 f32[1, 32, 512, 128]"
  t2075 = ltorch.mul(t2074, t2073)  # t2075: "cuda:0 f32[1, 32, 512, 128]"
    # t2075 = prims.mul(t2074, t2073)  # t2075: "cuda:0 f32[1, 32, 512, 128]"
  t2076 = ltorch.add(t2072, t2075, alpha=None)  # t2076: "cuda:0 f32[1, 32, 512, 128]"
    # t2076 = prims.add(t2072, t2075)  # t2076: "cuda:0 f32[1, 32, 512, 128]"
  t2077 = prims.convert_element_type(t2076, dtypes.bfloat16)  # t2077: "cuda:0 bf16[1, 32, 512, 128]"
  t2078 = prims.slice_prim(t2055, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2078: "cuda:0 bf16[1, 32, 512, 128]"
  t2079 = prims.slice_prim(t2078, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2079: "cuda:0 bf16[1, 32, 512, 64]"
  t2080 = prims.slice_prim(t2078, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2080: "cuda:0 bf16[1, 32, 512, 64]"
  t2081 = prims.convert_element_type(t2080, dtypes.float32)  # t2081: "cuda:0 f32[1, 32, 512, 64]"
  t2082 = prims.neg(t2081)  # t2082: "cuda:0 f32[1, 32, 512, 64]"
  t2083 = prims.convert_element_type(t2082, dtypes.bfloat16)  # t2083: "cuda:0 bf16[1, 32, 512, 64]"
  t2085 = prims.cat((t2083, t2079), -1)  # t2085: "cuda:0 bf16[1, 32, 512, 128]"
  t2086 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2086: "cuda:0 f32[1, 32, 512, 128]"
  t2087 = prims.convert_element_type(t2078, dtypes.float32)  # t2087: "cuda:0 f32[1, 32, 512, 128]"
  t2088 = ltorch.mul(t2087, t2086)  # t2088: "cuda:0 f32[1, 32, 512, 128]"
    # t2088 = prims.mul(t2087, t2086)  # t2088: "cuda:0 f32[1, 32, 512, 128]"
  t2089 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2089: "cuda:0 f32[1, 32, 512, 128]"
  t2090 = prims.convert_element_type(t2085, dtypes.float32)  # t2090: "cuda:0 f32[1, 32, 512, 128]"
  t2091 = ltorch.mul(t2090, t2089)  # t2091: "cuda:0 f32[1, 32, 512, 128]"
    # t2091 = prims.mul(t2090, t2089)  # t2091: "cuda:0 f32[1, 32, 512, 128]"
  t2092 = ltorch.add(t2088, t2091, alpha=None)  # t2092: "cuda:0 f32[1, 32, 512, 128]"
    # t2092 = prims.add(t2088, t2091)  # t2092: "cuda:0 f32[1, 32, 512, 128]"
  t2093 = prims.convert_element_type(t2092, dtypes.bfloat16)  # t2093: "cuda:0 bf16[1, 32, 512, 128]"
  t2094 = prims.slice_prim(t2049, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2094: "cuda:0 bf16[1, 32, 512, 0]"
  t2096 = prims.cat((t2077, t2094), -1)  # t2096: "cuda:0 bf16[1, 32, 512, 128]"
  t2097 = prims.slice_prim(t2055, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2097: "cuda:0 bf16[1, 32, 512, 0]"
  t2099 = prims.cat((t2093, t2097), -1)  # t2099: "cuda:0 bf16[1, 32, 512, 128]"
  (t2100, t2101, t2102, t2103) = cudnn_sdpa_fwd(t2096, t2099, t2061, None, 0.0, True, scale=0.08838834764831843)
  t2106 = prims.transpose(t2100, (0, 2, 1, 3))  # t2106: "cuda:0 bf16[1, 512, 32, 128]"
  t2110 = prims.reshape(t2106, (1, 512, 4096))  # t2110: "cuda:0 bf16[1, 512, 4096]"
  t2111 = prims.linear(t2110, t_transformer_h_14_attn_proj_weight, None)  # t2111: "cuda:0 bf16[1, 512, 4096]"
  t2112 = prims.convert_element_type(t2111, dtypes.float32)  # t2112: "cuda:0 f32[1, 512, 4096]"
  t2113 = prims.convert_element_type(t2009, dtypes.float32)  # t2113: "cuda:0 f32[1, 512, 4096]"
  t2114 = ltorch.add(t2112, t2113, alpha=None)  # t2114: "cuda:0 f32[1, 512, 4096]"
    # t2114 = prims.add(t2112, t2113)  # t2114: "cuda:0 f32[1, 512, 4096]"
  t2115 = prims.convert_element_type(t2114, dtypes.bfloat16)  # t2115: "cuda:0 bf16[1, 512, 4096]"
  t2116 = prims.convert_element_type(t2115, dtypes.float32)  # t2116: "cuda:0 f32[1, 512, 4096]"
  t2117 = ltorch.mul(t2116, t2116)  # t2117: "cuda:0 f32[1, 512, 4096]"
    # t2117 = prims.mul(t2116, t2116)  # t2117: "cuda:0 f32[1, 512, 4096]"
  t2119 = prims.sum(t2117, (2,))  # t2119: "cuda:0 f32[1, 512]"
  t2120 = prims.broadcast_in_dim(t2119, [1, 512, 1], [0, 1])  # t2120: "cuda:0 f32[1, 512, 1]"
  t2122 = ltorch.true_divide(t2120, 4096.0)  # t2122: "cuda:0 f32[1, 512, 1]"
    # t2122 = prims.div(t2120, 4096.0)  # t2122: "cuda:0 f32[1, 512, 1]"
  t2124 = ltorch.add(t2122, 1e-05, alpha=None)  # t2124: "cuda:0 f32[1, 512, 1]"
    # t2124 = prims.add(t2122, 1e-05)  # t2124: "cuda:0 f32[1, 512, 1]"
  t2125 = prims.rsqrt(t2124)  # t2125: "cuda:0 f32[1, 512, 1]"
  t2126 = prims.broadcast_in_dim(t2125, (1, 512, 4096), (0, 1, 2))  # t2126: "cuda:0 f32[1, 512, 4096]"
  t2127 = ltorch.mul(t2116, t2126)  # t2127: "cuda:0 f32[1, 512, 4096]"
    # t2127 = prims.mul(t2116, t2126)  # t2127: "cuda:0 f32[1, 512, 4096]"
  t2128 = prims.convert_element_type(t2127, dtypes.bfloat16)  # t2128: "cuda:0 bf16[1, 512, 4096]"
  t2129 = prims.broadcast_in_dim(t_transformer_h_14_norm_2_weight, (1, 512, 4096), (2,))  # t2129: "cuda:0 bf16[1, 512, 4096]"
  t2130 = prims.convert_element_type(t2128, dtypes.float32)  # t2130: "cuda:0 f32[1, 512, 4096]"
  t2131 = prims.convert_element_type(t2129, dtypes.float32)  # t2131: "cuda:0 f32[1, 512, 4096]"
  t2132 = ltorch.mul(t2130, t2131)  # t2132: "cuda:0 f32[1, 512, 4096]"
    # t2132 = prims.mul(t2130, t2131)  # t2132: "cuda:0 f32[1, 512, 4096]"
  t2133 = prims.convert_element_type(t2132, dtypes.bfloat16)  # t2133: "cuda:0 bf16[1, 512, 4096]"
  t2134 = prims.linear(t2133, t_transformer_h_14_mlp_fc_1_weight, None)  # t2134: "cuda:0 bf16[1, 512, 11008]"
  t2135 = prims.linear(t2133, t_transformer_h_14_mlp_fc_2_weight, None)  # t2135: "cuda:0 bf16[1, 512, 11008]"
  t2136 = prims.convert_element_type(t2134, dtypes.float32)  # t2136: "cuda:0 f32[1, 512, 11008]"
  t2137 = prims.neg(t2136)  # t2137: "cuda:0 f32[1, 512, 11008]"
  t2138 = prims.exp(t2137)  # t2138: "cuda:0 f32[1, 512, 11008]"
  t2139 = ltorch.add(1.0, t2138, alpha=None)  # t2139: "cuda:0 f32[1, 512, 11008]"
    # t2139 = prims.add(1.0, t2138)  # t2139: "cuda:0 f32[1, 512, 11008]"
  t2140 = prims.reciprocal(t2139)  # t2140: "cuda:0 f32[1, 512, 11008]"
  t2141 = prims.convert_element_type(t2140, dtypes.bfloat16)  # t2141: "cuda:0 bf16[1, 512, 11008]"
  t2142 = prims.convert_element_type(t2134, dtypes.float32)  # t2142: "cuda:0 f32[1, 512, 11008]"
  t2143 = prims.convert_element_type(t2141, dtypes.float32)  # t2143: "cuda:0 f32[1, 512, 11008]"
  t2144 = ltorch.mul(t2142, t2143)  # t2144: "cuda:0 f32[1, 512, 11008]"
    # t2144 = prims.mul(t2142, t2143)  # t2144: "cuda:0 f32[1, 512, 11008]"
  t2145 = prims.convert_element_type(t2144, dtypes.bfloat16)  # t2145: "cuda:0 bf16[1, 512, 11008]"
  t2146 = prims.convert_element_type(t2145, dtypes.float32)  # t2146: "cuda:0 f32[1, 512, 11008]"
  t2147 = prims.convert_element_type(t2135, dtypes.float32)  # t2147: "cuda:0 f32[1, 512, 11008]"
  t2148 = ltorch.mul(t2146, t2147)  # t2148: "cuda:0 f32[1, 512, 11008]"
    # t2148 = prims.mul(t2146, t2147)  # t2148: "cuda:0 f32[1, 512, 11008]"
  t2149 = prims.convert_element_type(t2148, dtypes.bfloat16)  # t2149: "cuda:0 bf16[1, 512, 11008]"
  t2150 = prims.linear(t2149, t_transformer_h_14_mlp_proj_weight, None)  # t2150: "cuda:0 bf16[1, 512, 4096]"
  t2151 = prims.convert_element_type(t2150, dtypes.float32)  # t2151: "cuda:0 f32[1, 512, 4096]"
  t2152 = prims.convert_element_type(t2115, dtypes.float32)  # t2152: "cuda:0 f32[1, 512, 4096]"
  t2153 = ltorch.add(t2151, t2152, alpha=None)  # t2153: "cuda:0 f32[1, 512, 4096]"
    # t2153 = prims.add(t2151, t2152)  # t2153: "cuda:0 f32[1, 512, 4096]"
  t2154 = prims.convert_element_type(t2153, dtypes.bfloat16)  # t2154: "cuda:0 bf16[1, 512, 4096]"
  t2155 = prims.convert_element_type(t2154, dtypes.float32)  # t2155: "cuda:0 f32[1, 512, 4096]"
  t2156 = ltorch.mul(t2155, t2155)  # t2156: "cuda:0 f32[1, 512, 4096]"
    # t2156 = prims.mul(t2155, t2155)  # t2156: "cuda:0 f32[1, 512, 4096]"
  t2158 = prims.sum(t2156, (2,))  # t2158: "cuda:0 f32[1, 512]"
  t2159 = prims.broadcast_in_dim(t2158, [1, 512, 1], [0, 1])  # t2159: "cuda:0 f32[1, 512, 1]"
  t2161 = ltorch.true_divide(t2159, 4096.0)  # t2161: "cuda:0 f32[1, 512, 1]"
    # t2161 = prims.div(t2159, 4096.0)  # t2161: "cuda:0 f32[1, 512, 1]"
  t2163 = ltorch.add(t2161, 1e-05, alpha=None)  # t2163: "cuda:0 f32[1, 512, 1]"
    # t2163 = prims.add(t2161, 1e-05)  # t2163: "cuda:0 f32[1, 512, 1]"
  t2164 = prims.rsqrt(t2163)  # t2164: "cuda:0 f32[1, 512, 1]"
  t2165 = prims.broadcast_in_dim(t2164, (1, 512, 4096), (0, 1, 2))  # t2165: "cuda:0 f32[1, 512, 4096]"
  t2166 = ltorch.mul(t2155, t2165)  # t2166: "cuda:0 f32[1, 512, 4096]"
    # t2166 = prims.mul(t2155, t2165)  # t2166: "cuda:0 f32[1, 512, 4096]"
  t2167 = prims.convert_element_type(t2166, dtypes.bfloat16)  # t2167: "cuda:0 bf16[1, 512, 4096]"
  t2168 = prims.broadcast_in_dim(t_transformer_h_15_norm_1_weight, (1, 512, 4096), (2,))  # t2168: "cuda:0 bf16[1, 512, 4096]"
  t2169 = prims.convert_element_type(t2167, dtypes.float32)  # t2169: "cuda:0 f32[1, 512, 4096]"
  t2170 = prims.convert_element_type(t2168, dtypes.float32)  # t2170: "cuda:0 f32[1, 512, 4096]"
  t2171 = ltorch.mul(t2169, t2170)  # t2171: "cuda:0 f32[1, 512, 4096]"
    # t2171 = prims.mul(t2169, t2170)  # t2171: "cuda:0 f32[1, 512, 4096]"
  t2172 = prims.convert_element_type(t2171, dtypes.bfloat16)  # t2172: "cuda:0 bf16[1, 512, 4096]"
  t2173 = prims.linear(t2172, t_transformer_h_15_attn_attn_weight, None)  # t2173: "cuda:0 bf16[1, 512, 12288]"
  t2179 = prims.reshape(t2173, (1, 512, 32, 3, 128))  # t2179: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t2185 = prims.transpose(t2179, (0, 2, 3, 1, 4))  # t2185: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t2186, t2187, t2188) = ltorch.split(t2185, (1, 1, 1), 2)
    # t2186 = prims.slice_prim(t2185, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2186: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2187 = prims.slice_prim(t2185, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2187: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2188 = prims.slice_prim(t2185, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2188: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t2194 = prims.reshape(t2186, (1, 32, 512, 128))  # t2194: "cuda:0 bf16[1, 32, 512, 128]"
  t2200 = prims.reshape(t2187, (1, 32, 512, 128))  # t2200: "cuda:0 bf16[1, 32, 512, 128]"
  t2206 = prims.reshape(t2188, (1, 32, 512, 128))  # t2206: "cuda:0 bf16[1, 32, 512, 128]"
  t2207 = prims.slice_prim(t2194, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2207: "cuda:0 bf16[1, 32, 512, 128]"
  t2208 = prims.slice_prim(t2207, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2208: "cuda:0 bf16[1, 32, 512, 64]"
  t2209 = prims.slice_prim(t2207, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2209: "cuda:0 bf16[1, 32, 512, 64]"
  t2210 = prims.convert_element_type(t2209, dtypes.float32)  # t2210: "cuda:0 f32[1, 32, 512, 64]"
  t2211 = prims.neg(t2210)  # t2211: "cuda:0 f32[1, 32, 512, 64]"
  t2212 = prims.convert_element_type(t2211, dtypes.bfloat16)  # t2212: "cuda:0 bf16[1, 32, 512, 64]"
  t2214 = prims.cat((t2212, t2208), -1)  # t2214: "cuda:0 bf16[1, 32, 512, 128]"
  t2215 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2215: "cuda:0 f32[1, 32, 512, 128]"
  t2216 = prims.convert_element_type(t2207, dtypes.float32)  # t2216: "cuda:0 f32[1, 32, 512, 128]"
  t2217 = ltorch.mul(t2216, t2215)  # t2217: "cuda:0 f32[1, 32, 512, 128]"
    # t2217 = prims.mul(t2216, t2215)  # t2217: "cuda:0 f32[1, 32, 512, 128]"
  t2218 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2218: "cuda:0 f32[1, 32, 512, 128]"
  t2219 = prims.convert_element_type(t2214, dtypes.float32)  # t2219: "cuda:0 f32[1, 32, 512, 128]"
  t2220 = ltorch.mul(t2219, t2218)  # t2220: "cuda:0 f32[1, 32, 512, 128]"
    # t2220 = prims.mul(t2219, t2218)  # t2220: "cuda:0 f32[1, 32, 512, 128]"
  t2221 = ltorch.add(t2217, t2220, alpha=None)  # t2221: "cuda:0 f32[1, 32, 512, 128]"
    # t2221 = prims.add(t2217, t2220)  # t2221: "cuda:0 f32[1, 32, 512, 128]"
  t2222 = prims.convert_element_type(t2221, dtypes.bfloat16)  # t2222: "cuda:0 bf16[1, 32, 512, 128]"
  t2223 = prims.slice_prim(t2200, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2223: "cuda:0 bf16[1, 32, 512, 128]"
  t2224 = prims.slice_prim(t2223, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2224: "cuda:0 bf16[1, 32, 512, 64]"
  t2225 = prims.slice_prim(t2223, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2225: "cuda:0 bf16[1, 32, 512, 64]"
  t2226 = prims.convert_element_type(t2225, dtypes.float32)  # t2226: "cuda:0 f32[1, 32, 512, 64]"
  t2227 = prims.neg(t2226)  # t2227: "cuda:0 f32[1, 32, 512, 64]"
  t2228 = prims.convert_element_type(t2227, dtypes.bfloat16)  # t2228: "cuda:0 bf16[1, 32, 512, 64]"
  t2230 = prims.cat((t2228, t2224), -1)  # t2230: "cuda:0 bf16[1, 32, 512, 128]"
  t2231 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2231: "cuda:0 f32[1, 32, 512, 128]"
  t2232 = prims.convert_element_type(t2223, dtypes.float32)  # t2232: "cuda:0 f32[1, 32, 512, 128]"
  t2233 = ltorch.mul(t2232, t2231)  # t2233: "cuda:0 f32[1, 32, 512, 128]"
    # t2233 = prims.mul(t2232, t2231)  # t2233: "cuda:0 f32[1, 32, 512, 128]"
  t2234 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2234: "cuda:0 f32[1, 32, 512, 128]"
  t2235 = prims.convert_element_type(t2230, dtypes.float32)  # t2235: "cuda:0 f32[1, 32, 512, 128]"
  t2236 = ltorch.mul(t2235, t2234)  # t2236: "cuda:0 f32[1, 32, 512, 128]"
    # t2236 = prims.mul(t2235, t2234)  # t2236: "cuda:0 f32[1, 32, 512, 128]"
  t2237 = ltorch.add(t2233, t2236, alpha=None)  # t2237: "cuda:0 f32[1, 32, 512, 128]"
    # t2237 = prims.add(t2233, t2236)  # t2237: "cuda:0 f32[1, 32, 512, 128]"
  t2238 = prims.convert_element_type(t2237, dtypes.bfloat16)  # t2238: "cuda:0 bf16[1, 32, 512, 128]"
  t2239 = prims.slice_prim(t2194, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2239: "cuda:0 bf16[1, 32, 512, 0]"
  t2241 = prims.cat((t2222, t2239), -1)  # t2241: "cuda:0 bf16[1, 32, 512, 128]"
  t2242 = prims.slice_prim(t2200, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2242: "cuda:0 bf16[1, 32, 512, 0]"
  t2244 = prims.cat((t2238, t2242), -1)  # t2244: "cuda:0 bf16[1, 32, 512, 128]"
  (t2245, t2246, t2247, t2248) = cudnn_sdpa_fwd(t2241, t2244, t2206, None, 0.0, True, scale=0.08838834764831843)
  t2251 = prims.transpose(t2245, (0, 2, 1, 3))  # t2251: "cuda:0 bf16[1, 512, 32, 128]"
  t2255 = prims.reshape(t2251, (1, 512, 4096))  # t2255: "cuda:0 bf16[1, 512, 4096]"
  t2256 = prims.linear(t2255, t_transformer_h_15_attn_proj_weight, None)  # t2256: "cuda:0 bf16[1, 512, 4096]"
  t2257 = prims.convert_element_type(t2256, dtypes.float32)  # t2257: "cuda:0 f32[1, 512, 4096]"
  t2258 = prims.convert_element_type(t2154, dtypes.float32)  # t2258: "cuda:0 f32[1, 512, 4096]"
  t2259 = ltorch.add(t2257, t2258, alpha=None)  # t2259: "cuda:0 f32[1, 512, 4096]"
    # t2259 = prims.add(t2257, t2258)  # t2259: "cuda:0 f32[1, 512, 4096]"
  t2260 = prims.convert_element_type(t2259, dtypes.bfloat16)  # t2260: "cuda:0 bf16[1, 512, 4096]"
  t2261 = prims.convert_element_type(t2260, dtypes.float32)  # t2261: "cuda:0 f32[1, 512, 4096]"
  t2262 = ltorch.mul(t2261, t2261)  # t2262: "cuda:0 f32[1, 512, 4096]"
    # t2262 = prims.mul(t2261, t2261)  # t2262: "cuda:0 f32[1, 512, 4096]"
  t2264 = prims.sum(t2262, (2,))  # t2264: "cuda:0 f32[1, 512]"
  t2265 = prims.broadcast_in_dim(t2264, [1, 512, 1], [0, 1])  # t2265: "cuda:0 f32[1, 512, 1]"
  t2267 = ltorch.true_divide(t2265, 4096.0)  # t2267: "cuda:0 f32[1, 512, 1]"
    # t2267 = prims.div(t2265, 4096.0)  # t2267: "cuda:0 f32[1, 512, 1]"
  t2269 = ltorch.add(t2267, 1e-05, alpha=None)  # t2269: "cuda:0 f32[1, 512, 1]"
    # t2269 = prims.add(t2267, 1e-05)  # t2269: "cuda:0 f32[1, 512, 1]"
  t2270 = prims.rsqrt(t2269)  # t2270: "cuda:0 f32[1, 512, 1]"
  t2271 = prims.broadcast_in_dim(t2270, (1, 512, 4096), (0, 1, 2))  # t2271: "cuda:0 f32[1, 512, 4096]"
  t2272 = ltorch.mul(t2261, t2271)  # t2272: "cuda:0 f32[1, 512, 4096]"
    # t2272 = prims.mul(t2261, t2271)  # t2272: "cuda:0 f32[1, 512, 4096]"
  t2273 = prims.convert_element_type(t2272, dtypes.bfloat16)  # t2273: "cuda:0 bf16[1, 512, 4096]"
  t2274 = prims.broadcast_in_dim(t_transformer_h_15_norm_2_weight, (1, 512, 4096), (2,))  # t2274: "cuda:0 bf16[1, 512, 4096]"
  t2275 = prims.convert_element_type(t2273, dtypes.float32)  # t2275: "cuda:0 f32[1, 512, 4096]"
  t2276 = prims.convert_element_type(t2274, dtypes.float32)  # t2276: "cuda:0 f32[1, 512, 4096]"
  t2277 = ltorch.mul(t2275, t2276)  # t2277: "cuda:0 f32[1, 512, 4096]"
    # t2277 = prims.mul(t2275, t2276)  # t2277: "cuda:0 f32[1, 512, 4096]"
  t2278 = prims.convert_element_type(t2277, dtypes.bfloat16)  # t2278: "cuda:0 bf16[1, 512, 4096]"
  t2279 = prims.linear(t2278, t_transformer_h_15_mlp_fc_1_weight, None)  # t2279: "cuda:0 bf16[1, 512, 11008]"
  t2280 = prims.linear(t2278, t_transformer_h_15_mlp_fc_2_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
  t2281 = prims.convert_element_type(t2279, dtypes.float32)  # t2281: "cuda:0 f32[1, 512, 11008]"
  t2282 = prims.neg(t2281)  # t2282: "cuda:0 f32[1, 512, 11008]"
  t2283 = prims.exp(t2282)  # t2283: "cuda:0 f32[1, 512, 11008]"
  t2284 = ltorch.add(1.0, t2283, alpha=None)  # t2284: "cuda:0 f32[1, 512, 11008]"
    # t2284 = prims.add(1.0, t2283)  # t2284: "cuda:0 f32[1, 512, 11008]"
  t2285 = prims.reciprocal(t2284)  # t2285: "cuda:0 f32[1, 512, 11008]"
  t2286 = prims.convert_element_type(t2285, dtypes.bfloat16)  # t2286: "cuda:0 bf16[1, 512, 11008]"
  t2287 = prims.convert_element_type(t2279, dtypes.float32)  # t2287: "cuda:0 f32[1, 512, 11008]"
  t2288 = prims.convert_element_type(t2286, dtypes.float32)  # t2288: "cuda:0 f32[1, 512, 11008]"
  t2289 = ltorch.mul(t2287, t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
    # t2289 = prims.mul(t2287, t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
  t2290 = prims.convert_element_type(t2289, dtypes.bfloat16)  # t2290: "cuda:0 bf16[1, 512, 11008]"
  t2291 = prims.convert_element_type(t2290, dtypes.float32)  # t2291: "cuda:0 f32[1, 512, 11008]"
  t2292 = prims.convert_element_type(t2280, dtypes.float32)  # t2292: "cuda:0 f32[1, 512, 11008]"
  t2293 = ltorch.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
    # t2293 = prims.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
  t2294 = prims.convert_element_type(t2293, dtypes.bfloat16)  # t2294: "cuda:0 bf16[1, 512, 11008]"
  t2295 = prims.linear(t2294, t_transformer_h_15_mlp_proj_weight, None)  # t2295: "cuda:0 bf16[1, 512, 4096]"
  t2296 = prims.convert_element_type(t2295, dtypes.float32)  # t2296: "cuda:0 f32[1, 512, 4096]"
  t2297 = prims.convert_element_type(t2260, dtypes.float32)  # t2297: "cuda:0 f32[1, 512, 4096]"
  t2298 = ltorch.add(t2296, t2297, alpha=None)  # t2298: "cuda:0 f32[1, 512, 4096]"
    # t2298 = prims.add(t2296, t2297)  # t2298: "cuda:0 f32[1, 512, 4096]"
  t2299 = prims.convert_element_type(t2298, dtypes.bfloat16)  # t2299: "cuda:0 bf16[1, 512, 4096]"
  t2300 = prims.convert_element_type(t2299, dtypes.float32)  # t2300: "cuda:0 f32[1, 512, 4096]"
  t2301 = ltorch.mul(t2300, t2300)  # t2301: "cuda:0 f32[1, 512, 4096]"
    # t2301 = prims.mul(t2300, t2300)  # t2301: "cuda:0 f32[1, 512, 4096]"
  t2303 = prims.sum(t2301, (2,))  # t2303: "cuda:0 f32[1, 512]"
  t2304 = prims.broadcast_in_dim(t2303, [1, 512, 1], [0, 1])  # t2304: "cuda:0 f32[1, 512, 1]"
  t2306 = ltorch.true_divide(t2304, 4096.0)  # t2306: "cuda:0 f32[1, 512, 1]"
    # t2306 = prims.div(t2304, 4096.0)  # t2306: "cuda:0 f32[1, 512, 1]"
  t2308 = ltorch.add(t2306, 1e-05, alpha=None)  # t2308: "cuda:0 f32[1, 512, 1]"
    # t2308 = prims.add(t2306, 1e-05)  # t2308: "cuda:0 f32[1, 512, 1]"
  t2309 = prims.rsqrt(t2308)  # t2309: "cuda:0 f32[1, 512, 1]"
  t2310 = prims.broadcast_in_dim(t2309, (1, 512, 4096), (0, 1, 2))  # t2310: "cuda:0 f32[1, 512, 4096]"
  t2311 = ltorch.mul(t2300, t2310)  # t2311: "cuda:0 f32[1, 512, 4096]"
    # t2311 = prims.mul(t2300, t2310)  # t2311: "cuda:0 f32[1, 512, 4096]"
  t2312 = prims.convert_element_type(t2311, dtypes.bfloat16)  # t2312: "cuda:0 bf16[1, 512, 4096]"
  t2313 = prims.broadcast_in_dim(t_transformer_ln_f_weight, (1, 512, 4096), (2,))  # t2313: "cuda:0 bf16[1, 512, 4096]"
  t2314 = prims.convert_element_type(t2312, dtypes.float32)  # t2314: "cuda:0 f32[1, 512, 4096]"
  t2315 = prims.convert_element_type(t2313, dtypes.float32)  # t2315: "cuda:0 f32[1, 512, 4096]"
  t2316 = ltorch.mul(t2314, t2315)  # t2316: "cuda:0 f32[1, 512, 4096]"
    # t2316 = prims.mul(t2314, t2315)  # t2316: "cuda:0 f32[1, 512, 4096]"
  t2317 = prims.convert_element_type(t2316, dtypes.bfloat16)  # t2317: "cuda:0 bf16[1, 512, 4096]"
  t2318 = prims.linear(t2317, t_lm_head_weight, None)  # t2318: "cuda:0 bf16[1, 512, 32000]"
  return {'output': t2318, 'flat_args': [idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight], 'flat_output': (t2318,)}, ((idx, t5, t11, t12, t17, t16, t19, t_transformer_h_0_attn_attn_weight, t46, t47, t49, t50, t62, t63, t65, t66, t71, t74, t38, t75, t76, t77, t78, t80, t_transformer_h_0_attn_proj_weight, t86, t95, t96, t101, t100, t103, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t108, t110, t113, t112, t117, t116, t119, t_transformer_h_0_mlp_proj_weight, t125, t134, t135, t140, t139, t142, t_transformer_h_1_attn_attn_weight, t185, t186, t188, t189, t201, t202, t204, t205, t211, t214, t176, t215, t216, t217, t218, t225, t_transformer_h_1_attn_proj_weight, t231, t240, t241, t246, t245, t248, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t253, t255, t258, t257, t262, t261, t264, t_transformer_h_1_mlp_proj_weight, t270, t279, t280, t285, t284, t287, t_transformer_h_2_attn_attn_weight, t330, t331, t333, t334, t346, t347, t349, t350, t356, t359, t321, t360, t361, t362, t363, t370, t_transformer_h_2_attn_proj_weight, t376, t385, t386, t391, t390, t393, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t398, t400, t403, t402, t407, t406, t409, t_transformer_h_2_mlp_proj_weight, t415, t424, t425, t430, t429, t432, t_transformer_h_3_attn_attn_weight, t475, t476, t478, t479, t491, t492, t494, t495, t501, t504, t466, t505, t506, t507, t508, t515, t_transformer_h_3_attn_proj_weight, t521, t530, t531, t536, t535, t538, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t543, t545, t548, t547, t552, t551, t554, t_transformer_h_3_mlp_proj_weight, t560, t569, t570, t575, t574, t577, t_transformer_h_4_attn_attn_weight, t620, t621, t623, t624, t636, t637, t639, t640, t646, t649, t611, t650, t651, t652, t653, t660, t_transformer_h_4_attn_proj_weight, t666, t675, t676, t681, t680, t683, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t688, t690, t693, t692, t697, t696, t699, t_transformer_h_4_mlp_proj_weight, t705, t714, t715, t720, t719, t722, t_transformer_h_5_attn_attn_weight, t765, t766, t768, t769, t781, t782, t784, t785, t791, t794, t756, t795, t796, t797, t798, t805, t_transformer_h_5_attn_proj_weight, t811, t820, t821, t826, t825, t828, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t833, t835, t838, t837, t842, t841, t844, t_transformer_h_5_mlp_proj_weight, t850, t859, t860, t865, t864, t867, t_transformer_h_6_attn_attn_weight, t910, t911, t913, t914, t926, t927, t929, t930, t936, t939, t901, t940, t941, t942, t943, t950, t_transformer_h_6_attn_proj_weight, t956, t965, t966, t971, t970, t973, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t978, t980, t983, t982, t987, t986, t989, t_transformer_h_6_mlp_proj_weight, t995, t1004, t1005, t1010, t1009, t1012, t_transformer_h_7_attn_attn_weight, t1055, t1056, t1058, t1059, t1071, t1072, t1074, t1075, t1081, t1084, t1046, t1085, t1086, t1087, t1088, t1095, t_transformer_h_7_attn_proj_weight, t1101, t1110, t1111, t1116, t1115, t1118, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t1123, t1125, t1128, t1127, t1132, t1131, t1134, t_transformer_h_7_mlp_proj_weight, t1140, t1149, t1150, t1155, t1154, t1157, t_transformer_h_8_attn_attn_weight, t1200, t1201, t1203, t1204, t1216, t1217, t1219, t1220, t1226, t1229, t1191, t1230, t1231, t1232, t1233, t1240, t_transformer_h_8_attn_proj_weight, t1246, t1255, t1256, t1261, t1260, t1263, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t1268, t1270, t1273, t1272, t1277, t1276, t1279, t_transformer_h_8_mlp_proj_weight, t1285, t1294, t1295, t1300, t1299, t1302, t_transformer_h_9_attn_attn_weight, t1345, t1346, t1348, t1349, t1361, t1362, t1364, t1365, t1371, t1374, t1336, t1375, t1376, t1377, t1378, t1385, t_transformer_h_9_attn_proj_weight, t1391, t1400, t1401, t1406, t1405, t1408, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t1413, t1415, t1418, t1417, t1422, t1421, t1424, t_transformer_h_9_mlp_proj_weight, t1430, t1439, t1440, t1445, t1444, t1447, t_transformer_h_10_attn_attn_weight, t1490, t1491, t1493, t1494, t1506, t1507, t1509, t1510, t1516, t1519, t1481, t1520, t1521, t1522, t1523, t1530, t_transformer_h_10_attn_proj_weight, t1536, t1545, t1546, t1551, t1550, t1553, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t1558, t1560, t1563, t1562, t1567, t1566, t1569, t_transformer_h_10_mlp_proj_weight, t1575, t1584, t1585, t1590, t1589, t1592, t_transformer_h_11_attn_attn_weight, t1635, t1636, t1638, t1639, t1651, t1652, t1654, t1655, t1661, t1664, t1626, t1665, t1666, t1667, t1668, t1675, t_transformer_h_11_attn_proj_weight, t1681, t1690, t1691, t1696, t1695, t1698, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t1703, t1705, t1708, t1707, t1712, t1711, t1714, t_transformer_h_11_mlp_proj_weight, t1720, t1729, t1730, t1735, t1734, t1737, t_transformer_h_12_attn_attn_weight, t1780, t1781, t1783, t1784, t1796, t1797, t1799, t1800, t1806, t1809, t1771, t1810, t1811, t1812, t1813, t1820, t_transformer_h_12_attn_proj_weight, t1826, t1835, t1836, t1841, t1840, t1843, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t1848, t1850, t1853, t1852, t1857, t1856, t1859, t_transformer_h_12_mlp_proj_weight, t1865, t1874, t1875, t1880, t1879, t1882, t_transformer_h_13_attn_attn_weight, t1925, t1926, t1928, t1929, t1941, t1942, t1944, t1945, t1951, t1954, t1916, t1955, t1956, t1957, t1958, t1965, t_transformer_h_13_attn_proj_weight, t1971, t1980, t1981, t1986, t1985, t1988, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t1993, t1995, t1998, t1997, t2002, t2001, t2004, t_transformer_h_13_mlp_proj_weight, t2010, t2019, t2020, t2025, t2024, t2027, t_transformer_h_14_attn_attn_weight, t2070, t2071, t2073, t2074, t2086, t2087, t2089, t2090, t2096, t2099, t2061, t2100, t2101, t2102, t2103, t2110, t_transformer_h_14_attn_proj_weight, t2116, t2125, t2126, t2131, t2130, t2133, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t2138, t2140, t2143, t2142, t2147, t2146, t2149, t_transformer_h_14_mlp_proj_weight, t2155, t2164, t2165, t2170, t2169, t2172, t_transformer_h_15_attn_attn_weight, t2215, t2216, t2218, t2219, t2231, t2232, t2234, t2235, t2241, t2244, t2206, t2245, t2246, t2247, t2248, t2255, t_transformer_h_15_attn_proj_weight, t2261, t2270, t2271, t2276, t2275, t2278, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t2283, t2285, t2288, t2287, t2292, t2291, t2294, t_transformer_h_15_mlp_proj_weight, t2300, t2309, t2310, t2315, t2314, t2317, t_lm_head_weight), (32000, False, False, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0))
============================================ END: primal_trace forward_and_backward_from_trace
============================================ START: before forward_trc transform_for_execution
# Constructed by Augmented forward pass
import thunder
import thunder.core.dtypes as dtypes
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight):
  # idx: "cuda:0 i64[1, 512]"
  # tos1: "cuda:0 f32[4096, 128]"
  # t_lm_head_weight: "cuda:0 bf16[32000, 4096]"
  # t_sin: "cuda:0 f32[4096, 128]"
  # t_transformer_h_0_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_0_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_0_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_0_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_0_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_1_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_1_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_1_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_2_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_2_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_2_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_3_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_3_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_3_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_4_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_4_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_4_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_5_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_5_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_5_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_6_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_6_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_6_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_7_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_7_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_7_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_8_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_8_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_8_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_9_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_9_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_9_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_10_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_10_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_10_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_11_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_11_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_11_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_12_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_12_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_12_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_13_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_13_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_13_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_14_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_14_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_14_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_15_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_15_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_15_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_ln_f_weight: "cuda:0 bf16[4096]"
  # t_transformer_wte_weight: "cuda:0 bf16[32000, 4096]"
  t0 = prims.slice_prim(tos1, [0, 0], [512, 128], [1, 1])  # t0: "cuda:0 f32[512, 128]"
  t1 = prims.slice_prim(t_sin, [0, 0], [512, 128], [1, 1])  # t1: "cuda:0 f32[512, 128]"
  t4 = ltorch.embedding(idx, t_transformer_wte_weight, None, None, 2.0, False, False)  # t4: "cuda:0 bf16[1, 512, 4096]"
    # t2 = ltorch.reshape(idx, [512])  # t2: "cuda:0 i64[512]"
      # t2 = prims.reshape(idx, (512,))  # t2: "cuda:0 i64[512]"
    # t3 = prims.take(t_transformer_wte_weight, t2, 0)  # t3: "cuda:0 bf16[512, 4096]"
    # t4 = ltorch.reshape(t3, [1, 512, 4096])  # t4: "cuda:0 bf16[1, 512, 4096]"
      # t4 = prims.reshape(t3, (1, 512, 4096))  # t4: "cuda:0 bf16[1, 512, 4096]"
  t5 = prims.convert_element_type(t4, dtypes.float32)  # t5: "cuda:0 f32[1, 512, 4096]"
  t6 = ltorch.mul(t5, t5)  # t6: "cuda:0 f32[1, 512, 4096]"
    # t6 = prims.mul(t5, t5)  # t6: "cuda:0 f32[1, 512, 4096]"
  t7 = prims.sum(t6, (2,))  # t7: "cuda:0 f32[1, 512]"
  t8 = prims.broadcast_in_dim(t7, [1, 512, 1], [0, 1])  # t8: "cuda:0 f32[1, 512, 1]"
  t9 = ltorch.true_divide(t8, 4096.0)  # t9: "cuda:0 f32[1, 512, 1]"
    # t9 = prims.div(t8, 4096.0)  # t9: "cuda:0 f32[1, 512, 1]"
  t10 = ltorch.add(t9, 1e-05, alpha=None)  # t10: "cuda:0 f32[1, 512, 1]"
    # t10 = prims.add(t9, 1e-05)  # t10: "cuda:0 f32[1, 512, 1]"
  t11 = prims.rsqrt(t10)  # t11: "cuda:0 f32[1, 512, 1]"
  t12 = prims.broadcast_in_dim(t11, (1, 512, 4096), (0, 1, 2))  # t12: "cuda:0 f32[1, 512, 4096]"
  t13 = ltorch.mul(t5, t12)  # t13: "cuda:0 f32[1, 512, 4096]"
    # t13 = prims.mul(t5, t12)  # t13: "cuda:0 f32[1, 512, 4096]"
  t14 = prims.convert_element_type(t13, dtypes.bfloat16)  # t14: "cuda:0 bf16[1, 512, 4096]"
  t15 = prims.broadcast_in_dim(t_transformer_h_0_norm_1_weight, (1, 512, 4096), (2,))  # t15: "cuda:0 bf16[1, 512, 4096]"
  t16 = prims.convert_element_type(t14, dtypes.float32)  # t16: "cuda:0 f32[1, 512, 4096]"
  t17 = prims.convert_element_type(t15, dtypes.float32)  # t17: "cuda:0 f32[1, 512, 4096]"
  t18 = ltorch.mul(t16, t17)  # t18: "cuda:0 f32[1, 512, 4096]"
    # t18 = prims.mul(t16, t17)  # t18: "cuda:0 f32[1, 512, 4096]"
  t19 = prims.convert_element_type(t18, dtypes.bfloat16)  # t19: "cuda:0 bf16[1, 512, 4096]"
  t20 = prims.linear(t19, t_transformer_h_0_attn_attn_weight, None)  # t20: "cuda:0 bf16[1, 512, 12288]"
  t21 = prims.reshape(t20, (1, 512, 32, 3, 128))  # t21: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t22 = prims.transpose(t21, (0, 2, 3, 1, 4))  # t22: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t23, t24, t25) = ltorch.split(t22, (1, 1, 1), 2)
    # t23 = prims.slice_prim(t22, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t23: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t24 = prims.slice_prim(t22, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t24: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t25 = prims.slice_prim(t22, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t25: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t26 = prims.reshape(t23, (1, 32, 512, 128))  # t26: "cuda:0 bf16[1, 32, 512, 128]"
  t32 = prims.reshape(t24, (1, 32, 512, 128))  # t32: "cuda:0 bf16[1, 32, 512, 128]"
  t38 = prims.reshape(t25, (1, 32, 512, 128))  # t38: "cuda:0 bf16[1, 32, 512, 128]"
  t39 = prims.slice_prim(t26, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t39: "cuda:0 bf16[1, 32, 512, 128]"
  t40 = prims.slice_prim(t39, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t40: "cuda:0 bf16[1, 32, 512, 64]"
  t41 = prims.slice_prim(t39, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t41: "cuda:0 bf16[1, 32, 512, 64]"
  t42 = prims.convert_element_type(t41, dtypes.float32)  # t42: "cuda:0 f32[1, 32, 512, 64]"
  t43 = prims.neg(t42)  # t43: "cuda:0 f32[1, 32, 512, 64]"
  t44 = prims.convert_element_type(t43, dtypes.bfloat16)  # t44: "cuda:0 bf16[1, 32, 512, 64]"
  t45 = prims.cat((t44, t40), -1)  # t45: "cuda:0 bf16[1, 32, 512, 128]"
  t46 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t46: "cuda:0 f32[1, 32, 512, 128]"
  t47 = prims.convert_element_type(t39, dtypes.float32)  # t47: "cuda:0 f32[1, 32, 512, 128]"
  t48 = ltorch.mul(t47, t46)  # t48: "cuda:0 f32[1, 32, 512, 128]"
    # t48 = prims.mul(t47, t46)  # t48: "cuda:0 f32[1, 32, 512, 128]"
  t49 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t49: "cuda:0 f32[1, 32, 512, 128]"
  t50 = prims.convert_element_type(t45, dtypes.float32)  # t50: "cuda:0 f32[1, 32, 512, 128]"
  t51 = ltorch.mul(t50, t49)  # t51: "cuda:0 f32[1, 32, 512, 128]"
    # t51 = prims.mul(t50, t49)  # t51: "cuda:0 f32[1, 32, 512, 128]"
  t52 = ltorch.add(t48, t51, alpha=None)  # t52: "cuda:0 f32[1, 32, 512, 128]"
    # t52 = prims.add(t48, t51)  # t52: "cuda:0 f32[1, 32, 512, 128]"
  t53 = prims.convert_element_type(t52, dtypes.bfloat16)  # t53: "cuda:0 bf16[1, 32, 512, 128]"
  t54 = prims.slice_prim(t32, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t54: "cuda:0 bf16[1, 32, 512, 128]"
  t55 = prims.slice_prim(t54, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t55: "cuda:0 bf16[1, 32, 512, 64]"
  t56 = prims.slice_prim(t54, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t56: "cuda:0 bf16[1, 32, 512, 64]"
  t57 = prims.convert_element_type(t56, dtypes.float32)  # t57: "cuda:0 f32[1, 32, 512, 64]"
  t58 = prims.neg(t57)  # t58: "cuda:0 f32[1, 32, 512, 64]"
  t59 = prims.convert_element_type(t58, dtypes.bfloat16)  # t59: "cuda:0 bf16[1, 32, 512, 64]"
  t61 = prims.cat((t59, t55), -1)  # t61: "cuda:0 bf16[1, 32, 512, 128]"
  t62 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t62: "cuda:0 f32[1, 32, 512, 128]"
  t63 = prims.convert_element_type(t54, dtypes.float32)  # t63: "cuda:0 f32[1, 32, 512, 128]"
  t64 = ltorch.mul(t63, t62)  # t64: "cuda:0 f32[1, 32, 512, 128]"
    # t64 = prims.mul(t63, t62)  # t64: "cuda:0 f32[1, 32, 512, 128]"
  t65 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t65: "cuda:0 f32[1, 32, 512, 128]"
  t66 = prims.convert_element_type(t61, dtypes.float32)  # t66: "cuda:0 f32[1, 32, 512, 128]"
  t67 = ltorch.mul(t66, t65)  # t67: "cuda:0 f32[1, 32, 512, 128]"
    # t67 = prims.mul(t66, t65)  # t67: "cuda:0 f32[1, 32, 512, 128]"
  t68 = ltorch.add(t64, t67, alpha=None)  # t68: "cuda:0 f32[1, 32, 512, 128]"
    # t68 = prims.add(t64, t67)  # t68: "cuda:0 f32[1, 32, 512, 128]"
  t69 = prims.convert_element_type(t68, dtypes.bfloat16)  # t69: "cuda:0 bf16[1, 32, 512, 128]"
  t70 = prims.slice_prim(t26, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t70: "cuda:0 bf16[1, 32, 512, 0]"
  t71 = prims.cat((t53, t70), -1)  # t71: "cuda:0 bf16[1, 32, 512, 128]"
  t72 = prims.slice_prim(t32, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t72: "cuda:0 bf16[1, 32, 512, 0]"
  t74 = prims.cat((t69, t72), -1)  # t74: "cuda:0 bf16[1, 32, 512, 128]"
  (t75, t76, t77, t78) = cudnn_sdpa_fwd(t71, t74, t38, None, 0.0, True, scale=0.08838834764831843)
  t79 = prims.transpose(t75, (0, 2, 1, 3))  # t79: "cuda:0 bf16[1, 512, 32, 128]"
  t80 = prims.reshape(t79, (1, 512, 4096))  # t80: "cuda:0 bf16[1, 512, 4096]"
  t81 = prims.linear(t80, t_transformer_h_0_attn_proj_weight, None)  # t81: "cuda:0 bf16[1, 512, 4096]"
  t82 = prims.convert_element_type(t81, dtypes.float32)  # t82: "cuda:0 f32[1, 512, 4096]"
  t83 = prims.convert_element_type(t4, dtypes.float32)  # t83: "cuda:0 f32[1, 512, 4096]"
  t84 = ltorch.add(t82, t83, alpha=None)  # t84: "cuda:0 f32[1, 512, 4096]"
    # t84 = prims.add(t82, t83)  # t84: "cuda:0 f32[1, 512, 4096]"
  t85 = prims.convert_element_type(t84, dtypes.bfloat16)  # t85: "cuda:0 bf16[1, 512, 4096]"
  t86 = prims.convert_element_type(t85, dtypes.float32)  # t86: "cuda:0 f32[1, 512, 4096]"
  t87 = ltorch.mul(t86, t86)  # t87: "cuda:0 f32[1, 512, 4096]"
    # t87 = prims.mul(t86, t86)  # t87: "cuda:0 f32[1, 512, 4096]"
  t89 = prims.sum(t87, (2,))  # t89: "cuda:0 f32[1, 512]"
  t90 = prims.broadcast_in_dim(t89, [1, 512, 1], [0, 1])  # t90: "cuda:0 f32[1, 512, 1]"
  t92 = ltorch.true_divide(t90, 4096.0)  # t92: "cuda:0 f32[1, 512, 1]"
    # t92 = prims.div(t90, 4096.0)  # t92: "cuda:0 f32[1, 512, 1]"
  t94 = ltorch.add(t92, 1e-05, alpha=None)  # t94: "cuda:0 f32[1, 512, 1]"
    # t94 = prims.add(t92, 1e-05)  # t94: "cuda:0 f32[1, 512, 1]"
  t95 = prims.rsqrt(t94)  # t95: "cuda:0 f32[1, 512, 1]"
  t96 = prims.broadcast_in_dim(t95, (1, 512, 4096), (0, 1, 2))  # t96: "cuda:0 f32[1, 512, 4096]"
  t97 = ltorch.mul(t86, t96)  # t97: "cuda:0 f32[1, 512, 4096]"
    # t97 = prims.mul(t86, t96)  # t97: "cuda:0 f32[1, 512, 4096]"
  t98 = prims.convert_element_type(t97, dtypes.bfloat16)  # t98: "cuda:0 bf16[1, 512, 4096]"
  t99 = prims.broadcast_in_dim(t_transformer_h_0_norm_2_weight, (1, 512, 4096), (2,))  # t99: "cuda:0 bf16[1, 512, 4096]"
  t100 = prims.convert_element_type(t98, dtypes.float32)  # t100: "cuda:0 f32[1, 512, 4096]"
  t101 = prims.convert_element_type(t99, dtypes.float32)  # t101: "cuda:0 f32[1, 512, 4096]"
  t102 = ltorch.mul(t100, t101)  # t102: "cuda:0 f32[1, 512, 4096]"
    # t102 = prims.mul(t100, t101)  # t102: "cuda:0 f32[1, 512, 4096]"
  t103 = prims.convert_element_type(t102, dtypes.bfloat16)  # t103: "cuda:0 bf16[1, 512, 4096]"
  t104 = prims.linear(t103, t_transformer_h_0_mlp_fc_1_weight, None)  # t104: "cuda:0 bf16[1, 512, 11008]"
  t105 = prims.linear(t103, t_transformer_h_0_mlp_fc_2_weight, None)  # t105: "cuda:0 bf16[1, 512, 11008]"
  t106 = prims.convert_element_type(t104, dtypes.float32)  # t106: "cuda:0 f32[1, 512, 11008]"
  t107 = prims.neg(t106)  # t107: "cuda:0 f32[1, 512, 11008]"
  t108 = prims.exp(t107)  # t108: "cuda:0 f32[1, 512, 11008]"
  t109 = ltorch.add(1.0, t108, alpha=None)  # t109: "cuda:0 f32[1, 512, 11008]"
    # t109 = prims.add(1.0, t108)  # t109: "cuda:0 f32[1, 512, 11008]"
  t110 = prims.reciprocal(t109)  # t110: "cuda:0 f32[1, 512, 11008]"
  t111 = prims.convert_element_type(t110, dtypes.bfloat16)  # t111: "cuda:0 bf16[1, 512, 11008]"
  t112 = prims.convert_element_type(t104, dtypes.float32)  # t112: "cuda:0 f32[1, 512, 11008]"
  t113 = prims.convert_element_type(t111, dtypes.float32)  # t113: "cuda:0 f32[1, 512, 11008]"
  t114 = ltorch.mul(t112, t113)  # t114: "cuda:0 f32[1, 512, 11008]"
    # t114 = prims.mul(t112, t113)  # t114: "cuda:0 f32[1, 512, 11008]"
  t115 = prims.convert_element_type(t114, dtypes.bfloat16)  # t115: "cuda:0 bf16[1, 512, 11008]"
  t116 = prims.convert_element_type(t115, dtypes.float32)  # t116: "cuda:0 f32[1, 512, 11008]"
  t117 = prims.convert_element_type(t105, dtypes.float32)  # t117: "cuda:0 f32[1, 512, 11008]"
  t118 = ltorch.mul(t116, t117)  # t118: "cuda:0 f32[1, 512, 11008]"
    # t118 = prims.mul(t116, t117)  # t118: "cuda:0 f32[1, 512, 11008]"
  t119 = prims.convert_element_type(t118, dtypes.bfloat16)  # t119: "cuda:0 bf16[1, 512, 11008]"
  t120 = prims.linear(t119, t_transformer_h_0_mlp_proj_weight, None)  # t120: "cuda:0 bf16[1, 512, 4096]"
  t121 = prims.convert_element_type(t120, dtypes.float32)  # t121: "cuda:0 f32[1, 512, 4096]"
  t122 = prims.convert_element_type(t85, dtypes.float32)  # t122: "cuda:0 f32[1, 512, 4096]"
  t123 = ltorch.add(t121, t122, alpha=None)  # t123: "cuda:0 f32[1, 512, 4096]"
    # t123 = prims.add(t121, t122)  # t123: "cuda:0 f32[1, 512, 4096]"
  t124 = prims.convert_element_type(t123, dtypes.bfloat16)  # t124: "cuda:0 bf16[1, 512, 4096]"
  t125 = prims.convert_element_type(t124, dtypes.float32)  # t125: "cuda:0 f32[1, 512, 4096]"
  t126 = ltorch.mul(t125, t125)  # t126: "cuda:0 f32[1, 512, 4096]"
    # t126 = prims.mul(t125, t125)  # t126: "cuda:0 f32[1, 512, 4096]"
  t128 = prims.sum(t126, (2,))  # t128: "cuda:0 f32[1, 512]"
  t129 = prims.broadcast_in_dim(t128, [1, 512, 1], [0, 1])  # t129: "cuda:0 f32[1, 512, 1]"
  t131 = ltorch.true_divide(t129, 4096.0)  # t131: "cuda:0 f32[1, 512, 1]"
    # t131 = prims.div(t129, 4096.0)  # t131: "cuda:0 f32[1, 512, 1]"
  t133 = ltorch.add(t131, 1e-05, alpha=None)  # t133: "cuda:0 f32[1, 512, 1]"
    # t133 = prims.add(t131, 1e-05)  # t133: "cuda:0 f32[1, 512, 1]"
  t134 = prims.rsqrt(t133)  # t134: "cuda:0 f32[1, 512, 1]"
  t135 = prims.broadcast_in_dim(t134, (1, 512, 4096), (0, 1, 2))  # t135: "cuda:0 f32[1, 512, 4096]"
  t136 = ltorch.mul(t125, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
    # t136 = prims.mul(t125, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
  t137 = prims.convert_element_type(t136, dtypes.bfloat16)  # t137: "cuda:0 bf16[1, 512, 4096]"
  t138 = prims.broadcast_in_dim(t_transformer_h_1_norm_1_weight, (1, 512, 4096), (2,))  # t138: "cuda:0 bf16[1, 512, 4096]"
  t139 = prims.convert_element_type(t137, dtypes.float32)  # t139: "cuda:0 f32[1, 512, 4096]"
  t140 = prims.convert_element_type(t138, dtypes.float32)  # t140: "cuda:0 f32[1, 512, 4096]"
  t141 = ltorch.mul(t139, t140)  # t141: "cuda:0 f32[1, 512, 4096]"
    # t141 = prims.mul(t139, t140)  # t141: "cuda:0 f32[1, 512, 4096]"
  t142 = prims.convert_element_type(t141, dtypes.bfloat16)  # t142: "cuda:0 bf16[1, 512, 4096]"
  t143 = prims.linear(t142, t_transformer_h_1_attn_attn_weight, None)  # t143: "cuda:0 bf16[1, 512, 12288]"
  t149 = prims.reshape(t143, (1, 512, 32, 3, 128))  # t149: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t155 = prims.transpose(t149, (0, 2, 3, 1, 4))  # t155: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t156, t157, t158) = ltorch.split(t155, (1, 1, 1), 2)
    # t156 = prims.slice_prim(t155, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t156: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t157 = prims.slice_prim(t155, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t157: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t158 = prims.slice_prim(t155, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t158: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t164 = prims.reshape(t156, (1, 32, 512, 128))  # t164: "cuda:0 bf16[1, 32, 512, 128]"
  t170 = prims.reshape(t157, (1, 32, 512, 128))  # t170: "cuda:0 bf16[1, 32, 512, 128]"
  t176 = prims.reshape(t158, (1, 32, 512, 128))  # t176: "cuda:0 bf16[1, 32, 512, 128]"
  t177 = prims.slice_prim(t164, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t177: "cuda:0 bf16[1, 32, 512, 128]"
  t178 = prims.slice_prim(t177, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t178: "cuda:0 bf16[1, 32, 512, 64]"
  t179 = prims.slice_prim(t177, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t179: "cuda:0 bf16[1, 32, 512, 64]"
  t180 = prims.convert_element_type(t179, dtypes.float32)  # t180: "cuda:0 f32[1, 32, 512, 64]"
  t181 = prims.neg(t180)  # t181: "cuda:0 f32[1, 32, 512, 64]"
  t182 = prims.convert_element_type(t181, dtypes.bfloat16)  # t182: "cuda:0 bf16[1, 32, 512, 64]"
  t184 = prims.cat((t182, t178), -1)  # t184: "cuda:0 bf16[1, 32, 512, 128]"
  t185 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t185: "cuda:0 f32[1, 32, 512, 128]"
  t186 = prims.convert_element_type(t177, dtypes.float32)  # t186: "cuda:0 f32[1, 32, 512, 128]"
  t187 = ltorch.mul(t186, t185)  # t187: "cuda:0 f32[1, 32, 512, 128]"
    # t187 = prims.mul(t186, t185)  # t187: "cuda:0 f32[1, 32, 512, 128]"
  t188 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t188: "cuda:0 f32[1, 32, 512, 128]"
  t189 = prims.convert_element_type(t184, dtypes.float32)  # t189: "cuda:0 f32[1, 32, 512, 128]"
  t190 = ltorch.mul(t189, t188)  # t190: "cuda:0 f32[1, 32, 512, 128]"
    # t190 = prims.mul(t189, t188)  # t190: "cuda:0 f32[1, 32, 512, 128]"
  t191 = ltorch.add(t187, t190, alpha=None)  # t191: "cuda:0 f32[1, 32, 512, 128]"
    # t191 = prims.add(t187, t190)  # t191: "cuda:0 f32[1, 32, 512, 128]"
  t192 = prims.convert_element_type(t191, dtypes.bfloat16)  # t192: "cuda:0 bf16[1, 32, 512, 128]"
  t193 = prims.slice_prim(t170, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t193: "cuda:0 bf16[1, 32, 512, 128]"
  t194 = prims.slice_prim(t193, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t194: "cuda:0 bf16[1, 32, 512, 64]"
  t195 = prims.slice_prim(t193, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t195: "cuda:0 bf16[1, 32, 512, 64]"
  t196 = prims.convert_element_type(t195, dtypes.float32)  # t196: "cuda:0 f32[1, 32, 512, 64]"
  t197 = prims.neg(t196)  # t197: "cuda:0 f32[1, 32, 512, 64]"
  t198 = prims.convert_element_type(t197, dtypes.bfloat16)  # t198: "cuda:0 bf16[1, 32, 512, 64]"
  t200 = prims.cat((t198, t194), -1)  # t200: "cuda:0 bf16[1, 32, 512, 128]"
  t201 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t201: "cuda:0 f32[1, 32, 512, 128]"
  t202 = prims.convert_element_type(t193, dtypes.float32)  # t202: "cuda:0 f32[1, 32, 512, 128]"
  t203 = ltorch.mul(t202, t201)  # t203: "cuda:0 f32[1, 32, 512, 128]"
    # t203 = prims.mul(t202, t201)  # t203: "cuda:0 f32[1, 32, 512, 128]"
  t204 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t204: "cuda:0 f32[1, 32, 512, 128]"
  t205 = prims.convert_element_type(t200, dtypes.float32)  # t205: "cuda:0 f32[1, 32, 512, 128]"
  t206 = ltorch.mul(t205, t204)  # t206: "cuda:0 f32[1, 32, 512, 128]"
    # t206 = prims.mul(t205, t204)  # t206: "cuda:0 f32[1, 32, 512, 128]"
  t207 = ltorch.add(t203, t206, alpha=None)  # t207: "cuda:0 f32[1, 32, 512, 128]"
    # t207 = prims.add(t203, t206)  # t207: "cuda:0 f32[1, 32, 512, 128]"
  t208 = prims.convert_element_type(t207, dtypes.bfloat16)  # t208: "cuda:0 bf16[1, 32, 512, 128]"
  t209 = prims.slice_prim(t164, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t209: "cuda:0 bf16[1, 32, 512, 0]"
  t211 = prims.cat((t192, t209), -1)  # t211: "cuda:0 bf16[1, 32, 512, 128]"
  t212 = prims.slice_prim(t170, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t212: "cuda:0 bf16[1, 32, 512, 0]"
  t214 = prims.cat((t208, t212), -1)  # t214: "cuda:0 bf16[1, 32, 512, 128]"
  (t215, t216, t217, t218) = cudnn_sdpa_fwd(t211, t214, t176, None, 0.0, True, scale=0.08838834764831843)
  t221 = prims.transpose(t215, (0, 2, 1, 3))  # t221: "cuda:0 bf16[1, 512, 32, 128]"
  t225 = prims.reshape(t221, (1, 512, 4096))  # t225: "cuda:0 bf16[1, 512, 4096]"
  t226 = prims.linear(t225, t_transformer_h_1_attn_proj_weight, None)  # t226: "cuda:0 bf16[1, 512, 4096]"
  t227 = prims.convert_element_type(t226, dtypes.float32)  # t227: "cuda:0 f32[1, 512, 4096]"
  t228 = prims.convert_element_type(t124, dtypes.float32)  # t228: "cuda:0 f32[1, 512, 4096]"
  t229 = ltorch.add(t227, t228, alpha=None)  # t229: "cuda:0 f32[1, 512, 4096]"
    # t229 = prims.add(t227, t228)  # t229: "cuda:0 f32[1, 512, 4096]"
  t230 = prims.convert_element_type(t229, dtypes.bfloat16)  # t230: "cuda:0 bf16[1, 512, 4096]"
  t231 = prims.convert_element_type(t230, dtypes.float32)  # t231: "cuda:0 f32[1, 512, 4096]"
  t232 = ltorch.mul(t231, t231)  # t232: "cuda:0 f32[1, 512, 4096]"
    # t232 = prims.mul(t231, t231)  # t232: "cuda:0 f32[1, 512, 4096]"
  t234 = prims.sum(t232, (2,))  # t234: "cuda:0 f32[1, 512]"
  t235 = prims.broadcast_in_dim(t234, [1, 512, 1], [0, 1])  # t235: "cuda:0 f32[1, 512, 1]"
  t237 = ltorch.true_divide(t235, 4096.0)  # t237: "cuda:0 f32[1, 512, 1]"
    # t237 = prims.div(t235, 4096.0)  # t237: "cuda:0 f32[1, 512, 1]"
  t239 = ltorch.add(t237, 1e-05, alpha=None)  # t239: "cuda:0 f32[1, 512, 1]"
    # t239 = prims.add(t237, 1e-05)  # t239: "cuda:0 f32[1, 512, 1]"
  t240 = prims.rsqrt(t239)  # t240: "cuda:0 f32[1, 512, 1]"
  t241 = prims.broadcast_in_dim(t240, (1, 512, 4096), (0, 1, 2))  # t241: "cuda:0 f32[1, 512, 4096]"
  t242 = ltorch.mul(t231, t241)  # t242: "cuda:0 f32[1, 512, 4096]"
    # t242 = prims.mul(t231, t241)  # t242: "cuda:0 f32[1, 512, 4096]"
  t243 = prims.convert_element_type(t242, dtypes.bfloat16)  # t243: "cuda:0 bf16[1, 512, 4096]"
  t244 = prims.broadcast_in_dim(t_transformer_h_1_norm_2_weight, (1, 512, 4096), (2,))  # t244: "cuda:0 bf16[1, 512, 4096]"
  t245 = prims.convert_element_type(t243, dtypes.float32)  # t245: "cuda:0 f32[1, 512, 4096]"
  t246 = prims.convert_element_type(t244, dtypes.float32)  # t246: "cuda:0 f32[1, 512, 4096]"
  t247 = ltorch.mul(t245, t246)  # t247: "cuda:0 f32[1, 512, 4096]"
    # t247 = prims.mul(t245, t246)  # t247: "cuda:0 f32[1, 512, 4096]"
  t248 = prims.convert_element_type(t247, dtypes.bfloat16)  # t248: "cuda:0 bf16[1, 512, 4096]"
  t249 = prims.linear(t248, t_transformer_h_1_mlp_fc_1_weight, None)  # t249: "cuda:0 bf16[1, 512, 11008]"
  t250 = prims.linear(t248, t_transformer_h_1_mlp_fc_2_weight, None)  # t250: "cuda:0 bf16[1, 512, 11008]"
  t251 = prims.convert_element_type(t249, dtypes.float32)  # t251: "cuda:0 f32[1, 512, 11008]"
  t252 = prims.neg(t251)  # t252: "cuda:0 f32[1, 512, 11008]"
  t253 = prims.exp(t252)  # t253: "cuda:0 f32[1, 512, 11008]"
  t254 = ltorch.add(1.0, t253, alpha=None)  # t254: "cuda:0 f32[1, 512, 11008]"
    # t254 = prims.add(1.0, t253)  # t254: "cuda:0 f32[1, 512, 11008]"
  t255 = prims.reciprocal(t254)  # t255: "cuda:0 f32[1, 512, 11008]"
  t256 = prims.convert_element_type(t255, dtypes.bfloat16)  # t256: "cuda:0 bf16[1, 512, 11008]"
  t257 = prims.convert_element_type(t249, dtypes.float32)  # t257: "cuda:0 f32[1, 512, 11008]"
  t258 = prims.convert_element_type(t256, dtypes.float32)  # t258: "cuda:0 f32[1, 512, 11008]"
  t259 = ltorch.mul(t257, t258)  # t259: "cuda:0 f32[1, 512, 11008]"
    # t259 = prims.mul(t257, t258)  # t259: "cuda:0 f32[1, 512, 11008]"
  t260 = prims.convert_element_type(t259, dtypes.bfloat16)  # t260: "cuda:0 bf16[1, 512, 11008]"
  t261 = prims.convert_element_type(t260, dtypes.float32)  # t261: "cuda:0 f32[1, 512, 11008]"
  t262 = prims.convert_element_type(t250, dtypes.float32)  # t262: "cuda:0 f32[1, 512, 11008]"
  t263 = ltorch.mul(t261, t262)  # t263: "cuda:0 f32[1, 512, 11008]"
    # t263 = prims.mul(t261, t262)  # t263: "cuda:0 f32[1, 512, 11008]"
  t264 = prims.convert_element_type(t263, dtypes.bfloat16)  # t264: "cuda:0 bf16[1, 512, 11008]"
  t265 = prims.linear(t264, t_transformer_h_1_mlp_proj_weight, None)  # t265: "cuda:0 bf16[1, 512, 4096]"
  t266 = prims.convert_element_type(t265, dtypes.float32)  # t266: "cuda:0 f32[1, 512, 4096]"
  t267 = prims.convert_element_type(t230, dtypes.float32)  # t267: "cuda:0 f32[1, 512, 4096]"
  t268 = ltorch.add(t266, t267, alpha=None)  # t268: "cuda:0 f32[1, 512, 4096]"
    # t268 = prims.add(t266, t267)  # t268: "cuda:0 f32[1, 512, 4096]"
  t269 = prims.convert_element_type(t268, dtypes.bfloat16)  # t269: "cuda:0 bf16[1, 512, 4096]"
  t270 = prims.convert_element_type(t269, dtypes.float32)  # t270: "cuda:0 f32[1, 512, 4096]"
  t271 = ltorch.mul(t270, t270)  # t271: "cuda:0 f32[1, 512, 4096]"
    # t271 = prims.mul(t270, t270)  # t271: "cuda:0 f32[1, 512, 4096]"
  t273 = prims.sum(t271, (2,))  # t273: "cuda:0 f32[1, 512]"
  t274 = prims.broadcast_in_dim(t273, [1, 512, 1], [0, 1])  # t274: "cuda:0 f32[1, 512, 1]"
  t276 = ltorch.true_divide(t274, 4096.0)  # t276: "cuda:0 f32[1, 512, 1]"
    # t276 = prims.div(t274, 4096.0)  # t276: "cuda:0 f32[1, 512, 1]"
  t278 = ltorch.add(t276, 1e-05, alpha=None)  # t278: "cuda:0 f32[1, 512, 1]"
    # t278 = prims.add(t276, 1e-05)  # t278: "cuda:0 f32[1, 512, 1]"
  t279 = prims.rsqrt(t278)  # t279: "cuda:0 f32[1, 512, 1]"
  t280 = prims.broadcast_in_dim(t279, (1, 512, 4096), (0, 1, 2))  # t280: "cuda:0 f32[1, 512, 4096]"
  t281 = ltorch.mul(t270, t280)  # t281: "cuda:0 f32[1, 512, 4096]"
    # t281 = prims.mul(t270, t280)  # t281: "cuda:0 f32[1, 512, 4096]"
  t282 = prims.convert_element_type(t281, dtypes.bfloat16)  # t282: "cuda:0 bf16[1, 512, 4096]"
  t283 = prims.broadcast_in_dim(t_transformer_h_2_norm_1_weight, (1, 512, 4096), (2,))  # t283: "cuda:0 bf16[1, 512, 4096]"
  t284 = prims.convert_element_type(t282, dtypes.float32)  # t284: "cuda:0 f32[1, 512, 4096]"
  t285 = prims.convert_element_type(t283, dtypes.float32)  # t285: "cuda:0 f32[1, 512, 4096]"
  t286 = ltorch.mul(t284, t285)  # t286: "cuda:0 f32[1, 512, 4096]"
    # t286 = prims.mul(t284, t285)  # t286: "cuda:0 f32[1, 512, 4096]"
  t287 = prims.convert_element_type(t286, dtypes.bfloat16)  # t287: "cuda:0 bf16[1, 512, 4096]"
  t288 = prims.linear(t287, t_transformer_h_2_attn_attn_weight, None)  # t288: "cuda:0 bf16[1, 512, 12288]"
  t294 = prims.reshape(t288, (1, 512, 32, 3, 128))  # t294: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t300 = prims.transpose(t294, (0, 2, 3, 1, 4))  # t300: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t301, t302, t303) = ltorch.split(t300, (1, 1, 1), 2)
    # t301 = prims.slice_prim(t300, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t301: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t302 = prims.slice_prim(t300, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t302: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t303 = prims.slice_prim(t300, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t303: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t309 = prims.reshape(t301, (1, 32, 512, 128))  # t309: "cuda:0 bf16[1, 32, 512, 128]"
  t315 = prims.reshape(t302, (1, 32, 512, 128))  # t315: "cuda:0 bf16[1, 32, 512, 128]"
  t321 = prims.reshape(t303, (1, 32, 512, 128))  # t321: "cuda:0 bf16[1, 32, 512, 128]"
  t322 = prims.slice_prim(t309, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t322: "cuda:0 bf16[1, 32, 512, 128]"
  t323 = prims.slice_prim(t322, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t323: "cuda:0 bf16[1, 32, 512, 64]"
  t324 = prims.slice_prim(t322, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t324: "cuda:0 bf16[1, 32, 512, 64]"
  t325 = prims.convert_element_type(t324, dtypes.float32)  # t325: "cuda:0 f32[1, 32, 512, 64]"
  t326 = prims.neg(t325)  # t326: "cuda:0 f32[1, 32, 512, 64]"
  t327 = prims.convert_element_type(t326, dtypes.bfloat16)  # t327: "cuda:0 bf16[1, 32, 512, 64]"
  t329 = prims.cat((t327, t323), -1)  # t329: "cuda:0 bf16[1, 32, 512, 128]"
  t330 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t330: "cuda:0 f32[1, 32, 512, 128]"
  t331 = prims.convert_element_type(t322, dtypes.float32)  # t331: "cuda:0 f32[1, 32, 512, 128]"
  t332 = ltorch.mul(t331, t330)  # t332: "cuda:0 f32[1, 32, 512, 128]"
    # t332 = prims.mul(t331, t330)  # t332: "cuda:0 f32[1, 32, 512, 128]"
  t333 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t333: "cuda:0 f32[1, 32, 512, 128]"
  t334 = prims.convert_element_type(t329, dtypes.float32)  # t334: "cuda:0 f32[1, 32, 512, 128]"
  t335 = ltorch.mul(t334, t333)  # t335: "cuda:0 f32[1, 32, 512, 128]"
    # t335 = prims.mul(t334, t333)  # t335: "cuda:0 f32[1, 32, 512, 128]"
  t336 = ltorch.add(t332, t335, alpha=None)  # t336: "cuda:0 f32[1, 32, 512, 128]"
    # t336 = prims.add(t332, t335)  # t336: "cuda:0 f32[1, 32, 512, 128]"
  t337 = prims.convert_element_type(t336, dtypes.bfloat16)  # t337: "cuda:0 bf16[1, 32, 512, 128]"
  t338 = prims.slice_prim(t315, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t338: "cuda:0 bf16[1, 32, 512, 128]"
  t339 = prims.slice_prim(t338, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t339: "cuda:0 bf16[1, 32, 512, 64]"
  t340 = prims.slice_prim(t338, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t340: "cuda:0 bf16[1, 32, 512, 64]"
  t341 = prims.convert_element_type(t340, dtypes.float32)  # t341: "cuda:0 f32[1, 32, 512, 64]"
  t342 = prims.neg(t341)  # t342: "cuda:0 f32[1, 32, 512, 64]"
  t343 = prims.convert_element_type(t342, dtypes.bfloat16)  # t343: "cuda:0 bf16[1, 32, 512, 64]"
  t345 = prims.cat((t343, t339), -1)  # t345: "cuda:0 bf16[1, 32, 512, 128]"
  t346 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t346: "cuda:0 f32[1, 32, 512, 128]"
  t347 = prims.convert_element_type(t338, dtypes.float32)  # t347: "cuda:0 f32[1, 32, 512, 128]"
  t348 = ltorch.mul(t347, t346)  # t348: "cuda:0 f32[1, 32, 512, 128]"
    # t348 = prims.mul(t347, t346)  # t348: "cuda:0 f32[1, 32, 512, 128]"
  t349 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t349: "cuda:0 f32[1, 32, 512, 128]"
  t350 = prims.convert_element_type(t345, dtypes.float32)  # t350: "cuda:0 f32[1, 32, 512, 128]"
  t351 = ltorch.mul(t350, t349)  # t351: "cuda:0 f32[1, 32, 512, 128]"
    # t351 = prims.mul(t350, t349)  # t351: "cuda:0 f32[1, 32, 512, 128]"
  t352 = ltorch.add(t348, t351, alpha=None)  # t352: "cuda:0 f32[1, 32, 512, 128]"
    # t352 = prims.add(t348, t351)  # t352: "cuda:0 f32[1, 32, 512, 128]"
  t353 = prims.convert_element_type(t352, dtypes.bfloat16)  # t353: "cuda:0 bf16[1, 32, 512, 128]"
  t354 = prims.slice_prim(t309, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t354: "cuda:0 bf16[1, 32, 512, 0]"
  t356 = prims.cat((t337, t354), -1)  # t356: "cuda:0 bf16[1, 32, 512, 128]"
  t357 = prims.slice_prim(t315, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t357: "cuda:0 bf16[1, 32, 512, 0]"
  t359 = prims.cat((t353, t357), -1)  # t359: "cuda:0 bf16[1, 32, 512, 128]"
  (t360, t361, t362, t363) = cudnn_sdpa_fwd(t356, t359, t321, None, 0.0, True, scale=0.08838834764831843)
  t366 = prims.transpose(t360, (0, 2, 1, 3))  # t366: "cuda:0 bf16[1, 512, 32, 128]"
  t370 = prims.reshape(t366, (1, 512, 4096))  # t370: "cuda:0 bf16[1, 512, 4096]"
  t371 = prims.linear(t370, t_transformer_h_2_attn_proj_weight, None)  # t371: "cuda:0 bf16[1, 512, 4096]"
  t372 = prims.convert_element_type(t371, dtypes.float32)  # t372: "cuda:0 f32[1, 512, 4096]"
  t373 = prims.convert_element_type(t269, dtypes.float32)  # t373: "cuda:0 f32[1, 512, 4096]"
  t374 = ltorch.add(t372, t373, alpha=None)  # t374: "cuda:0 f32[1, 512, 4096]"
    # t374 = prims.add(t372, t373)  # t374: "cuda:0 f32[1, 512, 4096]"
  t375 = prims.convert_element_type(t374, dtypes.bfloat16)  # t375: "cuda:0 bf16[1, 512, 4096]"
  t376 = prims.convert_element_type(t375, dtypes.float32)  # t376: "cuda:0 f32[1, 512, 4096]"
  t377 = ltorch.mul(t376, t376)  # t377: "cuda:0 f32[1, 512, 4096]"
    # t377 = prims.mul(t376, t376)  # t377: "cuda:0 f32[1, 512, 4096]"
  t379 = prims.sum(t377, (2,))  # t379: "cuda:0 f32[1, 512]"
  t380 = prims.broadcast_in_dim(t379, [1, 512, 1], [0, 1])  # t380: "cuda:0 f32[1, 512, 1]"
  t382 = ltorch.true_divide(t380, 4096.0)  # t382: "cuda:0 f32[1, 512, 1]"
    # t382 = prims.div(t380, 4096.0)  # t382: "cuda:0 f32[1, 512, 1]"
  t384 = ltorch.add(t382, 1e-05, alpha=None)  # t384: "cuda:0 f32[1, 512, 1]"
    # t384 = prims.add(t382, 1e-05)  # t384: "cuda:0 f32[1, 512, 1]"
  t385 = prims.rsqrt(t384)  # t385: "cuda:0 f32[1, 512, 1]"
  t386 = prims.broadcast_in_dim(t385, (1, 512, 4096), (0, 1, 2))  # t386: "cuda:0 f32[1, 512, 4096]"
  t387 = ltorch.mul(t376, t386)  # t387: "cuda:0 f32[1, 512, 4096]"
    # t387 = prims.mul(t376, t386)  # t387: "cuda:0 f32[1, 512, 4096]"
  t388 = prims.convert_element_type(t387, dtypes.bfloat16)  # t388: "cuda:0 bf16[1, 512, 4096]"
  t389 = prims.broadcast_in_dim(t_transformer_h_2_norm_2_weight, (1, 512, 4096), (2,))  # t389: "cuda:0 bf16[1, 512, 4096]"
  t390 = prims.convert_element_type(t388, dtypes.float32)  # t390: "cuda:0 f32[1, 512, 4096]"
  t391 = prims.convert_element_type(t389, dtypes.float32)  # t391: "cuda:0 f32[1, 512, 4096]"
  t392 = ltorch.mul(t390, t391)  # t392: "cuda:0 f32[1, 512, 4096]"
    # t392 = prims.mul(t390, t391)  # t392: "cuda:0 f32[1, 512, 4096]"
  t393 = prims.convert_element_type(t392, dtypes.bfloat16)  # t393: "cuda:0 bf16[1, 512, 4096]"
  t394 = prims.linear(t393, t_transformer_h_2_mlp_fc_1_weight, None)  # t394: "cuda:0 bf16[1, 512, 11008]"
  t395 = prims.linear(t393, t_transformer_h_2_mlp_fc_2_weight, None)  # t395: "cuda:0 bf16[1, 512, 11008]"
  t396 = prims.convert_element_type(t394, dtypes.float32)  # t396: "cuda:0 f32[1, 512, 11008]"
  t397 = prims.neg(t396)  # t397: "cuda:0 f32[1, 512, 11008]"
  t398 = prims.exp(t397)  # t398: "cuda:0 f32[1, 512, 11008]"
  t399 = ltorch.add(1.0, t398, alpha=None)  # t399: "cuda:0 f32[1, 512, 11008]"
    # t399 = prims.add(1.0, t398)  # t399: "cuda:0 f32[1, 512, 11008]"
  t400 = prims.reciprocal(t399)  # t400: "cuda:0 f32[1, 512, 11008]"
  t401 = prims.convert_element_type(t400, dtypes.bfloat16)  # t401: "cuda:0 bf16[1, 512, 11008]"
  t402 = prims.convert_element_type(t394, dtypes.float32)  # t402: "cuda:0 f32[1, 512, 11008]"
  t403 = prims.convert_element_type(t401, dtypes.float32)  # t403: "cuda:0 f32[1, 512, 11008]"
  t404 = ltorch.mul(t402, t403)  # t404: "cuda:0 f32[1, 512, 11008]"
    # t404 = prims.mul(t402, t403)  # t404: "cuda:0 f32[1, 512, 11008]"
  t405 = prims.convert_element_type(t404, dtypes.bfloat16)  # t405: "cuda:0 bf16[1, 512, 11008]"
  t406 = prims.convert_element_type(t405, dtypes.float32)  # t406: "cuda:0 f32[1, 512, 11008]"
  t407 = prims.convert_element_type(t395, dtypes.float32)  # t407: "cuda:0 f32[1, 512, 11008]"
  t408 = ltorch.mul(t406, t407)  # t408: "cuda:0 f32[1, 512, 11008]"
    # t408 = prims.mul(t406, t407)  # t408: "cuda:0 f32[1, 512, 11008]"
  t409 = prims.convert_element_type(t408, dtypes.bfloat16)  # t409: "cuda:0 bf16[1, 512, 11008]"
  t410 = prims.linear(t409, t_transformer_h_2_mlp_proj_weight, None)  # t410: "cuda:0 bf16[1, 512, 4096]"
  t411 = prims.convert_element_type(t410, dtypes.float32)  # t411: "cuda:0 f32[1, 512, 4096]"
  t412 = prims.convert_element_type(t375, dtypes.float32)  # t412: "cuda:0 f32[1, 512, 4096]"
  t413 = ltorch.add(t411, t412, alpha=None)  # t413: "cuda:0 f32[1, 512, 4096]"
    # t413 = prims.add(t411, t412)  # t413: "cuda:0 f32[1, 512, 4096]"
  t414 = prims.convert_element_type(t413, dtypes.bfloat16)  # t414: "cuda:0 bf16[1, 512, 4096]"
  t415 = prims.convert_element_type(t414, dtypes.float32)  # t415: "cuda:0 f32[1, 512, 4096]"
  t416 = ltorch.mul(t415, t415)  # t416: "cuda:0 f32[1, 512, 4096]"
    # t416 = prims.mul(t415, t415)  # t416: "cuda:0 f32[1, 512, 4096]"
  t418 = prims.sum(t416, (2,))  # t418: "cuda:0 f32[1, 512]"
  t419 = prims.broadcast_in_dim(t418, [1, 512, 1], [0, 1])  # t419: "cuda:0 f32[1, 512, 1]"
  t421 = ltorch.true_divide(t419, 4096.0)  # t421: "cuda:0 f32[1, 512, 1]"
    # t421 = prims.div(t419, 4096.0)  # t421: "cuda:0 f32[1, 512, 1]"
  t423 = ltorch.add(t421, 1e-05, alpha=None)  # t423: "cuda:0 f32[1, 512, 1]"
    # t423 = prims.add(t421, 1e-05)  # t423: "cuda:0 f32[1, 512, 1]"
  t424 = prims.rsqrt(t423)  # t424: "cuda:0 f32[1, 512, 1]"
  t425 = prims.broadcast_in_dim(t424, (1, 512, 4096), (0, 1, 2))  # t425: "cuda:0 f32[1, 512, 4096]"
  t426 = ltorch.mul(t415, t425)  # t426: "cuda:0 f32[1, 512, 4096]"
    # t426 = prims.mul(t415, t425)  # t426: "cuda:0 f32[1, 512, 4096]"
  t427 = prims.convert_element_type(t426, dtypes.bfloat16)  # t427: "cuda:0 bf16[1, 512, 4096]"
  t428 = prims.broadcast_in_dim(t_transformer_h_3_norm_1_weight, (1, 512, 4096), (2,))  # t428: "cuda:0 bf16[1, 512, 4096]"
  t429 = prims.convert_element_type(t427, dtypes.float32)  # t429: "cuda:0 f32[1, 512, 4096]"
  t430 = prims.convert_element_type(t428, dtypes.float32)  # t430: "cuda:0 f32[1, 512, 4096]"
  t431 = ltorch.mul(t429, t430)  # t431: "cuda:0 f32[1, 512, 4096]"
    # t431 = prims.mul(t429, t430)  # t431: "cuda:0 f32[1, 512, 4096]"
  t432 = prims.convert_element_type(t431, dtypes.bfloat16)  # t432: "cuda:0 bf16[1, 512, 4096]"
  t433 = prims.linear(t432, t_transformer_h_3_attn_attn_weight, None)  # t433: "cuda:0 bf16[1, 512, 12288]"
  t439 = prims.reshape(t433, (1, 512, 32, 3, 128))  # t439: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t445 = prims.transpose(t439, (0, 2, 3, 1, 4))  # t445: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t446, t447, t448) = ltorch.split(t445, (1, 1, 1), 2)
    # t446 = prims.slice_prim(t445, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t446: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t447 = prims.slice_prim(t445, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t447: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t448 = prims.slice_prim(t445, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t448: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t454 = prims.reshape(t446, (1, 32, 512, 128))  # t454: "cuda:0 bf16[1, 32, 512, 128]"
  t460 = prims.reshape(t447, (1, 32, 512, 128))  # t460: "cuda:0 bf16[1, 32, 512, 128]"
  t466 = prims.reshape(t448, (1, 32, 512, 128))  # t466: "cuda:0 bf16[1, 32, 512, 128]"
  t467 = prims.slice_prim(t454, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t467: "cuda:0 bf16[1, 32, 512, 128]"
  t468 = prims.slice_prim(t467, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t468: "cuda:0 bf16[1, 32, 512, 64]"
  t469 = prims.slice_prim(t467, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t469: "cuda:0 bf16[1, 32, 512, 64]"
  t470 = prims.convert_element_type(t469, dtypes.float32)  # t470: "cuda:0 f32[1, 32, 512, 64]"
  t471 = prims.neg(t470)  # t471: "cuda:0 f32[1, 32, 512, 64]"
  t472 = prims.convert_element_type(t471, dtypes.bfloat16)  # t472: "cuda:0 bf16[1, 32, 512, 64]"
  t474 = prims.cat((t472, t468), -1)  # t474: "cuda:0 bf16[1, 32, 512, 128]"
  t475 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t475: "cuda:0 f32[1, 32, 512, 128]"
  t476 = prims.convert_element_type(t467, dtypes.float32)  # t476: "cuda:0 f32[1, 32, 512, 128]"
  t477 = ltorch.mul(t476, t475)  # t477: "cuda:0 f32[1, 32, 512, 128]"
    # t477 = prims.mul(t476, t475)  # t477: "cuda:0 f32[1, 32, 512, 128]"
  t478 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t478: "cuda:0 f32[1, 32, 512, 128]"
  t479 = prims.convert_element_type(t474, dtypes.float32)  # t479: "cuda:0 f32[1, 32, 512, 128]"
  t480 = ltorch.mul(t479, t478)  # t480: "cuda:0 f32[1, 32, 512, 128]"
    # t480 = prims.mul(t479, t478)  # t480: "cuda:0 f32[1, 32, 512, 128]"
  t481 = ltorch.add(t477, t480, alpha=None)  # t481: "cuda:0 f32[1, 32, 512, 128]"
    # t481 = prims.add(t477, t480)  # t481: "cuda:0 f32[1, 32, 512, 128]"
  t482 = prims.convert_element_type(t481, dtypes.bfloat16)  # t482: "cuda:0 bf16[1, 32, 512, 128]"
  t483 = prims.slice_prim(t460, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t483: "cuda:0 bf16[1, 32, 512, 128]"
  t484 = prims.slice_prim(t483, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t484: "cuda:0 bf16[1, 32, 512, 64]"
  t485 = prims.slice_prim(t483, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t485: "cuda:0 bf16[1, 32, 512, 64]"
  t486 = prims.convert_element_type(t485, dtypes.float32)  # t486: "cuda:0 f32[1, 32, 512, 64]"
  t487 = prims.neg(t486)  # t487: "cuda:0 f32[1, 32, 512, 64]"
  t488 = prims.convert_element_type(t487, dtypes.bfloat16)  # t488: "cuda:0 bf16[1, 32, 512, 64]"
  t490 = prims.cat((t488, t484), -1)  # t490: "cuda:0 bf16[1, 32, 512, 128]"
  t491 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t491: "cuda:0 f32[1, 32, 512, 128]"
  t492 = prims.convert_element_type(t483, dtypes.float32)  # t492: "cuda:0 f32[1, 32, 512, 128]"
  t493 = ltorch.mul(t492, t491)  # t493: "cuda:0 f32[1, 32, 512, 128]"
    # t493 = prims.mul(t492, t491)  # t493: "cuda:0 f32[1, 32, 512, 128]"
  t494 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t494: "cuda:0 f32[1, 32, 512, 128]"
  t495 = prims.convert_element_type(t490, dtypes.float32)  # t495: "cuda:0 f32[1, 32, 512, 128]"
  t496 = ltorch.mul(t495, t494)  # t496: "cuda:0 f32[1, 32, 512, 128]"
    # t496 = prims.mul(t495, t494)  # t496: "cuda:0 f32[1, 32, 512, 128]"
  t497 = ltorch.add(t493, t496, alpha=None)  # t497: "cuda:0 f32[1, 32, 512, 128]"
    # t497 = prims.add(t493, t496)  # t497: "cuda:0 f32[1, 32, 512, 128]"
  t498 = prims.convert_element_type(t497, dtypes.bfloat16)  # t498: "cuda:0 bf16[1, 32, 512, 128]"
  t499 = prims.slice_prim(t454, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t499: "cuda:0 bf16[1, 32, 512, 0]"
  t501 = prims.cat((t482, t499), -1)  # t501: "cuda:0 bf16[1, 32, 512, 128]"
  t502 = prims.slice_prim(t460, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t502: "cuda:0 bf16[1, 32, 512, 0]"
  t504 = prims.cat((t498, t502), -1)  # t504: "cuda:0 bf16[1, 32, 512, 128]"
  (t505, t506, t507, t508) = cudnn_sdpa_fwd(t501, t504, t466, None, 0.0, True, scale=0.08838834764831843)
  t511 = prims.transpose(t505, (0, 2, 1, 3))  # t511: "cuda:0 bf16[1, 512, 32, 128]"
  t515 = prims.reshape(t511, (1, 512, 4096))  # t515: "cuda:0 bf16[1, 512, 4096]"
  t516 = prims.linear(t515, t_transformer_h_3_attn_proj_weight, None)  # t516: "cuda:0 bf16[1, 512, 4096]"
  t517 = prims.convert_element_type(t516, dtypes.float32)  # t517: "cuda:0 f32[1, 512, 4096]"
  t518 = prims.convert_element_type(t414, dtypes.float32)  # t518: "cuda:0 f32[1, 512, 4096]"
  t519 = ltorch.add(t517, t518, alpha=None)  # t519: "cuda:0 f32[1, 512, 4096]"
    # t519 = prims.add(t517, t518)  # t519: "cuda:0 f32[1, 512, 4096]"
  t520 = prims.convert_element_type(t519, dtypes.bfloat16)  # t520: "cuda:0 bf16[1, 512, 4096]"
  t521 = prims.convert_element_type(t520, dtypes.float32)  # t521: "cuda:0 f32[1, 512, 4096]"
  t522 = ltorch.mul(t521, t521)  # t522: "cuda:0 f32[1, 512, 4096]"
    # t522 = prims.mul(t521, t521)  # t522: "cuda:0 f32[1, 512, 4096]"
  t524 = prims.sum(t522, (2,))  # t524: "cuda:0 f32[1, 512]"
  t525 = prims.broadcast_in_dim(t524, [1, 512, 1], [0, 1])  # t525: "cuda:0 f32[1, 512, 1]"
  t527 = ltorch.true_divide(t525, 4096.0)  # t527: "cuda:0 f32[1, 512, 1]"
    # t527 = prims.div(t525, 4096.0)  # t527: "cuda:0 f32[1, 512, 1]"
  t529 = ltorch.add(t527, 1e-05, alpha=None)  # t529: "cuda:0 f32[1, 512, 1]"
    # t529 = prims.add(t527, 1e-05)  # t529: "cuda:0 f32[1, 512, 1]"
  t530 = prims.rsqrt(t529)  # t530: "cuda:0 f32[1, 512, 1]"
  t531 = prims.broadcast_in_dim(t530, (1, 512, 4096), (0, 1, 2))  # t531: "cuda:0 f32[1, 512, 4096]"
  t532 = ltorch.mul(t521, t531)  # t532: "cuda:0 f32[1, 512, 4096]"
    # t532 = prims.mul(t521, t531)  # t532: "cuda:0 f32[1, 512, 4096]"
  t533 = prims.convert_element_type(t532, dtypes.bfloat16)  # t533: "cuda:0 bf16[1, 512, 4096]"
  t534 = prims.broadcast_in_dim(t_transformer_h_3_norm_2_weight, (1, 512, 4096), (2,))  # t534: "cuda:0 bf16[1, 512, 4096]"
  t535 = prims.convert_element_type(t533, dtypes.float32)  # t535: "cuda:0 f32[1, 512, 4096]"
  t536 = prims.convert_element_type(t534, dtypes.float32)  # t536: "cuda:0 f32[1, 512, 4096]"
  t537 = ltorch.mul(t535, t536)  # t537: "cuda:0 f32[1, 512, 4096]"
    # t537 = prims.mul(t535, t536)  # t537: "cuda:0 f32[1, 512, 4096]"
  t538 = prims.convert_element_type(t537, dtypes.bfloat16)  # t538: "cuda:0 bf16[1, 512, 4096]"
  t539 = prims.linear(t538, t_transformer_h_3_mlp_fc_1_weight, None)  # t539: "cuda:0 bf16[1, 512, 11008]"
  t540 = prims.linear(t538, t_transformer_h_3_mlp_fc_2_weight, None)  # t540: "cuda:0 bf16[1, 512, 11008]"
  t541 = prims.convert_element_type(t539, dtypes.float32)  # t541: "cuda:0 f32[1, 512, 11008]"
  t542 = prims.neg(t541)  # t542: "cuda:0 f32[1, 512, 11008]"
  t543 = prims.exp(t542)  # t543: "cuda:0 f32[1, 512, 11008]"
  t544 = ltorch.add(1.0, t543, alpha=None)  # t544: "cuda:0 f32[1, 512, 11008]"
    # t544 = prims.add(1.0, t543)  # t544: "cuda:0 f32[1, 512, 11008]"
  t545 = prims.reciprocal(t544)  # t545: "cuda:0 f32[1, 512, 11008]"
  t546 = prims.convert_element_type(t545, dtypes.bfloat16)  # t546: "cuda:0 bf16[1, 512, 11008]"
  t547 = prims.convert_element_type(t539, dtypes.float32)  # t547: "cuda:0 f32[1, 512, 11008]"
  t548 = prims.convert_element_type(t546, dtypes.float32)  # t548: "cuda:0 f32[1, 512, 11008]"
  t549 = ltorch.mul(t547, t548)  # t549: "cuda:0 f32[1, 512, 11008]"
    # t549 = prims.mul(t547, t548)  # t549: "cuda:0 f32[1, 512, 11008]"
  t550 = prims.convert_element_type(t549, dtypes.bfloat16)  # t550: "cuda:0 bf16[1, 512, 11008]"
  t551 = prims.convert_element_type(t550, dtypes.float32)  # t551: "cuda:0 f32[1, 512, 11008]"
  t552 = prims.convert_element_type(t540, dtypes.float32)  # t552: "cuda:0 f32[1, 512, 11008]"
  t553 = ltorch.mul(t551, t552)  # t553: "cuda:0 f32[1, 512, 11008]"
    # t553 = prims.mul(t551, t552)  # t553: "cuda:0 f32[1, 512, 11008]"
  t554 = prims.convert_element_type(t553, dtypes.bfloat16)  # t554: "cuda:0 bf16[1, 512, 11008]"
  t555 = prims.linear(t554, t_transformer_h_3_mlp_proj_weight, None)  # t555: "cuda:0 bf16[1, 512, 4096]"
  t556 = prims.convert_element_type(t555, dtypes.float32)  # t556: "cuda:0 f32[1, 512, 4096]"
  t557 = prims.convert_element_type(t520, dtypes.float32)  # t557: "cuda:0 f32[1, 512, 4096]"
  t558 = ltorch.add(t556, t557, alpha=None)  # t558: "cuda:0 f32[1, 512, 4096]"
    # t558 = prims.add(t556, t557)  # t558: "cuda:0 f32[1, 512, 4096]"
  t559 = prims.convert_element_type(t558, dtypes.bfloat16)  # t559: "cuda:0 bf16[1, 512, 4096]"
  t560 = prims.convert_element_type(t559, dtypes.float32)  # t560: "cuda:0 f32[1, 512, 4096]"
  t561 = ltorch.mul(t560, t560)  # t561: "cuda:0 f32[1, 512, 4096]"
    # t561 = prims.mul(t560, t560)  # t561: "cuda:0 f32[1, 512, 4096]"
  t563 = prims.sum(t561, (2,))  # t563: "cuda:0 f32[1, 512]"
  t564 = prims.broadcast_in_dim(t563, [1, 512, 1], [0, 1])  # t564: "cuda:0 f32[1, 512, 1]"
  t566 = ltorch.true_divide(t564, 4096.0)  # t566: "cuda:0 f32[1, 512, 1]"
    # t566 = prims.div(t564, 4096.0)  # t566: "cuda:0 f32[1, 512, 1]"
  t568 = ltorch.add(t566, 1e-05, alpha=None)  # t568: "cuda:0 f32[1, 512, 1]"
    # t568 = prims.add(t566, 1e-05)  # t568: "cuda:0 f32[1, 512, 1]"
  t569 = prims.rsqrt(t568)  # t569: "cuda:0 f32[1, 512, 1]"
  t570 = prims.broadcast_in_dim(t569, (1, 512, 4096), (0, 1, 2))  # t570: "cuda:0 f32[1, 512, 4096]"
  t571 = ltorch.mul(t560, t570)  # t571: "cuda:0 f32[1, 512, 4096]"
    # t571 = prims.mul(t560, t570)  # t571: "cuda:0 f32[1, 512, 4096]"
  t572 = prims.convert_element_type(t571, dtypes.bfloat16)  # t572: "cuda:0 bf16[1, 512, 4096]"
  t573 = prims.broadcast_in_dim(t_transformer_h_4_norm_1_weight, (1, 512, 4096), (2,))  # t573: "cuda:0 bf16[1, 512, 4096]"
  t574 = prims.convert_element_type(t572, dtypes.float32)  # t574: "cuda:0 f32[1, 512, 4096]"
  t575 = prims.convert_element_type(t573, dtypes.float32)  # t575: "cuda:0 f32[1, 512, 4096]"
  t576 = ltorch.mul(t574, t575)  # t576: "cuda:0 f32[1, 512, 4096]"
    # t576 = prims.mul(t574, t575)  # t576: "cuda:0 f32[1, 512, 4096]"
  t577 = prims.convert_element_type(t576, dtypes.bfloat16)  # t577: "cuda:0 bf16[1, 512, 4096]"
  t578 = prims.linear(t577, t_transformer_h_4_attn_attn_weight, None)  # t578: "cuda:0 bf16[1, 512, 12288]"
  t584 = prims.reshape(t578, (1, 512, 32, 3, 128))  # t584: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t590 = prims.transpose(t584, (0, 2, 3, 1, 4))  # t590: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t591, t592, t593) = ltorch.split(t590, (1, 1, 1), 2)
    # t591 = prims.slice_prim(t590, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t591: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t592 = prims.slice_prim(t590, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t592: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t593 = prims.slice_prim(t590, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t593: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t599 = prims.reshape(t591, (1, 32, 512, 128))  # t599: "cuda:0 bf16[1, 32, 512, 128]"
  t605 = prims.reshape(t592, (1, 32, 512, 128))  # t605: "cuda:0 bf16[1, 32, 512, 128]"
  t611 = prims.reshape(t593, (1, 32, 512, 128))  # t611: "cuda:0 bf16[1, 32, 512, 128]"
  t612 = prims.slice_prim(t599, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t612: "cuda:0 bf16[1, 32, 512, 128]"
  t613 = prims.slice_prim(t612, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t613: "cuda:0 bf16[1, 32, 512, 64]"
  t614 = prims.slice_prim(t612, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t614: "cuda:0 bf16[1, 32, 512, 64]"
  t615 = prims.convert_element_type(t614, dtypes.float32)  # t615: "cuda:0 f32[1, 32, 512, 64]"
  t616 = prims.neg(t615)  # t616: "cuda:0 f32[1, 32, 512, 64]"
  t617 = prims.convert_element_type(t616, dtypes.bfloat16)  # t617: "cuda:0 bf16[1, 32, 512, 64]"
  t619 = prims.cat((t617, t613), -1)  # t619: "cuda:0 bf16[1, 32, 512, 128]"
  t620 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t620: "cuda:0 f32[1, 32, 512, 128]"
  t621 = prims.convert_element_type(t612, dtypes.float32)  # t621: "cuda:0 f32[1, 32, 512, 128]"
  t622 = ltorch.mul(t621, t620)  # t622: "cuda:0 f32[1, 32, 512, 128]"
    # t622 = prims.mul(t621, t620)  # t622: "cuda:0 f32[1, 32, 512, 128]"
  t623 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t623: "cuda:0 f32[1, 32, 512, 128]"
  t624 = prims.convert_element_type(t619, dtypes.float32)  # t624: "cuda:0 f32[1, 32, 512, 128]"
  t625 = ltorch.mul(t624, t623)  # t625: "cuda:0 f32[1, 32, 512, 128]"
    # t625 = prims.mul(t624, t623)  # t625: "cuda:0 f32[1, 32, 512, 128]"
  t626 = ltorch.add(t622, t625, alpha=None)  # t626: "cuda:0 f32[1, 32, 512, 128]"
    # t626 = prims.add(t622, t625)  # t626: "cuda:0 f32[1, 32, 512, 128]"
  t627 = prims.convert_element_type(t626, dtypes.bfloat16)  # t627: "cuda:0 bf16[1, 32, 512, 128]"
  t628 = prims.slice_prim(t605, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t628: "cuda:0 bf16[1, 32, 512, 128]"
  t629 = prims.slice_prim(t628, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t629: "cuda:0 bf16[1, 32, 512, 64]"
  t630 = prims.slice_prim(t628, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t630: "cuda:0 bf16[1, 32, 512, 64]"
  t631 = prims.convert_element_type(t630, dtypes.float32)  # t631: "cuda:0 f32[1, 32, 512, 64]"
  t632 = prims.neg(t631)  # t632: "cuda:0 f32[1, 32, 512, 64]"
  t633 = prims.convert_element_type(t632, dtypes.bfloat16)  # t633: "cuda:0 bf16[1, 32, 512, 64]"
  t635 = prims.cat((t633, t629), -1)  # t635: "cuda:0 bf16[1, 32, 512, 128]"
  t636 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t636: "cuda:0 f32[1, 32, 512, 128]"
  t637 = prims.convert_element_type(t628, dtypes.float32)  # t637: "cuda:0 f32[1, 32, 512, 128]"
  t638 = ltorch.mul(t637, t636)  # t638: "cuda:0 f32[1, 32, 512, 128]"
    # t638 = prims.mul(t637, t636)  # t638: "cuda:0 f32[1, 32, 512, 128]"
  t639 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t639: "cuda:0 f32[1, 32, 512, 128]"
  t640 = prims.convert_element_type(t635, dtypes.float32)  # t640: "cuda:0 f32[1, 32, 512, 128]"
  t641 = ltorch.mul(t640, t639)  # t641: "cuda:0 f32[1, 32, 512, 128]"
    # t641 = prims.mul(t640, t639)  # t641: "cuda:0 f32[1, 32, 512, 128]"
  t642 = ltorch.add(t638, t641, alpha=None)  # t642: "cuda:0 f32[1, 32, 512, 128]"
    # t642 = prims.add(t638, t641)  # t642: "cuda:0 f32[1, 32, 512, 128]"
  t643 = prims.convert_element_type(t642, dtypes.bfloat16)  # t643: "cuda:0 bf16[1, 32, 512, 128]"
  t644 = prims.slice_prim(t599, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t644: "cuda:0 bf16[1, 32, 512, 0]"
  t646 = prims.cat((t627, t644), -1)  # t646: "cuda:0 bf16[1, 32, 512, 128]"
  t647 = prims.slice_prim(t605, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t647: "cuda:0 bf16[1, 32, 512, 0]"
  t649 = prims.cat((t643, t647), -1)  # t649: "cuda:0 bf16[1, 32, 512, 128]"
  (t650, t651, t652, t653) = cudnn_sdpa_fwd(t646, t649, t611, None, 0.0, True, scale=0.08838834764831843)
  t656 = prims.transpose(t650, (0, 2, 1, 3))  # t656: "cuda:0 bf16[1, 512, 32, 128]"
  t660 = prims.reshape(t656, (1, 512, 4096))  # t660: "cuda:0 bf16[1, 512, 4096]"
  t661 = prims.linear(t660, t_transformer_h_4_attn_proj_weight, None)  # t661: "cuda:0 bf16[1, 512, 4096]"
  t662 = prims.convert_element_type(t661, dtypes.float32)  # t662: "cuda:0 f32[1, 512, 4096]"
  t663 = prims.convert_element_type(t559, dtypes.float32)  # t663: "cuda:0 f32[1, 512, 4096]"
  t664 = ltorch.add(t662, t663, alpha=None)  # t664: "cuda:0 f32[1, 512, 4096]"
    # t664 = prims.add(t662, t663)  # t664: "cuda:0 f32[1, 512, 4096]"
  t665 = prims.convert_element_type(t664, dtypes.bfloat16)  # t665: "cuda:0 bf16[1, 512, 4096]"
  t666 = prims.convert_element_type(t665, dtypes.float32)  # t666: "cuda:0 f32[1, 512, 4096]"
  t667 = ltorch.mul(t666, t666)  # t667: "cuda:0 f32[1, 512, 4096]"
    # t667 = prims.mul(t666, t666)  # t667: "cuda:0 f32[1, 512, 4096]"
  t669 = prims.sum(t667, (2,))  # t669: "cuda:0 f32[1, 512]"
  t670 = prims.broadcast_in_dim(t669, [1, 512, 1], [0, 1])  # t670: "cuda:0 f32[1, 512, 1]"
  t672 = ltorch.true_divide(t670, 4096.0)  # t672: "cuda:0 f32[1, 512, 1]"
    # t672 = prims.div(t670, 4096.0)  # t672: "cuda:0 f32[1, 512, 1]"
  t674 = ltorch.add(t672, 1e-05, alpha=None)  # t674: "cuda:0 f32[1, 512, 1]"
    # t674 = prims.add(t672, 1e-05)  # t674: "cuda:0 f32[1, 512, 1]"
  t675 = prims.rsqrt(t674)  # t675: "cuda:0 f32[1, 512, 1]"
  t676 = prims.broadcast_in_dim(t675, (1, 512, 4096), (0, 1, 2))  # t676: "cuda:0 f32[1, 512, 4096]"
  t677 = ltorch.mul(t666, t676)  # t677: "cuda:0 f32[1, 512, 4096]"
    # t677 = prims.mul(t666, t676)  # t677: "cuda:0 f32[1, 512, 4096]"
  t678 = prims.convert_element_type(t677, dtypes.bfloat16)  # t678: "cuda:0 bf16[1, 512, 4096]"
  t679 = prims.broadcast_in_dim(t_transformer_h_4_norm_2_weight, (1, 512, 4096), (2,))  # t679: "cuda:0 bf16[1, 512, 4096]"
  t680 = prims.convert_element_type(t678, dtypes.float32)  # t680: "cuda:0 f32[1, 512, 4096]"
  t681 = prims.convert_element_type(t679, dtypes.float32)  # t681: "cuda:0 f32[1, 512, 4096]"
  t682 = ltorch.mul(t680, t681)  # t682: "cuda:0 f32[1, 512, 4096]"
    # t682 = prims.mul(t680, t681)  # t682: "cuda:0 f32[1, 512, 4096]"
  t683 = prims.convert_element_type(t682, dtypes.bfloat16)  # t683: "cuda:0 bf16[1, 512, 4096]"
  t684 = prims.linear(t683, t_transformer_h_4_mlp_fc_1_weight, None)  # t684: "cuda:0 bf16[1, 512, 11008]"
  t685 = prims.linear(t683, t_transformer_h_4_mlp_fc_2_weight, None)  # t685: "cuda:0 bf16[1, 512, 11008]"
  t686 = prims.convert_element_type(t684, dtypes.float32)  # t686: "cuda:0 f32[1, 512, 11008]"
  t687 = prims.neg(t686)  # t687: "cuda:0 f32[1, 512, 11008]"
  t688 = prims.exp(t687)  # t688: "cuda:0 f32[1, 512, 11008]"
  t689 = ltorch.add(1.0, t688, alpha=None)  # t689: "cuda:0 f32[1, 512, 11008]"
    # t689 = prims.add(1.0, t688)  # t689: "cuda:0 f32[1, 512, 11008]"
  t690 = prims.reciprocal(t689)  # t690: "cuda:0 f32[1, 512, 11008]"
  t691 = prims.convert_element_type(t690, dtypes.bfloat16)  # t691: "cuda:0 bf16[1, 512, 11008]"
  t692 = prims.convert_element_type(t684, dtypes.float32)  # t692: "cuda:0 f32[1, 512, 11008]"
  t693 = prims.convert_element_type(t691, dtypes.float32)  # t693: "cuda:0 f32[1, 512, 11008]"
  t694 = ltorch.mul(t692, t693)  # t694: "cuda:0 f32[1, 512, 11008]"
    # t694 = prims.mul(t692, t693)  # t694: "cuda:0 f32[1, 512, 11008]"
  t695 = prims.convert_element_type(t694, dtypes.bfloat16)  # t695: "cuda:0 bf16[1, 512, 11008]"
  t696 = prims.convert_element_type(t695, dtypes.float32)  # t696: "cuda:0 f32[1, 512, 11008]"
  t697 = prims.convert_element_type(t685, dtypes.float32)  # t697: "cuda:0 f32[1, 512, 11008]"
  t698 = ltorch.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 11008]"
    # t698 = prims.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 11008]"
  t699 = prims.convert_element_type(t698, dtypes.bfloat16)  # t699: "cuda:0 bf16[1, 512, 11008]"
  t700 = prims.linear(t699, t_transformer_h_4_mlp_proj_weight, None)  # t700: "cuda:0 bf16[1, 512, 4096]"
  t701 = prims.convert_element_type(t700, dtypes.float32)  # t701: "cuda:0 f32[1, 512, 4096]"
  t702 = prims.convert_element_type(t665, dtypes.float32)  # t702: "cuda:0 f32[1, 512, 4096]"
  t703 = ltorch.add(t701, t702, alpha=None)  # t703: "cuda:0 f32[1, 512, 4096]"
    # t703 = prims.add(t701, t702)  # t703: "cuda:0 f32[1, 512, 4096]"
  t704 = prims.convert_element_type(t703, dtypes.bfloat16)  # t704: "cuda:0 bf16[1, 512, 4096]"
  t705 = prims.convert_element_type(t704, dtypes.float32)  # t705: "cuda:0 f32[1, 512, 4096]"
  t706 = ltorch.mul(t705, t705)  # t706: "cuda:0 f32[1, 512, 4096]"
    # t706 = prims.mul(t705, t705)  # t706: "cuda:0 f32[1, 512, 4096]"
  t708 = prims.sum(t706, (2,))  # t708: "cuda:0 f32[1, 512]"
  t709 = prims.broadcast_in_dim(t708, [1, 512, 1], [0, 1])  # t709: "cuda:0 f32[1, 512, 1]"
  t711 = ltorch.true_divide(t709, 4096.0)  # t711: "cuda:0 f32[1, 512, 1]"
    # t711 = prims.div(t709, 4096.0)  # t711: "cuda:0 f32[1, 512, 1]"
  t713 = ltorch.add(t711, 1e-05, alpha=None)  # t713: "cuda:0 f32[1, 512, 1]"
    # t713 = prims.add(t711, 1e-05)  # t713: "cuda:0 f32[1, 512, 1]"
  t714 = prims.rsqrt(t713)  # t714: "cuda:0 f32[1, 512, 1]"
  t715 = prims.broadcast_in_dim(t714, (1, 512, 4096), (0, 1, 2))  # t715: "cuda:0 f32[1, 512, 4096]"
  t716 = ltorch.mul(t705, t715)  # t716: "cuda:0 f32[1, 512, 4096]"
    # t716 = prims.mul(t705, t715)  # t716: "cuda:0 f32[1, 512, 4096]"
  t717 = prims.convert_element_type(t716, dtypes.bfloat16)  # t717: "cuda:0 bf16[1, 512, 4096]"
  t718 = prims.broadcast_in_dim(t_transformer_h_5_norm_1_weight, (1, 512, 4096), (2,))  # t718: "cuda:0 bf16[1, 512, 4096]"
  t719 = prims.convert_element_type(t717, dtypes.float32)  # t719: "cuda:0 f32[1, 512, 4096]"
  t720 = prims.convert_element_type(t718, dtypes.float32)  # t720: "cuda:0 f32[1, 512, 4096]"
  t721 = ltorch.mul(t719, t720)  # t721: "cuda:0 f32[1, 512, 4096]"
    # t721 = prims.mul(t719, t720)  # t721: "cuda:0 f32[1, 512, 4096]"
  t722 = prims.convert_element_type(t721, dtypes.bfloat16)  # t722: "cuda:0 bf16[1, 512, 4096]"
  t723 = prims.linear(t722, t_transformer_h_5_attn_attn_weight, None)  # t723: "cuda:0 bf16[1, 512, 12288]"
  t729 = prims.reshape(t723, (1, 512, 32, 3, 128))  # t729: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t735 = prims.transpose(t729, (0, 2, 3, 1, 4))  # t735: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t736, t737, t738) = ltorch.split(t735, (1, 1, 1), 2)
    # t736 = prims.slice_prim(t735, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t736: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t737 = prims.slice_prim(t735, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t737: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t738 = prims.slice_prim(t735, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t738: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t744 = prims.reshape(t736, (1, 32, 512, 128))  # t744: "cuda:0 bf16[1, 32, 512, 128]"
  t750 = prims.reshape(t737, (1, 32, 512, 128))  # t750: "cuda:0 bf16[1, 32, 512, 128]"
  t756 = prims.reshape(t738, (1, 32, 512, 128))  # t756: "cuda:0 bf16[1, 32, 512, 128]"
  t757 = prims.slice_prim(t744, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t757: "cuda:0 bf16[1, 32, 512, 128]"
  t758 = prims.slice_prim(t757, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t758: "cuda:0 bf16[1, 32, 512, 64]"
  t759 = prims.slice_prim(t757, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t759: "cuda:0 bf16[1, 32, 512, 64]"
  t760 = prims.convert_element_type(t759, dtypes.float32)  # t760: "cuda:0 f32[1, 32, 512, 64]"
  t761 = prims.neg(t760)  # t761: "cuda:0 f32[1, 32, 512, 64]"
  t762 = prims.convert_element_type(t761, dtypes.bfloat16)  # t762: "cuda:0 bf16[1, 32, 512, 64]"
  t764 = prims.cat((t762, t758), -1)  # t764: "cuda:0 bf16[1, 32, 512, 128]"
  t765 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t765: "cuda:0 f32[1, 32, 512, 128]"
  t766 = prims.convert_element_type(t757, dtypes.float32)  # t766: "cuda:0 f32[1, 32, 512, 128]"
  t767 = ltorch.mul(t766, t765)  # t767: "cuda:0 f32[1, 32, 512, 128]"
    # t767 = prims.mul(t766, t765)  # t767: "cuda:0 f32[1, 32, 512, 128]"
  t768 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t768: "cuda:0 f32[1, 32, 512, 128]"
  t769 = prims.convert_element_type(t764, dtypes.float32)  # t769: "cuda:0 f32[1, 32, 512, 128]"
  t770 = ltorch.mul(t769, t768)  # t770: "cuda:0 f32[1, 32, 512, 128]"
    # t770 = prims.mul(t769, t768)  # t770: "cuda:0 f32[1, 32, 512, 128]"
  t771 = ltorch.add(t767, t770, alpha=None)  # t771: "cuda:0 f32[1, 32, 512, 128]"
    # t771 = prims.add(t767, t770)  # t771: "cuda:0 f32[1, 32, 512, 128]"
  t772 = prims.convert_element_type(t771, dtypes.bfloat16)  # t772: "cuda:0 bf16[1, 32, 512, 128]"
  t773 = prims.slice_prim(t750, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t773: "cuda:0 bf16[1, 32, 512, 128]"
  t774 = prims.slice_prim(t773, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t774: "cuda:0 bf16[1, 32, 512, 64]"
  t775 = prims.slice_prim(t773, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t775: "cuda:0 bf16[1, 32, 512, 64]"
  t776 = prims.convert_element_type(t775, dtypes.float32)  # t776: "cuda:0 f32[1, 32, 512, 64]"
  t777 = prims.neg(t776)  # t777: "cuda:0 f32[1, 32, 512, 64]"
  t778 = prims.convert_element_type(t777, dtypes.bfloat16)  # t778: "cuda:0 bf16[1, 32, 512, 64]"
  t780 = prims.cat((t778, t774), -1)  # t780: "cuda:0 bf16[1, 32, 512, 128]"
  t781 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t781: "cuda:0 f32[1, 32, 512, 128]"
  t782 = prims.convert_element_type(t773, dtypes.float32)  # t782: "cuda:0 f32[1, 32, 512, 128]"
  t783 = ltorch.mul(t782, t781)  # t783: "cuda:0 f32[1, 32, 512, 128]"
    # t783 = prims.mul(t782, t781)  # t783: "cuda:0 f32[1, 32, 512, 128]"
  t784 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t784: "cuda:0 f32[1, 32, 512, 128]"
  t785 = prims.convert_element_type(t780, dtypes.float32)  # t785: "cuda:0 f32[1, 32, 512, 128]"
  t786 = ltorch.mul(t785, t784)  # t786: "cuda:0 f32[1, 32, 512, 128]"
    # t786 = prims.mul(t785, t784)  # t786: "cuda:0 f32[1, 32, 512, 128]"
  t787 = ltorch.add(t783, t786, alpha=None)  # t787: "cuda:0 f32[1, 32, 512, 128]"
    # t787 = prims.add(t783, t786)  # t787: "cuda:0 f32[1, 32, 512, 128]"
  t788 = prims.convert_element_type(t787, dtypes.bfloat16)  # t788: "cuda:0 bf16[1, 32, 512, 128]"
  t789 = prims.slice_prim(t744, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t789: "cuda:0 bf16[1, 32, 512, 0]"
  t791 = prims.cat((t772, t789), -1)  # t791: "cuda:0 bf16[1, 32, 512, 128]"
  t792 = prims.slice_prim(t750, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t792: "cuda:0 bf16[1, 32, 512, 0]"
  t794 = prims.cat((t788, t792), -1)  # t794: "cuda:0 bf16[1, 32, 512, 128]"
  (t795, t796, t797, t798) = cudnn_sdpa_fwd(t791, t794, t756, None, 0.0, True, scale=0.08838834764831843)
  t801 = prims.transpose(t795, (0, 2, 1, 3))  # t801: "cuda:0 bf16[1, 512, 32, 128]"
  t805 = prims.reshape(t801, (1, 512, 4096))  # t805: "cuda:0 bf16[1, 512, 4096]"
  t806 = prims.linear(t805, t_transformer_h_5_attn_proj_weight, None)  # t806: "cuda:0 bf16[1, 512, 4096]"
  t807 = prims.convert_element_type(t806, dtypes.float32)  # t807: "cuda:0 f32[1, 512, 4096]"
  t808 = prims.convert_element_type(t704, dtypes.float32)  # t808: "cuda:0 f32[1, 512, 4096]"
  t809 = ltorch.add(t807, t808, alpha=None)  # t809: "cuda:0 f32[1, 512, 4096]"
    # t809 = prims.add(t807, t808)  # t809: "cuda:0 f32[1, 512, 4096]"
  t810 = prims.convert_element_type(t809, dtypes.bfloat16)  # t810: "cuda:0 bf16[1, 512, 4096]"
  t811 = prims.convert_element_type(t810, dtypes.float32)  # t811: "cuda:0 f32[1, 512, 4096]"
  t812 = ltorch.mul(t811, t811)  # t812: "cuda:0 f32[1, 512, 4096]"
    # t812 = prims.mul(t811, t811)  # t812: "cuda:0 f32[1, 512, 4096]"
  t814 = prims.sum(t812, (2,))  # t814: "cuda:0 f32[1, 512]"
  t815 = prims.broadcast_in_dim(t814, [1, 512, 1], [0, 1])  # t815: "cuda:0 f32[1, 512, 1]"
  t817 = ltorch.true_divide(t815, 4096.0)  # t817: "cuda:0 f32[1, 512, 1]"
    # t817 = prims.div(t815, 4096.0)  # t817: "cuda:0 f32[1, 512, 1]"
  t819 = ltorch.add(t817, 1e-05, alpha=None)  # t819: "cuda:0 f32[1, 512, 1]"
    # t819 = prims.add(t817, 1e-05)  # t819: "cuda:0 f32[1, 512, 1]"
  t820 = prims.rsqrt(t819)  # t820: "cuda:0 f32[1, 512, 1]"
  t821 = prims.broadcast_in_dim(t820, (1, 512, 4096), (0, 1, 2))  # t821: "cuda:0 f32[1, 512, 4096]"
  t822 = ltorch.mul(t811, t821)  # t822: "cuda:0 f32[1, 512, 4096]"
    # t822 = prims.mul(t811, t821)  # t822: "cuda:0 f32[1, 512, 4096]"
  t823 = prims.convert_element_type(t822, dtypes.bfloat16)  # t823: "cuda:0 bf16[1, 512, 4096]"
  t824 = prims.broadcast_in_dim(t_transformer_h_5_norm_2_weight, (1, 512, 4096), (2,))  # t824: "cuda:0 bf16[1, 512, 4096]"
  t825 = prims.convert_element_type(t823, dtypes.float32)  # t825: "cuda:0 f32[1, 512, 4096]"
  t826 = prims.convert_element_type(t824, dtypes.float32)  # t826: "cuda:0 f32[1, 512, 4096]"
  t827 = ltorch.mul(t825, t826)  # t827: "cuda:0 f32[1, 512, 4096]"
    # t827 = prims.mul(t825, t826)  # t827: "cuda:0 f32[1, 512, 4096]"
  t828 = prims.convert_element_type(t827, dtypes.bfloat16)  # t828: "cuda:0 bf16[1, 512, 4096]"
  t829 = prims.linear(t828, t_transformer_h_5_mlp_fc_1_weight, None)  # t829: "cuda:0 bf16[1, 512, 11008]"
  t830 = prims.linear(t828, t_transformer_h_5_mlp_fc_2_weight, None)  # t830: "cuda:0 bf16[1, 512, 11008]"
  t831 = prims.convert_element_type(t829, dtypes.float32)  # t831: "cuda:0 f32[1, 512, 11008]"
  t832 = prims.neg(t831)  # t832: "cuda:0 f32[1, 512, 11008]"
  t833 = prims.exp(t832)  # t833: "cuda:0 f32[1, 512, 11008]"
  t834 = ltorch.add(1.0, t833, alpha=None)  # t834: "cuda:0 f32[1, 512, 11008]"
    # t834 = prims.add(1.0, t833)  # t834: "cuda:0 f32[1, 512, 11008]"
  t835 = prims.reciprocal(t834)  # t835: "cuda:0 f32[1, 512, 11008]"
  t836 = prims.convert_element_type(t835, dtypes.bfloat16)  # t836: "cuda:0 bf16[1, 512, 11008]"
  t837 = prims.convert_element_type(t829, dtypes.float32)  # t837: "cuda:0 f32[1, 512, 11008]"
  t838 = prims.convert_element_type(t836, dtypes.float32)  # t838: "cuda:0 f32[1, 512, 11008]"
  t839 = ltorch.mul(t837, t838)  # t839: "cuda:0 f32[1, 512, 11008]"
    # t839 = prims.mul(t837, t838)  # t839: "cuda:0 f32[1, 512, 11008]"
  t840 = prims.convert_element_type(t839, dtypes.bfloat16)  # t840: "cuda:0 bf16[1, 512, 11008]"
  t841 = prims.convert_element_type(t840, dtypes.float32)  # t841: "cuda:0 f32[1, 512, 11008]"
  t842 = prims.convert_element_type(t830, dtypes.float32)  # t842: "cuda:0 f32[1, 512, 11008]"
  t843 = ltorch.mul(t841, t842)  # t843: "cuda:0 f32[1, 512, 11008]"
    # t843 = prims.mul(t841, t842)  # t843: "cuda:0 f32[1, 512, 11008]"
  t844 = prims.convert_element_type(t843, dtypes.bfloat16)  # t844: "cuda:0 bf16[1, 512, 11008]"
  t845 = prims.linear(t844, t_transformer_h_5_mlp_proj_weight, None)  # t845: "cuda:0 bf16[1, 512, 4096]"
  t846 = prims.convert_element_type(t845, dtypes.float32)  # t846: "cuda:0 f32[1, 512, 4096]"
  t847 = prims.convert_element_type(t810, dtypes.float32)  # t847: "cuda:0 f32[1, 512, 4096]"
  t848 = ltorch.add(t846, t847, alpha=None)  # t848: "cuda:0 f32[1, 512, 4096]"
    # t848 = prims.add(t846, t847)  # t848: "cuda:0 f32[1, 512, 4096]"
  t849 = prims.convert_element_type(t848, dtypes.bfloat16)  # t849: "cuda:0 bf16[1, 512, 4096]"
  t850 = prims.convert_element_type(t849, dtypes.float32)  # t850: "cuda:0 f32[1, 512, 4096]"
  t851 = ltorch.mul(t850, t850)  # t851: "cuda:0 f32[1, 512, 4096]"
    # t851 = prims.mul(t850, t850)  # t851: "cuda:0 f32[1, 512, 4096]"
  t853 = prims.sum(t851, (2,))  # t853: "cuda:0 f32[1, 512]"
  t854 = prims.broadcast_in_dim(t853, [1, 512, 1], [0, 1])  # t854: "cuda:0 f32[1, 512, 1]"
  t856 = ltorch.true_divide(t854, 4096.0)  # t856: "cuda:0 f32[1, 512, 1]"
    # t856 = prims.div(t854, 4096.0)  # t856: "cuda:0 f32[1, 512, 1]"
  t858 = ltorch.add(t856, 1e-05, alpha=None)  # t858: "cuda:0 f32[1, 512, 1]"
    # t858 = prims.add(t856, 1e-05)  # t858: "cuda:0 f32[1, 512, 1]"
  t859 = prims.rsqrt(t858)  # t859: "cuda:0 f32[1, 512, 1]"
  t860 = prims.broadcast_in_dim(t859, (1, 512, 4096), (0, 1, 2))  # t860: "cuda:0 f32[1, 512, 4096]"
  t861 = ltorch.mul(t850, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
    # t861 = prims.mul(t850, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
  t862 = prims.convert_element_type(t861, dtypes.bfloat16)  # t862: "cuda:0 bf16[1, 512, 4096]"
  t863 = prims.broadcast_in_dim(t_transformer_h_6_norm_1_weight, (1, 512, 4096), (2,))  # t863: "cuda:0 bf16[1, 512, 4096]"
  t864 = prims.convert_element_type(t862, dtypes.float32)  # t864: "cuda:0 f32[1, 512, 4096]"
  t865 = prims.convert_element_type(t863, dtypes.float32)  # t865: "cuda:0 f32[1, 512, 4096]"
  t866 = ltorch.mul(t864, t865)  # t866: "cuda:0 f32[1, 512, 4096]"
    # t866 = prims.mul(t864, t865)  # t866: "cuda:0 f32[1, 512, 4096]"
  t867 = prims.convert_element_type(t866, dtypes.bfloat16)  # t867: "cuda:0 bf16[1, 512, 4096]"
  t868 = prims.linear(t867, t_transformer_h_6_attn_attn_weight, None)  # t868: "cuda:0 bf16[1, 512, 12288]"
  t874 = prims.reshape(t868, (1, 512, 32, 3, 128))  # t874: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t880 = prims.transpose(t874, (0, 2, 3, 1, 4))  # t880: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t881, t882, t883) = ltorch.split(t880, (1, 1, 1), 2)
    # t881 = prims.slice_prim(t880, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t881: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t882 = prims.slice_prim(t880, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t882: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t883 = prims.slice_prim(t880, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t883: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t889 = prims.reshape(t881, (1, 32, 512, 128))  # t889: "cuda:0 bf16[1, 32, 512, 128]"
  t895 = prims.reshape(t882, (1, 32, 512, 128))  # t895: "cuda:0 bf16[1, 32, 512, 128]"
  t901 = prims.reshape(t883, (1, 32, 512, 128))  # t901: "cuda:0 bf16[1, 32, 512, 128]"
  t902 = prims.slice_prim(t889, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t902: "cuda:0 bf16[1, 32, 512, 128]"
  t903 = prims.slice_prim(t902, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t903: "cuda:0 bf16[1, 32, 512, 64]"
  t904 = prims.slice_prim(t902, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t904: "cuda:0 bf16[1, 32, 512, 64]"
  t905 = prims.convert_element_type(t904, dtypes.float32)  # t905: "cuda:0 f32[1, 32, 512, 64]"
  t906 = prims.neg(t905)  # t906: "cuda:0 f32[1, 32, 512, 64]"
  t907 = prims.convert_element_type(t906, dtypes.bfloat16)  # t907: "cuda:0 bf16[1, 32, 512, 64]"
  t909 = prims.cat((t907, t903), -1)  # t909: "cuda:0 bf16[1, 32, 512, 128]"
  t910 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t910: "cuda:0 f32[1, 32, 512, 128]"
  t911 = prims.convert_element_type(t902, dtypes.float32)  # t911: "cuda:0 f32[1, 32, 512, 128]"
  t912 = ltorch.mul(t911, t910)  # t912: "cuda:0 f32[1, 32, 512, 128]"
    # t912 = prims.mul(t911, t910)  # t912: "cuda:0 f32[1, 32, 512, 128]"
  t913 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t913: "cuda:0 f32[1, 32, 512, 128]"
  t914 = prims.convert_element_type(t909, dtypes.float32)  # t914: "cuda:0 f32[1, 32, 512, 128]"
  t915 = ltorch.mul(t914, t913)  # t915: "cuda:0 f32[1, 32, 512, 128]"
    # t915 = prims.mul(t914, t913)  # t915: "cuda:0 f32[1, 32, 512, 128]"
  t916 = ltorch.add(t912, t915, alpha=None)  # t916: "cuda:0 f32[1, 32, 512, 128]"
    # t916 = prims.add(t912, t915)  # t916: "cuda:0 f32[1, 32, 512, 128]"
  t917 = prims.convert_element_type(t916, dtypes.bfloat16)  # t917: "cuda:0 bf16[1, 32, 512, 128]"
  t918 = prims.slice_prim(t895, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t918: "cuda:0 bf16[1, 32, 512, 128]"
  t919 = prims.slice_prim(t918, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t919: "cuda:0 bf16[1, 32, 512, 64]"
  t920 = prims.slice_prim(t918, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t920: "cuda:0 bf16[1, 32, 512, 64]"
  t921 = prims.convert_element_type(t920, dtypes.float32)  # t921: "cuda:0 f32[1, 32, 512, 64]"
  t922 = prims.neg(t921)  # t922: "cuda:0 f32[1, 32, 512, 64]"
  t923 = prims.convert_element_type(t922, dtypes.bfloat16)  # t923: "cuda:0 bf16[1, 32, 512, 64]"
  t925 = prims.cat((t923, t919), -1)  # t925: "cuda:0 bf16[1, 32, 512, 128]"
  t926 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t926: "cuda:0 f32[1, 32, 512, 128]"
  t927 = prims.convert_element_type(t918, dtypes.float32)  # t927: "cuda:0 f32[1, 32, 512, 128]"
  t928 = ltorch.mul(t927, t926)  # t928: "cuda:0 f32[1, 32, 512, 128]"
    # t928 = prims.mul(t927, t926)  # t928: "cuda:0 f32[1, 32, 512, 128]"
  t929 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t929: "cuda:0 f32[1, 32, 512, 128]"
  t930 = prims.convert_element_type(t925, dtypes.float32)  # t930: "cuda:0 f32[1, 32, 512, 128]"
  t931 = ltorch.mul(t930, t929)  # t931: "cuda:0 f32[1, 32, 512, 128]"
    # t931 = prims.mul(t930, t929)  # t931: "cuda:0 f32[1, 32, 512, 128]"
  t932 = ltorch.add(t928, t931, alpha=None)  # t932: "cuda:0 f32[1, 32, 512, 128]"
    # t932 = prims.add(t928, t931)  # t932: "cuda:0 f32[1, 32, 512, 128]"
  t933 = prims.convert_element_type(t932, dtypes.bfloat16)  # t933: "cuda:0 bf16[1, 32, 512, 128]"
  t934 = prims.slice_prim(t889, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t934: "cuda:0 bf16[1, 32, 512, 0]"
  t936 = prims.cat((t917, t934), -1)  # t936: "cuda:0 bf16[1, 32, 512, 128]"
  t937 = prims.slice_prim(t895, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t937: "cuda:0 bf16[1, 32, 512, 0]"
  t939 = prims.cat((t933, t937), -1)  # t939: "cuda:0 bf16[1, 32, 512, 128]"
  (t940, t941, t942, t943) = cudnn_sdpa_fwd(t936, t939, t901, None, 0.0, True, scale=0.08838834764831843)
  t946 = prims.transpose(t940, (0, 2, 1, 3))  # t946: "cuda:0 bf16[1, 512, 32, 128]"
  t950 = prims.reshape(t946, (1, 512, 4096))  # t950: "cuda:0 bf16[1, 512, 4096]"
  t951 = prims.linear(t950, t_transformer_h_6_attn_proj_weight, None)  # t951: "cuda:0 bf16[1, 512, 4096]"
  t952 = prims.convert_element_type(t951, dtypes.float32)  # t952: "cuda:0 f32[1, 512, 4096]"
  t953 = prims.convert_element_type(t849, dtypes.float32)  # t953: "cuda:0 f32[1, 512, 4096]"
  t954 = ltorch.add(t952, t953, alpha=None)  # t954: "cuda:0 f32[1, 512, 4096]"
    # t954 = prims.add(t952, t953)  # t954: "cuda:0 f32[1, 512, 4096]"
  t955 = prims.convert_element_type(t954, dtypes.bfloat16)  # t955: "cuda:0 bf16[1, 512, 4096]"
  t956 = prims.convert_element_type(t955, dtypes.float32)  # t956: "cuda:0 f32[1, 512, 4096]"
  t957 = ltorch.mul(t956, t956)  # t957: "cuda:0 f32[1, 512, 4096]"
    # t957 = prims.mul(t956, t956)  # t957: "cuda:0 f32[1, 512, 4096]"
  t959 = prims.sum(t957, (2,))  # t959: "cuda:0 f32[1, 512]"
  t960 = prims.broadcast_in_dim(t959, [1, 512, 1], [0, 1])  # t960: "cuda:0 f32[1, 512, 1]"
  t962 = ltorch.true_divide(t960, 4096.0)  # t962: "cuda:0 f32[1, 512, 1]"
    # t962 = prims.div(t960, 4096.0)  # t962: "cuda:0 f32[1, 512, 1]"
  t964 = ltorch.add(t962, 1e-05, alpha=None)  # t964: "cuda:0 f32[1, 512, 1]"
    # t964 = prims.add(t962, 1e-05)  # t964: "cuda:0 f32[1, 512, 1]"
  t965 = prims.rsqrt(t964)  # t965: "cuda:0 f32[1, 512, 1]"
  t966 = prims.broadcast_in_dim(t965, (1, 512, 4096), (0, 1, 2))  # t966: "cuda:0 f32[1, 512, 4096]"
  t967 = ltorch.mul(t956, t966)  # t967: "cuda:0 f32[1, 512, 4096]"
    # t967 = prims.mul(t956, t966)  # t967: "cuda:0 f32[1, 512, 4096]"
  t968 = prims.convert_element_type(t967, dtypes.bfloat16)  # t968: "cuda:0 bf16[1, 512, 4096]"
  t969 = prims.broadcast_in_dim(t_transformer_h_6_norm_2_weight, (1, 512, 4096), (2,))  # t969: "cuda:0 bf16[1, 512, 4096]"
  t970 = prims.convert_element_type(t968, dtypes.float32)  # t970: "cuda:0 f32[1, 512, 4096]"
  t971 = prims.convert_element_type(t969, dtypes.float32)  # t971: "cuda:0 f32[1, 512, 4096]"
  t972 = ltorch.mul(t970, t971)  # t972: "cuda:0 f32[1, 512, 4096]"
    # t972 = prims.mul(t970, t971)  # t972: "cuda:0 f32[1, 512, 4096]"
  t973 = prims.convert_element_type(t972, dtypes.bfloat16)  # t973: "cuda:0 bf16[1, 512, 4096]"
  t974 = prims.linear(t973, t_transformer_h_6_mlp_fc_1_weight, None)  # t974: "cuda:0 bf16[1, 512, 11008]"
  t975 = prims.linear(t973, t_transformer_h_6_mlp_fc_2_weight, None)  # t975: "cuda:0 bf16[1, 512, 11008]"
  t976 = prims.convert_element_type(t974, dtypes.float32)  # t976: "cuda:0 f32[1, 512, 11008]"
  t977 = prims.neg(t976)  # t977: "cuda:0 f32[1, 512, 11008]"
  t978 = prims.exp(t977)  # t978: "cuda:0 f32[1, 512, 11008]"
  t979 = ltorch.add(1.0, t978, alpha=None)  # t979: "cuda:0 f32[1, 512, 11008]"
    # t979 = prims.add(1.0, t978)  # t979: "cuda:0 f32[1, 512, 11008]"
  t980 = prims.reciprocal(t979)  # t980: "cuda:0 f32[1, 512, 11008]"
  t981 = prims.convert_element_type(t980, dtypes.bfloat16)  # t981: "cuda:0 bf16[1, 512, 11008]"
  t982 = prims.convert_element_type(t974, dtypes.float32)  # t982: "cuda:0 f32[1, 512, 11008]"
  t983 = prims.convert_element_type(t981, dtypes.float32)  # t983: "cuda:0 f32[1, 512, 11008]"
  t984 = ltorch.mul(t982, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
    # t984 = prims.mul(t982, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
  t985 = prims.convert_element_type(t984, dtypes.bfloat16)  # t985: "cuda:0 bf16[1, 512, 11008]"
  t986 = prims.convert_element_type(t985, dtypes.float32)  # t986: "cuda:0 f32[1, 512, 11008]"
  t987 = prims.convert_element_type(t975, dtypes.float32)  # t987: "cuda:0 f32[1, 512, 11008]"
  t988 = ltorch.mul(t986, t987)  # t988: "cuda:0 f32[1, 512, 11008]"
    # t988 = prims.mul(t986, t987)  # t988: "cuda:0 f32[1, 512, 11008]"
  t989 = prims.convert_element_type(t988, dtypes.bfloat16)  # t989: "cuda:0 bf16[1, 512, 11008]"
  t990 = prims.linear(t989, t_transformer_h_6_mlp_proj_weight, None)  # t990: "cuda:0 bf16[1, 512, 4096]"
  t991 = prims.convert_element_type(t990, dtypes.float32)  # t991: "cuda:0 f32[1, 512, 4096]"
  t992 = prims.convert_element_type(t955, dtypes.float32)  # t992: "cuda:0 f32[1, 512, 4096]"
  t993 = ltorch.add(t991, t992, alpha=None)  # t993: "cuda:0 f32[1, 512, 4096]"
    # t993 = prims.add(t991, t992)  # t993: "cuda:0 f32[1, 512, 4096]"
  t994 = prims.convert_element_type(t993, dtypes.bfloat16)  # t994: "cuda:0 bf16[1, 512, 4096]"
  t995 = prims.convert_element_type(t994, dtypes.float32)  # t995: "cuda:0 f32[1, 512, 4096]"
  t996 = ltorch.mul(t995, t995)  # t996: "cuda:0 f32[1, 512, 4096]"
    # t996 = prims.mul(t995, t995)  # t996: "cuda:0 f32[1, 512, 4096]"
  t998 = prims.sum(t996, (2,))  # t998: "cuda:0 f32[1, 512]"
  t999 = prims.broadcast_in_dim(t998, [1, 512, 1], [0, 1])  # t999: "cuda:0 f32[1, 512, 1]"
  t1001 = ltorch.true_divide(t999, 4096.0)  # t1001: "cuda:0 f32[1, 512, 1]"
    # t1001 = prims.div(t999, 4096.0)  # t1001: "cuda:0 f32[1, 512, 1]"
  t1003 = ltorch.add(t1001, 1e-05, alpha=None)  # t1003: "cuda:0 f32[1, 512, 1]"
    # t1003 = prims.add(t1001, 1e-05)  # t1003: "cuda:0 f32[1, 512, 1]"
  t1004 = prims.rsqrt(t1003)  # t1004: "cuda:0 f32[1, 512, 1]"
  t1005 = prims.broadcast_in_dim(t1004, (1, 512, 4096), (0, 1, 2))  # t1005: "cuda:0 f32[1, 512, 4096]"
  t1006 = ltorch.mul(t995, t1005)  # t1006: "cuda:0 f32[1, 512, 4096]"
    # t1006 = prims.mul(t995, t1005)  # t1006: "cuda:0 f32[1, 512, 4096]"
  t1007 = prims.convert_element_type(t1006, dtypes.bfloat16)  # t1007: "cuda:0 bf16[1, 512, 4096]"
  t1008 = prims.broadcast_in_dim(t_transformer_h_7_norm_1_weight, (1, 512, 4096), (2,))  # t1008: "cuda:0 bf16[1, 512, 4096]"
  t1009 = prims.convert_element_type(t1007, dtypes.float32)  # t1009: "cuda:0 f32[1, 512, 4096]"
  t1010 = prims.convert_element_type(t1008, dtypes.float32)  # t1010: "cuda:0 f32[1, 512, 4096]"
  t1011 = ltorch.mul(t1009, t1010)  # t1011: "cuda:0 f32[1, 512, 4096]"
    # t1011 = prims.mul(t1009, t1010)  # t1011: "cuda:0 f32[1, 512, 4096]"
  t1012 = prims.convert_element_type(t1011, dtypes.bfloat16)  # t1012: "cuda:0 bf16[1, 512, 4096]"
  t1013 = prims.linear(t1012, t_transformer_h_7_attn_attn_weight, None)  # t1013: "cuda:0 bf16[1, 512, 12288]"
  t1019 = prims.reshape(t1013, (1, 512, 32, 3, 128))  # t1019: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1025 = prims.transpose(t1019, (0, 2, 3, 1, 4))  # t1025: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1026, t1027, t1028) = ltorch.split(t1025, (1, 1, 1), 2)
    # t1026 = prims.slice_prim(t1025, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1026: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1027 = prims.slice_prim(t1025, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1027: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1028 = prims.slice_prim(t1025, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1028: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1034 = prims.reshape(t1026, (1, 32, 512, 128))  # t1034: "cuda:0 bf16[1, 32, 512, 128]"
  t1040 = prims.reshape(t1027, (1, 32, 512, 128))  # t1040: "cuda:0 bf16[1, 32, 512, 128]"
  t1046 = prims.reshape(t1028, (1, 32, 512, 128))  # t1046: "cuda:0 bf16[1, 32, 512, 128]"
  t1047 = prims.slice_prim(t1034, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1047: "cuda:0 bf16[1, 32, 512, 128]"
  t1048 = prims.slice_prim(t1047, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1048: "cuda:0 bf16[1, 32, 512, 64]"
  t1049 = prims.slice_prim(t1047, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1049: "cuda:0 bf16[1, 32, 512, 64]"
  t1050 = prims.convert_element_type(t1049, dtypes.float32)  # t1050: "cuda:0 f32[1, 32, 512, 64]"
  t1051 = prims.neg(t1050)  # t1051: "cuda:0 f32[1, 32, 512, 64]"
  t1052 = prims.convert_element_type(t1051, dtypes.bfloat16)  # t1052: "cuda:0 bf16[1, 32, 512, 64]"
  t1054 = prims.cat((t1052, t1048), -1)  # t1054: "cuda:0 bf16[1, 32, 512, 128]"
  t1055 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1055: "cuda:0 f32[1, 32, 512, 128]"
  t1056 = prims.convert_element_type(t1047, dtypes.float32)  # t1056: "cuda:0 f32[1, 32, 512, 128]"
  t1057 = ltorch.mul(t1056, t1055)  # t1057: "cuda:0 f32[1, 32, 512, 128]"
    # t1057 = prims.mul(t1056, t1055)  # t1057: "cuda:0 f32[1, 32, 512, 128]"
  t1058 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1058: "cuda:0 f32[1, 32, 512, 128]"
  t1059 = prims.convert_element_type(t1054, dtypes.float32)  # t1059: "cuda:0 f32[1, 32, 512, 128]"
  t1060 = ltorch.mul(t1059, t1058)  # t1060: "cuda:0 f32[1, 32, 512, 128]"
    # t1060 = prims.mul(t1059, t1058)  # t1060: "cuda:0 f32[1, 32, 512, 128]"
  t1061 = ltorch.add(t1057, t1060, alpha=None)  # t1061: "cuda:0 f32[1, 32, 512, 128]"
    # t1061 = prims.add(t1057, t1060)  # t1061: "cuda:0 f32[1, 32, 512, 128]"
  t1062 = prims.convert_element_type(t1061, dtypes.bfloat16)  # t1062: "cuda:0 bf16[1, 32, 512, 128]"
  t1063 = prims.slice_prim(t1040, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1063: "cuda:0 bf16[1, 32, 512, 128]"
  t1064 = prims.slice_prim(t1063, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1064: "cuda:0 bf16[1, 32, 512, 64]"
  t1065 = prims.slice_prim(t1063, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1065: "cuda:0 bf16[1, 32, 512, 64]"
  t1066 = prims.convert_element_type(t1065, dtypes.float32)  # t1066: "cuda:0 f32[1, 32, 512, 64]"
  t1067 = prims.neg(t1066)  # t1067: "cuda:0 f32[1, 32, 512, 64]"
  t1068 = prims.convert_element_type(t1067, dtypes.bfloat16)  # t1068: "cuda:0 bf16[1, 32, 512, 64]"
  t1070 = prims.cat((t1068, t1064), -1)  # t1070: "cuda:0 bf16[1, 32, 512, 128]"
  t1071 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1071: "cuda:0 f32[1, 32, 512, 128]"
  t1072 = prims.convert_element_type(t1063, dtypes.float32)  # t1072: "cuda:0 f32[1, 32, 512, 128]"
  t1073 = ltorch.mul(t1072, t1071)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
    # t1073 = prims.mul(t1072, t1071)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
  t1074 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1074: "cuda:0 f32[1, 32, 512, 128]"
  t1075 = prims.convert_element_type(t1070, dtypes.float32)  # t1075: "cuda:0 f32[1, 32, 512, 128]"
  t1076 = ltorch.mul(t1075, t1074)  # t1076: "cuda:0 f32[1, 32, 512, 128]"
    # t1076 = prims.mul(t1075, t1074)  # t1076: "cuda:0 f32[1, 32, 512, 128]"
  t1077 = ltorch.add(t1073, t1076, alpha=None)  # t1077: "cuda:0 f32[1, 32, 512, 128]"
    # t1077 = prims.add(t1073, t1076)  # t1077: "cuda:0 f32[1, 32, 512, 128]"
  t1078 = prims.convert_element_type(t1077, dtypes.bfloat16)  # t1078: "cuda:0 bf16[1, 32, 512, 128]"
  t1079 = prims.slice_prim(t1034, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1079: "cuda:0 bf16[1, 32, 512, 0]"
  t1081 = prims.cat((t1062, t1079), -1)  # t1081: "cuda:0 bf16[1, 32, 512, 128]"
  t1082 = prims.slice_prim(t1040, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1082: "cuda:0 bf16[1, 32, 512, 0]"
  t1084 = prims.cat((t1078, t1082), -1)  # t1084: "cuda:0 bf16[1, 32, 512, 128]"
  (t1085, t1086, t1087, t1088) = cudnn_sdpa_fwd(t1081, t1084, t1046, None, 0.0, True, scale=0.08838834764831843)
  t1091 = prims.transpose(t1085, (0, 2, 1, 3))  # t1091: "cuda:0 bf16[1, 512, 32, 128]"
  t1095 = prims.reshape(t1091, (1, 512, 4096))  # t1095: "cuda:0 bf16[1, 512, 4096]"
  t1096 = prims.linear(t1095, t_transformer_h_7_attn_proj_weight, None)  # t1096: "cuda:0 bf16[1, 512, 4096]"
  t1097 = prims.convert_element_type(t1096, dtypes.float32)  # t1097: "cuda:0 f32[1, 512, 4096]"
  t1098 = prims.convert_element_type(t994, dtypes.float32)  # t1098: "cuda:0 f32[1, 512, 4096]"
  t1099 = ltorch.add(t1097, t1098, alpha=None)  # t1099: "cuda:0 f32[1, 512, 4096]"
    # t1099 = prims.add(t1097, t1098)  # t1099: "cuda:0 f32[1, 512, 4096]"
  t1100 = prims.convert_element_type(t1099, dtypes.bfloat16)  # t1100: "cuda:0 bf16[1, 512, 4096]"
  t1101 = prims.convert_element_type(t1100, dtypes.float32)  # t1101: "cuda:0 f32[1, 512, 4096]"
  t1102 = ltorch.mul(t1101, t1101)  # t1102: "cuda:0 f32[1, 512, 4096]"
    # t1102 = prims.mul(t1101, t1101)  # t1102: "cuda:0 f32[1, 512, 4096]"
  t1104 = prims.sum(t1102, (2,))  # t1104: "cuda:0 f32[1, 512]"
  t1105 = prims.broadcast_in_dim(t1104, [1, 512, 1], [0, 1])  # t1105: "cuda:0 f32[1, 512, 1]"
  t1107 = ltorch.true_divide(t1105, 4096.0)  # t1107: "cuda:0 f32[1, 512, 1]"
    # t1107 = prims.div(t1105, 4096.0)  # t1107: "cuda:0 f32[1, 512, 1]"
  t1109 = ltorch.add(t1107, 1e-05, alpha=None)  # t1109: "cuda:0 f32[1, 512, 1]"
    # t1109 = prims.add(t1107, 1e-05)  # t1109: "cuda:0 f32[1, 512, 1]"
  t1110 = prims.rsqrt(t1109)  # t1110: "cuda:0 f32[1, 512, 1]"
  t1111 = prims.broadcast_in_dim(t1110, (1, 512, 4096), (0, 1, 2))  # t1111: "cuda:0 f32[1, 512, 4096]"
  t1112 = ltorch.mul(t1101, t1111)  # t1112: "cuda:0 f32[1, 512, 4096]"
    # t1112 = prims.mul(t1101, t1111)  # t1112: "cuda:0 f32[1, 512, 4096]"
  t1113 = prims.convert_element_type(t1112, dtypes.bfloat16)  # t1113: "cuda:0 bf16[1, 512, 4096]"
  t1114 = prims.broadcast_in_dim(t_transformer_h_7_norm_2_weight, (1, 512, 4096), (2,))  # t1114: "cuda:0 bf16[1, 512, 4096]"
  t1115 = prims.convert_element_type(t1113, dtypes.float32)  # t1115: "cuda:0 f32[1, 512, 4096]"
  t1116 = prims.convert_element_type(t1114, dtypes.float32)  # t1116: "cuda:0 f32[1, 512, 4096]"
  t1117 = ltorch.mul(t1115, t1116)  # t1117: "cuda:0 f32[1, 512, 4096]"
    # t1117 = prims.mul(t1115, t1116)  # t1117: "cuda:0 f32[1, 512, 4096]"
  t1118 = prims.convert_element_type(t1117, dtypes.bfloat16)  # t1118: "cuda:0 bf16[1, 512, 4096]"
  t1119 = prims.linear(t1118, t_transformer_h_7_mlp_fc_1_weight, None)  # t1119: "cuda:0 bf16[1, 512, 11008]"
  t1120 = prims.linear(t1118, t_transformer_h_7_mlp_fc_2_weight, None)  # t1120: "cuda:0 bf16[1, 512, 11008]"
  t1121 = prims.convert_element_type(t1119, dtypes.float32)  # t1121: "cuda:0 f32[1, 512, 11008]"
  t1122 = prims.neg(t1121)  # t1122: "cuda:0 f32[1, 512, 11008]"
  t1123 = prims.exp(t1122)  # t1123: "cuda:0 f32[1, 512, 11008]"
  t1124 = ltorch.add(1.0, t1123, alpha=None)  # t1124: "cuda:0 f32[1, 512, 11008]"
    # t1124 = prims.add(1.0, t1123)  # t1124: "cuda:0 f32[1, 512, 11008]"
  t1125 = prims.reciprocal(t1124)  # t1125: "cuda:0 f32[1, 512, 11008]"
  t1126 = prims.convert_element_type(t1125, dtypes.bfloat16)  # t1126: "cuda:0 bf16[1, 512, 11008]"
  t1127 = prims.convert_element_type(t1119, dtypes.float32)  # t1127: "cuda:0 f32[1, 512, 11008]"
  t1128 = prims.convert_element_type(t1126, dtypes.float32)  # t1128: "cuda:0 f32[1, 512, 11008]"
  t1129 = ltorch.mul(t1127, t1128)  # t1129: "cuda:0 f32[1, 512, 11008]"
    # t1129 = prims.mul(t1127, t1128)  # t1129: "cuda:0 f32[1, 512, 11008]"
  t1130 = prims.convert_element_type(t1129, dtypes.bfloat16)  # t1130: "cuda:0 bf16[1, 512, 11008]"
  t1131 = prims.convert_element_type(t1130, dtypes.float32)  # t1131: "cuda:0 f32[1, 512, 11008]"
  t1132 = prims.convert_element_type(t1120, dtypes.float32)  # t1132: "cuda:0 f32[1, 512, 11008]"
  t1133 = ltorch.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 11008]"
    # t1133 = prims.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 11008]"
  t1134 = prims.convert_element_type(t1133, dtypes.bfloat16)  # t1134: "cuda:0 bf16[1, 512, 11008]"
  t1135 = prims.linear(t1134, t_transformer_h_7_mlp_proj_weight, None)  # t1135: "cuda:0 bf16[1, 512, 4096]"
  t1136 = prims.convert_element_type(t1135, dtypes.float32)  # t1136: "cuda:0 f32[1, 512, 4096]"
  t1137 = prims.convert_element_type(t1100, dtypes.float32)  # t1137: "cuda:0 f32[1, 512, 4096]"
  t1138 = ltorch.add(t1136, t1137, alpha=None)  # t1138: "cuda:0 f32[1, 512, 4096]"
    # t1138 = prims.add(t1136, t1137)  # t1138: "cuda:0 f32[1, 512, 4096]"
  t1139 = prims.convert_element_type(t1138, dtypes.bfloat16)  # t1139: "cuda:0 bf16[1, 512, 4096]"
  t1140 = prims.convert_element_type(t1139, dtypes.float32)  # t1140: "cuda:0 f32[1, 512, 4096]"
  t1141 = ltorch.mul(t1140, t1140)  # t1141: "cuda:0 f32[1, 512, 4096]"
    # t1141 = prims.mul(t1140, t1140)  # t1141: "cuda:0 f32[1, 512, 4096]"
  t1143 = prims.sum(t1141, (2,))  # t1143: "cuda:0 f32[1, 512]"
  t1144 = prims.broadcast_in_dim(t1143, [1, 512, 1], [0, 1])  # t1144: "cuda:0 f32[1, 512, 1]"
  t1146 = ltorch.true_divide(t1144, 4096.0)  # t1146: "cuda:0 f32[1, 512, 1]"
    # t1146 = prims.div(t1144, 4096.0)  # t1146: "cuda:0 f32[1, 512, 1]"
  t1148 = ltorch.add(t1146, 1e-05, alpha=None)  # t1148: "cuda:0 f32[1, 512, 1]"
    # t1148 = prims.add(t1146, 1e-05)  # t1148: "cuda:0 f32[1, 512, 1]"
  t1149 = prims.rsqrt(t1148)  # t1149: "cuda:0 f32[1, 512, 1]"
  t1150 = prims.broadcast_in_dim(t1149, (1, 512, 4096), (0, 1, 2))  # t1150: "cuda:0 f32[1, 512, 4096]"
  t1151 = ltorch.mul(t1140, t1150)  # t1151: "cuda:0 f32[1, 512, 4096]"
    # t1151 = prims.mul(t1140, t1150)  # t1151: "cuda:0 f32[1, 512, 4096]"
  t1152 = prims.convert_element_type(t1151, dtypes.bfloat16)  # t1152: "cuda:0 bf16[1, 512, 4096]"
  t1153 = prims.broadcast_in_dim(t_transformer_h_8_norm_1_weight, (1, 512, 4096), (2,))  # t1153: "cuda:0 bf16[1, 512, 4096]"
  t1154 = prims.convert_element_type(t1152, dtypes.float32)  # t1154: "cuda:0 f32[1, 512, 4096]"
  t1155 = prims.convert_element_type(t1153, dtypes.float32)  # t1155: "cuda:0 f32[1, 512, 4096]"
  t1156 = ltorch.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 4096]"
    # t1156 = prims.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 4096]"
  t1157 = prims.convert_element_type(t1156, dtypes.bfloat16)  # t1157: "cuda:0 bf16[1, 512, 4096]"
  t1158 = prims.linear(t1157, t_transformer_h_8_attn_attn_weight, None)  # t1158: "cuda:0 bf16[1, 512, 12288]"
  t1164 = prims.reshape(t1158, (1, 512, 32, 3, 128))  # t1164: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1170 = prims.transpose(t1164, (0, 2, 3, 1, 4))  # t1170: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1171, t1172, t1173) = ltorch.split(t1170, (1, 1, 1), 2)
    # t1171 = prims.slice_prim(t1170, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1171: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1172 = prims.slice_prim(t1170, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1172: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1173 = prims.slice_prim(t1170, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1173: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1179 = prims.reshape(t1171, (1, 32, 512, 128))  # t1179: "cuda:0 bf16[1, 32, 512, 128]"
  t1185 = prims.reshape(t1172, (1, 32, 512, 128))  # t1185: "cuda:0 bf16[1, 32, 512, 128]"
  t1191 = prims.reshape(t1173, (1, 32, 512, 128))  # t1191: "cuda:0 bf16[1, 32, 512, 128]"
  t1192 = prims.slice_prim(t1179, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1192: "cuda:0 bf16[1, 32, 512, 128]"
  t1193 = prims.slice_prim(t1192, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1193: "cuda:0 bf16[1, 32, 512, 64]"
  t1194 = prims.slice_prim(t1192, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1194: "cuda:0 bf16[1, 32, 512, 64]"
  t1195 = prims.convert_element_type(t1194, dtypes.float32)  # t1195: "cuda:0 f32[1, 32, 512, 64]"
  t1196 = prims.neg(t1195)  # t1196: "cuda:0 f32[1, 32, 512, 64]"
  t1197 = prims.convert_element_type(t1196, dtypes.bfloat16)  # t1197: "cuda:0 bf16[1, 32, 512, 64]"
  t1199 = prims.cat((t1197, t1193), -1)  # t1199: "cuda:0 bf16[1, 32, 512, 128]"
  t1200 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1200: "cuda:0 f32[1, 32, 512, 128]"
  t1201 = prims.convert_element_type(t1192, dtypes.float32)  # t1201: "cuda:0 f32[1, 32, 512, 128]"
  t1202 = ltorch.mul(t1201, t1200)  # t1202: "cuda:0 f32[1, 32, 512, 128]"
    # t1202 = prims.mul(t1201, t1200)  # t1202: "cuda:0 f32[1, 32, 512, 128]"
  t1203 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1203: "cuda:0 f32[1, 32, 512, 128]"
  t1204 = prims.convert_element_type(t1199, dtypes.float32)  # t1204: "cuda:0 f32[1, 32, 512, 128]"
  t1205 = ltorch.mul(t1204, t1203)  # t1205: "cuda:0 f32[1, 32, 512, 128]"
    # t1205 = prims.mul(t1204, t1203)  # t1205: "cuda:0 f32[1, 32, 512, 128]"
  t1206 = ltorch.add(t1202, t1205, alpha=None)  # t1206: "cuda:0 f32[1, 32, 512, 128]"
    # t1206 = prims.add(t1202, t1205)  # t1206: "cuda:0 f32[1, 32, 512, 128]"
  t1207 = prims.convert_element_type(t1206, dtypes.bfloat16)  # t1207: "cuda:0 bf16[1, 32, 512, 128]"
  t1208 = prims.slice_prim(t1185, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1208: "cuda:0 bf16[1, 32, 512, 128]"
  t1209 = prims.slice_prim(t1208, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1209: "cuda:0 bf16[1, 32, 512, 64]"
  t1210 = prims.slice_prim(t1208, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1210: "cuda:0 bf16[1, 32, 512, 64]"
  t1211 = prims.convert_element_type(t1210, dtypes.float32)  # t1211: "cuda:0 f32[1, 32, 512, 64]"
  t1212 = prims.neg(t1211)  # t1212: "cuda:0 f32[1, 32, 512, 64]"
  t1213 = prims.convert_element_type(t1212, dtypes.bfloat16)  # t1213: "cuda:0 bf16[1, 32, 512, 64]"
  t1215 = prims.cat((t1213, t1209), -1)  # t1215: "cuda:0 bf16[1, 32, 512, 128]"
  t1216 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1216: "cuda:0 f32[1, 32, 512, 128]"
  t1217 = prims.convert_element_type(t1208, dtypes.float32)  # t1217: "cuda:0 f32[1, 32, 512, 128]"
  t1218 = ltorch.mul(t1217, t1216)  # t1218: "cuda:0 f32[1, 32, 512, 128]"
    # t1218 = prims.mul(t1217, t1216)  # t1218: "cuda:0 f32[1, 32, 512, 128]"
  t1219 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1219: "cuda:0 f32[1, 32, 512, 128]"
  t1220 = prims.convert_element_type(t1215, dtypes.float32)  # t1220: "cuda:0 f32[1, 32, 512, 128]"
  t1221 = ltorch.mul(t1220, t1219)  # t1221: "cuda:0 f32[1, 32, 512, 128]"
    # t1221 = prims.mul(t1220, t1219)  # t1221: "cuda:0 f32[1, 32, 512, 128]"
  t1222 = ltorch.add(t1218, t1221, alpha=None)  # t1222: "cuda:0 f32[1, 32, 512, 128]"
    # t1222 = prims.add(t1218, t1221)  # t1222: "cuda:0 f32[1, 32, 512, 128]"
  t1223 = prims.convert_element_type(t1222, dtypes.bfloat16)  # t1223: "cuda:0 bf16[1, 32, 512, 128]"
  t1224 = prims.slice_prim(t1179, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1224: "cuda:0 bf16[1, 32, 512, 0]"
  t1226 = prims.cat((t1207, t1224), -1)  # t1226: "cuda:0 bf16[1, 32, 512, 128]"
  t1227 = prims.slice_prim(t1185, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1227: "cuda:0 bf16[1, 32, 512, 0]"
  t1229 = prims.cat((t1223, t1227), -1)  # t1229: "cuda:0 bf16[1, 32, 512, 128]"
  (t1230, t1231, t1232, t1233) = cudnn_sdpa_fwd(t1226, t1229, t1191, None, 0.0, True, scale=0.08838834764831843)
  t1236 = prims.transpose(t1230, (0, 2, 1, 3))  # t1236: "cuda:0 bf16[1, 512, 32, 128]"
  t1240 = prims.reshape(t1236, (1, 512, 4096))  # t1240: "cuda:0 bf16[1, 512, 4096]"
  t1241 = prims.linear(t1240, t_transformer_h_8_attn_proj_weight, None)  # t1241: "cuda:0 bf16[1, 512, 4096]"
  t1242 = prims.convert_element_type(t1241, dtypes.float32)  # t1242: "cuda:0 f32[1, 512, 4096]"
  t1243 = prims.convert_element_type(t1139, dtypes.float32)  # t1243: "cuda:0 f32[1, 512, 4096]"
  t1244 = ltorch.add(t1242, t1243, alpha=None)  # t1244: "cuda:0 f32[1, 512, 4096]"
    # t1244 = prims.add(t1242, t1243)  # t1244: "cuda:0 f32[1, 512, 4096]"
  t1245 = prims.convert_element_type(t1244, dtypes.bfloat16)  # t1245: "cuda:0 bf16[1, 512, 4096]"
  t1246 = prims.convert_element_type(t1245, dtypes.float32)  # t1246: "cuda:0 f32[1, 512, 4096]"
  t1247 = ltorch.mul(t1246, t1246)  # t1247: "cuda:0 f32[1, 512, 4096]"
    # t1247 = prims.mul(t1246, t1246)  # t1247: "cuda:0 f32[1, 512, 4096]"
  t1249 = prims.sum(t1247, (2,))  # t1249: "cuda:0 f32[1, 512]"
  t1250 = prims.broadcast_in_dim(t1249, [1, 512, 1], [0, 1])  # t1250: "cuda:0 f32[1, 512, 1]"
  t1252 = ltorch.true_divide(t1250, 4096.0)  # t1252: "cuda:0 f32[1, 512, 1]"
    # t1252 = prims.div(t1250, 4096.0)  # t1252: "cuda:0 f32[1, 512, 1]"
  t1254 = ltorch.add(t1252, 1e-05, alpha=None)  # t1254: "cuda:0 f32[1, 512, 1]"
    # t1254 = prims.add(t1252, 1e-05)  # t1254: "cuda:0 f32[1, 512, 1]"
  t1255 = prims.rsqrt(t1254)  # t1255: "cuda:0 f32[1, 512, 1]"
  t1256 = prims.broadcast_in_dim(t1255, (1, 512, 4096), (0, 1, 2))  # t1256: "cuda:0 f32[1, 512, 4096]"
  t1257 = ltorch.mul(t1246, t1256)  # t1257: "cuda:0 f32[1, 512, 4096]"
    # t1257 = prims.mul(t1246, t1256)  # t1257: "cuda:0 f32[1, 512, 4096]"
  t1258 = prims.convert_element_type(t1257, dtypes.bfloat16)  # t1258: "cuda:0 bf16[1, 512, 4096]"
  t1259 = prims.broadcast_in_dim(t_transformer_h_8_norm_2_weight, (1, 512, 4096), (2,))  # t1259: "cuda:0 bf16[1, 512, 4096]"
  t1260 = prims.convert_element_type(t1258, dtypes.float32)  # t1260: "cuda:0 f32[1, 512, 4096]"
  t1261 = prims.convert_element_type(t1259, dtypes.float32)  # t1261: "cuda:0 f32[1, 512, 4096]"
  t1262 = ltorch.mul(t1260, t1261)  # t1262: "cuda:0 f32[1, 512, 4096]"
    # t1262 = prims.mul(t1260, t1261)  # t1262: "cuda:0 f32[1, 512, 4096]"
  t1263 = prims.convert_element_type(t1262, dtypes.bfloat16)  # t1263: "cuda:0 bf16[1, 512, 4096]"
  t1264 = prims.linear(t1263, t_transformer_h_8_mlp_fc_1_weight, None)  # t1264: "cuda:0 bf16[1, 512, 11008]"
  t1265 = prims.linear(t1263, t_transformer_h_8_mlp_fc_2_weight, None)  # t1265: "cuda:0 bf16[1, 512, 11008]"
  t1266 = prims.convert_element_type(t1264, dtypes.float32)  # t1266: "cuda:0 f32[1, 512, 11008]"
  t1267 = prims.neg(t1266)  # t1267: "cuda:0 f32[1, 512, 11008]"
  t1268 = prims.exp(t1267)  # t1268: "cuda:0 f32[1, 512, 11008]"
  t1269 = ltorch.add(1.0, t1268, alpha=None)  # t1269: "cuda:0 f32[1, 512, 11008]"
    # t1269 = prims.add(1.0, t1268)  # t1269: "cuda:0 f32[1, 512, 11008]"
  t1270 = prims.reciprocal(t1269)  # t1270: "cuda:0 f32[1, 512, 11008]"
  t1271 = prims.convert_element_type(t1270, dtypes.bfloat16)  # t1271: "cuda:0 bf16[1, 512, 11008]"
  t1272 = prims.convert_element_type(t1264, dtypes.float32)  # t1272: "cuda:0 f32[1, 512, 11008]"
  t1273 = prims.convert_element_type(t1271, dtypes.float32)  # t1273: "cuda:0 f32[1, 512, 11008]"
  t1274 = ltorch.mul(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 11008]"
    # t1274 = prims.mul(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 11008]"
  t1275 = prims.convert_element_type(t1274, dtypes.bfloat16)  # t1275: "cuda:0 bf16[1, 512, 11008]"
  t1276 = prims.convert_element_type(t1275, dtypes.float32)  # t1276: "cuda:0 f32[1, 512, 11008]"
  t1277 = prims.convert_element_type(t1265, dtypes.float32)  # t1277: "cuda:0 f32[1, 512, 11008]"
  t1278 = ltorch.mul(t1276, t1277)  # t1278: "cuda:0 f32[1, 512, 11008]"
    # t1278 = prims.mul(t1276, t1277)  # t1278: "cuda:0 f32[1, 512, 11008]"
  t1279 = prims.convert_element_type(t1278, dtypes.bfloat16)  # t1279: "cuda:0 bf16[1, 512, 11008]"
  t1280 = prims.linear(t1279, t_transformer_h_8_mlp_proj_weight, None)  # t1280: "cuda:0 bf16[1, 512, 4096]"
  t1281 = prims.convert_element_type(t1280, dtypes.float32)  # t1281: "cuda:0 f32[1, 512, 4096]"
  t1282 = prims.convert_element_type(t1245, dtypes.float32)  # t1282: "cuda:0 f32[1, 512, 4096]"
  t1283 = ltorch.add(t1281, t1282, alpha=None)  # t1283: "cuda:0 f32[1, 512, 4096]"
    # t1283 = prims.add(t1281, t1282)  # t1283: "cuda:0 f32[1, 512, 4096]"
  t1284 = prims.convert_element_type(t1283, dtypes.bfloat16)  # t1284: "cuda:0 bf16[1, 512, 4096]"
  t1285 = prims.convert_element_type(t1284, dtypes.float32)  # t1285: "cuda:0 f32[1, 512, 4096]"
  t1286 = ltorch.mul(t1285, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"
    # t1286 = prims.mul(t1285, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"
  t1288 = prims.sum(t1286, (2,))  # t1288: "cuda:0 f32[1, 512]"
  t1289 = prims.broadcast_in_dim(t1288, [1, 512, 1], [0, 1])  # t1289: "cuda:0 f32[1, 512, 1]"
  t1291 = ltorch.true_divide(t1289, 4096.0)  # t1291: "cuda:0 f32[1, 512, 1]"
    # t1291 = prims.div(t1289, 4096.0)  # t1291: "cuda:0 f32[1, 512, 1]"
  t1293 = ltorch.add(t1291, 1e-05, alpha=None)  # t1293: "cuda:0 f32[1, 512, 1]"
    # t1293 = prims.add(t1291, 1e-05)  # t1293: "cuda:0 f32[1, 512, 1]"
  t1294 = prims.rsqrt(t1293)  # t1294: "cuda:0 f32[1, 512, 1]"
  t1295 = prims.broadcast_in_dim(t1294, (1, 512, 4096), (0, 1, 2))  # t1295: "cuda:0 f32[1, 512, 4096]"
  t1296 = ltorch.mul(t1285, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
    # t1296 = prims.mul(t1285, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
  t1297 = prims.convert_element_type(t1296, dtypes.bfloat16)  # t1297: "cuda:0 bf16[1, 512, 4096]"
  t1298 = prims.broadcast_in_dim(t_transformer_h_9_norm_1_weight, (1, 512, 4096), (2,))  # t1298: "cuda:0 bf16[1, 512, 4096]"
  t1299 = prims.convert_element_type(t1297, dtypes.float32)  # t1299: "cuda:0 f32[1, 512, 4096]"
  t1300 = prims.convert_element_type(t1298, dtypes.float32)  # t1300: "cuda:0 f32[1, 512, 4096]"
  t1301 = ltorch.mul(t1299, t1300)  # t1301: "cuda:0 f32[1, 512, 4096]"
    # t1301 = prims.mul(t1299, t1300)  # t1301: "cuda:0 f32[1, 512, 4096]"
  t1302 = prims.convert_element_type(t1301, dtypes.bfloat16)  # t1302: "cuda:0 bf16[1, 512, 4096]"
  t1303 = prims.linear(t1302, t_transformer_h_9_attn_attn_weight, None)  # t1303: "cuda:0 bf16[1, 512, 12288]"
  t1309 = prims.reshape(t1303, (1, 512, 32, 3, 128))  # t1309: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1315 = prims.transpose(t1309, (0, 2, 3, 1, 4))  # t1315: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1316, t1317, t1318) = ltorch.split(t1315, (1, 1, 1), 2)
    # t1316 = prims.slice_prim(t1315, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1316: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1317 = prims.slice_prim(t1315, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1317: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1318 = prims.slice_prim(t1315, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1318: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1324 = prims.reshape(t1316, (1, 32, 512, 128))  # t1324: "cuda:0 bf16[1, 32, 512, 128]"
  t1330 = prims.reshape(t1317, (1, 32, 512, 128))  # t1330: "cuda:0 bf16[1, 32, 512, 128]"
  t1336 = prims.reshape(t1318, (1, 32, 512, 128))  # t1336: "cuda:0 bf16[1, 32, 512, 128]"
  t1337 = prims.slice_prim(t1324, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1337: "cuda:0 bf16[1, 32, 512, 128]"
  t1338 = prims.slice_prim(t1337, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1338: "cuda:0 bf16[1, 32, 512, 64]"
  t1339 = prims.slice_prim(t1337, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1339: "cuda:0 bf16[1, 32, 512, 64]"
  t1340 = prims.convert_element_type(t1339, dtypes.float32)  # t1340: "cuda:0 f32[1, 32, 512, 64]"
  t1341 = prims.neg(t1340)  # t1341: "cuda:0 f32[1, 32, 512, 64]"
  t1342 = prims.convert_element_type(t1341, dtypes.bfloat16)  # t1342: "cuda:0 bf16[1, 32, 512, 64]"
  t1344 = prims.cat((t1342, t1338), -1)  # t1344: "cuda:0 bf16[1, 32, 512, 128]"
  t1345 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1345: "cuda:0 f32[1, 32, 512, 128]"
  t1346 = prims.convert_element_type(t1337, dtypes.float32)  # t1346: "cuda:0 f32[1, 32, 512, 128]"
  t1347 = ltorch.mul(t1346, t1345)  # t1347: "cuda:0 f32[1, 32, 512, 128]"
    # t1347 = prims.mul(t1346, t1345)  # t1347: "cuda:0 f32[1, 32, 512, 128]"
  t1348 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1348: "cuda:0 f32[1, 32, 512, 128]"
  t1349 = prims.convert_element_type(t1344, dtypes.float32)  # t1349: "cuda:0 f32[1, 32, 512, 128]"
  t1350 = ltorch.mul(t1349, t1348)  # t1350: "cuda:0 f32[1, 32, 512, 128]"
    # t1350 = prims.mul(t1349, t1348)  # t1350: "cuda:0 f32[1, 32, 512, 128]"
  t1351 = ltorch.add(t1347, t1350, alpha=None)  # t1351: "cuda:0 f32[1, 32, 512, 128]"
    # t1351 = prims.add(t1347, t1350)  # t1351: "cuda:0 f32[1, 32, 512, 128]"
  t1352 = prims.convert_element_type(t1351, dtypes.bfloat16)  # t1352: "cuda:0 bf16[1, 32, 512, 128]"
  t1353 = prims.slice_prim(t1330, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1353: "cuda:0 bf16[1, 32, 512, 128]"
  t1354 = prims.slice_prim(t1353, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1354: "cuda:0 bf16[1, 32, 512, 64]"
  t1355 = prims.slice_prim(t1353, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1355: "cuda:0 bf16[1, 32, 512, 64]"
  t1356 = prims.convert_element_type(t1355, dtypes.float32)  # t1356: "cuda:0 f32[1, 32, 512, 64]"
  t1357 = prims.neg(t1356)  # t1357: "cuda:0 f32[1, 32, 512, 64]"
  t1358 = prims.convert_element_type(t1357, dtypes.bfloat16)  # t1358: "cuda:0 bf16[1, 32, 512, 64]"
  t1360 = prims.cat((t1358, t1354), -1)  # t1360: "cuda:0 bf16[1, 32, 512, 128]"
  t1361 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1361: "cuda:0 f32[1, 32, 512, 128]"
  t1362 = prims.convert_element_type(t1353, dtypes.float32)  # t1362: "cuda:0 f32[1, 32, 512, 128]"
  t1363 = ltorch.mul(t1362, t1361)  # t1363: "cuda:0 f32[1, 32, 512, 128]"
    # t1363 = prims.mul(t1362, t1361)  # t1363: "cuda:0 f32[1, 32, 512, 128]"
  t1364 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1364: "cuda:0 f32[1, 32, 512, 128]"
  t1365 = prims.convert_element_type(t1360, dtypes.float32)  # t1365: "cuda:0 f32[1, 32, 512, 128]"
  t1366 = ltorch.mul(t1365, t1364)  # t1366: "cuda:0 f32[1, 32, 512, 128]"
    # t1366 = prims.mul(t1365, t1364)  # t1366: "cuda:0 f32[1, 32, 512, 128]"
  t1367 = ltorch.add(t1363, t1366, alpha=None)  # t1367: "cuda:0 f32[1, 32, 512, 128]"
    # t1367 = prims.add(t1363, t1366)  # t1367: "cuda:0 f32[1, 32, 512, 128]"
  t1368 = prims.convert_element_type(t1367, dtypes.bfloat16)  # t1368: "cuda:0 bf16[1, 32, 512, 128]"
  t1369 = prims.slice_prim(t1324, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1369: "cuda:0 bf16[1, 32, 512, 0]"
  t1371 = prims.cat((t1352, t1369), -1)  # t1371: "cuda:0 bf16[1, 32, 512, 128]"
  t1372 = prims.slice_prim(t1330, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1372: "cuda:0 bf16[1, 32, 512, 0]"
  t1374 = prims.cat((t1368, t1372), -1)  # t1374: "cuda:0 bf16[1, 32, 512, 128]"
  (t1375, t1376, t1377, t1378) = cudnn_sdpa_fwd(t1371, t1374, t1336, None, 0.0, True, scale=0.08838834764831843)
  t1381 = prims.transpose(t1375, (0, 2, 1, 3))  # t1381: "cuda:0 bf16[1, 512, 32, 128]"
  t1385 = prims.reshape(t1381, (1, 512, 4096))  # t1385: "cuda:0 bf16[1, 512, 4096]"
  t1386 = prims.linear(t1385, t_transformer_h_9_attn_proj_weight, None)  # t1386: "cuda:0 bf16[1, 512, 4096]"
  t1387 = prims.convert_element_type(t1386, dtypes.float32)  # t1387: "cuda:0 f32[1, 512, 4096]"
  t1388 = prims.convert_element_type(t1284, dtypes.float32)  # t1388: "cuda:0 f32[1, 512, 4096]"
  t1389 = ltorch.add(t1387, t1388, alpha=None)  # t1389: "cuda:0 f32[1, 512, 4096]"
    # t1389 = prims.add(t1387, t1388)  # t1389: "cuda:0 f32[1, 512, 4096]"
  t1390 = prims.convert_element_type(t1389, dtypes.bfloat16)  # t1390: "cuda:0 bf16[1, 512, 4096]"
  t1391 = prims.convert_element_type(t1390, dtypes.float32)  # t1391: "cuda:0 f32[1, 512, 4096]"
  t1392 = ltorch.mul(t1391, t1391)  # t1392: "cuda:0 f32[1, 512, 4096]"
    # t1392 = prims.mul(t1391, t1391)  # t1392: "cuda:0 f32[1, 512, 4096]"
  t1394 = prims.sum(t1392, (2,))  # t1394: "cuda:0 f32[1, 512]"
  t1395 = prims.broadcast_in_dim(t1394, [1, 512, 1], [0, 1])  # t1395: "cuda:0 f32[1, 512, 1]"
  t1397 = ltorch.true_divide(t1395, 4096.0)  # t1397: "cuda:0 f32[1, 512, 1]"
    # t1397 = prims.div(t1395, 4096.0)  # t1397: "cuda:0 f32[1, 512, 1]"
  t1399 = ltorch.add(t1397, 1e-05, alpha=None)  # t1399: "cuda:0 f32[1, 512, 1]"
    # t1399 = prims.add(t1397, 1e-05)  # t1399: "cuda:0 f32[1, 512, 1]"
  t1400 = prims.rsqrt(t1399)  # t1400: "cuda:0 f32[1, 512, 1]"
  t1401 = prims.broadcast_in_dim(t1400, (1, 512, 4096), (0, 1, 2))  # t1401: "cuda:0 f32[1, 512, 4096]"
  t1402 = ltorch.mul(t1391, t1401)  # t1402: "cuda:0 f32[1, 512, 4096]"
    # t1402 = prims.mul(t1391, t1401)  # t1402: "cuda:0 f32[1, 512, 4096]"
  t1403 = prims.convert_element_type(t1402, dtypes.bfloat16)  # t1403: "cuda:0 bf16[1, 512, 4096]"
  t1404 = prims.broadcast_in_dim(t_transformer_h_9_norm_2_weight, (1, 512, 4096), (2,))  # t1404: "cuda:0 bf16[1, 512, 4096]"
  t1405 = prims.convert_element_type(t1403, dtypes.float32)  # t1405: "cuda:0 f32[1, 512, 4096]"
  t1406 = prims.convert_element_type(t1404, dtypes.float32)  # t1406: "cuda:0 f32[1, 512, 4096]"
  t1407 = ltorch.mul(t1405, t1406)  # t1407: "cuda:0 f32[1, 512, 4096]"
    # t1407 = prims.mul(t1405, t1406)  # t1407: "cuda:0 f32[1, 512, 4096]"
  t1408 = prims.convert_element_type(t1407, dtypes.bfloat16)  # t1408: "cuda:0 bf16[1, 512, 4096]"
  t1409 = prims.linear(t1408, t_transformer_h_9_mlp_fc_1_weight, None)  # t1409: "cuda:0 bf16[1, 512, 11008]"
  t1410 = prims.linear(t1408, t_transformer_h_9_mlp_fc_2_weight, None)  # t1410: "cuda:0 bf16[1, 512, 11008]"
  t1411 = prims.convert_element_type(t1409, dtypes.float32)  # t1411: "cuda:0 f32[1, 512, 11008]"
  t1412 = prims.neg(t1411)  # t1412: "cuda:0 f32[1, 512, 11008]"
  t1413 = prims.exp(t1412)  # t1413: "cuda:0 f32[1, 512, 11008]"
  t1414 = ltorch.add(1.0, t1413, alpha=None)  # t1414: "cuda:0 f32[1, 512, 11008]"
    # t1414 = prims.add(1.0, t1413)  # t1414: "cuda:0 f32[1, 512, 11008]"
  t1415 = prims.reciprocal(t1414)  # t1415: "cuda:0 f32[1, 512, 11008]"
  t1416 = prims.convert_element_type(t1415, dtypes.bfloat16)  # t1416: "cuda:0 bf16[1, 512, 11008]"
  t1417 = prims.convert_element_type(t1409, dtypes.float32)  # t1417: "cuda:0 f32[1, 512, 11008]"
  t1418 = prims.convert_element_type(t1416, dtypes.float32)  # t1418: "cuda:0 f32[1, 512, 11008]"
  t1419 = ltorch.mul(t1417, t1418)  # t1419: "cuda:0 f32[1, 512, 11008]"
    # t1419 = prims.mul(t1417, t1418)  # t1419: "cuda:0 f32[1, 512, 11008]"
  t1420 = prims.convert_element_type(t1419, dtypes.bfloat16)  # t1420: "cuda:0 bf16[1, 512, 11008]"
  t1421 = prims.convert_element_type(t1420, dtypes.float32)  # t1421: "cuda:0 f32[1, 512, 11008]"
  t1422 = prims.convert_element_type(t1410, dtypes.float32)  # t1422: "cuda:0 f32[1, 512, 11008]"
  t1423 = ltorch.mul(t1421, t1422)  # t1423: "cuda:0 f32[1, 512, 11008]"
    # t1423 = prims.mul(t1421, t1422)  # t1423: "cuda:0 f32[1, 512, 11008]"
  t1424 = prims.convert_element_type(t1423, dtypes.bfloat16)  # t1424: "cuda:0 bf16[1, 512, 11008]"
  t1425 = prims.linear(t1424, t_transformer_h_9_mlp_proj_weight, None)  # t1425: "cuda:0 bf16[1, 512, 4096]"
  t1426 = prims.convert_element_type(t1425, dtypes.float32)  # t1426: "cuda:0 f32[1, 512, 4096]"
  t1427 = prims.convert_element_type(t1390, dtypes.float32)  # t1427: "cuda:0 f32[1, 512, 4096]"
  t1428 = ltorch.add(t1426, t1427, alpha=None)  # t1428: "cuda:0 f32[1, 512, 4096]"
    # t1428 = prims.add(t1426, t1427)  # t1428: "cuda:0 f32[1, 512, 4096]"
  t1429 = prims.convert_element_type(t1428, dtypes.bfloat16)  # t1429: "cuda:0 bf16[1, 512, 4096]"
  t1430 = prims.convert_element_type(t1429, dtypes.float32)  # t1430: "cuda:0 f32[1, 512, 4096]"
  t1431 = ltorch.mul(t1430, t1430)  # t1431: "cuda:0 f32[1, 512, 4096]"
    # t1431 = prims.mul(t1430, t1430)  # t1431: "cuda:0 f32[1, 512, 4096]"
  t1433 = prims.sum(t1431, (2,))  # t1433: "cuda:0 f32[1, 512]"
  t1434 = prims.broadcast_in_dim(t1433, [1, 512, 1], [0, 1])  # t1434: "cuda:0 f32[1, 512, 1]"
  t1436 = ltorch.true_divide(t1434, 4096.0)  # t1436: "cuda:0 f32[1, 512, 1]"
    # t1436 = prims.div(t1434, 4096.0)  # t1436: "cuda:0 f32[1, 512, 1]"
  t1438 = ltorch.add(t1436, 1e-05, alpha=None)  # t1438: "cuda:0 f32[1, 512, 1]"
    # t1438 = prims.add(t1436, 1e-05)  # t1438: "cuda:0 f32[1, 512, 1]"
  t1439 = prims.rsqrt(t1438)  # t1439: "cuda:0 f32[1, 512, 1]"
  t1440 = prims.broadcast_in_dim(t1439, (1, 512, 4096), (0, 1, 2))  # t1440: "cuda:0 f32[1, 512, 4096]"
  t1441 = ltorch.mul(t1430, t1440)  # t1441: "cuda:0 f32[1, 512, 4096]"
    # t1441 = prims.mul(t1430, t1440)  # t1441: "cuda:0 f32[1, 512, 4096]"
  t1442 = prims.convert_element_type(t1441, dtypes.bfloat16)  # t1442: "cuda:0 bf16[1, 512, 4096]"
  t1443 = prims.broadcast_in_dim(t_transformer_h_10_norm_1_weight, (1, 512, 4096), (2,))  # t1443: "cuda:0 bf16[1, 512, 4096]"
  t1444 = prims.convert_element_type(t1442, dtypes.float32)  # t1444: "cuda:0 f32[1, 512, 4096]"
  t1445 = prims.convert_element_type(t1443, dtypes.float32)  # t1445: "cuda:0 f32[1, 512, 4096]"
  t1446 = ltorch.mul(t1444, t1445)  # t1446: "cuda:0 f32[1, 512, 4096]"
    # t1446 = prims.mul(t1444, t1445)  # t1446: "cuda:0 f32[1, 512, 4096]"
  t1447 = prims.convert_element_type(t1446, dtypes.bfloat16)  # t1447: "cuda:0 bf16[1, 512, 4096]"
  t1448 = prims.linear(t1447, t_transformer_h_10_attn_attn_weight, None)  # t1448: "cuda:0 bf16[1, 512, 12288]"
  t1454 = prims.reshape(t1448, (1, 512, 32, 3, 128))  # t1454: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1460 = prims.transpose(t1454, (0, 2, 3, 1, 4))  # t1460: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1461, t1462, t1463) = ltorch.split(t1460, (1, 1, 1), 2)
    # t1461 = prims.slice_prim(t1460, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1461: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1462 = prims.slice_prim(t1460, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1462: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1463 = prims.slice_prim(t1460, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1463: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1469 = prims.reshape(t1461, (1, 32, 512, 128))  # t1469: "cuda:0 bf16[1, 32, 512, 128]"
  t1475 = prims.reshape(t1462, (1, 32, 512, 128))  # t1475: "cuda:0 bf16[1, 32, 512, 128]"
  t1481 = prims.reshape(t1463, (1, 32, 512, 128))  # t1481: "cuda:0 bf16[1, 32, 512, 128]"
  t1482 = prims.slice_prim(t1469, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1482: "cuda:0 bf16[1, 32, 512, 128]"
  t1483 = prims.slice_prim(t1482, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1483: "cuda:0 bf16[1, 32, 512, 64]"
  t1484 = prims.slice_prim(t1482, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1484: "cuda:0 bf16[1, 32, 512, 64]"
  t1485 = prims.convert_element_type(t1484, dtypes.float32)  # t1485: "cuda:0 f32[1, 32, 512, 64]"
  t1486 = prims.neg(t1485)  # t1486: "cuda:0 f32[1, 32, 512, 64]"
  t1487 = prims.convert_element_type(t1486, dtypes.bfloat16)  # t1487: "cuda:0 bf16[1, 32, 512, 64]"
  t1489 = prims.cat((t1487, t1483), -1)  # t1489: "cuda:0 bf16[1, 32, 512, 128]"
  t1490 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1490: "cuda:0 f32[1, 32, 512, 128]"
  t1491 = prims.convert_element_type(t1482, dtypes.float32)  # t1491: "cuda:0 f32[1, 32, 512, 128]"
  t1492 = ltorch.mul(t1491, t1490)  # t1492: "cuda:0 f32[1, 32, 512, 128]"
    # t1492 = prims.mul(t1491, t1490)  # t1492: "cuda:0 f32[1, 32, 512, 128]"
  t1493 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1493: "cuda:0 f32[1, 32, 512, 128]"
  t1494 = prims.convert_element_type(t1489, dtypes.float32)  # t1494: "cuda:0 f32[1, 32, 512, 128]"
  t1495 = ltorch.mul(t1494, t1493)  # t1495: "cuda:0 f32[1, 32, 512, 128]"
    # t1495 = prims.mul(t1494, t1493)  # t1495: "cuda:0 f32[1, 32, 512, 128]"
  t1496 = ltorch.add(t1492, t1495, alpha=None)  # t1496: "cuda:0 f32[1, 32, 512, 128]"
    # t1496 = prims.add(t1492, t1495)  # t1496: "cuda:0 f32[1, 32, 512, 128]"
  t1497 = prims.convert_element_type(t1496, dtypes.bfloat16)  # t1497: "cuda:0 bf16[1, 32, 512, 128]"
  t1498 = prims.slice_prim(t1475, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1498: "cuda:0 bf16[1, 32, 512, 128]"
  t1499 = prims.slice_prim(t1498, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1499: "cuda:0 bf16[1, 32, 512, 64]"
  t1500 = prims.slice_prim(t1498, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1500: "cuda:0 bf16[1, 32, 512, 64]"
  t1501 = prims.convert_element_type(t1500, dtypes.float32)  # t1501: "cuda:0 f32[1, 32, 512, 64]"
  t1502 = prims.neg(t1501)  # t1502: "cuda:0 f32[1, 32, 512, 64]"
  t1503 = prims.convert_element_type(t1502, dtypes.bfloat16)  # t1503: "cuda:0 bf16[1, 32, 512, 64]"
  t1505 = prims.cat((t1503, t1499), -1)  # t1505: "cuda:0 bf16[1, 32, 512, 128]"
  t1506 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1506: "cuda:0 f32[1, 32, 512, 128]"
  t1507 = prims.convert_element_type(t1498, dtypes.float32)  # t1507: "cuda:0 f32[1, 32, 512, 128]"
  t1508 = ltorch.mul(t1507, t1506)  # t1508: "cuda:0 f32[1, 32, 512, 128]"
    # t1508 = prims.mul(t1507, t1506)  # t1508: "cuda:0 f32[1, 32, 512, 128]"
  t1509 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1509: "cuda:0 f32[1, 32, 512, 128]"
  t1510 = prims.convert_element_type(t1505, dtypes.float32)  # t1510: "cuda:0 f32[1, 32, 512, 128]"
  t1511 = ltorch.mul(t1510, t1509)  # t1511: "cuda:0 f32[1, 32, 512, 128]"
    # t1511 = prims.mul(t1510, t1509)  # t1511: "cuda:0 f32[1, 32, 512, 128]"
  t1512 = ltorch.add(t1508, t1511, alpha=None)  # t1512: "cuda:0 f32[1, 32, 512, 128]"
    # t1512 = prims.add(t1508, t1511)  # t1512: "cuda:0 f32[1, 32, 512, 128]"
  t1513 = prims.convert_element_type(t1512, dtypes.bfloat16)  # t1513: "cuda:0 bf16[1, 32, 512, 128]"
  t1514 = prims.slice_prim(t1469, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1514: "cuda:0 bf16[1, 32, 512, 0]"
  t1516 = prims.cat((t1497, t1514), -1)  # t1516: "cuda:0 bf16[1, 32, 512, 128]"
  t1517 = prims.slice_prim(t1475, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1517: "cuda:0 bf16[1, 32, 512, 0]"
  t1519 = prims.cat((t1513, t1517), -1)  # t1519: "cuda:0 bf16[1, 32, 512, 128]"
  (t1520, t1521, t1522, t1523) = cudnn_sdpa_fwd(t1516, t1519, t1481, None, 0.0, True, scale=0.08838834764831843)
  t1526 = prims.transpose(t1520, (0, 2, 1, 3))  # t1526: "cuda:0 bf16[1, 512, 32, 128]"
  t1530 = prims.reshape(t1526, (1, 512, 4096))  # t1530: "cuda:0 bf16[1, 512, 4096]"
  t1531 = prims.linear(t1530, t_transformer_h_10_attn_proj_weight, None)  # t1531: "cuda:0 bf16[1, 512, 4096]"
  t1532 = prims.convert_element_type(t1531, dtypes.float32)  # t1532: "cuda:0 f32[1, 512, 4096]"
  t1533 = prims.convert_element_type(t1429, dtypes.float32)  # t1533: "cuda:0 f32[1, 512, 4096]"
  t1534 = ltorch.add(t1532, t1533, alpha=None)  # t1534: "cuda:0 f32[1, 512, 4096]"
    # t1534 = prims.add(t1532, t1533)  # t1534: "cuda:0 f32[1, 512, 4096]"
  t1535 = prims.convert_element_type(t1534, dtypes.bfloat16)  # t1535: "cuda:0 bf16[1, 512, 4096]"
  t1536 = prims.convert_element_type(t1535, dtypes.float32)  # t1536: "cuda:0 f32[1, 512, 4096]"
  t1537 = ltorch.mul(t1536, t1536)  # t1537: "cuda:0 f32[1, 512, 4096]"
    # t1537 = prims.mul(t1536, t1536)  # t1537: "cuda:0 f32[1, 512, 4096]"
  t1539 = prims.sum(t1537, (2,))  # t1539: "cuda:0 f32[1, 512]"
  t1540 = prims.broadcast_in_dim(t1539, [1, 512, 1], [0, 1])  # t1540: "cuda:0 f32[1, 512, 1]"
  t1542 = ltorch.true_divide(t1540, 4096.0)  # t1542: "cuda:0 f32[1, 512, 1]"
    # t1542 = prims.div(t1540, 4096.0)  # t1542: "cuda:0 f32[1, 512, 1]"
  t1544 = ltorch.add(t1542, 1e-05, alpha=None)  # t1544: "cuda:0 f32[1, 512, 1]"
    # t1544 = prims.add(t1542, 1e-05)  # t1544: "cuda:0 f32[1, 512, 1]"
  t1545 = prims.rsqrt(t1544)  # t1545: "cuda:0 f32[1, 512, 1]"
  t1546 = prims.broadcast_in_dim(t1545, (1, 512, 4096), (0, 1, 2))  # t1546: "cuda:0 f32[1, 512, 4096]"
  t1547 = ltorch.mul(t1536, t1546)  # t1547: "cuda:0 f32[1, 512, 4096]"
    # t1547 = prims.mul(t1536, t1546)  # t1547: "cuda:0 f32[1, 512, 4096]"
  t1548 = prims.convert_element_type(t1547, dtypes.bfloat16)  # t1548: "cuda:0 bf16[1, 512, 4096]"
  t1549 = prims.broadcast_in_dim(t_transformer_h_10_norm_2_weight, (1, 512, 4096), (2,))  # t1549: "cuda:0 bf16[1, 512, 4096]"
  t1550 = prims.convert_element_type(t1548, dtypes.float32)  # t1550: "cuda:0 f32[1, 512, 4096]"
  t1551 = prims.convert_element_type(t1549, dtypes.float32)  # t1551: "cuda:0 f32[1, 512, 4096]"
  t1552 = ltorch.mul(t1550, t1551)  # t1552: "cuda:0 f32[1, 512, 4096]"
    # t1552 = prims.mul(t1550, t1551)  # t1552: "cuda:0 f32[1, 512, 4096]"
  t1553 = prims.convert_element_type(t1552, dtypes.bfloat16)  # t1553: "cuda:0 bf16[1, 512, 4096]"
  t1554 = prims.linear(t1553, t_transformer_h_10_mlp_fc_1_weight, None)  # t1554: "cuda:0 bf16[1, 512, 11008]"
  t1555 = prims.linear(t1553, t_transformer_h_10_mlp_fc_2_weight, None)  # t1555: "cuda:0 bf16[1, 512, 11008]"
  t1556 = prims.convert_element_type(t1554, dtypes.float32)  # t1556: "cuda:0 f32[1, 512, 11008]"
  t1557 = prims.neg(t1556)  # t1557: "cuda:0 f32[1, 512, 11008]"
  t1558 = prims.exp(t1557)  # t1558: "cuda:0 f32[1, 512, 11008]"
  t1559 = ltorch.add(1.0, t1558, alpha=None)  # t1559: "cuda:0 f32[1, 512, 11008]"
    # t1559 = prims.add(1.0, t1558)  # t1559: "cuda:0 f32[1, 512, 11008]"
  t1560 = prims.reciprocal(t1559)  # t1560: "cuda:0 f32[1, 512, 11008]"
  t1561 = prims.convert_element_type(t1560, dtypes.bfloat16)  # t1561: "cuda:0 bf16[1, 512, 11008]"
  t1562 = prims.convert_element_type(t1554, dtypes.float32)  # t1562: "cuda:0 f32[1, 512, 11008]"
  t1563 = prims.convert_element_type(t1561, dtypes.float32)  # t1563: "cuda:0 f32[1, 512, 11008]"
  t1564 = ltorch.mul(t1562, t1563)  # t1564: "cuda:0 f32[1, 512, 11008]"
    # t1564 = prims.mul(t1562, t1563)  # t1564: "cuda:0 f32[1, 512, 11008]"
  t1565 = prims.convert_element_type(t1564, dtypes.bfloat16)  # t1565: "cuda:0 bf16[1, 512, 11008]"
  t1566 = prims.convert_element_type(t1565, dtypes.float32)  # t1566: "cuda:0 f32[1, 512, 11008]"
  t1567 = prims.convert_element_type(t1555, dtypes.float32)  # t1567: "cuda:0 f32[1, 512, 11008]"
  t1568 = ltorch.mul(t1566, t1567)  # t1568: "cuda:0 f32[1, 512, 11008]"
    # t1568 = prims.mul(t1566, t1567)  # t1568: "cuda:0 f32[1, 512, 11008]"
  t1569 = prims.convert_element_type(t1568, dtypes.bfloat16)  # t1569: "cuda:0 bf16[1, 512, 11008]"
  t1570 = prims.linear(t1569, t_transformer_h_10_mlp_proj_weight, None)  # t1570: "cuda:0 bf16[1, 512, 4096]"
  t1571 = prims.convert_element_type(t1570, dtypes.float32)  # t1571: "cuda:0 f32[1, 512, 4096]"
  t1572 = prims.convert_element_type(t1535, dtypes.float32)  # t1572: "cuda:0 f32[1, 512, 4096]"
  t1573 = ltorch.add(t1571, t1572, alpha=None)  # t1573: "cuda:0 f32[1, 512, 4096]"
    # t1573 = prims.add(t1571, t1572)  # t1573: "cuda:0 f32[1, 512, 4096]"
  t1574 = prims.convert_element_type(t1573, dtypes.bfloat16)  # t1574: "cuda:0 bf16[1, 512, 4096]"
  t1575 = prims.convert_element_type(t1574, dtypes.float32)  # t1575: "cuda:0 f32[1, 512, 4096]"
  t1576 = ltorch.mul(t1575, t1575)  # t1576: "cuda:0 f32[1, 512, 4096]"
    # t1576 = prims.mul(t1575, t1575)  # t1576: "cuda:0 f32[1, 512, 4096]"
  t1578 = prims.sum(t1576, (2,))  # t1578: "cuda:0 f32[1, 512]"
  t1579 = prims.broadcast_in_dim(t1578, [1, 512, 1], [0, 1])  # t1579: "cuda:0 f32[1, 512, 1]"
  t1581 = ltorch.true_divide(t1579, 4096.0)  # t1581: "cuda:0 f32[1, 512, 1]"
    # t1581 = prims.div(t1579, 4096.0)  # t1581: "cuda:0 f32[1, 512, 1]"
  t1583 = ltorch.add(t1581, 1e-05, alpha=None)  # t1583: "cuda:0 f32[1, 512, 1]"
    # t1583 = prims.add(t1581, 1e-05)  # t1583: "cuda:0 f32[1, 512, 1]"
  t1584 = prims.rsqrt(t1583)  # t1584: "cuda:0 f32[1, 512, 1]"
  t1585 = prims.broadcast_in_dim(t1584, (1, 512, 4096), (0, 1, 2))  # t1585: "cuda:0 f32[1, 512, 4096]"
  t1586 = ltorch.mul(t1575, t1585)  # t1586: "cuda:0 f32[1, 512, 4096]"
    # t1586 = prims.mul(t1575, t1585)  # t1586: "cuda:0 f32[1, 512, 4096]"
  t1587 = prims.convert_element_type(t1586, dtypes.bfloat16)  # t1587: "cuda:0 bf16[1, 512, 4096]"
  t1588 = prims.broadcast_in_dim(t_transformer_h_11_norm_1_weight, (1, 512, 4096), (2,))  # t1588: "cuda:0 bf16[1, 512, 4096]"
  t1589 = prims.convert_element_type(t1587, dtypes.float32)  # t1589: "cuda:0 f32[1, 512, 4096]"
  t1590 = prims.convert_element_type(t1588, dtypes.float32)  # t1590: "cuda:0 f32[1, 512, 4096]"
  t1591 = ltorch.mul(t1589, t1590)  # t1591: "cuda:0 f32[1, 512, 4096]"
    # t1591 = prims.mul(t1589, t1590)  # t1591: "cuda:0 f32[1, 512, 4096]"
  t1592 = prims.convert_element_type(t1591, dtypes.bfloat16)  # t1592: "cuda:0 bf16[1, 512, 4096]"
  t1593 = prims.linear(t1592, t_transformer_h_11_attn_attn_weight, None)  # t1593: "cuda:0 bf16[1, 512, 12288]"
  t1599 = prims.reshape(t1593, (1, 512, 32, 3, 128))  # t1599: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1605 = prims.transpose(t1599, (0, 2, 3, 1, 4))  # t1605: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1606, t1607, t1608) = ltorch.split(t1605, (1, 1, 1), 2)
    # t1606 = prims.slice_prim(t1605, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1606: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1607 = prims.slice_prim(t1605, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1607: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1608 = prims.slice_prim(t1605, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1608: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1614 = prims.reshape(t1606, (1, 32, 512, 128))  # t1614: "cuda:0 bf16[1, 32, 512, 128]"
  t1620 = prims.reshape(t1607, (1, 32, 512, 128))  # t1620: "cuda:0 bf16[1, 32, 512, 128]"
  t1626 = prims.reshape(t1608, (1, 32, 512, 128))  # t1626: "cuda:0 bf16[1, 32, 512, 128]"
  t1627 = prims.slice_prim(t1614, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1627: "cuda:0 bf16[1, 32, 512, 128]"
  t1628 = prims.slice_prim(t1627, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1628: "cuda:0 bf16[1, 32, 512, 64]"
  t1629 = prims.slice_prim(t1627, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1629: "cuda:0 bf16[1, 32, 512, 64]"
  t1630 = prims.convert_element_type(t1629, dtypes.float32)  # t1630: "cuda:0 f32[1, 32, 512, 64]"
  t1631 = prims.neg(t1630)  # t1631: "cuda:0 f32[1, 32, 512, 64]"
  t1632 = prims.convert_element_type(t1631, dtypes.bfloat16)  # t1632: "cuda:0 bf16[1, 32, 512, 64]"
  t1634 = prims.cat((t1632, t1628), -1)  # t1634: "cuda:0 bf16[1, 32, 512, 128]"
  t1635 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1635: "cuda:0 f32[1, 32, 512, 128]"
  t1636 = prims.convert_element_type(t1627, dtypes.float32)  # t1636: "cuda:0 f32[1, 32, 512, 128]"
  t1637 = ltorch.mul(t1636, t1635)  # t1637: "cuda:0 f32[1, 32, 512, 128]"
    # t1637 = prims.mul(t1636, t1635)  # t1637: "cuda:0 f32[1, 32, 512, 128]"
  t1638 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1638: "cuda:0 f32[1, 32, 512, 128]"
  t1639 = prims.convert_element_type(t1634, dtypes.float32)  # t1639: "cuda:0 f32[1, 32, 512, 128]"
  t1640 = ltorch.mul(t1639, t1638)  # t1640: "cuda:0 f32[1, 32, 512, 128]"
    # t1640 = prims.mul(t1639, t1638)  # t1640: "cuda:0 f32[1, 32, 512, 128]"
  t1641 = ltorch.add(t1637, t1640, alpha=None)  # t1641: "cuda:0 f32[1, 32, 512, 128]"
    # t1641 = prims.add(t1637, t1640)  # t1641: "cuda:0 f32[1, 32, 512, 128]"
  t1642 = prims.convert_element_type(t1641, dtypes.bfloat16)  # t1642: "cuda:0 bf16[1, 32, 512, 128]"
  t1643 = prims.slice_prim(t1620, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1643: "cuda:0 bf16[1, 32, 512, 128]"
  t1644 = prims.slice_prim(t1643, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1644: "cuda:0 bf16[1, 32, 512, 64]"
  t1645 = prims.slice_prim(t1643, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1645: "cuda:0 bf16[1, 32, 512, 64]"
  t1646 = prims.convert_element_type(t1645, dtypes.float32)  # t1646: "cuda:0 f32[1, 32, 512, 64]"
  t1647 = prims.neg(t1646)  # t1647: "cuda:0 f32[1, 32, 512, 64]"
  t1648 = prims.convert_element_type(t1647, dtypes.bfloat16)  # t1648: "cuda:0 bf16[1, 32, 512, 64]"
  t1650 = prims.cat((t1648, t1644), -1)  # t1650: "cuda:0 bf16[1, 32, 512, 128]"
  t1651 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1651: "cuda:0 f32[1, 32, 512, 128]"
  t1652 = prims.convert_element_type(t1643, dtypes.float32)  # t1652: "cuda:0 f32[1, 32, 512, 128]"
  t1653 = ltorch.mul(t1652, t1651)  # t1653: "cuda:0 f32[1, 32, 512, 128]"
    # t1653 = prims.mul(t1652, t1651)  # t1653: "cuda:0 f32[1, 32, 512, 128]"
  t1654 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1654: "cuda:0 f32[1, 32, 512, 128]"
  t1655 = prims.convert_element_type(t1650, dtypes.float32)  # t1655: "cuda:0 f32[1, 32, 512, 128]"
  t1656 = ltorch.mul(t1655, t1654)  # t1656: "cuda:0 f32[1, 32, 512, 128]"
    # t1656 = prims.mul(t1655, t1654)  # t1656: "cuda:0 f32[1, 32, 512, 128]"
  t1657 = ltorch.add(t1653, t1656, alpha=None)  # t1657: "cuda:0 f32[1, 32, 512, 128]"
    # t1657 = prims.add(t1653, t1656)  # t1657: "cuda:0 f32[1, 32, 512, 128]"
  t1658 = prims.convert_element_type(t1657, dtypes.bfloat16)  # t1658: "cuda:0 bf16[1, 32, 512, 128]"
  t1659 = prims.slice_prim(t1614, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1659: "cuda:0 bf16[1, 32, 512, 0]"
  t1661 = prims.cat((t1642, t1659), -1)  # t1661: "cuda:0 bf16[1, 32, 512, 128]"
  t1662 = prims.slice_prim(t1620, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1662: "cuda:0 bf16[1, 32, 512, 0]"
  t1664 = prims.cat((t1658, t1662), -1)  # t1664: "cuda:0 bf16[1, 32, 512, 128]"
  (t1665, t1666, t1667, t1668) = cudnn_sdpa_fwd(t1661, t1664, t1626, None, 0.0, True, scale=0.08838834764831843)
  t1671 = prims.transpose(t1665, (0, 2, 1, 3))  # t1671: "cuda:0 bf16[1, 512, 32, 128]"
  t1675 = prims.reshape(t1671, (1, 512, 4096))  # t1675: "cuda:0 bf16[1, 512, 4096]"
  t1676 = prims.linear(t1675, t_transformer_h_11_attn_proj_weight, None)  # t1676: "cuda:0 bf16[1, 512, 4096]"
  t1677 = prims.convert_element_type(t1676, dtypes.float32)  # t1677: "cuda:0 f32[1, 512, 4096]"
  t1678 = prims.convert_element_type(t1574, dtypes.float32)  # t1678: "cuda:0 f32[1, 512, 4096]"
  t1679 = ltorch.add(t1677, t1678, alpha=None)  # t1679: "cuda:0 f32[1, 512, 4096]"
    # t1679 = prims.add(t1677, t1678)  # t1679: "cuda:0 f32[1, 512, 4096]"
  t1680 = prims.convert_element_type(t1679, dtypes.bfloat16)  # t1680: "cuda:0 bf16[1, 512, 4096]"
  t1681 = prims.convert_element_type(t1680, dtypes.float32)  # t1681: "cuda:0 f32[1, 512, 4096]"
  t1682 = ltorch.mul(t1681, t1681)  # t1682: "cuda:0 f32[1, 512, 4096]"
    # t1682 = prims.mul(t1681, t1681)  # t1682: "cuda:0 f32[1, 512, 4096]"
  t1684 = prims.sum(t1682, (2,))  # t1684: "cuda:0 f32[1, 512]"
  t1685 = prims.broadcast_in_dim(t1684, [1, 512, 1], [0, 1])  # t1685: "cuda:0 f32[1, 512, 1]"
  t1687 = ltorch.true_divide(t1685, 4096.0)  # t1687: "cuda:0 f32[1, 512, 1]"
    # t1687 = prims.div(t1685, 4096.0)  # t1687: "cuda:0 f32[1, 512, 1]"
  t1689 = ltorch.add(t1687, 1e-05, alpha=None)  # t1689: "cuda:0 f32[1, 512, 1]"
    # t1689 = prims.add(t1687, 1e-05)  # t1689: "cuda:0 f32[1, 512, 1]"
  t1690 = prims.rsqrt(t1689)  # t1690: "cuda:0 f32[1, 512, 1]"
  t1691 = prims.broadcast_in_dim(t1690, (1, 512, 4096), (0, 1, 2))  # t1691: "cuda:0 f32[1, 512, 4096]"
  t1692 = ltorch.mul(t1681, t1691)  # t1692: "cuda:0 f32[1, 512, 4096]"
    # t1692 = prims.mul(t1681, t1691)  # t1692: "cuda:0 f32[1, 512, 4096]"
  t1693 = prims.convert_element_type(t1692, dtypes.bfloat16)  # t1693: "cuda:0 bf16[1, 512, 4096]"
  t1694 = prims.broadcast_in_dim(t_transformer_h_11_norm_2_weight, (1, 512, 4096), (2,))  # t1694: "cuda:0 bf16[1, 512, 4096]"
  t1695 = prims.convert_element_type(t1693, dtypes.float32)  # t1695: "cuda:0 f32[1, 512, 4096]"
  t1696 = prims.convert_element_type(t1694, dtypes.float32)  # t1696: "cuda:0 f32[1, 512, 4096]"
  t1697 = ltorch.mul(t1695, t1696)  # t1697: "cuda:0 f32[1, 512, 4096]"
    # t1697 = prims.mul(t1695, t1696)  # t1697: "cuda:0 f32[1, 512, 4096]"
  t1698 = prims.convert_element_type(t1697, dtypes.bfloat16)  # t1698: "cuda:0 bf16[1, 512, 4096]"
  t1699 = prims.linear(t1698, t_transformer_h_11_mlp_fc_1_weight, None)  # t1699: "cuda:0 bf16[1, 512, 11008]"
  t1700 = prims.linear(t1698, t_transformer_h_11_mlp_fc_2_weight, None)  # t1700: "cuda:0 bf16[1, 512, 11008]"
  t1701 = prims.convert_element_type(t1699, dtypes.float32)  # t1701: "cuda:0 f32[1, 512, 11008]"
  t1702 = prims.neg(t1701)  # t1702: "cuda:0 f32[1, 512, 11008]"
  t1703 = prims.exp(t1702)  # t1703: "cuda:0 f32[1, 512, 11008]"
  t1704 = ltorch.add(1.0, t1703, alpha=None)  # t1704: "cuda:0 f32[1, 512, 11008]"
    # t1704 = prims.add(1.0, t1703)  # t1704: "cuda:0 f32[1, 512, 11008]"
  t1705 = prims.reciprocal(t1704)  # t1705: "cuda:0 f32[1, 512, 11008]"
  t1706 = prims.convert_element_type(t1705, dtypes.bfloat16)  # t1706: "cuda:0 bf16[1, 512, 11008]"
  t1707 = prims.convert_element_type(t1699, dtypes.float32)  # t1707: "cuda:0 f32[1, 512, 11008]"
  t1708 = prims.convert_element_type(t1706, dtypes.float32)  # t1708: "cuda:0 f32[1, 512, 11008]"
  t1709 = ltorch.mul(t1707, t1708)  # t1709: "cuda:0 f32[1, 512, 11008]"
    # t1709 = prims.mul(t1707, t1708)  # t1709: "cuda:0 f32[1, 512, 11008]"
  t1710 = prims.convert_element_type(t1709, dtypes.bfloat16)  # t1710: "cuda:0 bf16[1, 512, 11008]"
  t1711 = prims.convert_element_type(t1710, dtypes.float32)  # t1711: "cuda:0 f32[1, 512, 11008]"
  t1712 = prims.convert_element_type(t1700, dtypes.float32)  # t1712: "cuda:0 f32[1, 512, 11008]"
  t1713 = ltorch.mul(t1711, t1712)  # t1713: "cuda:0 f32[1, 512, 11008]"
    # t1713 = prims.mul(t1711, t1712)  # t1713: "cuda:0 f32[1, 512, 11008]"
  t1714 = prims.convert_element_type(t1713, dtypes.bfloat16)  # t1714: "cuda:0 bf16[1, 512, 11008]"
  t1715 = prims.linear(t1714, t_transformer_h_11_mlp_proj_weight, None)  # t1715: "cuda:0 bf16[1, 512, 4096]"
  t1716 = prims.convert_element_type(t1715, dtypes.float32)  # t1716: "cuda:0 f32[1, 512, 4096]"
  t1717 = prims.convert_element_type(t1680, dtypes.float32)  # t1717: "cuda:0 f32[1, 512, 4096]"
  t1718 = ltorch.add(t1716, t1717, alpha=None)  # t1718: "cuda:0 f32[1, 512, 4096]"
    # t1718 = prims.add(t1716, t1717)  # t1718: "cuda:0 f32[1, 512, 4096]"
  t1719 = prims.convert_element_type(t1718, dtypes.bfloat16)  # t1719: "cuda:0 bf16[1, 512, 4096]"
  t1720 = prims.convert_element_type(t1719, dtypes.float32)  # t1720: "cuda:0 f32[1, 512, 4096]"
  t1721 = ltorch.mul(t1720, t1720)  # t1721: "cuda:0 f32[1, 512, 4096]"
    # t1721 = prims.mul(t1720, t1720)  # t1721: "cuda:0 f32[1, 512, 4096]"
  t1723 = prims.sum(t1721, (2,))  # t1723: "cuda:0 f32[1, 512]"
  t1724 = prims.broadcast_in_dim(t1723, [1, 512, 1], [0, 1])  # t1724: "cuda:0 f32[1, 512, 1]"
  t1726 = ltorch.true_divide(t1724, 4096.0)  # t1726: "cuda:0 f32[1, 512, 1]"
    # t1726 = prims.div(t1724, 4096.0)  # t1726: "cuda:0 f32[1, 512, 1]"
  t1728 = ltorch.add(t1726, 1e-05, alpha=None)  # t1728: "cuda:0 f32[1, 512, 1]"
    # t1728 = prims.add(t1726, 1e-05)  # t1728: "cuda:0 f32[1, 512, 1]"
  t1729 = prims.rsqrt(t1728)  # t1729: "cuda:0 f32[1, 512, 1]"
  t1730 = prims.broadcast_in_dim(t1729, (1, 512, 4096), (0, 1, 2))  # t1730: "cuda:0 f32[1, 512, 4096]"
  t1731 = ltorch.mul(t1720, t1730)  # t1731: "cuda:0 f32[1, 512, 4096]"
    # t1731 = prims.mul(t1720, t1730)  # t1731: "cuda:0 f32[1, 512, 4096]"
  t1732 = prims.convert_element_type(t1731, dtypes.bfloat16)  # t1732: "cuda:0 bf16[1, 512, 4096]"
  t1733 = prims.broadcast_in_dim(t_transformer_h_12_norm_1_weight, (1, 512, 4096), (2,))  # t1733: "cuda:0 bf16[1, 512, 4096]"
  t1734 = prims.convert_element_type(t1732, dtypes.float32)  # t1734: "cuda:0 f32[1, 512, 4096]"
  t1735 = prims.convert_element_type(t1733, dtypes.float32)  # t1735: "cuda:0 f32[1, 512, 4096]"
  t1736 = ltorch.mul(t1734, t1735)  # t1736: "cuda:0 f32[1, 512, 4096]"
    # t1736 = prims.mul(t1734, t1735)  # t1736: "cuda:0 f32[1, 512, 4096]"
  t1737 = prims.convert_element_type(t1736, dtypes.bfloat16)  # t1737: "cuda:0 bf16[1, 512, 4096]"
  t1738 = prims.linear(t1737, t_transformer_h_12_attn_attn_weight, None)  # t1738: "cuda:0 bf16[1, 512, 12288]"
  t1744 = prims.reshape(t1738, (1, 512, 32, 3, 128))  # t1744: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1750 = prims.transpose(t1744, (0, 2, 3, 1, 4))  # t1750: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1751, t1752, t1753) = ltorch.split(t1750, (1, 1, 1), 2)
    # t1751 = prims.slice_prim(t1750, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1751: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1752 = prims.slice_prim(t1750, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1752: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1753 = prims.slice_prim(t1750, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1753: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1759 = prims.reshape(t1751, (1, 32, 512, 128))  # t1759: "cuda:0 bf16[1, 32, 512, 128]"
  t1765 = prims.reshape(t1752, (1, 32, 512, 128))  # t1765: "cuda:0 bf16[1, 32, 512, 128]"
  t1771 = prims.reshape(t1753, (1, 32, 512, 128))  # t1771: "cuda:0 bf16[1, 32, 512, 128]"
  t1772 = prims.slice_prim(t1759, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1772: "cuda:0 bf16[1, 32, 512, 128]"
  t1773 = prims.slice_prim(t1772, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1773: "cuda:0 bf16[1, 32, 512, 64]"
  t1774 = prims.slice_prim(t1772, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1774: "cuda:0 bf16[1, 32, 512, 64]"
  t1775 = prims.convert_element_type(t1774, dtypes.float32)  # t1775: "cuda:0 f32[1, 32, 512, 64]"
  t1776 = prims.neg(t1775)  # t1776: "cuda:0 f32[1, 32, 512, 64]"
  t1777 = prims.convert_element_type(t1776, dtypes.bfloat16)  # t1777: "cuda:0 bf16[1, 32, 512, 64]"
  t1779 = prims.cat((t1777, t1773), -1)  # t1779: "cuda:0 bf16[1, 32, 512, 128]"
  t1780 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1780: "cuda:0 f32[1, 32, 512, 128]"
  t1781 = prims.convert_element_type(t1772, dtypes.float32)  # t1781: "cuda:0 f32[1, 32, 512, 128]"
  t1782 = ltorch.mul(t1781, t1780)  # t1782: "cuda:0 f32[1, 32, 512, 128]"
    # t1782 = prims.mul(t1781, t1780)  # t1782: "cuda:0 f32[1, 32, 512, 128]"
  t1783 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1783: "cuda:0 f32[1, 32, 512, 128]"
  t1784 = prims.convert_element_type(t1779, dtypes.float32)  # t1784: "cuda:0 f32[1, 32, 512, 128]"
  t1785 = ltorch.mul(t1784, t1783)  # t1785: "cuda:0 f32[1, 32, 512, 128]"
    # t1785 = prims.mul(t1784, t1783)  # t1785: "cuda:0 f32[1, 32, 512, 128]"
  t1786 = ltorch.add(t1782, t1785, alpha=None)  # t1786: "cuda:0 f32[1, 32, 512, 128]"
    # t1786 = prims.add(t1782, t1785)  # t1786: "cuda:0 f32[1, 32, 512, 128]"
  t1787 = prims.convert_element_type(t1786, dtypes.bfloat16)  # t1787: "cuda:0 bf16[1, 32, 512, 128]"
  t1788 = prims.slice_prim(t1765, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1788: "cuda:0 bf16[1, 32, 512, 128]"
  t1789 = prims.slice_prim(t1788, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1789: "cuda:0 bf16[1, 32, 512, 64]"
  t1790 = prims.slice_prim(t1788, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1790: "cuda:0 bf16[1, 32, 512, 64]"
  t1791 = prims.convert_element_type(t1790, dtypes.float32)  # t1791: "cuda:0 f32[1, 32, 512, 64]"
  t1792 = prims.neg(t1791)  # t1792: "cuda:0 f32[1, 32, 512, 64]"
  t1793 = prims.convert_element_type(t1792, dtypes.bfloat16)  # t1793: "cuda:0 bf16[1, 32, 512, 64]"
  t1795 = prims.cat((t1793, t1789), -1)  # t1795: "cuda:0 bf16[1, 32, 512, 128]"
  t1796 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1796: "cuda:0 f32[1, 32, 512, 128]"
  t1797 = prims.convert_element_type(t1788, dtypes.float32)  # t1797: "cuda:0 f32[1, 32, 512, 128]"
  t1798 = ltorch.mul(t1797, t1796)  # t1798: "cuda:0 f32[1, 32, 512, 128]"
    # t1798 = prims.mul(t1797, t1796)  # t1798: "cuda:0 f32[1, 32, 512, 128]"
  t1799 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1799: "cuda:0 f32[1, 32, 512, 128]"
  t1800 = prims.convert_element_type(t1795, dtypes.float32)  # t1800: "cuda:0 f32[1, 32, 512, 128]"
  t1801 = ltorch.mul(t1800, t1799)  # t1801: "cuda:0 f32[1, 32, 512, 128]"
    # t1801 = prims.mul(t1800, t1799)  # t1801: "cuda:0 f32[1, 32, 512, 128]"
  t1802 = ltorch.add(t1798, t1801, alpha=None)  # t1802: "cuda:0 f32[1, 32, 512, 128]"
    # t1802 = prims.add(t1798, t1801)  # t1802: "cuda:0 f32[1, 32, 512, 128]"
  t1803 = prims.convert_element_type(t1802, dtypes.bfloat16)  # t1803: "cuda:0 bf16[1, 32, 512, 128]"
  t1804 = prims.slice_prim(t1759, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1804: "cuda:0 bf16[1, 32, 512, 0]"
  t1806 = prims.cat((t1787, t1804), -1)  # t1806: "cuda:0 bf16[1, 32, 512, 128]"
  t1807 = prims.slice_prim(t1765, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1807: "cuda:0 bf16[1, 32, 512, 0]"
  t1809 = prims.cat((t1803, t1807), -1)  # t1809: "cuda:0 bf16[1, 32, 512, 128]"
  (t1810, t1811, t1812, t1813) = cudnn_sdpa_fwd(t1806, t1809, t1771, None, 0.0, True, scale=0.08838834764831843)
  t1816 = prims.transpose(t1810, (0, 2, 1, 3))  # t1816: "cuda:0 bf16[1, 512, 32, 128]"
  t1820 = prims.reshape(t1816, (1, 512, 4096))  # t1820: "cuda:0 bf16[1, 512, 4096]"
  t1821 = prims.linear(t1820, t_transformer_h_12_attn_proj_weight, None)  # t1821: "cuda:0 bf16[1, 512, 4096]"
  t1822 = prims.convert_element_type(t1821, dtypes.float32)  # t1822: "cuda:0 f32[1, 512, 4096]"
  t1823 = prims.convert_element_type(t1719, dtypes.float32)  # t1823: "cuda:0 f32[1, 512, 4096]"
  t1824 = ltorch.add(t1822, t1823, alpha=None)  # t1824: "cuda:0 f32[1, 512, 4096]"
    # t1824 = prims.add(t1822, t1823)  # t1824: "cuda:0 f32[1, 512, 4096]"
  t1825 = prims.convert_element_type(t1824, dtypes.bfloat16)  # t1825: "cuda:0 bf16[1, 512, 4096]"
  t1826 = prims.convert_element_type(t1825, dtypes.float32)  # t1826: "cuda:0 f32[1, 512, 4096]"
  t1827 = ltorch.mul(t1826, t1826)  # t1827: "cuda:0 f32[1, 512, 4096]"
    # t1827 = prims.mul(t1826, t1826)  # t1827: "cuda:0 f32[1, 512, 4096]"
  t1829 = prims.sum(t1827, (2,))  # t1829: "cuda:0 f32[1, 512]"
  t1830 = prims.broadcast_in_dim(t1829, [1, 512, 1], [0, 1])  # t1830: "cuda:0 f32[1, 512, 1]"
  t1832 = ltorch.true_divide(t1830, 4096.0)  # t1832: "cuda:0 f32[1, 512, 1]"
    # t1832 = prims.div(t1830, 4096.0)  # t1832: "cuda:0 f32[1, 512, 1]"
  t1834 = ltorch.add(t1832, 1e-05, alpha=None)  # t1834: "cuda:0 f32[1, 512, 1]"
    # t1834 = prims.add(t1832, 1e-05)  # t1834: "cuda:0 f32[1, 512, 1]"
  t1835 = prims.rsqrt(t1834)  # t1835: "cuda:0 f32[1, 512, 1]"
  t1836 = prims.broadcast_in_dim(t1835, (1, 512, 4096), (0, 1, 2))  # t1836: "cuda:0 f32[1, 512, 4096]"
  t1837 = ltorch.mul(t1826, t1836)  # t1837: "cuda:0 f32[1, 512, 4096]"
    # t1837 = prims.mul(t1826, t1836)  # t1837: "cuda:0 f32[1, 512, 4096]"
  t1838 = prims.convert_element_type(t1837, dtypes.bfloat16)  # t1838: "cuda:0 bf16[1, 512, 4096]"
  t1839 = prims.broadcast_in_dim(t_transformer_h_12_norm_2_weight, (1, 512, 4096), (2,))  # t1839: "cuda:0 bf16[1, 512, 4096]"
  t1840 = prims.convert_element_type(t1838, dtypes.float32)  # t1840: "cuda:0 f32[1, 512, 4096]"
  t1841 = prims.convert_element_type(t1839, dtypes.float32)  # t1841: "cuda:0 f32[1, 512, 4096]"
  t1842 = ltorch.mul(t1840, t1841)  # t1842: "cuda:0 f32[1, 512, 4096]"
    # t1842 = prims.mul(t1840, t1841)  # t1842: "cuda:0 f32[1, 512, 4096]"
  t1843 = prims.convert_element_type(t1842, dtypes.bfloat16)  # t1843: "cuda:0 bf16[1, 512, 4096]"
  t1844 = prims.linear(t1843, t_transformer_h_12_mlp_fc_1_weight, None)  # t1844: "cuda:0 bf16[1, 512, 11008]"
  t1845 = prims.linear(t1843, t_transformer_h_12_mlp_fc_2_weight, None)  # t1845: "cuda:0 bf16[1, 512, 11008]"
  t1846 = prims.convert_element_type(t1844, dtypes.float32)  # t1846: "cuda:0 f32[1, 512, 11008]"
  t1847 = prims.neg(t1846)  # t1847: "cuda:0 f32[1, 512, 11008]"
  t1848 = prims.exp(t1847)  # t1848: "cuda:0 f32[1, 512, 11008]"
  t1849 = ltorch.add(1.0, t1848, alpha=None)  # t1849: "cuda:0 f32[1, 512, 11008]"
    # t1849 = prims.add(1.0, t1848)  # t1849: "cuda:0 f32[1, 512, 11008]"
  t1850 = prims.reciprocal(t1849)  # t1850: "cuda:0 f32[1, 512, 11008]"
  t1851 = prims.convert_element_type(t1850, dtypes.bfloat16)  # t1851: "cuda:0 bf16[1, 512, 11008]"
  t1852 = prims.convert_element_type(t1844, dtypes.float32)  # t1852: "cuda:0 f32[1, 512, 11008]"
  t1853 = prims.convert_element_type(t1851, dtypes.float32)  # t1853: "cuda:0 f32[1, 512, 11008]"
  t1854 = ltorch.mul(t1852, t1853)  # t1854: "cuda:0 f32[1, 512, 11008]"
    # t1854 = prims.mul(t1852, t1853)  # t1854: "cuda:0 f32[1, 512, 11008]"
  t1855 = prims.convert_element_type(t1854, dtypes.bfloat16)  # t1855: "cuda:0 bf16[1, 512, 11008]"
  t1856 = prims.convert_element_type(t1855, dtypes.float32)  # t1856: "cuda:0 f32[1, 512, 11008]"
  t1857 = prims.convert_element_type(t1845, dtypes.float32)  # t1857: "cuda:0 f32[1, 512, 11008]"
  t1858 = ltorch.mul(t1856, t1857)  # t1858: "cuda:0 f32[1, 512, 11008]"
    # t1858 = prims.mul(t1856, t1857)  # t1858: "cuda:0 f32[1, 512, 11008]"
  t1859 = prims.convert_element_type(t1858, dtypes.bfloat16)  # t1859: "cuda:0 bf16[1, 512, 11008]"
  t1860 = prims.linear(t1859, t_transformer_h_12_mlp_proj_weight, None)  # t1860: "cuda:0 bf16[1, 512, 4096]"
  t1861 = prims.convert_element_type(t1860, dtypes.float32)  # t1861: "cuda:0 f32[1, 512, 4096]"
  t1862 = prims.convert_element_type(t1825, dtypes.float32)  # t1862: "cuda:0 f32[1, 512, 4096]"
  t1863 = ltorch.add(t1861, t1862, alpha=None)  # t1863: "cuda:0 f32[1, 512, 4096]"
    # t1863 = prims.add(t1861, t1862)  # t1863: "cuda:0 f32[1, 512, 4096]"
  t1864 = prims.convert_element_type(t1863, dtypes.bfloat16)  # t1864: "cuda:0 bf16[1, 512, 4096]"
  t1865 = prims.convert_element_type(t1864, dtypes.float32)  # t1865: "cuda:0 f32[1, 512, 4096]"
  t1866 = ltorch.mul(t1865, t1865)  # t1866: "cuda:0 f32[1, 512, 4096]"
    # t1866 = prims.mul(t1865, t1865)  # t1866: "cuda:0 f32[1, 512, 4096]"
  t1868 = prims.sum(t1866, (2,))  # t1868: "cuda:0 f32[1, 512]"
  t1869 = prims.broadcast_in_dim(t1868, [1, 512, 1], [0, 1])  # t1869: "cuda:0 f32[1, 512, 1]"
  t1871 = ltorch.true_divide(t1869, 4096.0)  # t1871: "cuda:0 f32[1, 512, 1]"
    # t1871 = prims.div(t1869, 4096.0)  # t1871: "cuda:0 f32[1, 512, 1]"
  t1873 = ltorch.add(t1871, 1e-05, alpha=None)  # t1873: "cuda:0 f32[1, 512, 1]"
    # t1873 = prims.add(t1871, 1e-05)  # t1873: "cuda:0 f32[1, 512, 1]"
  t1874 = prims.rsqrt(t1873)  # t1874: "cuda:0 f32[1, 512, 1]"
  t1875 = prims.broadcast_in_dim(t1874, (1, 512, 4096), (0, 1, 2))  # t1875: "cuda:0 f32[1, 512, 4096]"
  t1876 = ltorch.mul(t1865, t1875)  # t1876: "cuda:0 f32[1, 512, 4096]"
    # t1876 = prims.mul(t1865, t1875)  # t1876: "cuda:0 f32[1, 512, 4096]"
  t1877 = prims.convert_element_type(t1876, dtypes.bfloat16)  # t1877: "cuda:0 bf16[1, 512, 4096]"
  t1878 = prims.broadcast_in_dim(t_transformer_h_13_norm_1_weight, (1, 512, 4096), (2,))  # t1878: "cuda:0 bf16[1, 512, 4096]"
  t1879 = prims.convert_element_type(t1877, dtypes.float32)  # t1879: "cuda:0 f32[1, 512, 4096]"
  t1880 = prims.convert_element_type(t1878, dtypes.float32)  # t1880: "cuda:0 f32[1, 512, 4096]"
  t1881 = ltorch.mul(t1879, t1880)  # t1881: "cuda:0 f32[1, 512, 4096]"
    # t1881 = prims.mul(t1879, t1880)  # t1881: "cuda:0 f32[1, 512, 4096]"
  t1882 = prims.convert_element_type(t1881, dtypes.bfloat16)  # t1882: "cuda:0 bf16[1, 512, 4096]"
  t1883 = prims.linear(t1882, t_transformer_h_13_attn_attn_weight, None)  # t1883: "cuda:0 bf16[1, 512, 12288]"
  t1889 = prims.reshape(t1883, (1, 512, 32, 3, 128))  # t1889: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1895 = prims.transpose(t1889, (0, 2, 3, 1, 4))  # t1895: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1896, t1897, t1898) = ltorch.split(t1895, (1, 1, 1), 2)
    # t1896 = prims.slice_prim(t1895, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1896: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1897 = prims.slice_prim(t1895, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1897: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1898 = prims.slice_prim(t1895, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1898: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1904 = prims.reshape(t1896, (1, 32, 512, 128))  # t1904: "cuda:0 bf16[1, 32, 512, 128]"
  t1910 = prims.reshape(t1897, (1, 32, 512, 128))  # t1910: "cuda:0 bf16[1, 32, 512, 128]"
  t1916 = prims.reshape(t1898, (1, 32, 512, 128))  # t1916: "cuda:0 bf16[1, 32, 512, 128]"
  t1917 = prims.slice_prim(t1904, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1917: "cuda:0 bf16[1, 32, 512, 128]"
  t1918 = prims.slice_prim(t1917, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1918: "cuda:0 bf16[1, 32, 512, 64]"
  t1919 = prims.slice_prim(t1917, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1919: "cuda:0 bf16[1, 32, 512, 64]"
  t1920 = prims.convert_element_type(t1919, dtypes.float32)  # t1920: "cuda:0 f32[1, 32, 512, 64]"
  t1921 = prims.neg(t1920)  # t1921: "cuda:0 f32[1, 32, 512, 64]"
  t1922 = prims.convert_element_type(t1921, dtypes.bfloat16)  # t1922: "cuda:0 bf16[1, 32, 512, 64]"
  t1924 = prims.cat((t1922, t1918), -1)  # t1924: "cuda:0 bf16[1, 32, 512, 128]"
  t1925 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1925: "cuda:0 f32[1, 32, 512, 128]"
  t1926 = prims.convert_element_type(t1917, dtypes.float32)  # t1926: "cuda:0 f32[1, 32, 512, 128]"
  t1927 = ltorch.mul(t1926, t1925)  # t1927: "cuda:0 f32[1, 32, 512, 128]"
    # t1927 = prims.mul(t1926, t1925)  # t1927: "cuda:0 f32[1, 32, 512, 128]"
  t1928 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1928: "cuda:0 f32[1, 32, 512, 128]"
  t1929 = prims.convert_element_type(t1924, dtypes.float32)  # t1929: "cuda:0 f32[1, 32, 512, 128]"
  t1930 = ltorch.mul(t1929, t1928)  # t1930: "cuda:0 f32[1, 32, 512, 128]"
    # t1930 = prims.mul(t1929, t1928)  # t1930: "cuda:0 f32[1, 32, 512, 128]"
  t1931 = ltorch.add(t1927, t1930, alpha=None)  # t1931: "cuda:0 f32[1, 32, 512, 128]"
    # t1931 = prims.add(t1927, t1930)  # t1931: "cuda:0 f32[1, 32, 512, 128]"
  t1932 = prims.convert_element_type(t1931, dtypes.bfloat16)  # t1932: "cuda:0 bf16[1, 32, 512, 128]"
  t1933 = prims.slice_prim(t1910, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1933: "cuda:0 bf16[1, 32, 512, 128]"
  t1934 = prims.slice_prim(t1933, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1934: "cuda:0 bf16[1, 32, 512, 64]"
  t1935 = prims.slice_prim(t1933, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1935: "cuda:0 bf16[1, 32, 512, 64]"
  t1936 = prims.convert_element_type(t1935, dtypes.float32)  # t1936: "cuda:0 f32[1, 32, 512, 64]"
  t1937 = prims.neg(t1936)  # t1937: "cuda:0 f32[1, 32, 512, 64]"
  t1938 = prims.convert_element_type(t1937, dtypes.bfloat16)  # t1938: "cuda:0 bf16[1, 32, 512, 64]"
  t1940 = prims.cat((t1938, t1934), -1)  # t1940: "cuda:0 bf16[1, 32, 512, 128]"
  t1941 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1941: "cuda:0 f32[1, 32, 512, 128]"
  t1942 = prims.convert_element_type(t1933, dtypes.float32)  # t1942: "cuda:0 f32[1, 32, 512, 128]"
  t1943 = ltorch.mul(t1942, t1941)  # t1943: "cuda:0 f32[1, 32, 512, 128]"
    # t1943 = prims.mul(t1942, t1941)  # t1943: "cuda:0 f32[1, 32, 512, 128]"
  t1944 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1944: "cuda:0 f32[1, 32, 512, 128]"
  t1945 = prims.convert_element_type(t1940, dtypes.float32)  # t1945: "cuda:0 f32[1, 32, 512, 128]"
  t1946 = ltorch.mul(t1945, t1944)  # t1946: "cuda:0 f32[1, 32, 512, 128]"
    # t1946 = prims.mul(t1945, t1944)  # t1946: "cuda:0 f32[1, 32, 512, 128]"
  t1947 = ltorch.add(t1943, t1946, alpha=None)  # t1947: "cuda:0 f32[1, 32, 512, 128]"
    # t1947 = prims.add(t1943, t1946)  # t1947: "cuda:0 f32[1, 32, 512, 128]"
  t1948 = prims.convert_element_type(t1947, dtypes.bfloat16)  # t1948: "cuda:0 bf16[1, 32, 512, 128]"
  t1949 = prims.slice_prim(t1904, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1949: "cuda:0 bf16[1, 32, 512, 0]"
  t1951 = prims.cat((t1932, t1949), -1)  # t1951: "cuda:0 bf16[1, 32, 512, 128]"
  t1952 = prims.slice_prim(t1910, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1952: "cuda:0 bf16[1, 32, 512, 0]"
  t1954 = prims.cat((t1948, t1952), -1)  # t1954: "cuda:0 bf16[1, 32, 512, 128]"
  (t1955, t1956, t1957, t1958) = cudnn_sdpa_fwd(t1951, t1954, t1916, None, 0.0, True, scale=0.08838834764831843)
  t1961 = prims.transpose(t1955, (0, 2, 1, 3))  # t1961: "cuda:0 bf16[1, 512, 32, 128]"
  t1965 = prims.reshape(t1961, (1, 512, 4096))  # t1965: "cuda:0 bf16[1, 512, 4096]"
  t1966 = prims.linear(t1965, t_transformer_h_13_attn_proj_weight, None)  # t1966: "cuda:0 bf16[1, 512, 4096]"
  t1967 = prims.convert_element_type(t1966, dtypes.float32)  # t1967: "cuda:0 f32[1, 512, 4096]"
  t1968 = prims.convert_element_type(t1864, dtypes.float32)  # t1968: "cuda:0 f32[1, 512, 4096]"
  t1969 = ltorch.add(t1967, t1968, alpha=None)  # t1969: "cuda:0 f32[1, 512, 4096]"
    # t1969 = prims.add(t1967, t1968)  # t1969: "cuda:0 f32[1, 512, 4096]"
  t1970 = prims.convert_element_type(t1969, dtypes.bfloat16)  # t1970: "cuda:0 bf16[1, 512, 4096]"
  t1971 = prims.convert_element_type(t1970, dtypes.float32)  # t1971: "cuda:0 f32[1, 512, 4096]"
  t1972 = ltorch.mul(t1971, t1971)  # t1972: "cuda:0 f32[1, 512, 4096]"
    # t1972 = prims.mul(t1971, t1971)  # t1972: "cuda:0 f32[1, 512, 4096]"
  t1974 = prims.sum(t1972, (2,))  # t1974: "cuda:0 f32[1, 512]"
  t1975 = prims.broadcast_in_dim(t1974, [1, 512, 1], [0, 1])  # t1975: "cuda:0 f32[1, 512, 1]"
  t1977 = ltorch.true_divide(t1975, 4096.0)  # t1977: "cuda:0 f32[1, 512, 1]"
    # t1977 = prims.div(t1975, 4096.0)  # t1977: "cuda:0 f32[1, 512, 1]"
  t1979 = ltorch.add(t1977, 1e-05, alpha=None)  # t1979: "cuda:0 f32[1, 512, 1]"
    # t1979 = prims.add(t1977, 1e-05)  # t1979: "cuda:0 f32[1, 512, 1]"
  t1980 = prims.rsqrt(t1979)  # t1980: "cuda:0 f32[1, 512, 1]"
  t1981 = prims.broadcast_in_dim(t1980, (1, 512, 4096), (0, 1, 2))  # t1981: "cuda:0 f32[1, 512, 4096]"
  t1982 = ltorch.mul(t1971, t1981)  # t1982: "cuda:0 f32[1, 512, 4096]"
    # t1982 = prims.mul(t1971, t1981)  # t1982: "cuda:0 f32[1, 512, 4096]"
  t1983 = prims.convert_element_type(t1982, dtypes.bfloat16)  # t1983: "cuda:0 bf16[1, 512, 4096]"
  t1984 = prims.broadcast_in_dim(t_transformer_h_13_norm_2_weight, (1, 512, 4096), (2,))  # t1984: "cuda:0 bf16[1, 512, 4096]"
  t1985 = prims.convert_element_type(t1983, dtypes.float32)  # t1985: "cuda:0 f32[1, 512, 4096]"
  t1986 = prims.convert_element_type(t1984, dtypes.float32)  # t1986: "cuda:0 f32[1, 512, 4096]"
  t1987 = ltorch.mul(t1985, t1986)  # t1987: "cuda:0 f32[1, 512, 4096]"
    # t1987 = prims.mul(t1985, t1986)  # t1987: "cuda:0 f32[1, 512, 4096]"
  t1988 = prims.convert_element_type(t1987, dtypes.bfloat16)  # t1988: "cuda:0 bf16[1, 512, 4096]"
  t1989 = prims.linear(t1988, t_transformer_h_13_mlp_fc_1_weight, None)  # t1989: "cuda:0 bf16[1, 512, 11008]"
  t1990 = prims.linear(t1988, t_transformer_h_13_mlp_fc_2_weight, None)  # t1990: "cuda:0 bf16[1, 512, 11008]"
  t1991 = prims.convert_element_type(t1989, dtypes.float32)  # t1991: "cuda:0 f32[1, 512, 11008]"
  t1992 = prims.neg(t1991)  # t1992: "cuda:0 f32[1, 512, 11008]"
  t1993 = prims.exp(t1992)  # t1993: "cuda:0 f32[1, 512, 11008]"
  t1994 = ltorch.add(1.0, t1993, alpha=None)  # t1994: "cuda:0 f32[1, 512, 11008]"
    # t1994 = prims.add(1.0, t1993)  # t1994: "cuda:0 f32[1, 512, 11008]"
  t1995 = prims.reciprocal(t1994)  # t1995: "cuda:0 f32[1, 512, 11008]"
  t1996 = prims.convert_element_type(t1995, dtypes.bfloat16)  # t1996: "cuda:0 bf16[1, 512, 11008]"
  t1997 = prims.convert_element_type(t1989, dtypes.float32)  # t1997: "cuda:0 f32[1, 512, 11008]"
  t1998 = prims.convert_element_type(t1996, dtypes.float32)  # t1998: "cuda:0 f32[1, 512, 11008]"
  t1999 = ltorch.mul(t1997, t1998)  # t1999: "cuda:0 f32[1, 512, 11008]"
    # t1999 = prims.mul(t1997, t1998)  # t1999: "cuda:0 f32[1, 512, 11008]"
  t2000 = prims.convert_element_type(t1999, dtypes.bfloat16)  # t2000: "cuda:0 bf16[1, 512, 11008]"
  t2001 = prims.convert_element_type(t2000, dtypes.float32)  # t2001: "cuda:0 f32[1, 512, 11008]"
  t2002 = prims.convert_element_type(t1990, dtypes.float32)  # t2002: "cuda:0 f32[1, 512, 11008]"
  t2003 = ltorch.mul(t2001, t2002)  # t2003: "cuda:0 f32[1, 512, 11008]"
    # t2003 = prims.mul(t2001, t2002)  # t2003: "cuda:0 f32[1, 512, 11008]"
  t2004 = prims.convert_element_type(t2003, dtypes.bfloat16)  # t2004: "cuda:0 bf16[1, 512, 11008]"
  t2005 = prims.linear(t2004, t_transformer_h_13_mlp_proj_weight, None)  # t2005: "cuda:0 bf16[1, 512, 4096]"
  t2006 = prims.convert_element_type(t2005, dtypes.float32)  # t2006: "cuda:0 f32[1, 512, 4096]"
  t2007 = prims.convert_element_type(t1970, dtypes.float32)  # t2007: "cuda:0 f32[1, 512, 4096]"
  t2008 = ltorch.add(t2006, t2007, alpha=None)  # t2008: "cuda:0 f32[1, 512, 4096]"
    # t2008 = prims.add(t2006, t2007)  # t2008: "cuda:0 f32[1, 512, 4096]"
  t2009 = prims.convert_element_type(t2008, dtypes.bfloat16)  # t2009: "cuda:0 bf16[1, 512, 4096]"
  t2010 = prims.convert_element_type(t2009, dtypes.float32)  # t2010: "cuda:0 f32[1, 512, 4096]"
  t2011 = ltorch.mul(t2010, t2010)  # t2011: "cuda:0 f32[1, 512, 4096]"
    # t2011 = prims.mul(t2010, t2010)  # t2011: "cuda:0 f32[1, 512, 4096]"
  t2013 = prims.sum(t2011, (2,))  # t2013: "cuda:0 f32[1, 512]"
  t2014 = prims.broadcast_in_dim(t2013, [1, 512, 1], [0, 1])  # t2014: "cuda:0 f32[1, 512, 1]"
  t2016 = ltorch.true_divide(t2014, 4096.0)  # t2016: "cuda:0 f32[1, 512, 1]"
    # t2016 = prims.div(t2014, 4096.0)  # t2016: "cuda:0 f32[1, 512, 1]"
  t2018 = ltorch.add(t2016, 1e-05, alpha=None)  # t2018: "cuda:0 f32[1, 512, 1]"
    # t2018 = prims.add(t2016, 1e-05)  # t2018: "cuda:0 f32[1, 512, 1]"
  t2019 = prims.rsqrt(t2018)  # t2019: "cuda:0 f32[1, 512, 1]"
  t2020 = prims.broadcast_in_dim(t2019, (1, 512, 4096), (0, 1, 2))  # t2020: "cuda:0 f32[1, 512, 4096]"
  t2021 = ltorch.mul(t2010, t2020)  # t2021: "cuda:0 f32[1, 512, 4096]"
    # t2021 = prims.mul(t2010, t2020)  # t2021: "cuda:0 f32[1, 512, 4096]"
  t2022 = prims.convert_element_type(t2021, dtypes.bfloat16)  # t2022: "cuda:0 bf16[1, 512, 4096]"
  t2023 = prims.broadcast_in_dim(t_transformer_h_14_norm_1_weight, (1, 512, 4096), (2,))  # t2023: "cuda:0 bf16[1, 512, 4096]"
  t2024 = prims.convert_element_type(t2022, dtypes.float32)  # t2024: "cuda:0 f32[1, 512, 4096]"
  t2025 = prims.convert_element_type(t2023, dtypes.float32)  # t2025: "cuda:0 f32[1, 512, 4096]"
  t2026 = ltorch.mul(t2024, t2025)  # t2026: "cuda:0 f32[1, 512, 4096]"
    # t2026 = prims.mul(t2024, t2025)  # t2026: "cuda:0 f32[1, 512, 4096]"
  t2027 = prims.convert_element_type(t2026, dtypes.bfloat16)  # t2027: "cuda:0 bf16[1, 512, 4096]"
  t2028 = prims.linear(t2027, t_transformer_h_14_attn_attn_weight, None)  # t2028: "cuda:0 bf16[1, 512, 12288]"
  t2034 = prims.reshape(t2028, (1, 512, 32, 3, 128))  # t2034: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t2040 = prims.transpose(t2034, (0, 2, 3, 1, 4))  # t2040: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t2041, t2042, t2043) = ltorch.split(t2040, (1, 1, 1), 2)
    # t2041 = prims.slice_prim(t2040, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2041: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2042 = prims.slice_prim(t2040, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2042: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2043 = prims.slice_prim(t2040, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2043: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t2049 = prims.reshape(t2041, (1, 32, 512, 128))  # t2049: "cuda:0 bf16[1, 32, 512, 128]"
  t2055 = prims.reshape(t2042, (1, 32, 512, 128))  # t2055: "cuda:0 bf16[1, 32, 512, 128]"
  t2061 = prims.reshape(t2043, (1, 32, 512, 128))  # t2061: "cuda:0 bf16[1, 32, 512, 128]"
  t2062 = prims.slice_prim(t2049, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2062: "cuda:0 bf16[1, 32, 512, 128]"
  t2063 = prims.slice_prim(t2062, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2063: "cuda:0 bf16[1, 32, 512, 64]"
  t2064 = prims.slice_prim(t2062, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2064: "cuda:0 bf16[1, 32, 512, 64]"
  t2065 = prims.convert_element_type(t2064, dtypes.float32)  # t2065: "cuda:0 f32[1, 32, 512, 64]"
  t2066 = prims.neg(t2065)  # t2066: "cuda:0 f32[1, 32, 512, 64]"
  t2067 = prims.convert_element_type(t2066, dtypes.bfloat16)  # t2067: "cuda:0 bf16[1, 32, 512, 64]"
  t2069 = prims.cat((t2067, t2063), -1)  # t2069: "cuda:0 bf16[1, 32, 512, 128]"
  t2070 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2070: "cuda:0 f32[1, 32, 512, 128]"
  t2071 = prims.convert_element_type(t2062, dtypes.float32)  # t2071: "cuda:0 f32[1, 32, 512, 128]"
  t2072 = ltorch.mul(t2071, t2070)  # t2072: "cuda:0 f32[1, 32, 512, 128]"
    # t2072 = prims.mul(t2071, t2070)  # t2072: "cuda:0 f32[1, 32, 512, 128]"
  t2073 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2073: "cuda:0 f32[1, 32, 512, 128]"
  t2074 = prims.convert_element_type(t2069, dtypes.float32)  # t2074: "cuda:0 f32[1, 32, 512, 128]"
  t2075 = ltorch.mul(t2074, t2073)  # t2075: "cuda:0 f32[1, 32, 512, 128]"
    # t2075 = prims.mul(t2074, t2073)  # t2075: "cuda:0 f32[1, 32, 512, 128]"
  t2076 = ltorch.add(t2072, t2075, alpha=None)  # t2076: "cuda:0 f32[1, 32, 512, 128]"
    # t2076 = prims.add(t2072, t2075)  # t2076: "cuda:0 f32[1, 32, 512, 128]"
  t2077 = prims.convert_element_type(t2076, dtypes.bfloat16)  # t2077: "cuda:0 bf16[1, 32, 512, 128]"
  t2078 = prims.slice_prim(t2055, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2078: "cuda:0 bf16[1, 32, 512, 128]"
  t2079 = prims.slice_prim(t2078, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2079: "cuda:0 bf16[1, 32, 512, 64]"
  t2080 = prims.slice_prim(t2078, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2080: "cuda:0 bf16[1, 32, 512, 64]"
  t2081 = prims.convert_element_type(t2080, dtypes.float32)  # t2081: "cuda:0 f32[1, 32, 512, 64]"
  t2082 = prims.neg(t2081)  # t2082: "cuda:0 f32[1, 32, 512, 64]"
  t2083 = prims.convert_element_type(t2082, dtypes.bfloat16)  # t2083: "cuda:0 bf16[1, 32, 512, 64]"
  t2085 = prims.cat((t2083, t2079), -1)  # t2085: "cuda:0 bf16[1, 32, 512, 128]"
  t2086 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2086: "cuda:0 f32[1, 32, 512, 128]"
  t2087 = prims.convert_element_type(t2078, dtypes.float32)  # t2087: "cuda:0 f32[1, 32, 512, 128]"
  t2088 = ltorch.mul(t2087, t2086)  # t2088: "cuda:0 f32[1, 32, 512, 128]"
    # t2088 = prims.mul(t2087, t2086)  # t2088: "cuda:0 f32[1, 32, 512, 128]"
  t2089 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2089: "cuda:0 f32[1, 32, 512, 128]"
  t2090 = prims.convert_element_type(t2085, dtypes.float32)  # t2090: "cuda:0 f32[1, 32, 512, 128]"
  t2091 = ltorch.mul(t2090, t2089)  # t2091: "cuda:0 f32[1, 32, 512, 128]"
    # t2091 = prims.mul(t2090, t2089)  # t2091: "cuda:0 f32[1, 32, 512, 128]"
  t2092 = ltorch.add(t2088, t2091, alpha=None)  # t2092: "cuda:0 f32[1, 32, 512, 128]"
    # t2092 = prims.add(t2088, t2091)  # t2092: "cuda:0 f32[1, 32, 512, 128]"
  t2093 = prims.convert_element_type(t2092, dtypes.bfloat16)  # t2093: "cuda:0 bf16[1, 32, 512, 128]"
  t2094 = prims.slice_prim(t2049, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2094: "cuda:0 bf16[1, 32, 512, 0]"
  t2096 = prims.cat((t2077, t2094), -1)  # t2096: "cuda:0 bf16[1, 32, 512, 128]"
  t2097 = prims.slice_prim(t2055, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2097: "cuda:0 bf16[1, 32, 512, 0]"
  t2099 = prims.cat((t2093, t2097), -1)  # t2099: "cuda:0 bf16[1, 32, 512, 128]"
  (t2100, t2101, t2102, t2103) = cudnn_sdpa_fwd(t2096, t2099, t2061, None, 0.0, True, scale=0.08838834764831843)
  t2106 = prims.transpose(t2100, (0, 2, 1, 3))  # t2106: "cuda:0 bf16[1, 512, 32, 128]"
  t2110 = prims.reshape(t2106, (1, 512, 4096))  # t2110: "cuda:0 bf16[1, 512, 4096]"
  t2111 = prims.linear(t2110, t_transformer_h_14_attn_proj_weight, None)  # t2111: "cuda:0 bf16[1, 512, 4096]"
  t2112 = prims.convert_element_type(t2111, dtypes.float32)  # t2112: "cuda:0 f32[1, 512, 4096]"
  t2113 = prims.convert_element_type(t2009, dtypes.float32)  # t2113: "cuda:0 f32[1, 512, 4096]"
  t2114 = ltorch.add(t2112, t2113, alpha=None)  # t2114: "cuda:0 f32[1, 512, 4096]"
    # t2114 = prims.add(t2112, t2113)  # t2114: "cuda:0 f32[1, 512, 4096]"
  t2115 = prims.convert_element_type(t2114, dtypes.bfloat16)  # t2115: "cuda:0 bf16[1, 512, 4096]"
  t2116 = prims.convert_element_type(t2115, dtypes.float32)  # t2116: "cuda:0 f32[1, 512, 4096]"
  t2117 = ltorch.mul(t2116, t2116)  # t2117: "cuda:0 f32[1, 512, 4096]"
    # t2117 = prims.mul(t2116, t2116)  # t2117: "cuda:0 f32[1, 512, 4096]"
  t2119 = prims.sum(t2117, (2,))  # t2119: "cuda:0 f32[1, 512]"
  t2120 = prims.broadcast_in_dim(t2119, [1, 512, 1], [0, 1])  # t2120: "cuda:0 f32[1, 512, 1]"
  t2122 = ltorch.true_divide(t2120, 4096.0)  # t2122: "cuda:0 f32[1, 512, 1]"
    # t2122 = prims.div(t2120, 4096.0)  # t2122: "cuda:0 f32[1, 512, 1]"
  t2124 = ltorch.add(t2122, 1e-05, alpha=None)  # t2124: "cuda:0 f32[1, 512, 1]"
    # t2124 = prims.add(t2122, 1e-05)  # t2124: "cuda:0 f32[1, 512, 1]"
  t2125 = prims.rsqrt(t2124)  # t2125: "cuda:0 f32[1, 512, 1]"
  t2126 = prims.broadcast_in_dim(t2125, (1, 512, 4096), (0, 1, 2))  # t2126: "cuda:0 f32[1, 512, 4096]"
  t2127 = ltorch.mul(t2116, t2126)  # t2127: "cuda:0 f32[1, 512, 4096]"
    # t2127 = prims.mul(t2116, t2126)  # t2127: "cuda:0 f32[1, 512, 4096]"
  t2128 = prims.convert_element_type(t2127, dtypes.bfloat16)  # t2128: "cuda:0 bf16[1, 512, 4096]"
  t2129 = prims.broadcast_in_dim(t_transformer_h_14_norm_2_weight, (1, 512, 4096), (2,))  # t2129: "cuda:0 bf16[1, 512, 4096]"
  t2130 = prims.convert_element_type(t2128, dtypes.float32)  # t2130: "cuda:0 f32[1, 512, 4096]"
  t2131 = prims.convert_element_type(t2129, dtypes.float32)  # t2131: "cuda:0 f32[1, 512, 4096]"
  t2132 = ltorch.mul(t2130, t2131)  # t2132: "cuda:0 f32[1, 512, 4096]"
    # t2132 = prims.mul(t2130, t2131)  # t2132: "cuda:0 f32[1, 512, 4096]"
  t2133 = prims.convert_element_type(t2132, dtypes.bfloat16)  # t2133: "cuda:0 bf16[1, 512, 4096]"
  t2134 = prims.linear(t2133, t_transformer_h_14_mlp_fc_1_weight, None)  # t2134: "cuda:0 bf16[1, 512, 11008]"
  t2135 = prims.linear(t2133, t_transformer_h_14_mlp_fc_2_weight, None)  # t2135: "cuda:0 bf16[1, 512, 11008]"
  t2136 = prims.convert_element_type(t2134, dtypes.float32)  # t2136: "cuda:0 f32[1, 512, 11008]"
  t2137 = prims.neg(t2136)  # t2137: "cuda:0 f32[1, 512, 11008]"
  t2138 = prims.exp(t2137)  # t2138: "cuda:0 f32[1, 512, 11008]"
  t2139 = ltorch.add(1.0, t2138, alpha=None)  # t2139: "cuda:0 f32[1, 512, 11008]"
    # t2139 = prims.add(1.0, t2138)  # t2139: "cuda:0 f32[1, 512, 11008]"
  t2140 = prims.reciprocal(t2139)  # t2140: "cuda:0 f32[1, 512, 11008]"
  t2141 = prims.convert_element_type(t2140, dtypes.bfloat16)  # t2141: "cuda:0 bf16[1, 512, 11008]"
  t2142 = prims.convert_element_type(t2134, dtypes.float32)  # t2142: "cuda:0 f32[1, 512, 11008]"
  t2143 = prims.convert_element_type(t2141, dtypes.float32)  # t2143: "cuda:0 f32[1, 512, 11008]"
  t2144 = ltorch.mul(t2142, t2143)  # t2144: "cuda:0 f32[1, 512, 11008]"
    # t2144 = prims.mul(t2142, t2143)  # t2144: "cuda:0 f32[1, 512, 11008]"
  t2145 = prims.convert_element_type(t2144, dtypes.bfloat16)  # t2145: "cuda:0 bf16[1, 512, 11008]"
  t2146 = prims.convert_element_type(t2145, dtypes.float32)  # t2146: "cuda:0 f32[1, 512, 11008]"
  t2147 = prims.convert_element_type(t2135, dtypes.float32)  # t2147: "cuda:0 f32[1, 512, 11008]"
  t2148 = ltorch.mul(t2146, t2147)  # t2148: "cuda:0 f32[1, 512, 11008]"
    # t2148 = prims.mul(t2146, t2147)  # t2148: "cuda:0 f32[1, 512, 11008]"
  t2149 = prims.convert_element_type(t2148, dtypes.bfloat16)  # t2149: "cuda:0 bf16[1, 512, 11008]"
  t2150 = prims.linear(t2149, t_transformer_h_14_mlp_proj_weight, None)  # t2150: "cuda:0 bf16[1, 512, 4096]"
  t2151 = prims.convert_element_type(t2150, dtypes.float32)  # t2151: "cuda:0 f32[1, 512, 4096]"
  t2152 = prims.convert_element_type(t2115, dtypes.float32)  # t2152: "cuda:0 f32[1, 512, 4096]"
  t2153 = ltorch.add(t2151, t2152, alpha=None)  # t2153: "cuda:0 f32[1, 512, 4096]"
    # t2153 = prims.add(t2151, t2152)  # t2153: "cuda:0 f32[1, 512, 4096]"
  t2154 = prims.convert_element_type(t2153, dtypes.bfloat16)  # t2154: "cuda:0 bf16[1, 512, 4096]"
  t2155 = prims.convert_element_type(t2154, dtypes.float32)  # t2155: "cuda:0 f32[1, 512, 4096]"
  t2156 = ltorch.mul(t2155, t2155)  # t2156: "cuda:0 f32[1, 512, 4096]"
    # t2156 = prims.mul(t2155, t2155)  # t2156: "cuda:0 f32[1, 512, 4096]"
  t2158 = prims.sum(t2156, (2,))  # t2158: "cuda:0 f32[1, 512]"
  t2159 = prims.broadcast_in_dim(t2158, [1, 512, 1], [0, 1])  # t2159: "cuda:0 f32[1, 512, 1]"
  t2161 = ltorch.true_divide(t2159, 4096.0)  # t2161: "cuda:0 f32[1, 512, 1]"
    # t2161 = prims.div(t2159, 4096.0)  # t2161: "cuda:0 f32[1, 512, 1]"
  t2163 = ltorch.add(t2161, 1e-05, alpha=None)  # t2163: "cuda:0 f32[1, 512, 1]"
    # t2163 = prims.add(t2161, 1e-05)  # t2163: "cuda:0 f32[1, 512, 1]"
  t2164 = prims.rsqrt(t2163)  # t2164: "cuda:0 f32[1, 512, 1]"
  t2165 = prims.broadcast_in_dim(t2164, (1, 512, 4096), (0, 1, 2))  # t2165: "cuda:0 f32[1, 512, 4096]"
  t2166 = ltorch.mul(t2155, t2165)  # t2166: "cuda:0 f32[1, 512, 4096]"
    # t2166 = prims.mul(t2155, t2165)  # t2166: "cuda:0 f32[1, 512, 4096]"
  t2167 = prims.convert_element_type(t2166, dtypes.bfloat16)  # t2167: "cuda:0 bf16[1, 512, 4096]"
  t2168 = prims.broadcast_in_dim(t_transformer_h_15_norm_1_weight, (1, 512, 4096), (2,))  # t2168: "cuda:0 bf16[1, 512, 4096]"
  t2169 = prims.convert_element_type(t2167, dtypes.float32)  # t2169: "cuda:0 f32[1, 512, 4096]"
  t2170 = prims.convert_element_type(t2168, dtypes.float32)  # t2170: "cuda:0 f32[1, 512, 4096]"
  t2171 = ltorch.mul(t2169, t2170)  # t2171: "cuda:0 f32[1, 512, 4096]"
    # t2171 = prims.mul(t2169, t2170)  # t2171: "cuda:0 f32[1, 512, 4096]"
  t2172 = prims.convert_element_type(t2171, dtypes.bfloat16)  # t2172: "cuda:0 bf16[1, 512, 4096]"
  t2173 = prims.linear(t2172, t_transformer_h_15_attn_attn_weight, None)  # t2173: "cuda:0 bf16[1, 512, 12288]"
  t2179 = prims.reshape(t2173, (1, 512, 32, 3, 128))  # t2179: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t2185 = prims.transpose(t2179, (0, 2, 3, 1, 4))  # t2185: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t2186, t2187, t2188) = ltorch.split(t2185, (1, 1, 1), 2)
    # t2186 = prims.slice_prim(t2185, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2186: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2187 = prims.slice_prim(t2185, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2187: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2188 = prims.slice_prim(t2185, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2188: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t2194 = prims.reshape(t2186, (1, 32, 512, 128))  # t2194: "cuda:0 bf16[1, 32, 512, 128]"
  t2200 = prims.reshape(t2187, (1, 32, 512, 128))  # t2200: "cuda:0 bf16[1, 32, 512, 128]"
  t2206 = prims.reshape(t2188, (1, 32, 512, 128))  # t2206: "cuda:0 bf16[1, 32, 512, 128]"
  t2207 = prims.slice_prim(t2194, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2207: "cuda:0 bf16[1, 32, 512, 128]"
  t2208 = prims.slice_prim(t2207, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2208: "cuda:0 bf16[1, 32, 512, 64]"
  t2209 = prims.slice_prim(t2207, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2209: "cuda:0 bf16[1, 32, 512, 64]"
  t2210 = prims.convert_element_type(t2209, dtypes.float32)  # t2210: "cuda:0 f32[1, 32, 512, 64]"
  t2211 = prims.neg(t2210)  # t2211: "cuda:0 f32[1, 32, 512, 64]"
  t2212 = prims.convert_element_type(t2211, dtypes.bfloat16)  # t2212: "cuda:0 bf16[1, 32, 512, 64]"
  t2214 = prims.cat((t2212, t2208), -1)  # t2214: "cuda:0 bf16[1, 32, 512, 128]"
  t2215 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2215: "cuda:0 f32[1, 32, 512, 128]"
  t2216 = prims.convert_element_type(t2207, dtypes.float32)  # t2216: "cuda:0 f32[1, 32, 512, 128]"
  t2217 = ltorch.mul(t2216, t2215)  # t2217: "cuda:0 f32[1, 32, 512, 128]"
    # t2217 = prims.mul(t2216, t2215)  # t2217: "cuda:0 f32[1, 32, 512, 128]"
  t2218 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2218: "cuda:0 f32[1, 32, 512, 128]"
  t2219 = prims.convert_element_type(t2214, dtypes.float32)  # t2219: "cuda:0 f32[1, 32, 512, 128]"
  t2220 = ltorch.mul(t2219, t2218)  # t2220: "cuda:0 f32[1, 32, 512, 128]"
    # t2220 = prims.mul(t2219, t2218)  # t2220: "cuda:0 f32[1, 32, 512, 128]"
  t2221 = ltorch.add(t2217, t2220, alpha=None)  # t2221: "cuda:0 f32[1, 32, 512, 128]"
    # t2221 = prims.add(t2217, t2220)  # t2221: "cuda:0 f32[1, 32, 512, 128]"
  t2222 = prims.convert_element_type(t2221, dtypes.bfloat16)  # t2222: "cuda:0 bf16[1, 32, 512, 128]"
  t2223 = prims.slice_prim(t2200, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2223: "cuda:0 bf16[1, 32, 512, 128]"
  t2224 = prims.slice_prim(t2223, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2224: "cuda:0 bf16[1, 32, 512, 64]"
  t2225 = prims.slice_prim(t2223, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2225: "cuda:0 bf16[1, 32, 512, 64]"
  t2226 = prims.convert_element_type(t2225, dtypes.float32)  # t2226: "cuda:0 f32[1, 32, 512, 64]"
  t2227 = prims.neg(t2226)  # t2227: "cuda:0 f32[1, 32, 512, 64]"
  t2228 = prims.convert_element_type(t2227, dtypes.bfloat16)  # t2228: "cuda:0 bf16[1, 32, 512, 64]"
  t2230 = prims.cat((t2228, t2224), -1)  # t2230: "cuda:0 bf16[1, 32, 512, 128]"
  t2231 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2231: "cuda:0 f32[1, 32, 512, 128]"
  t2232 = prims.convert_element_type(t2223, dtypes.float32)  # t2232: "cuda:0 f32[1, 32, 512, 128]"
  t2233 = ltorch.mul(t2232, t2231)  # t2233: "cuda:0 f32[1, 32, 512, 128]"
    # t2233 = prims.mul(t2232, t2231)  # t2233: "cuda:0 f32[1, 32, 512, 128]"
  t2234 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2234: "cuda:0 f32[1, 32, 512, 128]"
  t2235 = prims.convert_element_type(t2230, dtypes.float32)  # t2235: "cuda:0 f32[1, 32, 512, 128]"
  t2236 = ltorch.mul(t2235, t2234)  # t2236: "cuda:0 f32[1, 32, 512, 128]"
    # t2236 = prims.mul(t2235, t2234)  # t2236: "cuda:0 f32[1, 32, 512, 128]"
  t2237 = ltorch.add(t2233, t2236, alpha=None)  # t2237: "cuda:0 f32[1, 32, 512, 128]"
    # t2237 = prims.add(t2233, t2236)  # t2237: "cuda:0 f32[1, 32, 512, 128]"
  t2238 = prims.convert_element_type(t2237, dtypes.bfloat16)  # t2238: "cuda:0 bf16[1, 32, 512, 128]"
  t2239 = prims.slice_prim(t2194, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2239: "cuda:0 bf16[1, 32, 512, 0]"
  t2241 = prims.cat((t2222, t2239), -1)  # t2241: "cuda:0 bf16[1, 32, 512, 128]"
  t2242 = prims.slice_prim(t2200, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2242: "cuda:0 bf16[1, 32, 512, 0]"
  t2244 = prims.cat((t2238, t2242), -1)  # t2244: "cuda:0 bf16[1, 32, 512, 128]"
  (t2245, t2246, t2247, t2248) = cudnn_sdpa_fwd(t2241, t2244, t2206, None, 0.0, True, scale=0.08838834764831843)
  t2251 = prims.transpose(t2245, (0, 2, 1, 3))  # t2251: "cuda:0 bf16[1, 512, 32, 128]"
  t2255 = prims.reshape(t2251, (1, 512, 4096))  # t2255: "cuda:0 bf16[1, 512, 4096]"
  t2256 = prims.linear(t2255, t_transformer_h_15_attn_proj_weight, None)  # t2256: "cuda:0 bf16[1, 512, 4096]"
  t2257 = prims.convert_element_type(t2256, dtypes.float32)  # t2257: "cuda:0 f32[1, 512, 4096]"
  t2258 = prims.convert_element_type(t2154, dtypes.float32)  # t2258: "cuda:0 f32[1, 512, 4096]"
  t2259 = ltorch.add(t2257, t2258, alpha=None)  # t2259: "cuda:0 f32[1, 512, 4096]"
    # t2259 = prims.add(t2257, t2258)  # t2259: "cuda:0 f32[1, 512, 4096]"
  t2260 = prims.convert_element_type(t2259, dtypes.bfloat16)  # t2260: "cuda:0 bf16[1, 512, 4096]"
  t2261 = prims.convert_element_type(t2260, dtypes.float32)  # t2261: "cuda:0 f32[1, 512, 4096]"
  t2262 = ltorch.mul(t2261, t2261)  # t2262: "cuda:0 f32[1, 512, 4096]"
    # t2262 = prims.mul(t2261, t2261)  # t2262: "cuda:0 f32[1, 512, 4096]"
  t2264 = prims.sum(t2262, (2,))  # t2264: "cuda:0 f32[1, 512]"
  t2265 = prims.broadcast_in_dim(t2264, [1, 512, 1], [0, 1])  # t2265: "cuda:0 f32[1, 512, 1]"
  t2267 = ltorch.true_divide(t2265, 4096.0)  # t2267: "cuda:0 f32[1, 512, 1]"
    # t2267 = prims.div(t2265, 4096.0)  # t2267: "cuda:0 f32[1, 512, 1]"
  t2269 = ltorch.add(t2267, 1e-05, alpha=None)  # t2269: "cuda:0 f32[1, 512, 1]"
    # t2269 = prims.add(t2267, 1e-05)  # t2269: "cuda:0 f32[1, 512, 1]"
  t2270 = prims.rsqrt(t2269)  # t2270: "cuda:0 f32[1, 512, 1]"
  t2271 = prims.broadcast_in_dim(t2270, (1, 512, 4096), (0, 1, 2))  # t2271: "cuda:0 f32[1, 512, 4096]"
  t2272 = ltorch.mul(t2261, t2271)  # t2272: "cuda:0 f32[1, 512, 4096]"
    # t2272 = prims.mul(t2261, t2271)  # t2272: "cuda:0 f32[1, 512, 4096]"
  t2273 = prims.convert_element_type(t2272, dtypes.bfloat16)  # t2273: "cuda:0 bf16[1, 512, 4096]"
  t2274 = prims.broadcast_in_dim(t_transformer_h_15_norm_2_weight, (1, 512, 4096), (2,))  # t2274: "cuda:0 bf16[1, 512, 4096]"
  t2275 = prims.convert_element_type(t2273, dtypes.float32)  # t2275: "cuda:0 f32[1, 512, 4096]"
  t2276 = prims.convert_element_type(t2274, dtypes.float32)  # t2276: "cuda:0 f32[1, 512, 4096]"
  t2277 = ltorch.mul(t2275, t2276)  # t2277: "cuda:0 f32[1, 512, 4096]"
    # t2277 = prims.mul(t2275, t2276)  # t2277: "cuda:0 f32[1, 512, 4096]"
  t2278 = prims.convert_element_type(t2277, dtypes.bfloat16)  # t2278: "cuda:0 bf16[1, 512, 4096]"
  t2279 = prims.linear(t2278, t_transformer_h_15_mlp_fc_1_weight, None)  # t2279: "cuda:0 bf16[1, 512, 11008]"
  t2280 = prims.linear(t2278, t_transformer_h_15_mlp_fc_2_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
  t2281 = prims.convert_element_type(t2279, dtypes.float32)  # t2281: "cuda:0 f32[1, 512, 11008]"
  t2282 = prims.neg(t2281)  # t2282: "cuda:0 f32[1, 512, 11008]"
  t2283 = prims.exp(t2282)  # t2283: "cuda:0 f32[1, 512, 11008]"
  t2284 = ltorch.add(1.0, t2283, alpha=None)  # t2284: "cuda:0 f32[1, 512, 11008]"
    # t2284 = prims.add(1.0, t2283)  # t2284: "cuda:0 f32[1, 512, 11008]"
  t2285 = prims.reciprocal(t2284)  # t2285: "cuda:0 f32[1, 512, 11008]"
  t2286 = prims.convert_element_type(t2285, dtypes.bfloat16)  # t2286: "cuda:0 bf16[1, 512, 11008]"
  t2287 = prims.convert_element_type(t2279, dtypes.float32)  # t2287: "cuda:0 f32[1, 512, 11008]"
  t2288 = prims.convert_element_type(t2286, dtypes.float32)  # t2288: "cuda:0 f32[1, 512, 11008]"
  t2289 = ltorch.mul(t2287, t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
    # t2289 = prims.mul(t2287, t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
  t2290 = prims.convert_element_type(t2289, dtypes.bfloat16)  # t2290: "cuda:0 bf16[1, 512, 11008]"
  t2291 = prims.convert_element_type(t2290, dtypes.float32)  # t2291: "cuda:0 f32[1, 512, 11008]"
  t2292 = prims.convert_element_type(t2280, dtypes.float32)  # t2292: "cuda:0 f32[1, 512, 11008]"
  t2293 = ltorch.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
    # t2293 = prims.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
  t2294 = prims.convert_element_type(t2293, dtypes.bfloat16)  # t2294: "cuda:0 bf16[1, 512, 11008]"
  t2295 = prims.linear(t2294, t_transformer_h_15_mlp_proj_weight, None)  # t2295: "cuda:0 bf16[1, 512, 4096]"
  t2296 = prims.convert_element_type(t2295, dtypes.float32)  # t2296: "cuda:0 f32[1, 512, 4096]"
  t2297 = prims.convert_element_type(t2260, dtypes.float32)  # t2297: "cuda:0 f32[1, 512, 4096]"
  t2298 = ltorch.add(t2296, t2297, alpha=None)  # t2298: "cuda:0 f32[1, 512, 4096]"
    # t2298 = prims.add(t2296, t2297)  # t2298: "cuda:0 f32[1, 512, 4096]"
  t2299 = prims.convert_element_type(t2298, dtypes.bfloat16)  # t2299: "cuda:0 bf16[1, 512, 4096]"
  t2300 = prims.convert_element_type(t2299, dtypes.float32)  # t2300: "cuda:0 f32[1, 512, 4096]"
  t2301 = ltorch.mul(t2300, t2300)  # t2301: "cuda:0 f32[1, 512, 4096]"
    # t2301 = prims.mul(t2300, t2300)  # t2301: "cuda:0 f32[1, 512, 4096]"
  t2303 = prims.sum(t2301, (2,))  # t2303: "cuda:0 f32[1, 512]"
  t2304 = prims.broadcast_in_dim(t2303, [1, 512, 1], [0, 1])  # t2304: "cuda:0 f32[1, 512, 1]"
  t2306 = ltorch.true_divide(t2304, 4096.0)  # t2306: "cuda:0 f32[1, 512, 1]"
    # t2306 = prims.div(t2304, 4096.0)  # t2306: "cuda:0 f32[1, 512, 1]"
  t2308 = ltorch.add(t2306, 1e-05, alpha=None)  # t2308: "cuda:0 f32[1, 512, 1]"
    # t2308 = prims.add(t2306, 1e-05)  # t2308: "cuda:0 f32[1, 512, 1]"
  t2309 = prims.rsqrt(t2308)  # t2309: "cuda:0 f32[1, 512, 1]"
  t2310 = prims.broadcast_in_dim(t2309, (1, 512, 4096), (0, 1, 2))  # t2310: "cuda:0 f32[1, 512, 4096]"
  t2311 = ltorch.mul(t2300, t2310)  # t2311: "cuda:0 f32[1, 512, 4096]"
    # t2311 = prims.mul(t2300, t2310)  # t2311: "cuda:0 f32[1, 512, 4096]"
  t2312 = prims.convert_element_type(t2311, dtypes.bfloat16)  # t2312: "cuda:0 bf16[1, 512, 4096]"
  t2313 = prims.broadcast_in_dim(t_transformer_ln_f_weight, (1, 512, 4096), (2,))  # t2313: "cuda:0 bf16[1, 512, 4096]"
  t2314 = prims.convert_element_type(t2312, dtypes.float32)  # t2314: "cuda:0 f32[1, 512, 4096]"
  t2315 = prims.convert_element_type(t2313, dtypes.float32)  # t2315: "cuda:0 f32[1, 512, 4096]"
  t2316 = ltorch.mul(t2314, t2315)  # t2316: "cuda:0 f32[1, 512, 4096]"
    # t2316 = prims.mul(t2314, t2315)  # t2316: "cuda:0 f32[1, 512, 4096]"
  t2317 = prims.convert_element_type(t2316, dtypes.bfloat16)  # t2317: "cuda:0 bf16[1, 512, 4096]"
  t2318 = prims.linear(t2317, t_lm_head_weight, None)  # t2318: "cuda:0 bf16[1, 512, 32000]"
  return {'output': t2318, 'flat_args': [idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight], 'flat_output': (t2318,)}, ((idx, t5, t11, t12, t17, t16, t19, t_transformer_h_0_attn_attn_weight, t46, t47, t49, t50, t62, t63, t65, t66, t71, t74, t38, t75, t76, t77, t78, t80, t_transformer_h_0_attn_proj_weight, t86, t95, t96, t101, t100, t103, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t108, t110, t113, t112, t117, t116, t119, t_transformer_h_0_mlp_proj_weight, t125, t134, t135, t140, t139, t142, t_transformer_h_1_attn_attn_weight, t185, t186, t188, t189, t201, t202, t204, t205, t211, t214, t176, t215, t216, t217, t218, t225, t_transformer_h_1_attn_proj_weight, t231, t240, t241, t246, t245, t248, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t253, t255, t258, t257, t262, t261, t264, t_transformer_h_1_mlp_proj_weight, t270, t279, t280, t285, t284, t287, t_transformer_h_2_attn_attn_weight, t330, t331, t333, t334, t346, t347, t349, t350, t356, t359, t321, t360, t361, t362, t363, t370, t_transformer_h_2_attn_proj_weight, t376, t385, t386, t391, t390, t393, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t398, t400, t403, t402, t407, t406, t409, t_transformer_h_2_mlp_proj_weight, t415, t424, t425, t430, t429, t432, t_transformer_h_3_attn_attn_weight, t475, t476, t478, t479, t491, t492, t494, t495, t501, t504, t466, t505, t506, t507, t508, t515, t_transformer_h_3_attn_proj_weight, t521, t530, t531, t536, t535, t538, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t543, t545, t548, t547, t552, t551, t554, t_transformer_h_3_mlp_proj_weight, t560, t569, t570, t575, t574, t577, t_transformer_h_4_attn_attn_weight, t620, t621, t623, t624, t636, t637, t639, t640, t646, t649, t611, t650, t651, t652, t653, t660, t_transformer_h_4_attn_proj_weight, t666, t675, t676, t681, t680, t683, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t688, t690, t693, t692, t697, t696, t699, t_transformer_h_4_mlp_proj_weight, t705, t714, t715, t720, t719, t722, t_transformer_h_5_attn_attn_weight, t765, t766, t768, t769, t781, t782, t784, t785, t791, t794, t756, t795, t796, t797, t798, t805, t_transformer_h_5_attn_proj_weight, t811, t820, t821, t826, t825, t828, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t833, t835, t838, t837, t842, t841, t844, t_transformer_h_5_mlp_proj_weight, t850, t859, t860, t865, t864, t867, t_transformer_h_6_attn_attn_weight, t910, t911, t913, t914, t926, t927, t929, t930, t936, t939, t901, t940, t941, t942, t943, t950, t_transformer_h_6_attn_proj_weight, t956, t965, t966, t971, t970, t973, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t978, t980, t983, t982, t987, t986, t989, t_transformer_h_6_mlp_proj_weight, t995, t1004, t1005, t1010, t1009, t1012, t_transformer_h_7_attn_attn_weight, t1055, t1056, t1058, t1059, t1071, t1072, t1074, t1075, t1081, t1084, t1046, t1085, t1086, t1087, t1088, t1095, t_transformer_h_7_attn_proj_weight, t1101, t1110, t1111, t1116, t1115, t1118, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t1123, t1125, t1128, t1127, t1132, t1131, t1134, t_transformer_h_7_mlp_proj_weight, t1140, t1149, t1150, t1155, t1154, t1157, t_transformer_h_8_attn_attn_weight, t1200, t1201, t1203, t1204, t1216, t1217, t1219, t1220, t1226, t1229, t1191, t1230, t1231, t1232, t1233, t1240, t_transformer_h_8_attn_proj_weight, t1246, t1255, t1256, t1261, t1260, t1263, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t1268, t1270, t1273, t1272, t1277, t1276, t1279, t_transformer_h_8_mlp_proj_weight, t1285, t1294, t1295, t1300, t1299, t1302, t_transformer_h_9_attn_attn_weight, t1345, t1346, t1348, t1349, t1361, t1362, t1364, t1365, t1371, t1374, t1336, t1375, t1376, t1377, t1378, t1385, t_transformer_h_9_attn_proj_weight, t1391, t1400, t1401, t1406, t1405, t1408, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t1413, t1415, t1418, t1417, t1422, t1421, t1424, t_transformer_h_9_mlp_proj_weight, t1430, t1439, t1440, t1445, t1444, t1447, t_transformer_h_10_attn_attn_weight, t1490, t1491, t1493, t1494, t1506, t1507, t1509, t1510, t1516, t1519, t1481, t1520, t1521, t1522, t1523, t1530, t_transformer_h_10_attn_proj_weight, t1536, t1545, t1546, t1551, t1550, t1553, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t1558, t1560, t1563, t1562, t1567, t1566, t1569, t_transformer_h_10_mlp_proj_weight, t1575, t1584, t1585, t1590, t1589, t1592, t_transformer_h_11_attn_attn_weight, t1635, t1636, t1638, t1639, t1651, t1652, t1654, t1655, t1661, t1664, t1626, t1665, t1666, t1667, t1668, t1675, t_transformer_h_11_attn_proj_weight, t1681, t1690, t1691, t1696, t1695, t1698, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t1703, t1705, t1708, t1707, t1712, t1711, t1714, t_transformer_h_11_mlp_proj_weight, t1720, t1729, t1730, t1735, t1734, t1737, t_transformer_h_12_attn_attn_weight, t1780, t1781, t1783, t1784, t1796, t1797, t1799, t1800, t1806, t1809, t1771, t1810, t1811, t1812, t1813, t1820, t_transformer_h_12_attn_proj_weight, t1826, t1835, t1836, t1841, t1840, t1843, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t1848, t1850, t1853, t1852, t1857, t1856, t1859, t_transformer_h_12_mlp_proj_weight, t1865, t1874, t1875, t1880, t1879, t1882, t_transformer_h_13_attn_attn_weight, t1925, t1926, t1928, t1929, t1941, t1942, t1944, t1945, t1951, t1954, t1916, t1955, t1956, t1957, t1958, t1965, t_transformer_h_13_attn_proj_weight, t1971, t1980, t1981, t1986, t1985, t1988, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t1993, t1995, t1998, t1997, t2002, t2001, t2004, t_transformer_h_13_mlp_proj_weight, t2010, t2019, t2020, t2025, t2024, t2027, t_transformer_h_14_attn_attn_weight, t2070, t2071, t2073, t2074, t2086, t2087, t2089, t2090, t2096, t2099, t2061, t2100, t2101, t2102, t2103, t2110, t_transformer_h_14_attn_proj_weight, t2116, t2125, t2126, t2131, t2130, t2133, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t2138, t2140, t2143, t2142, t2147, t2146, t2149, t_transformer_h_14_mlp_proj_weight, t2155, t2164, t2165, t2170, t2169, t2172, t_transformer_h_15_attn_attn_weight, t2215, t2216, t2218, t2219, t2231, t2232, t2234, t2235, t2241, t2244, t2206, t2245, t2246, t2247, t2248, t2255, t_transformer_h_15_attn_proj_weight, t2261, t2270, t2271, t2276, t2275, t2278, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t2283, t2285, t2288, t2287, t2292, t2291, t2294, t_transformer_h_15_mlp_proj_weight, t2300, t2309, t2310, t2315, t2314, t2317, t_lm_head_weight), (32000, False, False, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0))
============================================ END: after forward_trc transform_for_execution
============================================ START: LABEL forward_trc
============================================ START: before _transform_for_operator_executor_execution
# Constructed by Dead Code Elimination (took 9 milliseconds)
import thunder
import thunder.core.dtypes as dtypes
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight):
  # idx: "cuda:0 i64[1, 512]"
  # tos1: "cuda:0 f32[4096, 128]"
  # t_lm_head_weight: "cuda:0 bf16[32000, 4096]"
  # t_sin: "cuda:0 f32[4096, 128]"
  # t_transformer_h_0_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_0_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_0_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_0_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_0_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_1_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_1_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_1_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_2_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_2_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_2_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_3_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_3_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_3_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_4_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_4_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_4_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_5_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_5_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_5_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_6_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_6_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_6_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_7_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_7_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_7_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_8_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_8_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_8_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_9_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_9_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_9_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_10_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_10_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_10_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_11_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_11_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_11_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_12_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_12_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_12_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_13_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_13_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_13_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_14_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_14_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_14_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_15_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_15_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_15_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_ln_f_weight: "cuda:0 bf16[4096]"
  # t_transformer_wte_weight: "cuda:0 bf16[32000, 4096]"
  t0 = prims.slice_prim(tos1, [0, 0], [512, 128], [1, 1])  # t0: "cuda:0 f32[512, 128]"
  t1 = prims.slice_prim(t_sin, [0, 0], [512, 128], [1, 1])  # t1: "cuda:0 f32[512, 128]"
  t4 = ltorch.embedding(idx, t_transformer_wte_weight, None, None, 2.0, False, False)  # t4: "cuda:0 bf16[1, 512, 4096]"
    # t2 = ltorch.reshape(idx, [512])  # t2: "cuda:0 i64[512]"
      # t2 = prims.reshape(idx, (512,))  # t2: "cuda:0 i64[512]"
    # t3 = prims.take(t_transformer_wte_weight, t2, 0)  # t3: "cuda:0 bf16[512, 4096]"
    # t4 = ltorch.reshape(t3, [1, 512, 4096])  # t4: "cuda:0 bf16[1, 512, 4096]"
      # t4 = prims.reshape(t3, (1, 512, 4096))  # t4: "cuda:0 bf16[1, 512, 4096]"
  t5 = prims.convert_element_type(t4, dtypes.float32)  # t5: "cuda:0 f32[1, 512, 4096]"
  t6 = ltorch.mul(t5, t5)  # t6: "cuda:0 f32[1, 512, 4096]"
    # t6 = prims.mul(t5, t5)  # t6: "cuda:0 f32[1, 512, 4096]"
  t7 = prims.sum(t6, (2,))  # t7: "cuda:0 f32[1, 512]"
  t8 = prims.broadcast_in_dim(t7, [1, 512, 1], [0, 1])  # t8: "cuda:0 f32[1, 512, 1]"
  t9 = ltorch.true_divide(t8, 4096.0)  # t9: "cuda:0 f32[1, 512, 1]"
    # t9 = prims.div(t8, 4096.0)  # t9: "cuda:0 f32[1, 512, 1]"
  t10 = ltorch.add(t9, 1e-05, alpha=None)  # t10: "cuda:0 f32[1, 512, 1]"
    # t10 = prims.add(t9, 1e-05)  # t10: "cuda:0 f32[1, 512, 1]"
  t11 = prims.rsqrt(t10)  # t11: "cuda:0 f32[1, 512, 1]"
  t12 = prims.broadcast_in_dim(t11, (1, 512, 4096), (0, 1, 2))  # t12: "cuda:0 f32[1, 512, 4096]"
  t13 = ltorch.mul(t5, t12)  # t13: "cuda:0 f32[1, 512, 4096]"
    # t13 = prims.mul(t5, t12)  # t13: "cuda:0 f32[1, 512, 4096]"
  t14 = prims.convert_element_type(t13, dtypes.bfloat16)  # t14: "cuda:0 bf16[1, 512, 4096]"
  t15 = prims.broadcast_in_dim(t_transformer_h_0_norm_1_weight, (1, 512, 4096), (2,))  # t15: "cuda:0 bf16[1, 512, 4096]"
  t16 = prims.convert_element_type(t14, dtypes.float32)  # t16: "cuda:0 f32[1, 512, 4096]"
  t17 = prims.convert_element_type(t15, dtypes.float32)  # t17: "cuda:0 f32[1, 512, 4096]"
  t18 = ltorch.mul(t16, t17)  # t18: "cuda:0 f32[1, 512, 4096]"
    # t18 = prims.mul(t16, t17)  # t18: "cuda:0 f32[1, 512, 4096]"
  t19 = prims.convert_element_type(t18, dtypes.bfloat16)  # t19: "cuda:0 bf16[1, 512, 4096]"
  t20 = prims.linear(t19, t_transformer_h_0_attn_attn_weight, None)  # t20: "cuda:0 bf16[1, 512, 12288]"
  t21 = prims.reshape(t20, (1, 512, 32, 3, 128))  # t21: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t22 = prims.transpose(t21, (0, 2, 3, 1, 4))  # t22: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t23, t24, t25) = ltorch.split(t22, (1, 1, 1), 2)
    # t23 = prims.slice_prim(t22, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t23: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t24 = prims.slice_prim(t22, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t24: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t25 = prims.slice_prim(t22, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t25: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t26 = prims.reshape(t23, (1, 32, 512, 128))  # t26: "cuda:0 bf16[1, 32, 512, 128]"
  t32 = prims.reshape(t24, (1, 32, 512, 128))  # t32: "cuda:0 bf16[1, 32, 512, 128]"
  t38 = prims.reshape(t25, (1, 32, 512, 128))  # t38: "cuda:0 bf16[1, 32, 512, 128]"
  t39 = prims.slice_prim(t26, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t39: "cuda:0 bf16[1, 32, 512, 128]"
  t40 = prims.slice_prim(t39, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t40: "cuda:0 bf16[1, 32, 512, 64]"
  t41 = prims.slice_prim(t39, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t41: "cuda:0 bf16[1, 32, 512, 64]"
  t42 = prims.convert_element_type(t41, dtypes.float32)  # t42: "cuda:0 f32[1, 32, 512, 64]"
  t43 = prims.neg(t42)  # t43: "cuda:0 f32[1, 32, 512, 64]"
  t44 = prims.convert_element_type(t43, dtypes.bfloat16)  # t44: "cuda:0 bf16[1, 32, 512, 64]"
  t45 = prims.cat((t44, t40), -1)  # t45: "cuda:0 bf16[1, 32, 512, 128]"
  t46 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t46: "cuda:0 f32[1, 32, 512, 128]"
  t47 = prims.convert_element_type(t39, dtypes.float32)  # t47: "cuda:0 f32[1, 32, 512, 128]"
  t48 = ltorch.mul(t47, t46)  # t48: "cuda:0 f32[1, 32, 512, 128]"
    # t48 = prims.mul(t47, t46)  # t48: "cuda:0 f32[1, 32, 512, 128]"
  t49 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t49: "cuda:0 f32[1, 32, 512, 128]"
  t50 = prims.convert_element_type(t45, dtypes.float32)  # t50: "cuda:0 f32[1, 32, 512, 128]"
  t51 = ltorch.mul(t50, t49)  # t51: "cuda:0 f32[1, 32, 512, 128]"
    # t51 = prims.mul(t50, t49)  # t51: "cuda:0 f32[1, 32, 512, 128]"
  t52 = ltorch.add(t48, t51, alpha=None)  # t52: "cuda:0 f32[1, 32, 512, 128]"
    # t52 = prims.add(t48, t51)  # t52: "cuda:0 f32[1, 32, 512, 128]"
  t53 = prims.convert_element_type(t52, dtypes.bfloat16)  # t53: "cuda:0 bf16[1, 32, 512, 128]"
  t54 = prims.slice_prim(t32, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t54: "cuda:0 bf16[1, 32, 512, 128]"
  t55 = prims.slice_prim(t54, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t55: "cuda:0 bf16[1, 32, 512, 64]"
  t56 = prims.slice_prim(t54, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t56: "cuda:0 bf16[1, 32, 512, 64]"
  t57 = prims.convert_element_type(t56, dtypes.float32)  # t57: "cuda:0 f32[1, 32, 512, 64]"
  t58 = prims.neg(t57)  # t58: "cuda:0 f32[1, 32, 512, 64]"
  t59 = prims.convert_element_type(t58, dtypes.bfloat16)  # t59: "cuda:0 bf16[1, 32, 512, 64]"
  t61 = prims.cat((t59, t55), -1)  # t61: "cuda:0 bf16[1, 32, 512, 128]"
  t62 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t62: "cuda:0 f32[1, 32, 512, 128]"
  t63 = prims.convert_element_type(t54, dtypes.float32)  # t63: "cuda:0 f32[1, 32, 512, 128]"
  t64 = ltorch.mul(t63, t62)  # t64: "cuda:0 f32[1, 32, 512, 128]"
    # t64 = prims.mul(t63, t62)  # t64: "cuda:0 f32[1, 32, 512, 128]"
  t65 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t65: "cuda:0 f32[1, 32, 512, 128]"
  t66 = prims.convert_element_type(t61, dtypes.float32)  # t66: "cuda:0 f32[1, 32, 512, 128]"
  t67 = ltorch.mul(t66, t65)  # t67: "cuda:0 f32[1, 32, 512, 128]"
    # t67 = prims.mul(t66, t65)  # t67: "cuda:0 f32[1, 32, 512, 128]"
  t68 = ltorch.add(t64, t67, alpha=None)  # t68: "cuda:0 f32[1, 32, 512, 128]"
    # t68 = prims.add(t64, t67)  # t68: "cuda:0 f32[1, 32, 512, 128]"
  t69 = prims.convert_element_type(t68, dtypes.bfloat16)  # t69: "cuda:0 bf16[1, 32, 512, 128]"
  t70 = prims.slice_prim(t26, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t70: "cuda:0 bf16[1, 32, 512, 0]"
  t71 = prims.cat((t53, t70), -1)  # t71: "cuda:0 bf16[1, 32, 512, 128]"
  t72 = prims.slice_prim(t32, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t72: "cuda:0 bf16[1, 32, 512, 0]"
  t74 = prims.cat((t69, t72), -1)  # t74: "cuda:0 bf16[1, 32, 512, 128]"
  (t75, t76, t77, t78) = cudnn_sdpa_fwd(t71, t74, t38, None, 0.0, True, scale=0.08838834764831843)
  t79 = prims.transpose(t75, (0, 2, 1, 3))  # t79: "cuda:0 bf16[1, 512, 32, 128]"
  t80 = prims.reshape(t79, (1, 512, 4096))  # t80: "cuda:0 bf16[1, 512, 4096]"
  t81 = prims.linear(t80, t_transformer_h_0_attn_proj_weight, None)  # t81: "cuda:0 bf16[1, 512, 4096]"
  t82 = prims.convert_element_type(t81, dtypes.float32)  # t82: "cuda:0 f32[1, 512, 4096]"
  t83 = prims.convert_element_type(t4, dtypes.float32)  # t83: "cuda:0 f32[1, 512, 4096]"
  t84 = ltorch.add(t82, t83, alpha=None)  # t84: "cuda:0 f32[1, 512, 4096]"
    # t84 = prims.add(t82, t83)  # t84: "cuda:0 f32[1, 512, 4096]"
  t85 = prims.convert_element_type(t84, dtypes.bfloat16)  # t85: "cuda:0 bf16[1, 512, 4096]"
  t86 = prims.convert_element_type(t85, dtypes.float32)  # t86: "cuda:0 f32[1, 512, 4096]"
  t87 = ltorch.mul(t86, t86)  # t87: "cuda:0 f32[1, 512, 4096]"
    # t87 = prims.mul(t86, t86)  # t87: "cuda:0 f32[1, 512, 4096]"
  t89 = prims.sum(t87, (2,))  # t89: "cuda:0 f32[1, 512]"
  t90 = prims.broadcast_in_dim(t89, [1, 512, 1], [0, 1])  # t90: "cuda:0 f32[1, 512, 1]"
  t92 = ltorch.true_divide(t90, 4096.0)  # t92: "cuda:0 f32[1, 512, 1]"
    # t92 = prims.div(t90, 4096.0)  # t92: "cuda:0 f32[1, 512, 1]"
  t94 = ltorch.add(t92, 1e-05, alpha=None)  # t94: "cuda:0 f32[1, 512, 1]"
    # t94 = prims.add(t92, 1e-05)  # t94: "cuda:0 f32[1, 512, 1]"
  t95 = prims.rsqrt(t94)  # t95: "cuda:0 f32[1, 512, 1]"
  t96 = prims.broadcast_in_dim(t95, (1, 512, 4096), (0, 1, 2))  # t96: "cuda:0 f32[1, 512, 4096]"
  t97 = ltorch.mul(t86, t96)  # t97: "cuda:0 f32[1, 512, 4096]"
    # t97 = prims.mul(t86, t96)  # t97: "cuda:0 f32[1, 512, 4096]"
  t98 = prims.convert_element_type(t97, dtypes.bfloat16)  # t98: "cuda:0 bf16[1, 512, 4096]"
  t99 = prims.broadcast_in_dim(t_transformer_h_0_norm_2_weight, (1, 512, 4096), (2,))  # t99: "cuda:0 bf16[1, 512, 4096]"
  t100 = prims.convert_element_type(t98, dtypes.float32)  # t100: "cuda:0 f32[1, 512, 4096]"
  t101 = prims.convert_element_type(t99, dtypes.float32)  # t101: "cuda:0 f32[1, 512, 4096]"
  t102 = ltorch.mul(t100, t101)  # t102: "cuda:0 f32[1, 512, 4096]"
    # t102 = prims.mul(t100, t101)  # t102: "cuda:0 f32[1, 512, 4096]"
  t103 = prims.convert_element_type(t102, dtypes.bfloat16)  # t103: "cuda:0 bf16[1, 512, 4096]"
  t104 = prims.linear(t103, t_transformer_h_0_mlp_fc_1_weight, None)  # t104: "cuda:0 bf16[1, 512, 11008]"
  t105 = prims.linear(t103, t_transformer_h_0_mlp_fc_2_weight, None)  # t105: "cuda:0 bf16[1, 512, 11008]"
  t106 = prims.convert_element_type(t104, dtypes.float32)  # t106: "cuda:0 f32[1, 512, 11008]"
  t107 = prims.neg(t106)  # t107: "cuda:0 f32[1, 512, 11008]"
  t108 = prims.exp(t107)  # t108: "cuda:0 f32[1, 512, 11008]"
  t109 = ltorch.add(1.0, t108, alpha=None)  # t109: "cuda:0 f32[1, 512, 11008]"
    # t109 = prims.add(1.0, t108)  # t109: "cuda:0 f32[1, 512, 11008]"
  t110 = prims.reciprocal(t109)  # t110: "cuda:0 f32[1, 512, 11008]"
  t111 = prims.convert_element_type(t110, dtypes.bfloat16)  # t111: "cuda:0 bf16[1, 512, 11008]"
  t112 = prims.convert_element_type(t104, dtypes.float32)  # t112: "cuda:0 f32[1, 512, 11008]"
  t113 = prims.convert_element_type(t111, dtypes.float32)  # t113: "cuda:0 f32[1, 512, 11008]"
  t114 = ltorch.mul(t112, t113)  # t114: "cuda:0 f32[1, 512, 11008]"
    # t114 = prims.mul(t112, t113)  # t114: "cuda:0 f32[1, 512, 11008]"
  t115 = prims.convert_element_type(t114, dtypes.bfloat16)  # t115: "cuda:0 bf16[1, 512, 11008]"
  t116 = prims.convert_element_type(t115, dtypes.float32)  # t116: "cuda:0 f32[1, 512, 11008]"
  t117 = prims.convert_element_type(t105, dtypes.float32)  # t117: "cuda:0 f32[1, 512, 11008]"
  t118 = ltorch.mul(t116, t117)  # t118: "cuda:0 f32[1, 512, 11008]"
    # t118 = prims.mul(t116, t117)  # t118: "cuda:0 f32[1, 512, 11008]"
  t119 = prims.convert_element_type(t118, dtypes.bfloat16)  # t119: "cuda:0 bf16[1, 512, 11008]"
  t120 = prims.linear(t119, t_transformer_h_0_mlp_proj_weight, None)  # t120: "cuda:0 bf16[1, 512, 4096]"
  t121 = prims.convert_element_type(t120, dtypes.float32)  # t121: "cuda:0 f32[1, 512, 4096]"
  t122 = prims.convert_element_type(t85, dtypes.float32)  # t122: "cuda:0 f32[1, 512, 4096]"
  t123 = ltorch.add(t121, t122, alpha=None)  # t123: "cuda:0 f32[1, 512, 4096]"
    # t123 = prims.add(t121, t122)  # t123: "cuda:0 f32[1, 512, 4096]"
  t124 = prims.convert_element_type(t123, dtypes.bfloat16)  # t124: "cuda:0 bf16[1, 512, 4096]"
  t125 = prims.convert_element_type(t124, dtypes.float32)  # t125: "cuda:0 f32[1, 512, 4096]"
  t126 = ltorch.mul(t125, t125)  # t126: "cuda:0 f32[1, 512, 4096]"
    # t126 = prims.mul(t125, t125)  # t126: "cuda:0 f32[1, 512, 4096]"
  t128 = prims.sum(t126, (2,))  # t128: "cuda:0 f32[1, 512]"
  t129 = prims.broadcast_in_dim(t128, [1, 512, 1], [0, 1])  # t129: "cuda:0 f32[1, 512, 1]"
  t131 = ltorch.true_divide(t129, 4096.0)  # t131: "cuda:0 f32[1, 512, 1]"
    # t131 = prims.div(t129, 4096.0)  # t131: "cuda:0 f32[1, 512, 1]"
  t133 = ltorch.add(t131, 1e-05, alpha=None)  # t133: "cuda:0 f32[1, 512, 1]"
    # t133 = prims.add(t131, 1e-05)  # t133: "cuda:0 f32[1, 512, 1]"
  t134 = prims.rsqrt(t133)  # t134: "cuda:0 f32[1, 512, 1]"
  t135 = prims.broadcast_in_dim(t134, (1, 512, 4096), (0, 1, 2))  # t135: "cuda:0 f32[1, 512, 4096]"
  t136 = ltorch.mul(t125, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
    # t136 = prims.mul(t125, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
  t137 = prims.convert_element_type(t136, dtypes.bfloat16)  # t137: "cuda:0 bf16[1, 512, 4096]"
  t138 = prims.broadcast_in_dim(t_transformer_h_1_norm_1_weight, (1, 512, 4096), (2,))  # t138: "cuda:0 bf16[1, 512, 4096]"
  t139 = prims.convert_element_type(t137, dtypes.float32)  # t139: "cuda:0 f32[1, 512, 4096]"
  t140 = prims.convert_element_type(t138, dtypes.float32)  # t140: "cuda:0 f32[1, 512, 4096]"
  t141 = ltorch.mul(t139, t140)  # t141: "cuda:0 f32[1, 512, 4096]"
    # t141 = prims.mul(t139, t140)  # t141: "cuda:0 f32[1, 512, 4096]"
  t142 = prims.convert_element_type(t141, dtypes.bfloat16)  # t142: "cuda:0 bf16[1, 512, 4096]"
  t143 = prims.linear(t142, t_transformer_h_1_attn_attn_weight, None)  # t143: "cuda:0 bf16[1, 512, 12288]"
  t149 = prims.reshape(t143, (1, 512, 32, 3, 128))  # t149: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t155 = prims.transpose(t149, (0, 2, 3, 1, 4))  # t155: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t156, t157, t158) = ltorch.split(t155, (1, 1, 1), 2)
    # t156 = prims.slice_prim(t155, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t156: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t157 = prims.slice_prim(t155, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t157: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t158 = prims.slice_prim(t155, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t158: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t164 = prims.reshape(t156, (1, 32, 512, 128))  # t164: "cuda:0 bf16[1, 32, 512, 128]"
  t170 = prims.reshape(t157, (1, 32, 512, 128))  # t170: "cuda:0 bf16[1, 32, 512, 128]"
  t176 = prims.reshape(t158, (1, 32, 512, 128))  # t176: "cuda:0 bf16[1, 32, 512, 128]"
  t177 = prims.slice_prim(t164, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t177: "cuda:0 bf16[1, 32, 512, 128]"
  t178 = prims.slice_prim(t177, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t178: "cuda:0 bf16[1, 32, 512, 64]"
  t179 = prims.slice_prim(t177, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t179: "cuda:0 bf16[1, 32, 512, 64]"
  t180 = prims.convert_element_type(t179, dtypes.float32)  # t180: "cuda:0 f32[1, 32, 512, 64]"
  t181 = prims.neg(t180)  # t181: "cuda:0 f32[1, 32, 512, 64]"
  t182 = prims.convert_element_type(t181, dtypes.bfloat16)  # t182: "cuda:0 bf16[1, 32, 512, 64]"
  t184 = prims.cat((t182, t178), -1)  # t184: "cuda:0 bf16[1, 32, 512, 128]"
  t185 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t185: "cuda:0 f32[1, 32, 512, 128]"
  t186 = prims.convert_element_type(t177, dtypes.float32)  # t186: "cuda:0 f32[1, 32, 512, 128]"
  t187 = ltorch.mul(t186, t185)  # t187: "cuda:0 f32[1, 32, 512, 128]"
    # t187 = prims.mul(t186, t185)  # t187: "cuda:0 f32[1, 32, 512, 128]"
  t188 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t188: "cuda:0 f32[1, 32, 512, 128]"
  t189 = prims.convert_element_type(t184, dtypes.float32)  # t189: "cuda:0 f32[1, 32, 512, 128]"
  t190 = ltorch.mul(t189, t188)  # t190: "cuda:0 f32[1, 32, 512, 128]"
    # t190 = prims.mul(t189, t188)  # t190: "cuda:0 f32[1, 32, 512, 128]"
  t191 = ltorch.add(t187, t190, alpha=None)  # t191: "cuda:0 f32[1, 32, 512, 128]"
    # t191 = prims.add(t187, t190)  # t191: "cuda:0 f32[1, 32, 512, 128]"
  t192 = prims.convert_element_type(t191, dtypes.bfloat16)  # t192: "cuda:0 bf16[1, 32, 512, 128]"
  t193 = prims.slice_prim(t170, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t193: "cuda:0 bf16[1, 32, 512, 128]"
  t194 = prims.slice_prim(t193, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t194: "cuda:0 bf16[1, 32, 512, 64]"
  t195 = prims.slice_prim(t193, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t195: "cuda:0 bf16[1, 32, 512, 64]"
  t196 = prims.convert_element_type(t195, dtypes.float32)  # t196: "cuda:0 f32[1, 32, 512, 64]"
  t197 = prims.neg(t196)  # t197: "cuda:0 f32[1, 32, 512, 64]"
  t198 = prims.convert_element_type(t197, dtypes.bfloat16)  # t198: "cuda:0 bf16[1, 32, 512, 64]"
  t200 = prims.cat((t198, t194), -1)  # t200: "cuda:0 bf16[1, 32, 512, 128]"
  t201 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t201: "cuda:0 f32[1, 32, 512, 128]"
  t202 = prims.convert_element_type(t193, dtypes.float32)  # t202: "cuda:0 f32[1, 32, 512, 128]"
  t203 = ltorch.mul(t202, t201)  # t203: "cuda:0 f32[1, 32, 512, 128]"
    # t203 = prims.mul(t202, t201)  # t203: "cuda:0 f32[1, 32, 512, 128]"
  t204 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t204: "cuda:0 f32[1, 32, 512, 128]"
  t205 = prims.convert_element_type(t200, dtypes.float32)  # t205: "cuda:0 f32[1, 32, 512, 128]"
  t206 = ltorch.mul(t205, t204)  # t206: "cuda:0 f32[1, 32, 512, 128]"
    # t206 = prims.mul(t205, t204)  # t206: "cuda:0 f32[1, 32, 512, 128]"
  t207 = ltorch.add(t203, t206, alpha=None)  # t207: "cuda:0 f32[1, 32, 512, 128]"
    # t207 = prims.add(t203, t206)  # t207: "cuda:0 f32[1, 32, 512, 128]"
  t208 = prims.convert_element_type(t207, dtypes.bfloat16)  # t208: "cuda:0 bf16[1, 32, 512, 128]"
  t209 = prims.slice_prim(t164, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t209: "cuda:0 bf16[1, 32, 512, 0]"
  t211 = prims.cat((t192, t209), -1)  # t211: "cuda:0 bf16[1, 32, 512, 128]"
  t212 = prims.slice_prim(t170, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t212: "cuda:0 bf16[1, 32, 512, 0]"
  t214 = prims.cat((t208, t212), -1)  # t214: "cuda:0 bf16[1, 32, 512, 128]"
  (t215, t216, t217, t218) = cudnn_sdpa_fwd(t211, t214, t176, None, 0.0, True, scale=0.08838834764831843)
  t221 = prims.transpose(t215, (0, 2, 1, 3))  # t221: "cuda:0 bf16[1, 512, 32, 128]"
  t225 = prims.reshape(t221, (1, 512, 4096))  # t225: "cuda:0 bf16[1, 512, 4096]"
  t226 = prims.linear(t225, t_transformer_h_1_attn_proj_weight, None)  # t226: "cuda:0 bf16[1, 512, 4096]"
  t227 = prims.convert_element_type(t226, dtypes.float32)  # t227: "cuda:0 f32[1, 512, 4096]"
  t228 = prims.convert_element_type(t124, dtypes.float32)  # t228: "cuda:0 f32[1, 512, 4096]"
  t229 = ltorch.add(t227, t228, alpha=None)  # t229: "cuda:0 f32[1, 512, 4096]"
    # t229 = prims.add(t227, t228)  # t229: "cuda:0 f32[1, 512, 4096]"
  t230 = prims.convert_element_type(t229, dtypes.bfloat16)  # t230: "cuda:0 bf16[1, 512, 4096]"
  t231 = prims.convert_element_type(t230, dtypes.float32)  # t231: "cuda:0 f32[1, 512, 4096]"
  t232 = ltorch.mul(t231, t231)  # t232: "cuda:0 f32[1, 512, 4096]"
    # t232 = prims.mul(t231, t231)  # t232: "cuda:0 f32[1, 512, 4096]"
  t234 = prims.sum(t232, (2,))  # t234: "cuda:0 f32[1, 512]"
  t235 = prims.broadcast_in_dim(t234, [1, 512, 1], [0, 1])  # t235: "cuda:0 f32[1, 512, 1]"
  t237 = ltorch.true_divide(t235, 4096.0)  # t237: "cuda:0 f32[1, 512, 1]"
    # t237 = prims.div(t235, 4096.0)  # t237: "cuda:0 f32[1, 512, 1]"
  t239 = ltorch.add(t237, 1e-05, alpha=None)  # t239: "cuda:0 f32[1, 512, 1]"
    # t239 = prims.add(t237, 1e-05)  # t239: "cuda:0 f32[1, 512, 1]"
  t240 = prims.rsqrt(t239)  # t240: "cuda:0 f32[1, 512, 1]"
  t241 = prims.broadcast_in_dim(t240, (1, 512, 4096), (0, 1, 2))  # t241: "cuda:0 f32[1, 512, 4096]"
  t242 = ltorch.mul(t231, t241)  # t242: "cuda:0 f32[1, 512, 4096]"
    # t242 = prims.mul(t231, t241)  # t242: "cuda:0 f32[1, 512, 4096]"
  t243 = prims.convert_element_type(t242, dtypes.bfloat16)  # t243: "cuda:0 bf16[1, 512, 4096]"
  t244 = prims.broadcast_in_dim(t_transformer_h_1_norm_2_weight, (1, 512, 4096), (2,))  # t244: "cuda:0 bf16[1, 512, 4096]"
  t245 = prims.convert_element_type(t243, dtypes.float32)  # t245: "cuda:0 f32[1, 512, 4096]"
  t246 = prims.convert_element_type(t244, dtypes.float32)  # t246: "cuda:0 f32[1, 512, 4096]"
  t247 = ltorch.mul(t245, t246)  # t247: "cuda:0 f32[1, 512, 4096]"
    # t247 = prims.mul(t245, t246)  # t247: "cuda:0 f32[1, 512, 4096]"
  t248 = prims.convert_element_type(t247, dtypes.bfloat16)  # t248: "cuda:0 bf16[1, 512, 4096]"
  t249 = prims.linear(t248, t_transformer_h_1_mlp_fc_1_weight, None)  # t249: "cuda:0 bf16[1, 512, 11008]"
  t250 = prims.linear(t248, t_transformer_h_1_mlp_fc_2_weight, None)  # t250: "cuda:0 bf16[1, 512, 11008]"
  t251 = prims.convert_element_type(t249, dtypes.float32)  # t251: "cuda:0 f32[1, 512, 11008]"
  t252 = prims.neg(t251)  # t252: "cuda:0 f32[1, 512, 11008]"
  t253 = prims.exp(t252)  # t253: "cuda:0 f32[1, 512, 11008]"
  t254 = ltorch.add(1.0, t253, alpha=None)  # t254: "cuda:0 f32[1, 512, 11008]"
    # t254 = prims.add(1.0, t253)  # t254: "cuda:0 f32[1, 512, 11008]"
  t255 = prims.reciprocal(t254)  # t255: "cuda:0 f32[1, 512, 11008]"
  t256 = prims.convert_element_type(t255, dtypes.bfloat16)  # t256: "cuda:0 bf16[1, 512, 11008]"
  t257 = prims.convert_element_type(t249, dtypes.float32)  # t257: "cuda:0 f32[1, 512, 11008]"
  t258 = prims.convert_element_type(t256, dtypes.float32)  # t258: "cuda:0 f32[1, 512, 11008]"
  t259 = ltorch.mul(t257, t258)  # t259: "cuda:0 f32[1, 512, 11008]"
    # t259 = prims.mul(t257, t258)  # t259: "cuda:0 f32[1, 512, 11008]"
  t260 = prims.convert_element_type(t259, dtypes.bfloat16)  # t260: "cuda:0 bf16[1, 512, 11008]"
  t261 = prims.convert_element_type(t260, dtypes.float32)  # t261: "cuda:0 f32[1, 512, 11008]"
  t262 = prims.convert_element_type(t250, dtypes.float32)  # t262: "cuda:0 f32[1, 512, 11008]"
  t263 = ltorch.mul(t261, t262)  # t263: "cuda:0 f32[1, 512, 11008]"
    # t263 = prims.mul(t261, t262)  # t263: "cuda:0 f32[1, 512, 11008]"
  t264 = prims.convert_element_type(t263, dtypes.bfloat16)  # t264: "cuda:0 bf16[1, 512, 11008]"
  t265 = prims.linear(t264, t_transformer_h_1_mlp_proj_weight, None)  # t265: "cuda:0 bf16[1, 512, 4096]"
  t266 = prims.convert_element_type(t265, dtypes.float32)  # t266: "cuda:0 f32[1, 512, 4096]"
  t267 = prims.convert_element_type(t230, dtypes.float32)  # t267: "cuda:0 f32[1, 512, 4096]"
  t268 = ltorch.add(t266, t267, alpha=None)  # t268: "cuda:0 f32[1, 512, 4096]"
    # t268 = prims.add(t266, t267)  # t268: "cuda:0 f32[1, 512, 4096]"
  t269 = prims.convert_element_type(t268, dtypes.bfloat16)  # t269: "cuda:0 bf16[1, 512, 4096]"
  t270 = prims.convert_element_type(t269, dtypes.float32)  # t270: "cuda:0 f32[1, 512, 4096]"
  t271 = ltorch.mul(t270, t270)  # t271: "cuda:0 f32[1, 512, 4096]"
    # t271 = prims.mul(t270, t270)  # t271: "cuda:0 f32[1, 512, 4096]"
  t273 = prims.sum(t271, (2,))  # t273: "cuda:0 f32[1, 512]"
  t274 = prims.broadcast_in_dim(t273, [1, 512, 1], [0, 1])  # t274: "cuda:0 f32[1, 512, 1]"
  t276 = ltorch.true_divide(t274, 4096.0)  # t276: "cuda:0 f32[1, 512, 1]"
    # t276 = prims.div(t274, 4096.0)  # t276: "cuda:0 f32[1, 512, 1]"
  t278 = ltorch.add(t276, 1e-05, alpha=None)  # t278: "cuda:0 f32[1, 512, 1]"
    # t278 = prims.add(t276, 1e-05)  # t278: "cuda:0 f32[1, 512, 1]"
  t279 = prims.rsqrt(t278)  # t279: "cuda:0 f32[1, 512, 1]"
  t280 = prims.broadcast_in_dim(t279, (1, 512, 4096), (0, 1, 2))  # t280: "cuda:0 f32[1, 512, 4096]"
  t281 = ltorch.mul(t270, t280)  # t281: "cuda:0 f32[1, 512, 4096]"
    # t281 = prims.mul(t270, t280)  # t281: "cuda:0 f32[1, 512, 4096]"
  t282 = prims.convert_element_type(t281, dtypes.bfloat16)  # t282: "cuda:0 bf16[1, 512, 4096]"
  t283 = prims.broadcast_in_dim(t_transformer_h_2_norm_1_weight, (1, 512, 4096), (2,))  # t283: "cuda:0 bf16[1, 512, 4096]"
  t284 = prims.convert_element_type(t282, dtypes.float32)  # t284: "cuda:0 f32[1, 512, 4096]"
  t285 = prims.convert_element_type(t283, dtypes.float32)  # t285: "cuda:0 f32[1, 512, 4096]"
  t286 = ltorch.mul(t284, t285)  # t286: "cuda:0 f32[1, 512, 4096]"
    # t286 = prims.mul(t284, t285)  # t286: "cuda:0 f32[1, 512, 4096]"
  t287 = prims.convert_element_type(t286, dtypes.bfloat16)  # t287: "cuda:0 bf16[1, 512, 4096]"
  t288 = prims.linear(t287, t_transformer_h_2_attn_attn_weight, None)  # t288: "cuda:0 bf16[1, 512, 12288]"
  t294 = prims.reshape(t288, (1, 512, 32, 3, 128))  # t294: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t300 = prims.transpose(t294, (0, 2, 3, 1, 4))  # t300: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t301, t302, t303) = ltorch.split(t300, (1, 1, 1), 2)
    # t301 = prims.slice_prim(t300, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t301: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t302 = prims.slice_prim(t300, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t302: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t303 = prims.slice_prim(t300, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t303: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t309 = prims.reshape(t301, (1, 32, 512, 128))  # t309: "cuda:0 bf16[1, 32, 512, 128]"
  t315 = prims.reshape(t302, (1, 32, 512, 128))  # t315: "cuda:0 bf16[1, 32, 512, 128]"
  t321 = prims.reshape(t303, (1, 32, 512, 128))  # t321: "cuda:0 bf16[1, 32, 512, 128]"
  t322 = prims.slice_prim(t309, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t322: "cuda:0 bf16[1, 32, 512, 128]"
  t323 = prims.slice_prim(t322, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t323: "cuda:0 bf16[1, 32, 512, 64]"
  t324 = prims.slice_prim(t322, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t324: "cuda:0 bf16[1, 32, 512, 64]"
  t325 = prims.convert_element_type(t324, dtypes.float32)  # t325: "cuda:0 f32[1, 32, 512, 64]"
  t326 = prims.neg(t325)  # t326: "cuda:0 f32[1, 32, 512, 64]"
  t327 = prims.convert_element_type(t326, dtypes.bfloat16)  # t327: "cuda:0 bf16[1, 32, 512, 64]"
  t329 = prims.cat((t327, t323), -1)  # t329: "cuda:0 bf16[1, 32, 512, 128]"
  t330 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t330: "cuda:0 f32[1, 32, 512, 128]"
  t331 = prims.convert_element_type(t322, dtypes.float32)  # t331: "cuda:0 f32[1, 32, 512, 128]"
  t332 = ltorch.mul(t331, t330)  # t332: "cuda:0 f32[1, 32, 512, 128]"
    # t332 = prims.mul(t331, t330)  # t332: "cuda:0 f32[1, 32, 512, 128]"
  t333 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t333: "cuda:0 f32[1, 32, 512, 128]"
  t334 = prims.convert_element_type(t329, dtypes.float32)  # t334: "cuda:0 f32[1, 32, 512, 128]"
  t335 = ltorch.mul(t334, t333)  # t335: "cuda:0 f32[1, 32, 512, 128]"
    # t335 = prims.mul(t334, t333)  # t335: "cuda:0 f32[1, 32, 512, 128]"
  t336 = ltorch.add(t332, t335, alpha=None)  # t336: "cuda:0 f32[1, 32, 512, 128]"
    # t336 = prims.add(t332, t335)  # t336: "cuda:0 f32[1, 32, 512, 128]"
  t337 = prims.convert_element_type(t336, dtypes.bfloat16)  # t337: "cuda:0 bf16[1, 32, 512, 128]"
  t338 = prims.slice_prim(t315, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t338: "cuda:0 bf16[1, 32, 512, 128]"
  t339 = prims.slice_prim(t338, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t339: "cuda:0 bf16[1, 32, 512, 64]"
  t340 = prims.slice_prim(t338, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t340: "cuda:0 bf16[1, 32, 512, 64]"
  t341 = prims.convert_element_type(t340, dtypes.float32)  # t341: "cuda:0 f32[1, 32, 512, 64]"
  t342 = prims.neg(t341)  # t342: "cuda:0 f32[1, 32, 512, 64]"
  t343 = prims.convert_element_type(t342, dtypes.bfloat16)  # t343: "cuda:0 bf16[1, 32, 512, 64]"
  t345 = prims.cat((t343, t339), -1)  # t345: "cuda:0 bf16[1, 32, 512, 128]"
  t346 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t346: "cuda:0 f32[1, 32, 512, 128]"
  t347 = prims.convert_element_type(t338, dtypes.float32)  # t347: "cuda:0 f32[1, 32, 512, 128]"
  t348 = ltorch.mul(t347, t346)  # t348: "cuda:0 f32[1, 32, 512, 128]"
    # t348 = prims.mul(t347, t346)  # t348: "cuda:0 f32[1, 32, 512, 128]"
  t349 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t349: "cuda:0 f32[1, 32, 512, 128]"
  t350 = prims.convert_element_type(t345, dtypes.float32)  # t350: "cuda:0 f32[1, 32, 512, 128]"
  t351 = ltorch.mul(t350, t349)  # t351: "cuda:0 f32[1, 32, 512, 128]"
    # t351 = prims.mul(t350, t349)  # t351: "cuda:0 f32[1, 32, 512, 128]"
  t352 = ltorch.add(t348, t351, alpha=None)  # t352: "cuda:0 f32[1, 32, 512, 128]"
    # t352 = prims.add(t348, t351)  # t352: "cuda:0 f32[1, 32, 512, 128]"
  t353 = prims.convert_element_type(t352, dtypes.bfloat16)  # t353: "cuda:0 bf16[1, 32, 512, 128]"
  t354 = prims.slice_prim(t309, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t354: "cuda:0 bf16[1, 32, 512, 0]"
  t356 = prims.cat((t337, t354), -1)  # t356: "cuda:0 bf16[1, 32, 512, 128]"
  t357 = prims.slice_prim(t315, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t357: "cuda:0 bf16[1, 32, 512, 0]"
  t359 = prims.cat((t353, t357), -1)  # t359: "cuda:0 bf16[1, 32, 512, 128]"
  (t360, t361, t362, t363) = cudnn_sdpa_fwd(t356, t359, t321, None, 0.0, True, scale=0.08838834764831843)
  t366 = prims.transpose(t360, (0, 2, 1, 3))  # t366: "cuda:0 bf16[1, 512, 32, 128]"
  t370 = prims.reshape(t366, (1, 512, 4096))  # t370: "cuda:0 bf16[1, 512, 4096]"
  t371 = prims.linear(t370, t_transformer_h_2_attn_proj_weight, None)  # t371: "cuda:0 bf16[1, 512, 4096]"
  t372 = prims.convert_element_type(t371, dtypes.float32)  # t372: "cuda:0 f32[1, 512, 4096]"
  t373 = prims.convert_element_type(t269, dtypes.float32)  # t373: "cuda:0 f32[1, 512, 4096]"
  t374 = ltorch.add(t372, t373, alpha=None)  # t374: "cuda:0 f32[1, 512, 4096]"
    # t374 = prims.add(t372, t373)  # t374: "cuda:0 f32[1, 512, 4096]"
  t375 = prims.convert_element_type(t374, dtypes.bfloat16)  # t375: "cuda:0 bf16[1, 512, 4096]"
  t376 = prims.convert_element_type(t375, dtypes.float32)  # t376: "cuda:0 f32[1, 512, 4096]"
  t377 = ltorch.mul(t376, t376)  # t377: "cuda:0 f32[1, 512, 4096]"
    # t377 = prims.mul(t376, t376)  # t377: "cuda:0 f32[1, 512, 4096]"
  t379 = prims.sum(t377, (2,))  # t379: "cuda:0 f32[1, 512]"
  t380 = prims.broadcast_in_dim(t379, [1, 512, 1], [0, 1])  # t380: "cuda:0 f32[1, 512, 1]"
  t382 = ltorch.true_divide(t380, 4096.0)  # t382: "cuda:0 f32[1, 512, 1]"
    # t382 = prims.div(t380, 4096.0)  # t382: "cuda:0 f32[1, 512, 1]"
  t384 = ltorch.add(t382, 1e-05, alpha=None)  # t384: "cuda:0 f32[1, 512, 1]"
    # t384 = prims.add(t382, 1e-05)  # t384: "cuda:0 f32[1, 512, 1]"
  t385 = prims.rsqrt(t384)  # t385: "cuda:0 f32[1, 512, 1]"
  t386 = prims.broadcast_in_dim(t385, (1, 512, 4096), (0, 1, 2))  # t386: "cuda:0 f32[1, 512, 4096]"
  t387 = ltorch.mul(t376, t386)  # t387: "cuda:0 f32[1, 512, 4096]"
    # t387 = prims.mul(t376, t386)  # t387: "cuda:0 f32[1, 512, 4096]"
  t388 = prims.convert_element_type(t387, dtypes.bfloat16)  # t388: "cuda:0 bf16[1, 512, 4096]"
  t389 = prims.broadcast_in_dim(t_transformer_h_2_norm_2_weight, (1, 512, 4096), (2,))  # t389: "cuda:0 bf16[1, 512, 4096]"
  t390 = prims.convert_element_type(t388, dtypes.float32)  # t390: "cuda:0 f32[1, 512, 4096]"
  t391 = prims.convert_element_type(t389, dtypes.float32)  # t391: "cuda:0 f32[1, 512, 4096]"
  t392 = ltorch.mul(t390, t391)  # t392: "cuda:0 f32[1, 512, 4096]"
    # t392 = prims.mul(t390, t391)  # t392: "cuda:0 f32[1, 512, 4096]"
  t393 = prims.convert_element_type(t392, dtypes.bfloat16)  # t393: "cuda:0 bf16[1, 512, 4096]"
  t394 = prims.linear(t393, t_transformer_h_2_mlp_fc_1_weight, None)  # t394: "cuda:0 bf16[1, 512, 11008]"
  t395 = prims.linear(t393, t_transformer_h_2_mlp_fc_2_weight, None)  # t395: "cuda:0 bf16[1, 512, 11008]"
  t396 = prims.convert_element_type(t394, dtypes.float32)  # t396: "cuda:0 f32[1, 512, 11008]"
  t397 = prims.neg(t396)  # t397: "cuda:0 f32[1, 512, 11008]"
  t398 = prims.exp(t397)  # t398: "cuda:0 f32[1, 512, 11008]"
  t399 = ltorch.add(1.0, t398, alpha=None)  # t399: "cuda:0 f32[1, 512, 11008]"
    # t399 = prims.add(1.0, t398)  # t399: "cuda:0 f32[1, 512, 11008]"
  t400 = prims.reciprocal(t399)  # t400: "cuda:0 f32[1, 512, 11008]"
  t401 = prims.convert_element_type(t400, dtypes.bfloat16)  # t401: "cuda:0 bf16[1, 512, 11008]"
  t402 = prims.convert_element_type(t394, dtypes.float32)  # t402: "cuda:0 f32[1, 512, 11008]"
  t403 = prims.convert_element_type(t401, dtypes.float32)  # t403: "cuda:0 f32[1, 512, 11008]"
  t404 = ltorch.mul(t402, t403)  # t404: "cuda:0 f32[1, 512, 11008]"
    # t404 = prims.mul(t402, t403)  # t404: "cuda:0 f32[1, 512, 11008]"
  t405 = prims.convert_element_type(t404, dtypes.bfloat16)  # t405: "cuda:0 bf16[1, 512, 11008]"
  t406 = prims.convert_element_type(t405, dtypes.float32)  # t406: "cuda:0 f32[1, 512, 11008]"
  t407 = prims.convert_element_type(t395, dtypes.float32)  # t407: "cuda:0 f32[1, 512, 11008]"
  t408 = ltorch.mul(t406, t407)  # t408: "cuda:0 f32[1, 512, 11008]"
    # t408 = prims.mul(t406, t407)  # t408: "cuda:0 f32[1, 512, 11008]"
  t409 = prims.convert_element_type(t408, dtypes.bfloat16)  # t409: "cuda:0 bf16[1, 512, 11008]"
  t410 = prims.linear(t409, t_transformer_h_2_mlp_proj_weight, None)  # t410: "cuda:0 bf16[1, 512, 4096]"
  t411 = prims.convert_element_type(t410, dtypes.float32)  # t411: "cuda:0 f32[1, 512, 4096]"
  t412 = prims.convert_element_type(t375, dtypes.float32)  # t412: "cuda:0 f32[1, 512, 4096]"
  t413 = ltorch.add(t411, t412, alpha=None)  # t413: "cuda:0 f32[1, 512, 4096]"
    # t413 = prims.add(t411, t412)  # t413: "cuda:0 f32[1, 512, 4096]"
  t414 = prims.convert_element_type(t413, dtypes.bfloat16)  # t414: "cuda:0 bf16[1, 512, 4096]"
  t415 = prims.convert_element_type(t414, dtypes.float32)  # t415: "cuda:0 f32[1, 512, 4096]"
  t416 = ltorch.mul(t415, t415)  # t416: "cuda:0 f32[1, 512, 4096]"
    # t416 = prims.mul(t415, t415)  # t416: "cuda:0 f32[1, 512, 4096]"
  t418 = prims.sum(t416, (2,))  # t418: "cuda:0 f32[1, 512]"
  t419 = prims.broadcast_in_dim(t418, [1, 512, 1], [0, 1])  # t419: "cuda:0 f32[1, 512, 1]"
  t421 = ltorch.true_divide(t419, 4096.0)  # t421: "cuda:0 f32[1, 512, 1]"
    # t421 = prims.div(t419, 4096.0)  # t421: "cuda:0 f32[1, 512, 1]"
  t423 = ltorch.add(t421, 1e-05, alpha=None)  # t423: "cuda:0 f32[1, 512, 1]"
    # t423 = prims.add(t421, 1e-05)  # t423: "cuda:0 f32[1, 512, 1]"
  t424 = prims.rsqrt(t423)  # t424: "cuda:0 f32[1, 512, 1]"
  t425 = prims.broadcast_in_dim(t424, (1, 512, 4096), (0, 1, 2))  # t425: "cuda:0 f32[1, 512, 4096]"
  t426 = ltorch.mul(t415, t425)  # t426: "cuda:0 f32[1, 512, 4096]"
    # t426 = prims.mul(t415, t425)  # t426: "cuda:0 f32[1, 512, 4096]"
  t427 = prims.convert_element_type(t426, dtypes.bfloat16)  # t427: "cuda:0 bf16[1, 512, 4096]"
  t428 = prims.broadcast_in_dim(t_transformer_h_3_norm_1_weight, (1, 512, 4096), (2,))  # t428: "cuda:0 bf16[1, 512, 4096]"
  t429 = prims.convert_element_type(t427, dtypes.float32)  # t429: "cuda:0 f32[1, 512, 4096]"
  t430 = prims.convert_element_type(t428, dtypes.float32)  # t430: "cuda:0 f32[1, 512, 4096]"
  t431 = ltorch.mul(t429, t430)  # t431: "cuda:0 f32[1, 512, 4096]"
    # t431 = prims.mul(t429, t430)  # t431: "cuda:0 f32[1, 512, 4096]"
  t432 = prims.convert_element_type(t431, dtypes.bfloat16)  # t432: "cuda:0 bf16[1, 512, 4096]"
  t433 = prims.linear(t432, t_transformer_h_3_attn_attn_weight, None)  # t433: "cuda:0 bf16[1, 512, 12288]"
  t439 = prims.reshape(t433, (1, 512, 32, 3, 128))  # t439: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t445 = prims.transpose(t439, (0, 2, 3, 1, 4))  # t445: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t446, t447, t448) = ltorch.split(t445, (1, 1, 1), 2)
    # t446 = prims.slice_prim(t445, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t446: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t447 = prims.slice_prim(t445, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t447: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t448 = prims.slice_prim(t445, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t448: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t454 = prims.reshape(t446, (1, 32, 512, 128))  # t454: "cuda:0 bf16[1, 32, 512, 128]"
  t460 = prims.reshape(t447, (1, 32, 512, 128))  # t460: "cuda:0 bf16[1, 32, 512, 128]"
  t466 = prims.reshape(t448, (1, 32, 512, 128))  # t466: "cuda:0 bf16[1, 32, 512, 128]"
  t467 = prims.slice_prim(t454, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t467: "cuda:0 bf16[1, 32, 512, 128]"
  t468 = prims.slice_prim(t467, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t468: "cuda:0 bf16[1, 32, 512, 64]"
  t469 = prims.slice_prim(t467, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t469: "cuda:0 bf16[1, 32, 512, 64]"
  t470 = prims.convert_element_type(t469, dtypes.float32)  # t470: "cuda:0 f32[1, 32, 512, 64]"
  t471 = prims.neg(t470)  # t471: "cuda:0 f32[1, 32, 512, 64]"
  t472 = prims.convert_element_type(t471, dtypes.bfloat16)  # t472: "cuda:0 bf16[1, 32, 512, 64]"
  t474 = prims.cat((t472, t468), -1)  # t474: "cuda:0 bf16[1, 32, 512, 128]"
  t475 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t475: "cuda:0 f32[1, 32, 512, 128]"
  t476 = prims.convert_element_type(t467, dtypes.float32)  # t476: "cuda:0 f32[1, 32, 512, 128]"
  t477 = ltorch.mul(t476, t475)  # t477: "cuda:0 f32[1, 32, 512, 128]"
    # t477 = prims.mul(t476, t475)  # t477: "cuda:0 f32[1, 32, 512, 128]"
  t478 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t478: "cuda:0 f32[1, 32, 512, 128]"
  t479 = prims.convert_element_type(t474, dtypes.float32)  # t479: "cuda:0 f32[1, 32, 512, 128]"
  t480 = ltorch.mul(t479, t478)  # t480: "cuda:0 f32[1, 32, 512, 128]"
    # t480 = prims.mul(t479, t478)  # t480: "cuda:0 f32[1, 32, 512, 128]"
  t481 = ltorch.add(t477, t480, alpha=None)  # t481: "cuda:0 f32[1, 32, 512, 128]"
    # t481 = prims.add(t477, t480)  # t481: "cuda:0 f32[1, 32, 512, 128]"
  t482 = prims.convert_element_type(t481, dtypes.bfloat16)  # t482: "cuda:0 bf16[1, 32, 512, 128]"
  t483 = prims.slice_prim(t460, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t483: "cuda:0 bf16[1, 32, 512, 128]"
  t484 = prims.slice_prim(t483, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t484: "cuda:0 bf16[1, 32, 512, 64]"
  t485 = prims.slice_prim(t483, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t485: "cuda:0 bf16[1, 32, 512, 64]"
  t486 = prims.convert_element_type(t485, dtypes.float32)  # t486: "cuda:0 f32[1, 32, 512, 64]"
  t487 = prims.neg(t486)  # t487: "cuda:0 f32[1, 32, 512, 64]"
  t488 = prims.convert_element_type(t487, dtypes.bfloat16)  # t488: "cuda:0 bf16[1, 32, 512, 64]"
  t490 = prims.cat((t488, t484), -1)  # t490: "cuda:0 bf16[1, 32, 512, 128]"
  t491 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t491: "cuda:0 f32[1, 32, 512, 128]"
  t492 = prims.convert_element_type(t483, dtypes.float32)  # t492: "cuda:0 f32[1, 32, 512, 128]"
  t493 = ltorch.mul(t492, t491)  # t493: "cuda:0 f32[1, 32, 512, 128]"
    # t493 = prims.mul(t492, t491)  # t493: "cuda:0 f32[1, 32, 512, 128]"
  t494 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t494: "cuda:0 f32[1, 32, 512, 128]"
  t495 = prims.convert_element_type(t490, dtypes.float32)  # t495: "cuda:0 f32[1, 32, 512, 128]"
  t496 = ltorch.mul(t495, t494)  # t496: "cuda:0 f32[1, 32, 512, 128]"
    # t496 = prims.mul(t495, t494)  # t496: "cuda:0 f32[1, 32, 512, 128]"
  t497 = ltorch.add(t493, t496, alpha=None)  # t497: "cuda:0 f32[1, 32, 512, 128]"
    # t497 = prims.add(t493, t496)  # t497: "cuda:0 f32[1, 32, 512, 128]"
  t498 = prims.convert_element_type(t497, dtypes.bfloat16)  # t498: "cuda:0 bf16[1, 32, 512, 128]"
  t499 = prims.slice_prim(t454, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t499: "cuda:0 bf16[1, 32, 512, 0]"
  t501 = prims.cat((t482, t499), -1)  # t501: "cuda:0 bf16[1, 32, 512, 128]"
  t502 = prims.slice_prim(t460, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t502: "cuda:0 bf16[1, 32, 512, 0]"
  t504 = prims.cat((t498, t502), -1)  # t504: "cuda:0 bf16[1, 32, 512, 128]"
  (t505, t506, t507, t508) = cudnn_sdpa_fwd(t501, t504, t466, None, 0.0, True, scale=0.08838834764831843)
  t511 = prims.transpose(t505, (0, 2, 1, 3))  # t511: "cuda:0 bf16[1, 512, 32, 128]"
  t515 = prims.reshape(t511, (1, 512, 4096))  # t515: "cuda:0 bf16[1, 512, 4096]"
  t516 = prims.linear(t515, t_transformer_h_3_attn_proj_weight, None)  # t516: "cuda:0 bf16[1, 512, 4096]"
  t517 = prims.convert_element_type(t516, dtypes.float32)  # t517: "cuda:0 f32[1, 512, 4096]"
  t518 = prims.convert_element_type(t414, dtypes.float32)  # t518: "cuda:0 f32[1, 512, 4096]"
  t519 = ltorch.add(t517, t518, alpha=None)  # t519: "cuda:0 f32[1, 512, 4096]"
    # t519 = prims.add(t517, t518)  # t519: "cuda:0 f32[1, 512, 4096]"
  t520 = prims.convert_element_type(t519, dtypes.bfloat16)  # t520: "cuda:0 bf16[1, 512, 4096]"
  t521 = prims.convert_element_type(t520, dtypes.float32)  # t521: "cuda:0 f32[1, 512, 4096]"
  t522 = ltorch.mul(t521, t521)  # t522: "cuda:0 f32[1, 512, 4096]"
    # t522 = prims.mul(t521, t521)  # t522: "cuda:0 f32[1, 512, 4096]"
  t524 = prims.sum(t522, (2,))  # t524: "cuda:0 f32[1, 512]"
  t525 = prims.broadcast_in_dim(t524, [1, 512, 1], [0, 1])  # t525: "cuda:0 f32[1, 512, 1]"
  t527 = ltorch.true_divide(t525, 4096.0)  # t527: "cuda:0 f32[1, 512, 1]"
    # t527 = prims.div(t525, 4096.0)  # t527: "cuda:0 f32[1, 512, 1]"
  t529 = ltorch.add(t527, 1e-05, alpha=None)  # t529: "cuda:0 f32[1, 512, 1]"
    # t529 = prims.add(t527, 1e-05)  # t529: "cuda:0 f32[1, 512, 1]"
  t530 = prims.rsqrt(t529)  # t530: "cuda:0 f32[1, 512, 1]"
  t531 = prims.broadcast_in_dim(t530, (1, 512, 4096), (0, 1, 2))  # t531: "cuda:0 f32[1, 512, 4096]"
  t532 = ltorch.mul(t521, t531)  # t532: "cuda:0 f32[1, 512, 4096]"
    # t532 = prims.mul(t521, t531)  # t532: "cuda:0 f32[1, 512, 4096]"
  t533 = prims.convert_element_type(t532, dtypes.bfloat16)  # t533: "cuda:0 bf16[1, 512, 4096]"
  t534 = prims.broadcast_in_dim(t_transformer_h_3_norm_2_weight, (1, 512, 4096), (2,))  # t534: "cuda:0 bf16[1, 512, 4096]"
  t535 = prims.convert_element_type(t533, dtypes.float32)  # t535: "cuda:0 f32[1, 512, 4096]"
  t536 = prims.convert_element_type(t534, dtypes.float32)  # t536: "cuda:0 f32[1, 512, 4096]"
  t537 = ltorch.mul(t535, t536)  # t537: "cuda:0 f32[1, 512, 4096]"
    # t537 = prims.mul(t535, t536)  # t537: "cuda:0 f32[1, 512, 4096]"
  t538 = prims.convert_element_type(t537, dtypes.bfloat16)  # t538: "cuda:0 bf16[1, 512, 4096]"
  t539 = prims.linear(t538, t_transformer_h_3_mlp_fc_1_weight, None)  # t539: "cuda:0 bf16[1, 512, 11008]"
  t540 = prims.linear(t538, t_transformer_h_3_mlp_fc_2_weight, None)  # t540: "cuda:0 bf16[1, 512, 11008]"
  t541 = prims.convert_element_type(t539, dtypes.float32)  # t541: "cuda:0 f32[1, 512, 11008]"
  t542 = prims.neg(t541)  # t542: "cuda:0 f32[1, 512, 11008]"
  t543 = prims.exp(t542)  # t543: "cuda:0 f32[1, 512, 11008]"
  t544 = ltorch.add(1.0, t543, alpha=None)  # t544: "cuda:0 f32[1, 512, 11008]"
    # t544 = prims.add(1.0, t543)  # t544: "cuda:0 f32[1, 512, 11008]"
  t545 = prims.reciprocal(t544)  # t545: "cuda:0 f32[1, 512, 11008]"
  t546 = prims.convert_element_type(t545, dtypes.bfloat16)  # t546: "cuda:0 bf16[1, 512, 11008]"
  t547 = prims.convert_element_type(t539, dtypes.float32)  # t547: "cuda:0 f32[1, 512, 11008]"
  t548 = prims.convert_element_type(t546, dtypes.float32)  # t548: "cuda:0 f32[1, 512, 11008]"
  t549 = ltorch.mul(t547, t548)  # t549: "cuda:0 f32[1, 512, 11008]"
    # t549 = prims.mul(t547, t548)  # t549: "cuda:0 f32[1, 512, 11008]"
  t550 = prims.convert_element_type(t549, dtypes.bfloat16)  # t550: "cuda:0 bf16[1, 512, 11008]"
  t551 = prims.convert_element_type(t550, dtypes.float32)  # t551: "cuda:0 f32[1, 512, 11008]"
  t552 = prims.convert_element_type(t540, dtypes.float32)  # t552: "cuda:0 f32[1, 512, 11008]"
  t553 = ltorch.mul(t551, t552)  # t553: "cuda:0 f32[1, 512, 11008]"
    # t553 = prims.mul(t551, t552)  # t553: "cuda:0 f32[1, 512, 11008]"
  t554 = prims.convert_element_type(t553, dtypes.bfloat16)  # t554: "cuda:0 bf16[1, 512, 11008]"
  t555 = prims.linear(t554, t_transformer_h_3_mlp_proj_weight, None)  # t555: "cuda:0 bf16[1, 512, 4096]"
  t556 = prims.convert_element_type(t555, dtypes.float32)  # t556: "cuda:0 f32[1, 512, 4096]"
  t557 = prims.convert_element_type(t520, dtypes.float32)  # t557: "cuda:0 f32[1, 512, 4096]"
  t558 = ltorch.add(t556, t557, alpha=None)  # t558: "cuda:0 f32[1, 512, 4096]"
    # t558 = prims.add(t556, t557)  # t558: "cuda:0 f32[1, 512, 4096]"
  t559 = prims.convert_element_type(t558, dtypes.bfloat16)  # t559: "cuda:0 bf16[1, 512, 4096]"
  t560 = prims.convert_element_type(t559, dtypes.float32)  # t560: "cuda:0 f32[1, 512, 4096]"
  t561 = ltorch.mul(t560, t560)  # t561: "cuda:0 f32[1, 512, 4096]"
    # t561 = prims.mul(t560, t560)  # t561: "cuda:0 f32[1, 512, 4096]"
  t563 = prims.sum(t561, (2,))  # t563: "cuda:0 f32[1, 512]"
  t564 = prims.broadcast_in_dim(t563, [1, 512, 1], [0, 1])  # t564: "cuda:0 f32[1, 512, 1]"
  t566 = ltorch.true_divide(t564, 4096.0)  # t566: "cuda:0 f32[1, 512, 1]"
    # t566 = prims.div(t564, 4096.0)  # t566: "cuda:0 f32[1, 512, 1]"
  t568 = ltorch.add(t566, 1e-05, alpha=None)  # t568: "cuda:0 f32[1, 512, 1]"
    # t568 = prims.add(t566, 1e-05)  # t568: "cuda:0 f32[1, 512, 1]"
  t569 = prims.rsqrt(t568)  # t569: "cuda:0 f32[1, 512, 1]"
  t570 = prims.broadcast_in_dim(t569, (1, 512, 4096), (0, 1, 2))  # t570: "cuda:0 f32[1, 512, 4096]"
  t571 = ltorch.mul(t560, t570)  # t571: "cuda:0 f32[1, 512, 4096]"
    # t571 = prims.mul(t560, t570)  # t571: "cuda:0 f32[1, 512, 4096]"
  t572 = prims.convert_element_type(t571, dtypes.bfloat16)  # t572: "cuda:0 bf16[1, 512, 4096]"
  t573 = prims.broadcast_in_dim(t_transformer_h_4_norm_1_weight, (1, 512, 4096), (2,))  # t573: "cuda:0 bf16[1, 512, 4096]"
  t574 = prims.convert_element_type(t572, dtypes.float32)  # t574: "cuda:0 f32[1, 512, 4096]"
  t575 = prims.convert_element_type(t573, dtypes.float32)  # t575: "cuda:0 f32[1, 512, 4096]"
  t576 = ltorch.mul(t574, t575)  # t576: "cuda:0 f32[1, 512, 4096]"
    # t576 = prims.mul(t574, t575)  # t576: "cuda:0 f32[1, 512, 4096]"
  t577 = prims.convert_element_type(t576, dtypes.bfloat16)  # t577: "cuda:0 bf16[1, 512, 4096]"
  t578 = prims.linear(t577, t_transformer_h_4_attn_attn_weight, None)  # t578: "cuda:0 bf16[1, 512, 12288]"
  t584 = prims.reshape(t578, (1, 512, 32, 3, 128))  # t584: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t590 = prims.transpose(t584, (0, 2, 3, 1, 4))  # t590: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t591, t592, t593) = ltorch.split(t590, (1, 1, 1), 2)
    # t591 = prims.slice_prim(t590, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t591: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t592 = prims.slice_prim(t590, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t592: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t593 = prims.slice_prim(t590, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t593: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t599 = prims.reshape(t591, (1, 32, 512, 128))  # t599: "cuda:0 bf16[1, 32, 512, 128]"
  t605 = prims.reshape(t592, (1, 32, 512, 128))  # t605: "cuda:0 bf16[1, 32, 512, 128]"
  t611 = prims.reshape(t593, (1, 32, 512, 128))  # t611: "cuda:0 bf16[1, 32, 512, 128]"
  t612 = prims.slice_prim(t599, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t612: "cuda:0 bf16[1, 32, 512, 128]"
  t613 = prims.slice_prim(t612, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t613: "cuda:0 bf16[1, 32, 512, 64]"
  t614 = prims.slice_prim(t612, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t614: "cuda:0 bf16[1, 32, 512, 64]"
  t615 = prims.convert_element_type(t614, dtypes.float32)  # t615: "cuda:0 f32[1, 32, 512, 64]"
  t616 = prims.neg(t615)  # t616: "cuda:0 f32[1, 32, 512, 64]"
  t617 = prims.convert_element_type(t616, dtypes.bfloat16)  # t617: "cuda:0 bf16[1, 32, 512, 64]"
  t619 = prims.cat((t617, t613), -1)  # t619: "cuda:0 bf16[1, 32, 512, 128]"
  t620 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t620: "cuda:0 f32[1, 32, 512, 128]"
  t621 = prims.convert_element_type(t612, dtypes.float32)  # t621: "cuda:0 f32[1, 32, 512, 128]"
  t622 = ltorch.mul(t621, t620)  # t622: "cuda:0 f32[1, 32, 512, 128]"
    # t622 = prims.mul(t621, t620)  # t622: "cuda:0 f32[1, 32, 512, 128]"
  t623 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t623: "cuda:0 f32[1, 32, 512, 128]"
  t624 = prims.convert_element_type(t619, dtypes.float32)  # t624: "cuda:0 f32[1, 32, 512, 128]"
  t625 = ltorch.mul(t624, t623)  # t625: "cuda:0 f32[1, 32, 512, 128]"
    # t625 = prims.mul(t624, t623)  # t625: "cuda:0 f32[1, 32, 512, 128]"
  t626 = ltorch.add(t622, t625, alpha=None)  # t626: "cuda:0 f32[1, 32, 512, 128]"
    # t626 = prims.add(t622, t625)  # t626: "cuda:0 f32[1, 32, 512, 128]"
  t627 = prims.convert_element_type(t626, dtypes.bfloat16)  # t627: "cuda:0 bf16[1, 32, 512, 128]"
  t628 = prims.slice_prim(t605, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t628: "cuda:0 bf16[1, 32, 512, 128]"
  t629 = prims.slice_prim(t628, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t629: "cuda:0 bf16[1, 32, 512, 64]"
  t630 = prims.slice_prim(t628, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t630: "cuda:0 bf16[1, 32, 512, 64]"
  t631 = prims.convert_element_type(t630, dtypes.float32)  # t631: "cuda:0 f32[1, 32, 512, 64]"
  t632 = prims.neg(t631)  # t632: "cuda:0 f32[1, 32, 512, 64]"
  t633 = prims.convert_element_type(t632, dtypes.bfloat16)  # t633: "cuda:0 bf16[1, 32, 512, 64]"
  t635 = prims.cat((t633, t629), -1)  # t635: "cuda:0 bf16[1, 32, 512, 128]"
  t636 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t636: "cuda:0 f32[1, 32, 512, 128]"
  t637 = prims.convert_element_type(t628, dtypes.float32)  # t637: "cuda:0 f32[1, 32, 512, 128]"
  t638 = ltorch.mul(t637, t636)  # t638: "cuda:0 f32[1, 32, 512, 128]"
    # t638 = prims.mul(t637, t636)  # t638: "cuda:0 f32[1, 32, 512, 128]"
  t639 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t639: "cuda:0 f32[1, 32, 512, 128]"
  t640 = prims.convert_element_type(t635, dtypes.float32)  # t640: "cuda:0 f32[1, 32, 512, 128]"
  t641 = ltorch.mul(t640, t639)  # t641: "cuda:0 f32[1, 32, 512, 128]"
    # t641 = prims.mul(t640, t639)  # t641: "cuda:0 f32[1, 32, 512, 128]"
  t642 = ltorch.add(t638, t641, alpha=None)  # t642: "cuda:0 f32[1, 32, 512, 128]"
    # t642 = prims.add(t638, t641)  # t642: "cuda:0 f32[1, 32, 512, 128]"
  t643 = prims.convert_element_type(t642, dtypes.bfloat16)  # t643: "cuda:0 bf16[1, 32, 512, 128]"
  t644 = prims.slice_prim(t599, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t644: "cuda:0 bf16[1, 32, 512, 0]"
  t646 = prims.cat((t627, t644), -1)  # t646: "cuda:0 bf16[1, 32, 512, 128]"
  t647 = prims.slice_prim(t605, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t647: "cuda:0 bf16[1, 32, 512, 0]"
  t649 = prims.cat((t643, t647), -1)  # t649: "cuda:0 bf16[1, 32, 512, 128]"
  (t650, t651, t652, t653) = cudnn_sdpa_fwd(t646, t649, t611, None, 0.0, True, scale=0.08838834764831843)
  t656 = prims.transpose(t650, (0, 2, 1, 3))  # t656: "cuda:0 bf16[1, 512, 32, 128]"
  t660 = prims.reshape(t656, (1, 512, 4096))  # t660: "cuda:0 bf16[1, 512, 4096]"
  t661 = prims.linear(t660, t_transformer_h_4_attn_proj_weight, None)  # t661: "cuda:0 bf16[1, 512, 4096]"
  t662 = prims.convert_element_type(t661, dtypes.float32)  # t662: "cuda:0 f32[1, 512, 4096]"
  t663 = prims.convert_element_type(t559, dtypes.float32)  # t663: "cuda:0 f32[1, 512, 4096]"
  t664 = ltorch.add(t662, t663, alpha=None)  # t664: "cuda:0 f32[1, 512, 4096]"
    # t664 = prims.add(t662, t663)  # t664: "cuda:0 f32[1, 512, 4096]"
  t665 = prims.convert_element_type(t664, dtypes.bfloat16)  # t665: "cuda:0 bf16[1, 512, 4096]"
  t666 = prims.convert_element_type(t665, dtypes.float32)  # t666: "cuda:0 f32[1, 512, 4096]"
  t667 = ltorch.mul(t666, t666)  # t667: "cuda:0 f32[1, 512, 4096]"
    # t667 = prims.mul(t666, t666)  # t667: "cuda:0 f32[1, 512, 4096]"
  t669 = prims.sum(t667, (2,))  # t669: "cuda:0 f32[1, 512]"
  t670 = prims.broadcast_in_dim(t669, [1, 512, 1], [0, 1])  # t670: "cuda:0 f32[1, 512, 1]"
  t672 = ltorch.true_divide(t670, 4096.0)  # t672: "cuda:0 f32[1, 512, 1]"
    # t672 = prims.div(t670, 4096.0)  # t672: "cuda:0 f32[1, 512, 1]"
  t674 = ltorch.add(t672, 1e-05, alpha=None)  # t674: "cuda:0 f32[1, 512, 1]"
    # t674 = prims.add(t672, 1e-05)  # t674: "cuda:0 f32[1, 512, 1]"
  t675 = prims.rsqrt(t674)  # t675: "cuda:0 f32[1, 512, 1]"
  t676 = prims.broadcast_in_dim(t675, (1, 512, 4096), (0, 1, 2))  # t676: "cuda:0 f32[1, 512, 4096]"
  t677 = ltorch.mul(t666, t676)  # t677: "cuda:0 f32[1, 512, 4096]"
    # t677 = prims.mul(t666, t676)  # t677: "cuda:0 f32[1, 512, 4096]"
  t678 = prims.convert_element_type(t677, dtypes.bfloat16)  # t678: "cuda:0 bf16[1, 512, 4096]"
  t679 = prims.broadcast_in_dim(t_transformer_h_4_norm_2_weight, (1, 512, 4096), (2,))  # t679: "cuda:0 bf16[1, 512, 4096]"
  t680 = prims.convert_element_type(t678, dtypes.float32)  # t680: "cuda:0 f32[1, 512, 4096]"
  t681 = prims.convert_element_type(t679, dtypes.float32)  # t681: "cuda:0 f32[1, 512, 4096]"
  t682 = ltorch.mul(t680, t681)  # t682: "cuda:0 f32[1, 512, 4096]"
    # t682 = prims.mul(t680, t681)  # t682: "cuda:0 f32[1, 512, 4096]"
  t683 = prims.convert_element_type(t682, dtypes.bfloat16)  # t683: "cuda:0 bf16[1, 512, 4096]"
  t684 = prims.linear(t683, t_transformer_h_4_mlp_fc_1_weight, None)  # t684: "cuda:0 bf16[1, 512, 11008]"
  t685 = prims.linear(t683, t_transformer_h_4_mlp_fc_2_weight, None)  # t685: "cuda:0 bf16[1, 512, 11008]"
  t686 = prims.convert_element_type(t684, dtypes.float32)  # t686: "cuda:0 f32[1, 512, 11008]"
  t687 = prims.neg(t686)  # t687: "cuda:0 f32[1, 512, 11008]"
  t688 = prims.exp(t687)  # t688: "cuda:0 f32[1, 512, 11008]"
  t689 = ltorch.add(1.0, t688, alpha=None)  # t689: "cuda:0 f32[1, 512, 11008]"
    # t689 = prims.add(1.0, t688)  # t689: "cuda:0 f32[1, 512, 11008]"
  t690 = prims.reciprocal(t689)  # t690: "cuda:0 f32[1, 512, 11008]"
  t691 = prims.convert_element_type(t690, dtypes.bfloat16)  # t691: "cuda:0 bf16[1, 512, 11008]"
  t692 = prims.convert_element_type(t684, dtypes.float32)  # t692: "cuda:0 f32[1, 512, 11008]"
  t693 = prims.convert_element_type(t691, dtypes.float32)  # t693: "cuda:0 f32[1, 512, 11008]"
  t694 = ltorch.mul(t692, t693)  # t694: "cuda:0 f32[1, 512, 11008]"
    # t694 = prims.mul(t692, t693)  # t694: "cuda:0 f32[1, 512, 11008]"
  t695 = prims.convert_element_type(t694, dtypes.bfloat16)  # t695: "cuda:0 bf16[1, 512, 11008]"
  t696 = prims.convert_element_type(t695, dtypes.float32)  # t696: "cuda:0 f32[1, 512, 11008]"
  t697 = prims.convert_element_type(t685, dtypes.float32)  # t697: "cuda:0 f32[1, 512, 11008]"
  t698 = ltorch.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 11008]"
    # t698 = prims.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 11008]"
  t699 = prims.convert_element_type(t698, dtypes.bfloat16)  # t699: "cuda:0 bf16[1, 512, 11008]"
  t700 = prims.linear(t699, t_transformer_h_4_mlp_proj_weight, None)  # t700: "cuda:0 bf16[1, 512, 4096]"
  t701 = prims.convert_element_type(t700, dtypes.float32)  # t701: "cuda:0 f32[1, 512, 4096]"
  t702 = prims.convert_element_type(t665, dtypes.float32)  # t702: "cuda:0 f32[1, 512, 4096]"
  t703 = ltorch.add(t701, t702, alpha=None)  # t703: "cuda:0 f32[1, 512, 4096]"
    # t703 = prims.add(t701, t702)  # t703: "cuda:0 f32[1, 512, 4096]"
  t704 = prims.convert_element_type(t703, dtypes.bfloat16)  # t704: "cuda:0 bf16[1, 512, 4096]"
  t705 = prims.convert_element_type(t704, dtypes.float32)  # t705: "cuda:0 f32[1, 512, 4096]"
  t706 = ltorch.mul(t705, t705)  # t706: "cuda:0 f32[1, 512, 4096]"
    # t706 = prims.mul(t705, t705)  # t706: "cuda:0 f32[1, 512, 4096]"
  t708 = prims.sum(t706, (2,))  # t708: "cuda:0 f32[1, 512]"
  t709 = prims.broadcast_in_dim(t708, [1, 512, 1], [0, 1])  # t709: "cuda:0 f32[1, 512, 1]"
  t711 = ltorch.true_divide(t709, 4096.0)  # t711: "cuda:0 f32[1, 512, 1]"
    # t711 = prims.div(t709, 4096.0)  # t711: "cuda:0 f32[1, 512, 1]"
  t713 = ltorch.add(t711, 1e-05, alpha=None)  # t713: "cuda:0 f32[1, 512, 1]"
    # t713 = prims.add(t711, 1e-05)  # t713: "cuda:0 f32[1, 512, 1]"
  t714 = prims.rsqrt(t713)  # t714: "cuda:0 f32[1, 512, 1]"
  t715 = prims.broadcast_in_dim(t714, (1, 512, 4096), (0, 1, 2))  # t715: "cuda:0 f32[1, 512, 4096]"
  t716 = ltorch.mul(t705, t715)  # t716: "cuda:0 f32[1, 512, 4096]"
    # t716 = prims.mul(t705, t715)  # t716: "cuda:0 f32[1, 512, 4096]"
  t717 = prims.convert_element_type(t716, dtypes.bfloat16)  # t717: "cuda:0 bf16[1, 512, 4096]"
  t718 = prims.broadcast_in_dim(t_transformer_h_5_norm_1_weight, (1, 512, 4096), (2,))  # t718: "cuda:0 bf16[1, 512, 4096]"
  t719 = prims.convert_element_type(t717, dtypes.float32)  # t719: "cuda:0 f32[1, 512, 4096]"
  t720 = prims.convert_element_type(t718, dtypes.float32)  # t720: "cuda:0 f32[1, 512, 4096]"
  t721 = ltorch.mul(t719, t720)  # t721: "cuda:0 f32[1, 512, 4096]"
    # t721 = prims.mul(t719, t720)  # t721: "cuda:0 f32[1, 512, 4096]"
  t722 = prims.convert_element_type(t721, dtypes.bfloat16)  # t722: "cuda:0 bf16[1, 512, 4096]"
  t723 = prims.linear(t722, t_transformer_h_5_attn_attn_weight, None)  # t723: "cuda:0 bf16[1, 512, 12288]"
  t729 = prims.reshape(t723, (1, 512, 32, 3, 128))  # t729: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t735 = prims.transpose(t729, (0, 2, 3, 1, 4))  # t735: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t736, t737, t738) = ltorch.split(t735, (1, 1, 1), 2)
    # t736 = prims.slice_prim(t735, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t736: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t737 = prims.slice_prim(t735, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t737: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t738 = prims.slice_prim(t735, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t738: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t744 = prims.reshape(t736, (1, 32, 512, 128))  # t744: "cuda:0 bf16[1, 32, 512, 128]"
  t750 = prims.reshape(t737, (1, 32, 512, 128))  # t750: "cuda:0 bf16[1, 32, 512, 128]"
  t756 = prims.reshape(t738, (1, 32, 512, 128))  # t756: "cuda:0 bf16[1, 32, 512, 128]"
  t757 = prims.slice_prim(t744, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t757: "cuda:0 bf16[1, 32, 512, 128]"
  t758 = prims.slice_prim(t757, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t758: "cuda:0 bf16[1, 32, 512, 64]"
  t759 = prims.slice_prim(t757, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t759: "cuda:0 bf16[1, 32, 512, 64]"
  t760 = prims.convert_element_type(t759, dtypes.float32)  # t760: "cuda:0 f32[1, 32, 512, 64]"
  t761 = prims.neg(t760)  # t761: "cuda:0 f32[1, 32, 512, 64]"
  t762 = prims.convert_element_type(t761, dtypes.bfloat16)  # t762: "cuda:0 bf16[1, 32, 512, 64]"
  t764 = prims.cat((t762, t758), -1)  # t764: "cuda:0 bf16[1, 32, 512, 128]"
  t765 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t765: "cuda:0 f32[1, 32, 512, 128]"
  t766 = prims.convert_element_type(t757, dtypes.float32)  # t766: "cuda:0 f32[1, 32, 512, 128]"
  t767 = ltorch.mul(t766, t765)  # t767: "cuda:0 f32[1, 32, 512, 128]"
    # t767 = prims.mul(t766, t765)  # t767: "cuda:0 f32[1, 32, 512, 128]"
  t768 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t768: "cuda:0 f32[1, 32, 512, 128]"
  t769 = prims.convert_element_type(t764, dtypes.float32)  # t769: "cuda:0 f32[1, 32, 512, 128]"
  t770 = ltorch.mul(t769, t768)  # t770: "cuda:0 f32[1, 32, 512, 128]"
    # t770 = prims.mul(t769, t768)  # t770: "cuda:0 f32[1, 32, 512, 128]"
  t771 = ltorch.add(t767, t770, alpha=None)  # t771: "cuda:0 f32[1, 32, 512, 128]"
    # t771 = prims.add(t767, t770)  # t771: "cuda:0 f32[1, 32, 512, 128]"
  t772 = prims.convert_element_type(t771, dtypes.bfloat16)  # t772: "cuda:0 bf16[1, 32, 512, 128]"
  t773 = prims.slice_prim(t750, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t773: "cuda:0 bf16[1, 32, 512, 128]"
  t774 = prims.slice_prim(t773, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t774: "cuda:0 bf16[1, 32, 512, 64]"
  t775 = prims.slice_prim(t773, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t775: "cuda:0 bf16[1, 32, 512, 64]"
  t776 = prims.convert_element_type(t775, dtypes.float32)  # t776: "cuda:0 f32[1, 32, 512, 64]"
  t777 = prims.neg(t776)  # t777: "cuda:0 f32[1, 32, 512, 64]"
  t778 = prims.convert_element_type(t777, dtypes.bfloat16)  # t778: "cuda:0 bf16[1, 32, 512, 64]"
  t780 = prims.cat((t778, t774), -1)  # t780: "cuda:0 bf16[1, 32, 512, 128]"
  t781 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t781: "cuda:0 f32[1, 32, 512, 128]"
  t782 = prims.convert_element_type(t773, dtypes.float32)  # t782: "cuda:0 f32[1, 32, 512, 128]"
  t783 = ltorch.mul(t782, t781)  # t783: "cuda:0 f32[1, 32, 512, 128]"
    # t783 = prims.mul(t782, t781)  # t783: "cuda:0 f32[1, 32, 512, 128]"
  t784 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t784: "cuda:0 f32[1, 32, 512, 128]"
  t785 = prims.convert_element_type(t780, dtypes.float32)  # t785: "cuda:0 f32[1, 32, 512, 128]"
  t786 = ltorch.mul(t785, t784)  # t786: "cuda:0 f32[1, 32, 512, 128]"
    # t786 = prims.mul(t785, t784)  # t786: "cuda:0 f32[1, 32, 512, 128]"
  t787 = ltorch.add(t783, t786, alpha=None)  # t787: "cuda:0 f32[1, 32, 512, 128]"
    # t787 = prims.add(t783, t786)  # t787: "cuda:0 f32[1, 32, 512, 128]"
  t788 = prims.convert_element_type(t787, dtypes.bfloat16)  # t788: "cuda:0 bf16[1, 32, 512, 128]"
  t789 = prims.slice_prim(t744, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t789: "cuda:0 bf16[1, 32, 512, 0]"
  t791 = prims.cat((t772, t789), -1)  # t791: "cuda:0 bf16[1, 32, 512, 128]"
  t792 = prims.slice_prim(t750, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t792: "cuda:0 bf16[1, 32, 512, 0]"
  t794 = prims.cat((t788, t792), -1)  # t794: "cuda:0 bf16[1, 32, 512, 128]"
  (t795, t796, t797, t798) = cudnn_sdpa_fwd(t791, t794, t756, None, 0.0, True, scale=0.08838834764831843)
  t801 = prims.transpose(t795, (0, 2, 1, 3))  # t801: "cuda:0 bf16[1, 512, 32, 128]"
  t805 = prims.reshape(t801, (1, 512, 4096))  # t805: "cuda:0 bf16[1, 512, 4096]"
  t806 = prims.linear(t805, t_transformer_h_5_attn_proj_weight, None)  # t806: "cuda:0 bf16[1, 512, 4096]"
  t807 = prims.convert_element_type(t806, dtypes.float32)  # t807: "cuda:0 f32[1, 512, 4096]"
  t808 = prims.convert_element_type(t704, dtypes.float32)  # t808: "cuda:0 f32[1, 512, 4096]"
  t809 = ltorch.add(t807, t808, alpha=None)  # t809: "cuda:0 f32[1, 512, 4096]"
    # t809 = prims.add(t807, t808)  # t809: "cuda:0 f32[1, 512, 4096]"
  t810 = prims.convert_element_type(t809, dtypes.bfloat16)  # t810: "cuda:0 bf16[1, 512, 4096]"
  t811 = prims.convert_element_type(t810, dtypes.float32)  # t811: "cuda:0 f32[1, 512, 4096]"
  t812 = ltorch.mul(t811, t811)  # t812: "cuda:0 f32[1, 512, 4096]"
    # t812 = prims.mul(t811, t811)  # t812: "cuda:0 f32[1, 512, 4096]"
  t814 = prims.sum(t812, (2,))  # t814: "cuda:0 f32[1, 512]"
  t815 = prims.broadcast_in_dim(t814, [1, 512, 1], [0, 1])  # t815: "cuda:0 f32[1, 512, 1]"
  t817 = ltorch.true_divide(t815, 4096.0)  # t817: "cuda:0 f32[1, 512, 1]"
    # t817 = prims.div(t815, 4096.0)  # t817: "cuda:0 f32[1, 512, 1]"
  t819 = ltorch.add(t817, 1e-05, alpha=None)  # t819: "cuda:0 f32[1, 512, 1]"
    # t819 = prims.add(t817, 1e-05)  # t819: "cuda:0 f32[1, 512, 1]"
  t820 = prims.rsqrt(t819)  # t820: "cuda:0 f32[1, 512, 1]"
  t821 = prims.broadcast_in_dim(t820, (1, 512, 4096), (0, 1, 2))  # t821: "cuda:0 f32[1, 512, 4096]"
  t822 = ltorch.mul(t811, t821)  # t822: "cuda:0 f32[1, 512, 4096]"
    # t822 = prims.mul(t811, t821)  # t822: "cuda:0 f32[1, 512, 4096]"
  t823 = prims.convert_element_type(t822, dtypes.bfloat16)  # t823: "cuda:0 bf16[1, 512, 4096]"
  t824 = prims.broadcast_in_dim(t_transformer_h_5_norm_2_weight, (1, 512, 4096), (2,))  # t824: "cuda:0 bf16[1, 512, 4096]"
  t825 = prims.convert_element_type(t823, dtypes.float32)  # t825: "cuda:0 f32[1, 512, 4096]"
  t826 = prims.convert_element_type(t824, dtypes.float32)  # t826: "cuda:0 f32[1, 512, 4096]"
  t827 = ltorch.mul(t825, t826)  # t827: "cuda:0 f32[1, 512, 4096]"
    # t827 = prims.mul(t825, t826)  # t827: "cuda:0 f32[1, 512, 4096]"
  t828 = prims.convert_element_type(t827, dtypes.bfloat16)  # t828: "cuda:0 bf16[1, 512, 4096]"
  t829 = prims.linear(t828, t_transformer_h_5_mlp_fc_1_weight, None)  # t829: "cuda:0 bf16[1, 512, 11008]"
  t830 = prims.linear(t828, t_transformer_h_5_mlp_fc_2_weight, None)  # t830: "cuda:0 bf16[1, 512, 11008]"
  t831 = prims.convert_element_type(t829, dtypes.float32)  # t831: "cuda:0 f32[1, 512, 11008]"
  t832 = prims.neg(t831)  # t832: "cuda:0 f32[1, 512, 11008]"
  t833 = prims.exp(t832)  # t833: "cuda:0 f32[1, 512, 11008]"
  t834 = ltorch.add(1.0, t833, alpha=None)  # t834: "cuda:0 f32[1, 512, 11008]"
    # t834 = prims.add(1.0, t833)  # t834: "cuda:0 f32[1, 512, 11008]"
  t835 = prims.reciprocal(t834)  # t835: "cuda:0 f32[1, 512, 11008]"
  t836 = prims.convert_element_type(t835, dtypes.bfloat16)  # t836: "cuda:0 bf16[1, 512, 11008]"
  t837 = prims.convert_element_type(t829, dtypes.float32)  # t837: "cuda:0 f32[1, 512, 11008]"
  t838 = prims.convert_element_type(t836, dtypes.float32)  # t838: "cuda:0 f32[1, 512, 11008]"
  t839 = ltorch.mul(t837, t838)  # t839: "cuda:0 f32[1, 512, 11008]"
    # t839 = prims.mul(t837, t838)  # t839: "cuda:0 f32[1, 512, 11008]"
  t840 = prims.convert_element_type(t839, dtypes.bfloat16)  # t840: "cuda:0 bf16[1, 512, 11008]"
  t841 = prims.convert_element_type(t840, dtypes.float32)  # t841: "cuda:0 f32[1, 512, 11008]"
  t842 = prims.convert_element_type(t830, dtypes.float32)  # t842: "cuda:0 f32[1, 512, 11008]"
  t843 = ltorch.mul(t841, t842)  # t843: "cuda:0 f32[1, 512, 11008]"
    # t843 = prims.mul(t841, t842)  # t843: "cuda:0 f32[1, 512, 11008]"
  t844 = prims.convert_element_type(t843, dtypes.bfloat16)  # t844: "cuda:0 bf16[1, 512, 11008]"
  t845 = prims.linear(t844, t_transformer_h_5_mlp_proj_weight, None)  # t845: "cuda:0 bf16[1, 512, 4096]"
  t846 = prims.convert_element_type(t845, dtypes.float32)  # t846: "cuda:0 f32[1, 512, 4096]"
  t847 = prims.convert_element_type(t810, dtypes.float32)  # t847: "cuda:0 f32[1, 512, 4096]"
  t848 = ltorch.add(t846, t847, alpha=None)  # t848: "cuda:0 f32[1, 512, 4096]"
    # t848 = prims.add(t846, t847)  # t848: "cuda:0 f32[1, 512, 4096]"
  t849 = prims.convert_element_type(t848, dtypes.bfloat16)  # t849: "cuda:0 bf16[1, 512, 4096]"
  t850 = prims.convert_element_type(t849, dtypes.float32)  # t850: "cuda:0 f32[1, 512, 4096]"
  t851 = ltorch.mul(t850, t850)  # t851: "cuda:0 f32[1, 512, 4096]"
    # t851 = prims.mul(t850, t850)  # t851: "cuda:0 f32[1, 512, 4096]"
  t853 = prims.sum(t851, (2,))  # t853: "cuda:0 f32[1, 512]"
  t854 = prims.broadcast_in_dim(t853, [1, 512, 1], [0, 1])  # t854: "cuda:0 f32[1, 512, 1]"
  t856 = ltorch.true_divide(t854, 4096.0)  # t856: "cuda:0 f32[1, 512, 1]"
    # t856 = prims.div(t854, 4096.0)  # t856: "cuda:0 f32[1, 512, 1]"
  t858 = ltorch.add(t856, 1e-05, alpha=None)  # t858: "cuda:0 f32[1, 512, 1]"
    # t858 = prims.add(t856, 1e-05)  # t858: "cuda:0 f32[1, 512, 1]"
  t859 = prims.rsqrt(t858)  # t859: "cuda:0 f32[1, 512, 1]"
  t860 = prims.broadcast_in_dim(t859, (1, 512, 4096), (0, 1, 2))  # t860: "cuda:0 f32[1, 512, 4096]"
  t861 = ltorch.mul(t850, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
    # t861 = prims.mul(t850, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
  t862 = prims.convert_element_type(t861, dtypes.bfloat16)  # t862: "cuda:0 bf16[1, 512, 4096]"
  t863 = prims.broadcast_in_dim(t_transformer_h_6_norm_1_weight, (1, 512, 4096), (2,))  # t863: "cuda:0 bf16[1, 512, 4096]"
  t864 = prims.convert_element_type(t862, dtypes.float32)  # t864: "cuda:0 f32[1, 512, 4096]"
  t865 = prims.convert_element_type(t863, dtypes.float32)  # t865: "cuda:0 f32[1, 512, 4096]"
  t866 = ltorch.mul(t864, t865)  # t866: "cuda:0 f32[1, 512, 4096]"
    # t866 = prims.mul(t864, t865)  # t866: "cuda:0 f32[1, 512, 4096]"
  t867 = prims.convert_element_type(t866, dtypes.bfloat16)  # t867: "cuda:0 bf16[1, 512, 4096]"
  t868 = prims.linear(t867, t_transformer_h_6_attn_attn_weight, None)  # t868: "cuda:0 bf16[1, 512, 12288]"
  t874 = prims.reshape(t868, (1, 512, 32, 3, 128))  # t874: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t880 = prims.transpose(t874, (0, 2, 3, 1, 4))  # t880: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t881, t882, t883) = ltorch.split(t880, (1, 1, 1), 2)
    # t881 = prims.slice_prim(t880, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t881: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t882 = prims.slice_prim(t880, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t882: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t883 = prims.slice_prim(t880, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t883: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t889 = prims.reshape(t881, (1, 32, 512, 128))  # t889: "cuda:0 bf16[1, 32, 512, 128]"
  t895 = prims.reshape(t882, (1, 32, 512, 128))  # t895: "cuda:0 bf16[1, 32, 512, 128]"
  t901 = prims.reshape(t883, (1, 32, 512, 128))  # t901: "cuda:0 bf16[1, 32, 512, 128]"
  t902 = prims.slice_prim(t889, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t902: "cuda:0 bf16[1, 32, 512, 128]"
  t903 = prims.slice_prim(t902, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t903: "cuda:0 bf16[1, 32, 512, 64]"
  t904 = prims.slice_prim(t902, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t904: "cuda:0 bf16[1, 32, 512, 64]"
  t905 = prims.convert_element_type(t904, dtypes.float32)  # t905: "cuda:0 f32[1, 32, 512, 64]"
  t906 = prims.neg(t905)  # t906: "cuda:0 f32[1, 32, 512, 64]"
  t907 = prims.convert_element_type(t906, dtypes.bfloat16)  # t907: "cuda:0 bf16[1, 32, 512, 64]"
  t909 = prims.cat((t907, t903), -1)  # t909: "cuda:0 bf16[1, 32, 512, 128]"
  t910 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t910: "cuda:0 f32[1, 32, 512, 128]"
  t911 = prims.convert_element_type(t902, dtypes.float32)  # t911: "cuda:0 f32[1, 32, 512, 128]"
  t912 = ltorch.mul(t911, t910)  # t912: "cuda:0 f32[1, 32, 512, 128]"
    # t912 = prims.mul(t911, t910)  # t912: "cuda:0 f32[1, 32, 512, 128]"
  t913 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t913: "cuda:0 f32[1, 32, 512, 128]"
  t914 = prims.convert_element_type(t909, dtypes.float32)  # t914: "cuda:0 f32[1, 32, 512, 128]"
  t915 = ltorch.mul(t914, t913)  # t915: "cuda:0 f32[1, 32, 512, 128]"
    # t915 = prims.mul(t914, t913)  # t915: "cuda:0 f32[1, 32, 512, 128]"
  t916 = ltorch.add(t912, t915, alpha=None)  # t916: "cuda:0 f32[1, 32, 512, 128]"
    # t916 = prims.add(t912, t915)  # t916: "cuda:0 f32[1, 32, 512, 128]"
  t917 = prims.convert_element_type(t916, dtypes.bfloat16)  # t917: "cuda:0 bf16[1, 32, 512, 128]"
  t918 = prims.slice_prim(t895, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t918: "cuda:0 bf16[1, 32, 512, 128]"
  t919 = prims.slice_prim(t918, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t919: "cuda:0 bf16[1, 32, 512, 64]"
  t920 = prims.slice_prim(t918, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t920: "cuda:0 bf16[1, 32, 512, 64]"
  t921 = prims.convert_element_type(t920, dtypes.float32)  # t921: "cuda:0 f32[1, 32, 512, 64]"
  t922 = prims.neg(t921)  # t922: "cuda:0 f32[1, 32, 512, 64]"
  t923 = prims.convert_element_type(t922, dtypes.bfloat16)  # t923: "cuda:0 bf16[1, 32, 512, 64]"
  t925 = prims.cat((t923, t919), -1)  # t925: "cuda:0 bf16[1, 32, 512, 128]"
  t926 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t926: "cuda:0 f32[1, 32, 512, 128]"
  t927 = prims.convert_element_type(t918, dtypes.float32)  # t927: "cuda:0 f32[1, 32, 512, 128]"
  t928 = ltorch.mul(t927, t926)  # t928: "cuda:0 f32[1, 32, 512, 128]"
    # t928 = prims.mul(t927, t926)  # t928: "cuda:0 f32[1, 32, 512, 128]"
  t929 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t929: "cuda:0 f32[1, 32, 512, 128]"
  t930 = prims.convert_element_type(t925, dtypes.float32)  # t930: "cuda:0 f32[1, 32, 512, 128]"
  t931 = ltorch.mul(t930, t929)  # t931: "cuda:0 f32[1, 32, 512, 128]"
    # t931 = prims.mul(t930, t929)  # t931: "cuda:0 f32[1, 32, 512, 128]"
  t932 = ltorch.add(t928, t931, alpha=None)  # t932: "cuda:0 f32[1, 32, 512, 128]"
    # t932 = prims.add(t928, t931)  # t932: "cuda:0 f32[1, 32, 512, 128]"
  t933 = prims.convert_element_type(t932, dtypes.bfloat16)  # t933: "cuda:0 bf16[1, 32, 512, 128]"
  t934 = prims.slice_prim(t889, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t934: "cuda:0 bf16[1, 32, 512, 0]"
  t936 = prims.cat((t917, t934), -1)  # t936: "cuda:0 bf16[1, 32, 512, 128]"
  t937 = prims.slice_prim(t895, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t937: "cuda:0 bf16[1, 32, 512, 0]"
  t939 = prims.cat((t933, t937), -1)  # t939: "cuda:0 bf16[1, 32, 512, 128]"
  (t940, t941, t942, t943) = cudnn_sdpa_fwd(t936, t939, t901, None, 0.0, True, scale=0.08838834764831843)
  t946 = prims.transpose(t940, (0, 2, 1, 3))  # t946: "cuda:0 bf16[1, 512, 32, 128]"
  t950 = prims.reshape(t946, (1, 512, 4096))  # t950: "cuda:0 bf16[1, 512, 4096]"
  t951 = prims.linear(t950, t_transformer_h_6_attn_proj_weight, None)  # t951: "cuda:0 bf16[1, 512, 4096]"
  t952 = prims.convert_element_type(t951, dtypes.float32)  # t952: "cuda:0 f32[1, 512, 4096]"
  t953 = prims.convert_element_type(t849, dtypes.float32)  # t953: "cuda:0 f32[1, 512, 4096]"
  t954 = ltorch.add(t952, t953, alpha=None)  # t954: "cuda:0 f32[1, 512, 4096]"
    # t954 = prims.add(t952, t953)  # t954: "cuda:0 f32[1, 512, 4096]"
  t955 = prims.convert_element_type(t954, dtypes.bfloat16)  # t955: "cuda:0 bf16[1, 512, 4096]"
  t956 = prims.convert_element_type(t955, dtypes.float32)  # t956: "cuda:0 f32[1, 512, 4096]"
  t957 = ltorch.mul(t956, t956)  # t957: "cuda:0 f32[1, 512, 4096]"
    # t957 = prims.mul(t956, t956)  # t957: "cuda:0 f32[1, 512, 4096]"
  t959 = prims.sum(t957, (2,))  # t959: "cuda:0 f32[1, 512]"
  t960 = prims.broadcast_in_dim(t959, [1, 512, 1], [0, 1])  # t960: "cuda:0 f32[1, 512, 1]"
  t962 = ltorch.true_divide(t960, 4096.0)  # t962: "cuda:0 f32[1, 512, 1]"
    # t962 = prims.div(t960, 4096.0)  # t962: "cuda:0 f32[1, 512, 1]"
  t964 = ltorch.add(t962, 1e-05, alpha=None)  # t964: "cuda:0 f32[1, 512, 1]"
    # t964 = prims.add(t962, 1e-05)  # t964: "cuda:0 f32[1, 512, 1]"
  t965 = prims.rsqrt(t964)  # t965: "cuda:0 f32[1, 512, 1]"
  t966 = prims.broadcast_in_dim(t965, (1, 512, 4096), (0, 1, 2))  # t966: "cuda:0 f32[1, 512, 4096]"
  t967 = ltorch.mul(t956, t966)  # t967: "cuda:0 f32[1, 512, 4096]"
    # t967 = prims.mul(t956, t966)  # t967: "cuda:0 f32[1, 512, 4096]"
  t968 = prims.convert_element_type(t967, dtypes.bfloat16)  # t968: "cuda:0 bf16[1, 512, 4096]"
  t969 = prims.broadcast_in_dim(t_transformer_h_6_norm_2_weight, (1, 512, 4096), (2,))  # t969: "cuda:0 bf16[1, 512, 4096]"
  t970 = prims.convert_element_type(t968, dtypes.float32)  # t970: "cuda:0 f32[1, 512, 4096]"
  t971 = prims.convert_element_type(t969, dtypes.float32)  # t971: "cuda:0 f32[1, 512, 4096]"
  t972 = ltorch.mul(t970, t971)  # t972: "cuda:0 f32[1, 512, 4096]"
    # t972 = prims.mul(t970, t971)  # t972: "cuda:0 f32[1, 512, 4096]"
  t973 = prims.convert_element_type(t972, dtypes.bfloat16)  # t973: "cuda:0 bf16[1, 512, 4096]"
  t974 = prims.linear(t973, t_transformer_h_6_mlp_fc_1_weight, None)  # t974: "cuda:0 bf16[1, 512, 11008]"
  t975 = prims.linear(t973, t_transformer_h_6_mlp_fc_2_weight, None)  # t975: "cuda:0 bf16[1, 512, 11008]"
  t976 = prims.convert_element_type(t974, dtypes.float32)  # t976: "cuda:0 f32[1, 512, 11008]"
  t977 = prims.neg(t976)  # t977: "cuda:0 f32[1, 512, 11008]"
  t978 = prims.exp(t977)  # t978: "cuda:0 f32[1, 512, 11008]"
  t979 = ltorch.add(1.0, t978, alpha=None)  # t979: "cuda:0 f32[1, 512, 11008]"
    # t979 = prims.add(1.0, t978)  # t979: "cuda:0 f32[1, 512, 11008]"
  t980 = prims.reciprocal(t979)  # t980: "cuda:0 f32[1, 512, 11008]"
  t981 = prims.convert_element_type(t980, dtypes.bfloat16)  # t981: "cuda:0 bf16[1, 512, 11008]"
  t982 = prims.convert_element_type(t974, dtypes.float32)  # t982: "cuda:0 f32[1, 512, 11008]"
  t983 = prims.convert_element_type(t981, dtypes.float32)  # t983: "cuda:0 f32[1, 512, 11008]"
  t984 = ltorch.mul(t982, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
    # t984 = prims.mul(t982, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
  t985 = prims.convert_element_type(t984, dtypes.bfloat16)  # t985: "cuda:0 bf16[1, 512, 11008]"
  t986 = prims.convert_element_type(t985, dtypes.float32)  # t986: "cuda:0 f32[1, 512, 11008]"
  t987 = prims.convert_element_type(t975, dtypes.float32)  # t987: "cuda:0 f32[1, 512, 11008]"
  t988 = ltorch.mul(t986, t987)  # t988: "cuda:0 f32[1, 512, 11008]"
    # t988 = prims.mul(t986, t987)  # t988: "cuda:0 f32[1, 512, 11008]"
  t989 = prims.convert_element_type(t988, dtypes.bfloat16)  # t989: "cuda:0 bf16[1, 512, 11008]"
  t990 = prims.linear(t989, t_transformer_h_6_mlp_proj_weight, None)  # t990: "cuda:0 bf16[1, 512, 4096]"
  t991 = prims.convert_element_type(t990, dtypes.float32)  # t991: "cuda:0 f32[1, 512, 4096]"
  t992 = prims.convert_element_type(t955, dtypes.float32)  # t992: "cuda:0 f32[1, 512, 4096]"
  t993 = ltorch.add(t991, t992, alpha=None)  # t993: "cuda:0 f32[1, 512, 4096]"
    # t993 = prims.add(t991, t992)  # t993: "cuda:0 f32[1, 512, 4096]"
  t994 = prims.convert_element_type(t993, dtypes.bfloat16)  # t994: "cuda:0 bf16[1, 512, 4096]"
  t995 = prims.convert_element_type(t994, dtypes.float32)  # t995: "cuda:0 f32[1, 512, 4096]"
  t996 = ltorch.mul(t995, t995)  # t996: "cuda:0 f32[1, 512, 4096]"
    # t996 = prims.mul(t995, t995)  # t996: "cuda:0 f32[1, 512, 4096]"
  t998 = prims.sum(t996, (2,))  # t998: "cuda:0 f32[1, 512]"
  t999 = prims.broadcast_in_dim(t998, [1, 512, 1], [0, 1])  # t999: "cuda:0 f32[1, 512, 1]"
  t1001 = ltorch.true_divide(t999, 4096.0)  # t1001: "cuda:0 f32[1, 512, 1]"
    # t1001 = prims.div(t999, 4096.0)  # t1001: "cuda:0 f32[1, 512, 1]"
  t1003 = ltorch.add(t1001, 1e-05, alpha=None)  # t1003: "cuda:0 f32[1, 512, 1]"
    # t1003 = prims.add(t1001, 1e-05)  # t1003: "cuda:0 f32[1, 512, 1]"
  t1004 = prims.rsqrt(t1003)  # t1004: "cuda:0 f32[1, 512, 1]"
  t1005 = prims.broadcast_in_dim(t1004, (1, 512, 4096), (0, 1, 2))  # t1005: "cuda:0 f32[1, 512, 4096]"
  t1006 = ltorch.mul(t995, t1005)  # t1006: "cuda:0 f32[1, 512, 4096]"
    # t1006 = prims.mul(t995, t1005)  # t1006: "cuda:0 f32[1, 512, 4096]"
  t1007 = prims.convert_element_type(t1006, dtypes.bfloat16)  # t1007: "cuda:0 bf16[1, 512, 4096]"
  t1008 = prims.broadcast_in_dim(t_transformer_h_7_norm_1_weight, (1, 512, 4096), (2,))  # t1008: "cuda:0 bf16[1, 512, 4096]"
  t1009 = prims.convert_element_type(t1007, dtypes.float32)  # t1009: "cuda:0 f32[1, 512, 4096]"
  t1010 = prims.convert_element_type(t1008, dtypes.float32)  # t1010: "cuda:0 f32[1, 512, 4096]"
  t1011 = ltorch.mul(t1009, t1010)  # t1011: "cuda:0 f32[1, 512, 4096]"
    # t1011 = prims.mul(t1009, t1010)  # t1011: "cuda:0 f32[1, 512, 4096]"
  t1012 = prims.convert_element_type(t1011, dtypes.bfloat16)  # t1012: "cuda:0 bf16[1, 512, 4096]"
  t1013 = prims.linear(t1012, t_transformer_h_7_attn_attn_weight, None)  # t1013: "cuda:0 bf16[1, 512, 12288]"
  t1019 = prims.reshape(t1013, (1, 512, 32, 3, 128))  # t1019: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1025 = prims.transpose(t1019, (0, 2, 3, 1, 4))  # t1025: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1026, t1027, t1028) = ltorch.split(t1025, (1, 1, 1), 2)
    # t1026 = prims.slice_prim(t1025, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1026: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1027 = prims.slice_prim(t1025, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1027: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1028 = prims.slice_prim(t1025, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1028: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1034 = prims.reshape(t1026, (1, 32, 512, 128))  # t1034: "cuda:0 bf16[1, 32, 512, 128]"
  t1040 = prims.reshape(t1027, (1, 32, 512, 128))  # t1040: "cuda:0 bf16[1, 32, 512, 128]"
  t1046 = prims.reshape(t1028, (1, 32, 512, 128))  # t1046: "cuda:0 bf16[1, 32, 512, 128]"
  t1047 = prims.slice_prim(t1034, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1047: "cuda:0 bf16[1, 32, 512, 128]"
  t1048 = prims.slice_prim(t1047, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1048: "cuda:0 bf16[1, 32, 512, 64]"
  t1049 = prims.slice_prim(t1047, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1049: "cuda:0 bf16[1, 32, 512, 64]"
  t1050 = prims.convert_element_type(t1049, dtypes.float32)  # t1050: "cuda:0 f32[1, 32, 512, 64]"
  t1051 = prims.neg(t1050)  # t1051: "cuda:0 f32[1, 32, 512, 64]"
  t1052 = prims.convert_element_type(t1051, dtypes.bfloat16)  # t1052: "cuda:0 bf16[1, 32, 512, 64]"
  t1054 = prims.cat((t1052, t1048), -1)  # t1054: "cuda:0 bf16[1, 32, 512, 128]"
  t1055 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1055: "cuda:0 f32[1, 32, 512, 128]"
  t1056 = prims.convert_element_type(t1047, dtypes.float32)  # t1056: "cuda:0 f32[1, 32, 512, 128]"
  t1057 = ltorch.mul(t1056, t1055)  # t1057: "cuda:0 f32[1, 32, 512, 128]"
    # t1057 = prims.mul(t1056, t1055)  # t1057: "cuda:0 f32[1, 32, 512, 128]"
  t1058 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1058: "cuda:0 f32[1, 32, 512, 128]"
  t1059 = prims.convert_element_type(t1054, dtypes.float32)  # t1059: "cuda:0 f32[1, 32, 512, 128]"
  t1060 = ltorch.mul(t1059, t1058)  # t1060: "cuda:0 f32[1, 32, 512, 128]"
    # t1060 = prims.mul(t1059, t1058)  # t1060: "cuda:0 f32[1, 32, 512, 128]"
  t1061 = ltorch.add(t1057, t1060, alpha=None)  # t1061: "cuda:0 f32[1, 32, 512, 128]"
    # t1061 = prims.add(t1057, t1060)  # t1061: "cuda:0 f32[1, 32, 512, 128]"
  t1062 = prims.convert_element_type(t1061, dtypes.bfloat16)  # t1062: "cuda:0 bf16[1, 32, 512, 128]"
  t1063 = prims.slice_prim(t1040, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1063: "cuda:0 bf16[1, 32, 512, 128]"
  t1064 = prims.slice_prim(t1063, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1064: "cuda:0 bf16[1, 32, 512, 64]"
  t1065 = prims.slice_prim(t1063, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1065: "cuda:0 bf16[1, 32, 512, 64]"
  t1066 = prims.convert_element_type(t1065, dtypes.float32)  # t1066: "cuda:0 f32[1, 32, 512, 64]"
  t1067 = prims.neg(t1066)  # t1067: "cuda:0 f32[1, 32, 512, 64]"
  t1068 = prims.convert_element_type(t1067, dtypes.bfloat16)  # t1068: "cuda:0 bf16[1, 32, 512, 64]"
  t1070 = prims.cat((t1068, t1064), -1)  # t1070: "cuda:0 bf16[1, 32, 512, 128]"
  t1071 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1071: "cuda:0 f32[1, 32, 512, 128]"
  t1072 = prims.convert_element_type(t1063, dtypes.float32)  # t1072: "cuda:0 f32[1, 32, 512, 128]"
  t1073 = ltorch.mul(t1072, t1071)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
    # t1073 = prims.mul(t1072, t1071)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
  t1074 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1074: "cuda:0 f32[1, 32, 512, 128]"
  t1075 = prims.convert_element_type(t1070, dtypes.float32)  # t1075: "cuda:0 f32[1, 32, 512, 128]"
  t1076 = ltorch.mul(t1075, t1074)  # t1076: "cuda:0 f32[1, 32, 512, 128]"
    # t1076 = prims.mul(t1075, t1074)  # t1076: "cuda:0 f32[1, 32, 512, 128]"
  t1077 = ltorch.add(t1073, t1076, alpha=None)  # t1077: "cuda:0 f32[1, 32, 512, 128]"
    # t1077 = prims.add(t1073, t1076)  # t1077: "cuda:0 f32[1, 32, 512, 128]"
  t1078 = prims.convert_element_type(t1077, dtypes.bfloat16)  # t1078: "cuda:0 bf16[1, 32, 512, 128]"
  t1079 = prims.slice_prim(t1034, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1079: "cuda:0 bf16[1, 32, 512, 0]"
  t1081 = prims.cat((t1062, t1079), -1)  # t1081: "cuda:0 bf16[1, 32, 512, 128]"
  t1082 = prims.slice_prim(t1040, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1082: "cuda:0 bf16[1, 32, 512, 0]"
  t1084 = prims.cat((t1078, t1082), -1)  # t1084: "cuda:0 bf16[1, 32, 512, 128]"
  (t1085, t1086, t1087, t1088) = cudnn_sdpa_fwd(t1081, t1084, t1046, None, 0.0, True, scale=0.08838834764831843)
  t1091 = prims.transpose(t1085, (0, 2, 1, 3))  # t1091: "cuda:0 bf16[1, 512, 32, 128]"
  t1095 = prims.reshape(t1091, (1, 512, 4096))  # t1095: "cuda:0 bf16[1, 512, 4096]"
  t1096 = prims.linear(t1095, t_transformer_h_7_attn_proj_weight, None)  # t1096: "cuda:0 bf16[1, 512, 4096]"
  t1097 = prims.convert_element_type(t1096, dtypes.float32)  # t1097: "cuda:0 f32[1, 512, 4096]"
  t1098 = prims.convert_element_type(t994, dtypes.float32)  # t1098: "cuda:0 f32[1, 512, 4096]"
  t1099 = ltorch.add(t1097, t1098, alpha=None)  # t1099: "cuda:0 f32[1, 512, 4096]"
    # t1099 = prims.add(t1097, t1098)  # t1099: "cuda:0 f32[1, 512, 4096]"
  t1100 = prims.convert_element_type(t1099, dtypes.bfloat16)  # t1100: "cuda:0 bf16[1, 512, 4096]"
  t1101 = prims.convert_element_type(t1100, dtypes.float32)  # t1101: "cuda:0 f32[1, 512, 4096]"
  t1102 = ltorch.mul(t1101, t1101)  # t1102: "cuda:0 f32[1, 512, 4096]"
    # t1102 = prims.mul(t1101, t1101)  # t1102: "cuda:0 f32[1, 512, 4096]"
  t1104 = prims.sum(t1102, (2,))  # t1104: "cuda:0 f32[1, 512]"
  t1105 = prims.broadcast_in_dim(t1104, [1, 512, 1], [0, 1])  # t1105: "cuda:0 f32[1, 512, 1]"
  t1107 = ltorch.true_divide(t1105, 4096.0)  # t1107: "cuda:0 f32[1, 512, 1]"
    # t1107 = prims.div(t1105, 4096.0)  # t1107: "cuda:0 f32[1, 512, 1]"
  t1109 = ltorch.add(t1107, 1e-05, alpha=None)  # t1109: "cuda:0 f32[1, 512, 1]"
    # t1109 = prims.add(t1107, 1e-05)  # t1109: "cuda:0 f32[1, 512, 1]"
  t1110 = prims.rsqrt(t1109)  # t1110: "cuda:0 f32[1, 512, 1]"
  t1111 = prims.broadcast_in_dim(t1110, (1, 512, 4096), (0, 1, 2))  # t1111: "cuda:0 f32[1, 512, 4096]"
  t1112 = ltorch.mul(t1101, t1111)  # t1112: "cuda:0 f32[1, 512, 4096]"
    # t1112 = prims.mul(t1101, t1111)  # t1112: "cuda:0 f32[1, 512, 4096]"
  t1113 = prims.convert_element_type(t1112, dtypes.bfloat16)  # t1113: "cuda:0 bf16[1, 512, 4096]"
  t1114 = prims.broadcast_in_dim(t_transformer_h_7_norm_2_weight, (1, 512, 4096), (2,))  # t1114: "cuda:0 bf16[1, 512, 4096]"
  t1115 = prims.convert_element_type(t1113, dtypes.float32)  # t1115: "cuda:0 f32[1, 512, 4096]"
  t1116 = prims.convert_element_type(t1114, dtypes.float32)  # t1116: "cuda:0 f32[1, 512, 4096]"
  t1117 = ltorch.mul(t1115, t1116)  # t1117: "cuda:0 f32[1, 512, 4096]"
    # t1117 = prims.mul(t1115, t1116)  # t1117: "cuda:0 f32[1, 512, 4096]"
  t1118 = prims.convert_element_type(t1117, dtypes.bfloat16)  # t1118: "cuda:0 bf16[1, 512, 4096]"
  t1119 = prims.linear(t1118, t_transformer_h_7_mlp_fc_1_weight, None)  # t1119: "cuda:0 bf16[1, 512, 11008]"
  t1120 = prims.linear(t1118, t_transformer_h_7_mlp_fc_2_weight, None)  # t1120: "cuda:0 bf16[1, 512, 11008]"
  t1121 = prims.convert_element_type(t1119, dtypes.float32)  # t1121: "cuda:0 f32[1, 512, 11008]"
  t1122 = prims.neg(t1121)  # t1122: "cuda:0 f32[1, 512, 11008]"
  t1123 = prims.exp(t1122)  # t1123: "cuda:0 f32[1, 512, 11008]"
  t1124 = ltorch.add(1.0, t1123, alpha=None)  # t1124: "cuda:0 f32[1, 512, 11008]"
    # t1124 = prims.add(1.0, t1123)  # t1124: "cuda:0 f32[1, 512, 11008]"
  t1125 = prims.reciprocal(t1124)  # t1125: "cuda:0 f32[1, 512, 11008]"
  t1126 = prims.convert_element_type(t1125, dtypes.bfloat16)  # t1126: "cuda:0 bf16[1, 512, 11008]"
  t1127 = prims.convert_element_type(t1119, dtypes.float32)  # t1127: "cuda:0 f32[1, 512, 11008]"
  t1128 = prims.convert_element_type(t1126, dtypes.float32)  # t1128: "cuda:0 f32[1, 512, 11008]"
  t1129 = ltorch.mul(t1127, t1128)  # t1129: "cuda:0 f32[1, 512, 11008]"
    # t1129 = prims.mul(t1127, t1128)  # t1129: "cuda:0 f32[1, 512, 11008]"
  t1130 = prims.convert_element_type(t1129, dtypes.bfloat16)  # t1130: "cuda:0 bf16[1, 512, 11008]"
  t1131 = prims.convert_element_type(t1130, dtypes.float32)  # t1131: "cuda:0 f32[1, 512, 11008]"
  t1132 = prims.convert_element_type(t1120, dtypes.float32)  # t1132: "cuda:0 f32[1, 512, 11008]"
  t1133 = ltorch.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 11008]"
    # t1133 = prims.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 11008]"
  t1134 = prims.convert_element_type(t1133, dtypes.bfloat16)  # t1134: "cuda:0 bf16[1, 512, 11008]"
  t1135 = prims.linear(t1134, t_transformer_h_7_mlp_proj_weight, None)  # t1135: "cuda:0 bf16[1, 512, 4096]"
  t1136 = prims.convert_element_type(t1135, dtypes.float32)  # t1136: "cuda:0 f32[1, 512, 4096]"
  t1137 = prims.convert_element_type(t1100, dtypes.float32)  # t1137: "cuda:0 f32[1, 512, 4096]"
  t1138 = ltorch.add(t1136, t1137, alpha=None)  # t1138: "cuda:0 f32[1, 512, 4096]"
    # t1138 = prims.add(t1136, t1137)  # t1138: "cuda:0 f32[1, 512, 4096]"
  t1139 = prims.convert_element_type(t1138, dtypes.bfloat16)  # t1139: "cuda:0 bf16[1, 512, 4096]"
  t1140 = prims.convert_element_type(t1139, dtypes.float32)  # t1140: "cuda:0 f32[1, 512, 4096]"
  t1141 = ltorch.mul(t1140, t1140)  # t1141: "cuda:0 f32[1, 512, 4096]"
    # t1141 = prims.mul(t1140, t1140)  # t1141: "cuda:0 f32[1, 512, 4096]"
  t1143 = prims.sum(t1141, (2,))  # t1143: "cuda:0 f32[1, 512]"
  t1144 = prims.broadcast_in_dim(t1143, [1, 512, 1], [0, 1])  # t1144: "cuda:0 f32[1, 512, 1]"
  t1146 = ltorch.true_divide(t1144, 4096.0)  # t1146: "cuda:0 f32[1, 512, 1]"
    # t1146 = prims.div(t1144, 4096.0)  # t1146: "cuda:0 f32[1, 512, 1]"
  t1148 = ltorch.add(t1146, 1e-05, alpha=None)  # t1148: "cuda:0 f32[1, 512, 1]"
    # t1148 = prims.add(t1146, 1e-05)  # t1148: "cuda:0 f32[1, 512, 1]"
  t1149 = prims.rsqrt(t1148)  # t1149: "cuda:0 f32[1, 512, 1]"
  t1150 = prims.broadcast_in_dim(t1149, (1, 512, 4096), (0, 1, 2))  # t1150: "cuda:0 f32[1, 512, 4096]"
  t1151 = ltorch.mul(t1140, t1150)  # t1151: "cuda:0 f32[1, 512, 4096]"
    # t1151 = prims.mul(t1140, t1150)  # t1151: "cuda:0 f32[1, 512, 4096]"
  t1152 = prims.convert_element_type(t1151, dtypes.bfloat16)  # t1152: "cuda:0 bf16[1, 512, 4096]"
  t1153 = prims.broadcast_in_dim(t_transformer_h_8_norm_1_weight, (1, 512, 4096), (2,))  # t1153: "cuda:0 bf16[1, 512, 4096]"
  t1154 = prims.convert_element_type(t1152, dtypes.float32)  # t1154: "cuda:0 f32[1, 512, 4096]"
  t1155 = prims.convert_element_type(t1153, dtypes.float32)  # t1155: "cuda:0 f32[1, 512, 4096]"
  t1156 = ltorch.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 4096]"
    # t1156 = prims.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 4096]"
  t1157 = prims.convert_element_type(t1156, dtypes.bfloat16)  # t1157: "cuda:0 bf16[1, 512, 4096]"
  t1158 = prims.linear(t1157, t_transformer_h_8_attn_attn_weight, None)  # t1158: "cuda:0 bf16[1, 512, 12288]"
  t1164 = prims.reshape(t1158, (1, 512, 32, 3, 128))  # t1164: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1170 = prims.transpose(t1164, (0, 2, 3, 1, 4))  # t1170: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1171, t1172, t1173) = ltorch.split(t1170, (1, 1, 1), 2)
    # t1171 = prims.slice_prim(t1170, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1171: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1172 = prims.slice_prim(t1170, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1172: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1173 = prims.slice_prim(t1170, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1173: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1179 = prims.reshape(t1171, (1, 32, 512, 128))  # t1179: "cuda:0 bf16[1, 32, 512, 128]"
  t1185 = prims.reshape(t1172, (1, 32, 512, 128))  # t1185: "cuda:0 bf16[1, 32, 512, 128]"
  t1191 = prims.reshape(t1173, (1, 32, 512, 128))  # t1191: "cuda:0 bf16[1, 32, 512, 128]"
  t1192 = prims.slice_prim(t1179, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1192: "cuda:0 bf16[1, 32, 512, 128]"
  t1193 = prims.slice_prim(t1192, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1193: "cuda:0 bf16[1, 32, 512, 64]"
  t1194 = prims.slice_prim(t1192, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1194: "cuda:0 bf16[1, 32, 512, 64]"
  t1195 = prims.convert_element_type(t1194, dtypes.float32)  # t1195: "cuda:0 f32[1, 32, 512, 64]"
  t1196 = prims.neg(t1195)  # t1196: "cuda:0 f32[1, 32, 512, 64]"
  t1197 = prims.convert_element_type(t1196, dtypes.bfloat16)  # t1197: "cuda:0 bf16[1, 32, 512, 64]"
  t1199 = prims.cat((t1197, t1193), -1)  # t1199: "cuda:0 bf16[1, 32, 512, 128]"
  t1200 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1200: "cuda:0 f32[1, 32, 512, 128]"
  t1201 = prims.convert_element_type(t1192, dtypes.float32)  # t1201: "cuda:0 f32[1, 32, 512, 128]"
  t1202 = ltorch.mul(t1201, t1200)  # t1202: "cuda:0 f32[1, 32, 512, 128]"
    # t1202 = prims.mul(t1201, t1200)  # t1202: "cuda:0 f32[1, 32, 512, 128]"
  t1203 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1203: "cuda:0 f32[1, 32, 512, 128]"
  t1204 = prims.convert_element_type(t1199, dtypes.float32)  # t1204: "cuda:0 f32[1, 32, 512, 128]"
  t1205 = ltorch.mul(t1204, t1203)  # t1205: "cuda:0 f32[1, 32, 512, 128]"
    # t1205 = prims.mul(t1204, t1203)  # t1205: "cuda:0 f32[1, 32, 512, 128]"
  t1206 = ltorch.add(t1202, t1205, alpha=None)  # t1206: "cuda:0 f32[1, 32, 512, 128]"
    # t1206 = prims.add(t1202, t1205)  # t1206: "cuda:0 f32[1, 32, 512, 128]"
  t1207 = prims.convert_element_type(t1206, dtypes.bfloat16)  # t1207: "cuda:0 bf16[1, 32, 512, 128]"
  t1208 = prims.slice_prim(t1185, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1208: "cuda:0 bf16[1, 32, 512, 128]"
  t1209 = prims.slice_prim(t1208, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1209: "cuda:0 bf16[1, 32, 512, 64]"
  t1210 = prims.slice_prim(t1208, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1210: "cuda:0 bf16[1, 32, 512, 64]"
  t1211 = prims.convert_element_type(t1210, dtypes.float32)  # t1211: "cuda:0 f32[1, 32, 512, 64]"
  t1212 = prims.neg(t1211)  # t1212: "cuda:0 f32[1, 32, 512, 64]"
  t1213 = prims.convert_element_type(t1212, dtypes.bfloat16)  # t1213: "cuda:0 bf16[1, 32, 512, 64]"
  t1215 = prims.cat((t1213, t1209), -1)  # t1215: "cuda:0 bf16[1, 32, 512, 128]"
  t1216 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1216: "cuda:0 f32[1, 32, 512, 128]"
  t1217 = prims.convert_element_type(t1208, dtypes.float32)  # t1217: "cuda:0 f32[1, 32, 512, 128]"
  t1218 = ltorch.mul(t1217, t1216)  # t1218: "cuda:0 f32[1, 32, 512, 128]"
    # t1218 = prims.mul(t1217, t1216)  # t1218: "cuda:0 f32[1, 32, 512, 128]"
  t1219 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1219: "cuda:0 f32[1, 32, 512, 128]"
  t1220 = prims.convert_element_type(t1215, dtypes.float32)  # t1220: "cuda:0 f32[1, 32, 512, 128]"
  t1221 = ltorch.mul(t1220, t1219)  # t1221: "cuda:0 f32[1, 32, 512, 128]"
    # t1221 = prims.mul(t1220, t1219)  # t1221: "cuda:0 f32[1, 32, 512, 128]"
  t1222 = ltorch.add(t1218, t1221, alpha=None)  # t1222: "cuda:0 f32[1, 32, 512, 128]"
    # t1222 = prims.add(t1218, t1221)  # t1222: "cuda:0 f32[1, 32, 512, 128]"
  t1223 = prims.convert_element_type(t1222, dtypes.bfloat16)  # t1223: "cuda:0 bf16[1, 32, 512, 128]"
  t1224 = prims.slice_prim(t1179, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1224: "cuda:0 bf16[1, 32, 512, 0]"
  t1226 = prims.cat((t1207, t1224), -1)  # t1226: "cuda:0 bf16[1, 32, 512, 128]"
  t1227 = prims.slice_prim(t1185, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1227: "cuda:0 bf16[1, 32, 512, 0]"
  t1229 = prims.cat((t1223, t1227), -1)  # t1229: "cuda:0 bf16[1, 32, 512, 128]"
  (t1230, t1231, t1232, t1233) = cudnn_sdpa_fwd(t1226, t1229, t1191, None, 0.0, True, scale=0.08838834764831843)
  t1236 = prims.transpose(t1230, (0, 2, 1, 3))  # t1236: "cuda:0 bf16[1, 512, 32, 128]"
  t1240 = prims.reshape(t1236, (1, 512, 4096))  # t1240: "cuda:0 bf16[1, 512, 4096]"
  t1241 = prims.linear(t1240, t_transformer_h_8_attn_proj_weight, None)  # t1241: "cuda:0 bf16[1, 512, 4096]"
  t1242 = prims.convert_element_type(t1241, dtypes.float32)  # t1242: "cuda:0 f32[1, 512, 4096]"
  t1243 = prims.convert_element_type(t1139, dtypes.float32)  # t1243: "cuda:0 f32[1, 512, 4096]"
  t1244 = ltorch.add(t1242, t1243, alpha=None)  # t1244: "cuda:0 f32[1, 512, 4096]"
    # t1244 = prims.add(t1242, t1243)  # t1244: "cuda:0 f32[1, 512, 4096]"
  t1245 = prims.convert_element_type(t1244, dtypes.bfloat16)  # t1245: "cuda:0 bf16[1, 512, 4096]"
  t1246 = prims.convert_element_type(t1245, dtypes.float32)  # t1246: "cuda:0 f32[1, 512, 4096]"
  t1247 = ltorch.mul(t1246, t1246)  # t1247: "cuda:0 f32[1, 512, 4096]"
    # t1247 = prims.mul(t1246, t1246)  # t1247: "cuda:0 f32[1, 512, 4096]"
  t1249 = prims.sum(t1247, (2,))  # t1249: "cuda:0 f32[1, 512]"
  t1250 = prims.broadcast_in_dim(t1249, [1, 512, 1], [0, 1])  # t1250: "cuda:0 f32[1, 512, 1]"
  t1252 = ltorch.true_divide(t1250, 4096.0)  # t1252: "cuda:0 f32[1, 512, 1]"
    # t1252 = prims.div(t1250, 4096.0)  # t1252: "cuda:0 f32[1, 512, 1]"
  t1254 = ltorch.add(t1252, 1e-05, alpha=None)  # t1254: "cuda:0 f32[1, 512, 1]"
    # t1254 = prims.add(t1252, 1e-05)  # t1254: "cuda:0 f32[1, 512, 1]"
  t1255 = prims.rsqrt(t1254)  # t1255: "cuda:0 f32[1, 512, 1]"
  t1256 = prims.broadcast_in_dim(t1255, (1, 512, 4096), (0, 1, 2))  # t1256: "cuda:0 f32[1, 512, 4096]"
  t1257 = ltorch.mul(t1246, t1256)  # t1257: "cuda:0 f32[1, 512, 4096]"
    # t1257 = prims.mul(t1246, t1256)  # t1257: "cuda:0 f32[1, 512, 4096]"
  t1258 = prims.convert_element_type(t1257, dtypes.bfloat16)  # t1258: "cuda:0 bf16[1, 512, 4096]"
  t1259 = prims.broadcast_in_dim(t_transformer_h_8_norm_2_weight, (1, 512, 4096), (2,))  # t1259: "cuda:0 bf16[1, 512, 4096]"
  t1260 = prims.convert_element_type(t1258, dtypes.float32)  # t1260: "cuda:0 f32[1, 512, 4096]"
  t1261 = prims.convert_element_type(t1259, dtypes.float32)  # t1261: "cuda:0 f32[1, 512, 4096]"
  t1262 = ltorch.mul(t1260, t1261)  # t1262: "cuda:0 f32[1, 512, 4096]"
    # t1262 = prims.mul(t1260, t1261)  # t1262: "cuda:0 f32[1, 512, 4096]"
  t1263 = prims.convert_element_type(t1262, dtypes.bfloat16)  # t1263: "cuda:0 bf16[1, 512, 4096]"
  t1264 = prims.linear(t1263, t_transformer_h_8_mlp_fc_1_weight, None)  # t1264: "cuda:0 bf16[1, 512, 11008]"
  t1265 = prims.linear(t1263, t_transformer_h_8_mlp_fc_2_weight, None)  # t1265: "cuda:0 bf16[1, 512, 11008]"
  t1266 = prims.convert_element_type(t1264, dtypes.float32)  # t1266: "cuda:0 f32[1, 512, 11008]"
  t1267 = prims.neg(t1266)  # t1267: "cuda:0 f32[1, 512, 11008]"
  t1268 = prims.exp(t1267)  # t1268: "cuda:0 f32[1, 512, 11008]"
  t1269 = ltorch.add(1.0, t1268, alpha=None)  # t1269: "cuda:0 f32[1, 512, 11008]"
    # t1269 = prims.add(1.0, t1268)  # t1269: "cuda:0 f32[1, 512, 11008]"
  t1270 = prims.reciprocal(t1269)  # t1270: "cuda:0 f32[1, 512, 11008]"
  t1271 = prims.convert_element_type(t1270, dtypes.bfloat16)  # t1271: "cuda:0 bf16[1, 512, 11008]"
  t1272 = prims.convert_element_type(t1264, dtypes.float32)  # t1272: "cuda:0 f32[1, 512, 11008]"
  t1273 = prims.convert_element_type(t1271, dtypes.float32)  # t1273: "cuda:0 f32[1, 512, 11008]"
  t1274 = ltorch.mul(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 11008]"
    # t1274 = prims.mul(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 11008]"
  t1275 = prims.convert_element_type(t1274, dtypes.bfloat16)  # t1275: "cuda:0 bf16[1, 512, 11008]"
  t1276 = prims.convert_element_type(t1275, dtypes.float32)  # t1276: "cuda:0 f32[1, 512, 11008]"
  t1277 = prims.convert_element_type(t1265, dtypes.float32)  # t1277: "cuda:0 f32[1, 512, 11008]"
  t1278 = ltorch.mul(t1276, t1277)  # t1278: "cuda:0 f32[1, 512, 11008]"
    # t1278 = prims.mul(t1276, t1277)  # t1278: "cuda:0 f32[1, 512, 11008]"
  t1279 = prims.convert_element_type(t1278, dtypes.bfloat16)  # t1279: "cuda:0 bf16[1, 512, 11008]"
  t1280 = prims.linear(t1279, t_transformer_h_8_mlp_proj_weight, None)  # t1280: "cuda:0 bf16[1, 512, 4096]"
  t1281 = prims.convert_element_type(t1280, dtypes.float32)  # t1281: "cuda:0 f32[1, 512, 4096]"
  t1282 = prims.convert_element_type(t1245, dtypes.float32)  # t1282: "cuda:0 f32[1, 512, 4096]"
  t1283 = ltorch.add(t1281, t1282, alpha=None)  # t1283: "cuda:0 f32[1, 512, 4096]"
    # t1283 = prims.add(t1281, t1282)  # t1283: "cuda:0 f32[1, 512, 4096]"
  t1284 = prims.convert_element_type(t1283, dtypes.bfloat16)  # t1284: "cuda:0 bf16[1, 512, 4096]"
  t1285 = prims.convert_element_type(t1284, dtypes.float32)  # t1285: "cuda:0 f32[1, 512, 4096]"
  t1286 = ltorch.mul(t1285, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"
    # t1286 = prims.mul(t1285, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"
  t1288 = prims.sum(t1286, (2,))  # t1288: "cuda:0 f32[1, 512]"
  t1289 = prims.broadcast_in_dim(t1288, [1, 512, 1], [0, 1])  # t1289: "cuda:0 f32[1, 512, 1]"
  t1291 = ltorch.true_divide(t1289, 4096.0)  # t1291: "cuda:0 f32[1, 512, 1]"
    # t1291 = prims.div(t1289, 4096.0)  # t1291: "cuda:0 f32[1, 512, 1]"
  t1293 = ltorch.add(t1291, 1e-05, alpha=None)  # t1293: "cuda:0 f32[1, 512, 1]"
    # t1293 = prims.add(t1291, 1e-05)  # t1293: "cuda:0 f32[1, 512, 1]"
  t1294 = prims.rsqrt(t1293)  # t1294: "cuda:0 f32[1, 512, 1]"
  t1295 = prims.broadcast_in_dim(t1294, (1, 512, 4096), (0, 1, 2))  # t1295: "cuda:0 f32[1, 512, 4096]"
  t1296 = ltorch.mul(t1285, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
    # t1296 = prims.mul(t1285, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
  t1297 = prims.convert_element_type(t1296, dtypes.bfloat16)  # t1297: "cuda:0 bf16[1, 512, 4096]"
  t1298 = prims.broadcast_in_dim(t_transformer_h_9_norm_1_weight, (1, 512, 4096), (2,))  # t1298: "cuda:0 bf16[1, 512, 4096]"
  t1299 = prims.convert_element_type(t1297, dtypes.float32)  # t1299: "cuda:0 f32[1, 512, 4096]"
  t1300 = prims.convert_element_type(t1298, dtypes.float32)  # t1300: "cuda:0 f32[1, 512, 4096]"
  t1301 = ltorch.mul(t1299, t1300)  # t1301: "cuda:0 f32[1, 512, 4096]"
    # t1301 = prims.mul(t1299, t1300)  # t1301: "cuda:0 f32[1, 512, 4096]"
  t1302 = prims.convert_element_type(t1301, dtypes.bfloat16)  # t1302: "cuda:0 bf16[1, 512, 4096]"
  t1303 = prims.linear(t1302, t_transformer_h_9_attn_attn_weight, None)  # t1303: "cuda:0 bf16[1, 512, 12288]"
  t1309 = prims.reshape(t1303, (1, 512, 32, 3, 128))  # t1309: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1315 = prims.transpose(t1309, (0, 2, 3, 1, 4))  # t1315: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1316, t1317, t1318) = ltorch.split(t1315, (1, 1, 1), 2)
    # t1316 = prims.slice_prim(t1315, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1316: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1317 = prims.slice_prim(t1315, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1317: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1318 = prims.slice_prim(t1315, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1318: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1324 = prims.reshape(t1316, (1, 32, 512, 128))  # t1324: "cuda:0 bf16[1, 32, 512, 128]"
  t1330 = prims.reshape(t1317, (1, 32, 512, 128))  # t1330: "cuda:0 bf16[1, 32, 512, 128]"
  t1336 = prims.reshape(t1318, (1, 32, 512, 128))  # t1336: "cuda:0 bf16[1, 32, 512, 128]"
  t1337 = prims.slice_prim(t1324, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1337: "cuda:0 bf16[1, 32, 512, 128]"
  t1338 = prims.slice_prim(t1337, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1338: "cuda:0 bf16[1, 32, 512, 64]"
  t1339 = prims.slice_prim(t1337, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1339: "cuda:0 bf16[1, 32, 512, 64]"
  t1340 = prims.convert_element_type(t1339, dtypes.float32)  # t1340: "cuda:0 f32[1, 32, 512, 64]"
  t1341 = prims.neg(t1340)  # t1341: "cuda:0 f32[1, 32, 512, 64]"
  t1342 = prims.convert_element_type(t1341, dtypes.bfloat16)  # t1342: "cuda:0 bf16[1, 32, 512, 64]"
  t1344 = prims.cat((t1342, t1338), -1)  # t1344: "cuda:0 bf16[1, 32, 512, 128]"
  t1345 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1345: "cuda:0 f32[1, 32, 512, 128]"
  t1346 = prims.convert_element_type(t1337, dtypes.float32)  # t1346: "cuda:0 f32[1, 32, 512, 128]"
  t1347 = ltorch.mul(t1346, t1345)  # t1347: "cuda:0 f32[1, 32, 512, 128]"
    # t1347 = prims.mul(t1346, t1345)  # t1347: "cuda:0 f32[1, 32, 512, 128]"
  t1348 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1348: "cuda:0 f32[1, 32, 512, 128]"
  t1349 = prims.convert_element_type(t1344, dtypes.float32)  # t1349: "cuda:0 f32[1, 32, 512, 128]"
  t1350 = ltorch.mul(t1349, t1348)  # t1350: "cuda:0 f32[1, 32, 512, 128]"
    # t1350 = prims.mul(t1349, t1348)  # t1350: "cuda:0 f32[1, 32, 512, 128]"
  t1351 = ltorch.add(t1347, t1350, alpha=None)  # t1351: "cuda:0 f32[1, 32, 512, 128]"
    # t1351 = prims.add(t1347, t1350)  # t1351: "cuda:0 f32[1, 32, 512, 128]"
  t1352 = prims.convert_element_type(t1351, dtypes.bfloat16)  # t1352: "cuda:0 bf16[1, 32, 512, 128]"
  t1353 = prims.slice_prim(t1330, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1353: "cuda:0 bf16[1, 32, 512, 128]"
  t1354 = prims.slice_prim(t1353, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1354: "cuda:0 bf16[1, 32, 512, 64]"
  t1355 = prims.slice_prim(t1353, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1355: "cuda:0 bf16[1, 32, 512, 64]"
  t1356 = prims.convert_element_type(t1355, dtypes.float32)  # t1356: "cuda:0 f32[1, 32, 512, 64]"
  t1357 = prims.neg(t1356)  # t1357: "cuda:0 f32[1, 32, 512, 64]"
  t1358 = prims.convert_element_type(t1357, dtypes.bfloat16)  # t1358: "cuda:0 bf16[1, 32, 512, 64]"
  t1360 = prims.cat((t1358, t1354), -1)  # t1360: "cuda:0 bf16[1, 32, 512, 128]"
  t1361 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1361: "cuda:0 f32[1, 32, 512, 128]"
  t1362 = prims.convert_element_type(t1353, dtypes.float32)  # t1362: "cuda:0 f32[1, 32, 512, 128]"
  t1363 = ltorch.mul(t1362, t1361)  # t1363: "cuda:0 f32[1, 32, 512, 128]"
    # t1363 = prims.mul(t1362, t1361)  # t1363: "cuda:0 f32[1, 32, 512, 128]"
  t1364 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1364: "cuda:0 f32[1, 32, 512, 128]"
  t1365 = prims.convert_element_type(t1360, dtypes.float32)  # t1365: "cuda:0 f32[1, 32, 512, 128]"
  t1366 = ltorch.mul(t1365, t1364)  # t1366: "cuda:0 f32[1, 32, 512, 128]"
    # t1366 = prims.mul(t1365, t1364)  # t1366: "cuda:0 f32[1, 32, 512, 128]"
  t1367 = ltorch.add(t1363, t1366, alpha=None)  # t1367: "cuda:0 f32[1, 32, 512, 128]"
    # t1367 = prims.add(t1363, t1366)  # t1367: "cuda:0 f32[1, 32, 512, 128]"
  t1368 = prims.convert_element_type(t1367, dtypes.bfloat16)  # t1368: "cuda:0 bf16[1, 32, 512, 128]"
  t1369 = prims.slice_prim(t1324, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1369: "cuda:0 bf16[1, 32, 512, 0]"
  t1371 = prims.cat((t1352, t1369), -1)  # t1371: "cuda:0 bf16[1, 32, 512, 128]"
  t1372 = prims.slice_prim(t1330, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1372: "cuda:0 bf16[1, 32, 512, 0]"
  t1374 = prims.cat((t1368, t1372), -1)  # t1374: "cuda:0 bf16[1, 32, 512, 128]"
  (t1375, t1376, t1377, t1378) = cudnn_sdpa_fwd(t1371, t1374, t1336, None, 0.0, True, scale=0.08838834764831843)
  t1381 = prims.transpose(t1375, (0, 2, 1, 3))  # t1381: "cuda:0 bf16[1, 512, 32, 128]"
  t1385 = prims.reshape(t1381, (1, 512, 4096))  # t1385: "cuda:0 bf16[1, 512, 4096]"
  t1386 = prims.linear(t1385, t_transformer_h_9_attn_proj_weight, None)  # t1386: "cuda:0 bf16[1, 512, 4096]"
  t1387 = prims.convert_element_type(t1386, dtypes.float32)  # t1387: "cuda:0 f32[1, 512, 4096]"
  t1388 = prims.convert_element_type(t1284, dtypes.float32)  # t1388: "cuda:0 f32[1, 512, 4096]"
  t1389 = ltorch.add(t1387, t1388, alpha=None)  # t1389: "cuda:0 f32[1, 512, 4096]"
    # t1389 = prims.add(t1387, t1388)  # t1389: "cuda:0 f32[1, 512, 4096]"
  t1390 = prims.convert_element_type(t1389, dtypes.bfloat16)  # t1390: "cuda:0 bf16[1, 512, 4096]"
  t1391 = prims.convert_element_type(t1390, dtypes.float32)  # t1391: "cuda:0 f32[1, 512, 4096]"
  t1392 = ltorch.mul(t1391, t1391)  # t1392: "cuda:0 f32[1, 512, 4096]"
    # t1392 = prims.mul(t1391, t1391)  # t1392: "cuda:0 f32[1, 512, 4096]"
  t1394 = prims.sum(t1392, (2,))  # t1394: "cuda:0 f32[1, 512]"
  t1395 = prims.broadcast_in_dim(t1394, [1, 512, 1], [0, 1])  # t1395: "cuda:0 f32[1, 512, 1]"
  t1397 = ltorch.true_divide(t1395, 4096.0)  # t1397: "cuda:0 f32[1, 512, 1]"
    # t1397 = prims.div(t1395, 4096.0)  # t1397: "cuda:0 f32[1, 512, 1]"
  t1399 = ltorch.add(t1397, 1e-05, alpha=None)  # t1399: "cuda:0 f32[1, 512, 1]"
    # t1399 = prims.add(t1397, 1e-05)  # t1399: "cuda:0 f32[1, 512, 1]"
  t1400 = prims.rsqrt(t1399)  # t1400: "cuda:0 f32[1, 512, 1]"
  t1401 = prims.broadcast_in_dim(t1400, (1, 512, 4096), (0, 1, 2))  # t1401: "cuda:0 f32[1, 512, 4096]"
  t1402 = ltorch.mul(t1391, t1401)  # t1402: "cuda:0 f32[1, 512, 4096]"
    # t1402 = prims.mul(t1391, t1401)  # t1402: "cuda:0 f32[1, 512, 4096]"
  t1403 = prims.convert_element_type(t1402, dtypes.bfloat16)  # t1403: "cuda:0 bf16[1, 512, 4096]"
  t1404 = prims.broadcast_in_dim(t_transformer_h_9_norm_2_weight, (1, 512, 4096), (2,))  # t1404: "cuda:0 bf16[1, 512, 4096]"
  t1405 = prims.convert_element_type(t1403, dtypes.float32)  # t1405: "cuda:0 f32[1, 512, 4096]"
  t1406 = prims.convert_element_type(t1404, dtypes.float32)  # t1406: "cuda:0 f32[1, 512, 4096]"
  t1407 = ltorch.mul(t1405, t1406)  # t1407: "cuda:0 f32[1, 512, 4096]"
    # t1407 = prims.mul(t1405, t1406)  # t1407: "cuda:0 f32[1, 512, 4096]"
  t1408 = prims.convert_element_type(t1407, dtypes.bfloat16)  # t1408: "cuda:0 bf16[1, 512, 4096]"
  t1409 = prims.linear(t1408, t_transformer_h_9_mlp_fc_1_weight, None)  # t1409: "cuda:0 bf16[1, 512, 11008]"
  t1410 = prims.linear(t1408, t_transformer_h_9_mlp_fc_2_weight, None)  # t1410: "cuda:0 bf16[1, 512, 11008]"
  t1411 = prims.convert_element_type(t1409, dtypes.float32)  # t1411: "cuda:0 f32[1, 512, 11008]"
  t1412 = prims.neg(t1411)  # t1412: "cuda:0 f32[1, 512, 11008]"
  t1413 = prims.exp(t1412)  # t1413: "cuda:0 f32[1, 512, 11008]"
  t1414 = ltorch.add(1.0, t1413, alpha=None)  # t1414: "cuda:0 f32[1, 512, 11008]"
    # t1414 = prims.add(1.0, t1413)  # t1414: "cuda:0 f32[1, 512, 11008]"
  t1415 = prims.reciprocal(t1414)  # t1415: "cuda:0 f32[1, 512, 11008]"
  t1416 = prims.convert_element_type(t1415, dtypes.bfloat16)  # t1416: "cuda:0 bf16[1, 512, 11008]"
  t1417 = prims.convert_element_type(t1409, dtypes.float32)  # t1417: "cuda:0 f32[1, 512, 11008]"
  t1418 = prims.convert_element_type(t1416, dtypes.float32)  # t1418: "cuda:0 f32[1, 512, 11008]"
  t1419 = ltorch.mul(t1417, t1418)  # t1419: "cuda:0 f32[1, 512, 11008]"
    # t1419 = prims.mul(t1417, t1418)  # t1419: "cuda:0 f32[1, 512, 11008]"
  t1420 = prims.convert_element_type(t1419, dtypes.bfloat16)  # t1420: "cuda:0 bf16[1, 512, 11008]"
  t1421 = prims.convert_element_type(t1420, dtypes.float32)  # t1421: "cuda:0 f32[1, 512, 11008]"
  t1422 = prims.convert_element_type(t1410, dtypes.float32)  # t1422: "cuda:0 f32[1, 512, 11008]"
  t1423 = ltorch.mul(t1421, t1422)  # t1423: "cuda:0 f32[1, 512, 11008]"
    # t1423 = prims.mul(t1421, t1422)  # t1423: "cuda:0 f32[1, 512, 11008]"
  t1424 = prims.convert_element_type(t1423, dtypes.bfloat16)  # t1424: "cuda:0 bf16[1, 512, 11008]"
  t1425 = prims.linear(t1424, t_transformer_h_9_mlp_proj_weight, None)  # t1425: "cuda:0 bf16[1, 512, 4096]"
  t1426 = prims.convert_element_type(t1425, dtypes.float32)  # t1426: "cuda:0 f32[1, 512, 4096]"
  t1427 = prims.convert_element_type(t1390, dtypes.float32)  # t1427: "cuda:0 f32[1, 512, 4096]"
  t1428 = ltorch.add(t1426, t1427, alpha=None)  # t1428: "cuda:0 f32[1, 512, 4096]"
    # t1428 = prims.add(t1426, t1427)  # t1428: "cuda:0 f32[1, 512, 4096]"
  t1429 = prims.convert_element_type(t1428, dtypes.bfloat16)  # t1429: "cuda:0 bf16[1, 512, 4096]"
  t1430 = prims.convert_element_type(t1429, dtypes.float32)  # t1430: "cuda:0 f32[1, 512, 4096]"
  t1431 = ltorch.mul(t1430, t1430)  # t1431: "cuda:0 f32[1, 512, 4096]"
    # t1431 = prims.mul(t1430, t1430)  # t1431: "cuda:0 f32[1, 512, 4096]"
  t1433 = prims.sum(t1431, (2,))  # t1433: "cuda:0 f32[1, 512]"
  t1434 = prims.broadcast_in_dim(t1433, [1, 512, 1], [0, 1])  # t1434: "cuda:0 f32[1, 512, 1]"
  t1436 = ltorch.true_divide(t1434, 4096.0)  # t1436: "cuda:0 f32[1, 512, 1]"
    # t1436 = prims.div(t1434, 4096.0)  # t1436: "cuda:0 f32[1, 512, 1]"
  t1438 = ltorch.add(t1436, 1e-05, alpha=None)  # t1438: "cuda:0 f32[1, 512, 1]"
    # t1438 = prims.add(t1436, 1e-05)  # t1438: "cuda:0 f32[1, 512, 1]"
  t1439 = prims.rsqrt(t1438)  # t1439: "cuda:0 f32[1, 512, 1]"
  t1440 = prims.broadcast_in_dim(t1439, (1, 512, 4096), (0, 1, 2))  # t1440: "cuda:0 f32[1, 512, 4096]"
  t1441 = ltorch.mul(t1430, t1440)  # t1441: "cuda:0 f32[1, 512, 4096]"
    # t1441 = prims.mul(t1430, t1440)  # t1441: "cuda:0 f32[1, 512, 4096]"
  t1442 = prims.convert_element_type(t1441, dtypes.bfloat16)  # t1442: "cuda:0 bf16[1, 512, 4096]"
  t1443 = prims.broadcast_in_dim(t_transformer_h_10_norm_1_weight, (1, 512, 4096), (2,))  # t1443: "cuda:0 bf16[1, 512, 4096]"
  t1444 = prims.convert_element_type(t1442, dtypes.float32)  # t1444: "cuda:0 f32[1, 512, 4096]"
  t1445 = prims.convert_element_type(t1443, dtypes.float32)  # t1445: "cuda:0 f32[1, 512, 4096]"
  t1446 = ltorch.mul(t1444, t1445)  # t1446: "cuda:0 f32[1, 512, 4096]"
    # t1446 = prims.mul(t1444, t1445)  # t1446: "cuda:0 f32[1, 512, 4096]"
  t1447 = prims.convert_element_type(t1446, dtypes.bfloat16)  # t1447: "cuda:0 bf16[1, 512, 4096]"
  t1448 = prims.linear(t1447, t_transformer_h_10_attn_attn_weight, None)  # t1448: "cuda:0 bf16[1, 512, 12288]"
  t1454 = prims.reshape(t1448, (1, 512, 32, 3, 128))  # t1454: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1460 = prims.transpose(t1454, (0, 2, 3, 1, 4))  # t1460: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1461, t1462, t1463) = ltorch.split(t1460, (1, 1, 1), 2)
    # t1461 = prims.slice_prim(t1460, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1461: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1462 = prims.slice_prim(t1460, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1462: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1463 = prims.slice_prim(t1460, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1463: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1469 = prims.reshape(t1461, (1, 32, 512, 128))  # t1469: "cuda:0 bf16[1, 32, 512, 128]"
  t1475 = prims.reshape(t1462, (1, 32, 512, 128))  # t1475: "cuda:0 bf16[1, 32, 512, 128]"
  t1481 = prims.reshape(t1463, (1, 32, 512, 128))  # t1481: "cuda:0 bf16[1, 32, 512, 128]"
  t1482 = prims.slice_prim(t1469, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1482: "cuda:0 bf16[1, 32, 512, 128]"
  t1483 = prims.slice_prim(t1482, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1483: "cuda:0 bf16[1, 32, 512, 64]"
  t1484 = prims.slice_prim(t1482, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1484: "cuda:0 bf16[1, 32, 512, 64]"
  t1485 = prims.convert_element_type(t1484, dtypes.float32)  # t1485: "cuda:0 f32[1, 32, 512, 64]"
  t1486 = prims.neg(t1485)  # t1486: "cuda:0 f32[1, 32, 512, 64]"
  t1487 = prims.convert_element_type(t1486, dtypes.bfloat16)  # t1487: "cuda:0 bf16[1, 32, 512, 64]"
  t1489 = prims.cat((t1487, t1483), -1)  # t1489: "cuda:0 bf16[1, 32, 512, 128]"
  t1490 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1490: "cuda:0 f32[1, 32, 512, 128]"
  t1491 = prims.convert_element_type(t1482, dtypes.float32)  # t1491: "cuda:0 f32[1, 32, 512, 128]"
  t1492 = ltorch.mul(t1491, t1490)  # t1492: "cuda:0 f32[1, 32, 512, 128]"
    # t1492 = prims.mul(t1491, t1490)  # t1492: "cuda:0 f32[1, 32, 512, 128]"
  t1493 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1493: "cuda:0 f32[1, 32, 512, 128]"
  t1494 = prims.convert_element_type(t1489, dtypes.float32)  # t1494: "cuda:0 f32[1, 32, 512, 128]"
  t1495 = ltorch.mul(t1494, t1493)  # t1495: "cuda:0 f32[1, 32, 512, 128]"
    # t1495 = prims.mul(t1494, t1493)  # t1495: "cuda:0 f32[1, 32, 512, 128]"
  t1496 = ltorch.add(t1492, t1495, alpha=None)  # t1496: "cuda:0 f32[1, 32, 512, 128]"
    # t1496 = prims.add(t1492, t1495)  # t1496: "cuda:0 f32[1, 32, 512, 128]"
  t1497 = prims.convert_element_type(t1496, dtypes.bfloat16)  # t1497: "cuda:0 bf16[1, 32, 512, 128]"
  t1498 = prims.slice_prim(t1475, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1498: "cuda:0 bf16[1, 32, 512, 128]"
  t1499 = prims.slice_prim(t1498, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1499: "cuda:0 bf16[1, 32, 512, 64]"
  t1500 = prims.slice_prim(t1498, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1500: "cuda:0 bf16[1, 32, 512, 64]"
  t1501 = prims.convert_element_type(t1500, dtypes.float32)  # t1501: "cuda:0 f32[1, 32, 512, 64]"
  t1502 = prims.neg(t1501)  # t1502: "cuda:0 f32[1, 32, 512, 64]"
  t1503 = prims.convert_element_type(t1502, dtypes.bfloat16)  # t1503: "cuda:0 bf16[1, 32, 512, 64]"
  t1505 = prims.cat((t1503, t1499), -1)  # t1505: "cuda:0 bf16[1, 32, 512, 128]"
  t1506 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1506: "cuda:0 f32[1, 32, 512, 128]"
  t1507 = prims.convert_element_type(t1498, dtypes.float32)  # t1507: "cuda:0 f32[1, 32, 512, 128]"
  t1508 = ltorch.mul(t1507, t1506)  # t1508: "cuda:0 f32[1, 32, 512, 128]"
    # t1508 = prims.mul(t1507, t1506)  # t1508: "cuda:0 f32[1, 32, 512, 128]"
  t1509 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1509: "cuda:0 f32[1, 32, 512, 128]"
  t1510 = prims.convert_element_type(t1505, dtypes.float32)  # t1510: "cuda:0 f32[1, 32, 512, 128]"
  t1511 = ltorch.mul(t1510, t1509)  # t1511: "cuda:0 f32[1, 32, 512, 128]"
    # t1511 = prims.mul(t1510, t1509)  # t1511: "cuda:0 f32[1, 32, 512, 128]"
  t1512 = ltorch.add(t1508, t1511, alpha=None)  # t1512: "cuda:0 f32[1, 32, 512, 128]"
    # t1512 = prims.add(t1508, t1511)  # t1512: "cuda:0 f32[1, 32, 512, 128]"
  t1513 = prims.convert_element_type(t1512, dtypes.bfloat16)  # t1513: "cuda:0 bf16[1, 32, 512, 128]"
  t1514 = prims.slice_prim(t1469, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1514: "cuda:0 bf16[1, 32, 512, 0]"
  t1516 = prims.cat((t1497, t1514), -1)  # t1516: "cuda:0 bf16[1, 32, 512, 128]"
  t1517 = prims.slice_prim(t1475, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1517: "cuda:0 bf16[1, 32, 512, 0]"
  t1519 = prims.cat((t1513, t1517), -1)  # t1519: "cuda:0 bf16[1, 32, 512, 128]"
  (t1520, t1521, t1522, t1523) = cudnn_sdpa_fwd(t1516, t1519, t1481, None, 0.0, True, scale=0.08838834764831843)
  t1526 = prims.transpose(t1520, (0, 2, 1, 3))  # t1526: "cuda:0 bf16[1, 512, 32, 128]"
  t1530 = prims.reshape(t1526, (1, 512, 4096))  # t1530: "cuda:0 bf16[1, 512, 4096]"
  t1531 = prims.linear(t1530, t_transformer_h_10_attn_proj_weight, None)  # t1531: "cuda:0 bf16[1, 512, 4096]"
  t1532 = prims.convert_element_type(t1531, dtypes.float32)  # t1532: "cuda:0 f32[1, 512, 4096]"
  t1533 = prims.convert_element_type(t1429, dtypes.float32)  # t1533: "cuda:0 f32[1, 512, 4096]"
  t1534 = ltorch.add(t1532, t1533, alpha=None)  # t1534: "cuda:0 f32[1, 512, 4096]"
    # t1534 = prims.add(t1532, t1533)  # t1534: "cuda:0 f32[1, 512, 4096]"
  t1535 = prims.convert_element_type(t1534, dtypes.bfloat16)  # t1535: "cuda:0 bf16[1, 512, 4096]"
  t1536 = prims.convert_element_type(t1535, dtypes.float32)  # t1536: "cuda:0 f32[1, 512, 4096]"
  t1537 = ltorch.mul(t1536, t1536)  # t1537: "cuda:0 f32[1, 512, 4096]"
    # t1537 = prims.mul(t1536, t1536)  # t1537: "cuda:0 f32[1, 512, 4096]"
  t1539 = prims.sum(t1537, (2,))  # t1539: "cuda:0 f32[1, 512]"
  t1540 = prims.broadcast_in_dim(t1539, [1, 512, 1], [0, 1])  # t1540: "cuda:0 f32[1, 512, 1]"
  t1542 = ltorch.true_divide(t1540, 4096.0)  # t1542: "cuda:0 f32[1, 512, 1]"
    # t1542 = prims.div(t1540, 4096.0)  # t1542: "cuda:0 f32[1, 512, 1]"
  t1544 = ltorch.add(t1542, 1e-05, alpha=None)  # t1544: "cuda:0 f32[1, 512, 1]"
    # t1544 = prims.add(t1542, 1e-05)  # t1544: "cuda:0 f32[1, 512, 1]"
  t1545 = prims.rsqrt(t1544)  # t1545: "cuda:0 f32[1, 512, 1]"
  t1546 = prims.broadcast_in_dim(t1545, (1, 512, 4096), (0, 1, 2))  # t1546: "cuda:0 f32[1, 512, 4096]"
  t1547 = ltorch.mul(t1536, t1546)  # t1547: "cuda:0 f32[1, 512, 4096]"
    # t1547 = prims.mul(t1536, t1546)  # t1547: "cuda:0 f32[1, 512, 4096]"
  t1548 = prims.convert_element_type(t1547, dtypes.bfloat16)  # t1548: "cuda:0 bf16[1, 512, 4096]"
  t1549 = prims.broadcast_in_dim(t_transformer_h_10_norm_2_weight, (1, 512, 4096), (2,))  # t1549: "cuda:0 bf16[1, 512, 4096]"
  t1550 = prims.convert_element_type(t1548, dtypes.float32)  # t1550: "cuda:0 f32[1, 512, 4096]"
  t1551 = prims.convert_element_type(t1549, dtypes.float32)  # t1551: "cuda:0 f32[1, 512, 4096]"
  t1552 = ltorch.mul(t1550, t1551)  # t1552: "cuda:0 f32[1, 512, 4096]"
    # t1552 = prims.mul(t1550, t1551)  # t1552: "cuda:0 f32[1, 512, 4096]"
  t1553 = prims.convert_element_type(t1552, dtypes.bfloat16)  # t1553: "cuda:0 bf16[1, 512, 4096]"
  t1554 = prims.linear(t1553, t_transformer_h_10_mlp_fc_1_weight, None)  # t1554: "cuda:0 bf16[1, 512, 11008]"
  t1555 = prims.linear(t1553, t_transformer_h_10_mlp_fc_2_weight, None)  # t1555: "cuda:0 bf16[1, 512, 11008]"
  t1556 = prims.convert_element_type(t1554, dtypes.float32)  # t1556: "cuda:0 f32[1, 512, 11008]"
  t1557 = prims.neg(t1556)  # t1557: "cuda:0 f32[1, 512, 11008]"
  t1558 = prims.exp(t1557)  # t1558: "cuda:0 f32[1, 512, 11008]"
  t1559 = ltorch.add(1.0, t1558, alpha=None)  # t1559: "cuda:0 f32[1, 512, 11008]"
    # t1559 = prims.add(1.0, t1558)  # t1559: "cuda:0 f32[1, 512, 11008]"
  t1560 = prims.reciprocal(t1559)  # t1560: "cuda:0 f32[1, 512, 11008]"
  t1561 = prims.convert_element_type(t1560, dtypes.bfloat16)  # t1561: "cuda:0 bf16[1, 512, 11008]"
  t1562 = prims.convert_element_type(t1554, dtypes.float32)  # t1562: "cuda:0 f32[1, 512, 11008]"
  t1563 = prims.convert_element_type(t1561, dtypes.float32)  # t1563: "cuda:0 f32[1, 512, 11008]"
  t1564 = ltorch.mul(t1562, t1563)  # t1564: "cuda:0 f32[1, 512, 11008]"
    # t1564 = prims.mul(t1562, t1563)  # t1564: "cuda:0 f32[1, 512, 11008]"
  t1565 = prims.convert_element_type(t1564, dtypes.bfloat16)  # t1565: "cuda:0 bf16[1, 512, 11008]"
  t1566 = prims.convert_element_type(t1565, dtypes.float32)  # t1566: "cuda:0 f32[1, 512, 11008]"
  t1567 = prims.convert_element_type(t1555, dtypes.float32)  # t1567: "cuda:0 f32[1, 512, 11008]"
  t1568 = ltorch.mul(t1566, t1567)  # t1568: "cuda:0 f32[1, 512, 11008]"
    # t1568 = prims.mul(t1566, t1567)  # t1568: "cuda:0 f32[1, 512, 11008]"
  t1569 = prims.convert_element_type(t1568, dtypes.bfloat16)  # t1569: "cuda:0 bf16[1, 512, 11008]"
  t1570 = prims.linear(t1569, t_transformer_h_10_mlp_proj_weight, None)  # t1570: "cuda:0 bf16[1, 512, 4096]"
  t1571 = prims.convert_element_type(t1570, dtypes.float32)  # t1571: "cuda:0 f32[1, 512, 4096]"
  t1572 = prims.convert_element_type(t1535, dtypes.float32)  # t1572: "cuda:0 f32[1, 512, 4096]"
  t1573 = ltorch.add(t1571, t1572, alpha=None)  # t1573: "cuda:0 f32[1, 512, 4096]"
    # t1573 = prims.add(t1571, t1572)  # t1573: "cuda:0 f32[1, 512, 4096]"
  t1574 = prims.convert_element_type(t1573, dtypes.bfloat16)  # t1574: "cuda:0 bf16[1, 512, 4096]"
  t1575 = prims.convert_element_type(t1574, dtypes.float32)  # t1575: "cuda:0 f32[1, 512, 4096]"
  t1576 = ltorch.mul(t1575, t1575)  # t1576: "cuda:0 f32[1, 512, 4096]"
    # t1576 = prims.mul(t1575, t1575)  # t1576: "cuda:0 f32[1, 512, 4096]"
  t1578 = prims.sum(t1576, (2,))  # t1578: "cuda:0 f32[1, 512]"
  t1579 = prims.broadcast_in_dim(t1578, [1, 512, 1], [0, 1])  # t1579: "cuda:0 f32[1, 512, 1]"
  t1581 = ltorch.true_divide(t1579, 4096.0)  # t1581: "cuda:0 f32[1, 512, 1]"
    # t1581 = prims.div(t1579, 4096.0)  # t1581: "cuda:0 f32[1, 512, 1]"
  t1583 = ltorch.add(t1581, 1e-05, alpha=None)  # t1583: "cuda:0 f32[1, 512, 1]"
    # t1583 = prims.add(t1581, 1e-05)  # t1583: "cuda:0 f32[1, 512, 1]"
  t1584 = prims.rsqrt(t1583)  # t1584: "cuda:0 f32[1, 512, 1]"
  t1585 = prims.broadcast_in_dim(t1584, (1, 512, 4096), (0, 1, 2))  # t1585: "cuda:0 f32[1, 512, 4096]"
  t1586 = ltorch.mul(t1575, t1585)  # t1586: "cuda:0 f32[1, 512, 4096]"
    # t1586 = prims.mul(t1575, t1585)  # t1586: "cuda:0 f32[1, 512, 4096]"
  t1587 = prims.convert_element_type(t1586, dtypes.bfloat16)  # t1587: "cuda:0 bf16[1, 512, 4096]"
  t1588 = prims.broadcast_in_dim(t_transformer_h_11_norm_1_weight, (1, 512, 4096), (2,))  # t1588: "cuda:0 bf16[1, 512, 4096]"
  t1589 = prims.convert_element_type(t1587, dtypes.float32)  # t1589: "cuda:0 f32[1, 512, 4096]"
  t1590 = prims.convert_element_type(t1588, dtypes.float32)  # t1590: "cuda:0 f32[1, 512, 4096]"
  t1591 = ltorch.mul(t1589, t1590)  # t1591: "cuda:0 f32[1, 512, 4096]"
    # t1591 = prims.mul(t1589, t1590)  # t1591: "cuda:0 f32[1, 512, 4096]"
  t1592 = prims.convert_element_type(t1591, dtypes.bfloat16)  # t1592: "cuda:0 bf16[1, 512, 4096]"
  t1593 = prims.linear(t1592, t_transformer_h_11_attn_attn_weight, None)  # t1593: "cuda:0 bf16[1, 512, 12288]"
  t1599 = prims.reshape(t1593, (1, 512, 32, 3, 128))  # t1599: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1605 = prims.transpose(t1599, (0, 2, 3, 1, 4))  # t1605: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1606, t1607, t1608) = ltorch.split(t1605, (1, 1, 1), 2)
    # t1606 = prims.slice_prim(t1605, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1606: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1607 = prims.slice_prim(t1605, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1607: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1608 = prims.slice_prim(t1605, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1608: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1614 = prims.reshape(t1606, (1, 32, 512, 128))  # t1614: "cuda:0 bf16[1, 32, 512, 128]"
  t1620 = prims.reshape(t1607, (1, 32, 512, 128))  # t1620: "cuda:0 bf16[1, 32, 512, 128]"
  t1626 = prims.reshape(t1608, (1, 32, 512, 128))  # t1626: "cuda:0 bf16[1, 32, 512, 128]"
  t1627 = prims.slice_prim(t1614, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1627: "cuda:0 bf16[1, 32, 512, 128]"
  t1628 = prims.slice_prim(t1627, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1628: "cuda:0 bf16[1, 32, 512, 64]"
  t1629 = prims.slice_prim(t1627, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1629: "cuda:0 bf16[1, 32, 512, 64]"
  t1630 = prims.convert_element_type(t1629, dtypes.float32)  # t1630: "cuda:0 f32[1, 32, 512, 64]"
  t1631 = prims.neg(t1630)  # t1631: "cuda:0 f32[1, 32, 512, 64]"
  t1632 = prims.convert_element_type(t1631, dtypes.bfloat16)  # t1632: "cuda:0 bf16[1, 32, 512, 64]"
  t1634 = prims.cat((t1632, t1628), -1)  # t1634: "cuda:0 bf16[1, 32, 512, 128]"
  t1635 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1635: "cuda:0 f32[1, 32, 512, 128]"
  t1636 = prims.convert_element_type(t1627, dtypes.float32)  # t1636: "cuda:0 f32[1, 32, 512, 128]"
  t1637 = ltorch.mul(t1636, t1635)  # t1637: "cuda:0 f32[1, 32, 512, 128]"
    # t1637 = prims.mul(t1636, t1635)  # t1637: "cuda:0 f32[1, 32, 512, 128]"
  t1638 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1638: "cuda:0 f32[1, 32, 512, 128]"
  t1639 = prims.convert_element_type(t1634, dtypes.float32)  # t1639: "cuda:0 f32[1, 32, 512, 128]"
  t1640 = ltorch.mul(t1639, t1638)  # t1640: "cuda:0 f32[1, 32, 512, 128]"
    # t1640 = prims.mul(t1639, t1638)  # t1640: "cuda:0 f32[1, 32, 512, 128]"
  t1641 = ltorch.add(t1637, t1640, alpha=None)  # t1641: "cuda:0 f32[1, 32, 512, 128]"
    # t1641 = prims.add(t1637, t1640)  # t1641: "cuda:0 f32[1, 32, 512, 128]"
  t1642 = prims.convert_element_type(t1641, dtypes.bfloat16)  # t1642: "cuda:0 bf16[1, 32, 512, 128]"
  t1643 = prims.slice_prim(t1620, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1643: "cuda:0 bf16[1, 32, 512, 128]"
  t1644 = prims.slice_prim(t1643, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1644: "cuda:0 bf16[1, 32, 512, 64]"
  t1645 = prims.slice_prim(t1643, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1645: "cuda:0 bf16[1, 32, 512, 64]"
  t1646 = prims.convert_element_type(t1645, dtypes.float32)  # t1646: "cuda:0 f32[1, 32, 512, 64]"
  t1647 = prims.neg(t1646)  # t1647: "cuda:0 f32[1, 32, 512, 64]"
  t1648 = prims.convert_element_type(t1647, dtypes.bfloat16)  # t1648: "cuda:0 bf16[1, 32, 512, 64]"
  t1650 = prims.cat((t1648, t1644), -1)  # t1650: "cuda:0 bf16[1, 32, 512, 128]"
  t1651 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1651: "cuda:0 f32[1, 32, 512, 128]"
  t1652 = prims.convert_element_type(t1643, dtypes.float32)  # t1652: "cuda:0 f32[1, 32, 512, 128]"
  t1653 = ltorch.mul(t1652, t1651)  # t1653: "cuda:0 f32[1, 32, 512, 128]"
    # t1653 = prims.mul(t1652, t1651)  # t1653: "cuda:0 f32[1, 32, 512, 128]"
  t1654 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1654: "cuda:0 f32[1, 32, 512, 128]"
  t1655 = prims.convert_element_type(t1650, dtypes.float32)  # t1655: "cuda:0 f32[1, 32, 512, 128]"
  t1656 = ltorch.mul(t1655, t1654)  # t1656: "cuda:0 f32[1, 32, 512, 128]"
    # t1656 = prims.mul(t1655, t1654)  # t1656: "cuda:0 f32[1, 32, 512, 128]"
  t1657 = ltorch.add(t1653, t1656, alpha=None)  # t1657: "cuda:0 f32[1, 32, 512, 128]"
    # t1657 = prims.add(t1653, t1656)  # t1657: "cuda:0 f32[1, 32, 512, 128]"
  t1658 = prims.convert_element_type(t1657, dtypes.bfloat16)  # t1658: "cuda:0 bf16[1, 32, 512, 128]"
  t1659 = prims.slice_prim(t1614, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1659: "cuda:0 bf16[1, 32, 512, 0]"
  t1661 = prims.cat((t1642, t1659), -1)  # t1661: "cuda:0 bf16[1, 32, 512, 128]"
  t1662 = prims.slice_prim(t1620, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1662: "cuda:0 bf16[1, 32, 512, 0]"
  t1664 = prims.cat((t1658, t1662), -1)  # t1664: "cuda:0 bf16[1, 32, 512, 128]"
  (t1665, t1666, t1667, t1668) = cudnn_sdpa_fwd(t1661, t1664, t1626, None, 0.0, True, scale=0.08838834764831843)
  t1671 = prims.transpose(t1665, (0, 2, 1, 3))  # t1671: "cuda:0 bf16[1, 512, 32, 128]"
  t1675 = prims.reshape(t1671, (1, 512, 4096))  # t1675: "cuda:0 bf16[1, 512, 4096]"
  t1676 = prims.linear(t1675, t_transformer_h_11_attn_proj_weight, None)  # t1676: "cuda:0 bf16[1, 512, 4096]"
  t1677 = prims.convert_element_type(t1676, dtypes.float32)  # t1677: "cuda:0 f32[1, 512, 4096]"
  t1678 = prims.convert_element_type(t1574, dtypes.float32)  # t1678: "cuda:0 f32[1, 512, 4096]"
  t1679 = ltorch.add(t1677, t1678, alpha=None)  # t1679: "cuda:0 f32[1, 512, 4096]"
    # t1679 = prims.add(t1677, t1678)  # t1679: "cuda:0 f32[1, 512, 4096]"
  t1680 = prims.convert_element_type(t1679, dtypes.bfloat16)  # t1680: "cuda:0 bf16[1, 512, 4096]"
  t1681 = prims.convert_element_type(t1680, dtypes.float32)  # t1681: "cuda:0 f32[1, 512, 4096]"
  t1682 = ltorch.mul(t1681, t1681)  # t1682: "cuda:0 f32[1, 512, 4096]"
    # t1682 = prims.mul(t1681, t1681)  # t1682: "cuda:0 f32[1, 512, 4096]"
  t1684 = prims.sum(t1682, (2,))  # t1684: "cuda:0 f32[1, 512]"
  t1685 = prims.broadcast_in_dim(t1684, [1, 512, 1], [0, 1])  # t1685: "cuda:0 f32[1, 512, 1]"
  t1687 = ltorch.true_divide(t1685, 4096.0)  # t1687: "cuda:0 f32[1, 512, 1]"
    # t1687 = prims.div(t1685, 4096.0)  # t1687: "cuda:0 f32[1, 512, 1]"
  t1689 = ltorch.add(t1687, 1e-05, alpha=None)  # t1689: "cuda:0 f32[1, 512, 1]"
    # t1689 = prims.add(t1687, 1e-05)  # t1689: "cuda:0 f32[1, 512, 1]"
  t1690 = prims.rsqrt(t1689)  # t1690: "cuda:0 f32[1, 512, 1]"
  t1691 = prims.broadcast_in_dim(t1690, (1, 512, 4096), (0, 1, 2))  # t1691: "cuda:0 f32[1, 512, 4096]"
  t1692 = ltorch.mul(t1681, t1691)  # t1692: "cuda:0 f32[1, 512, 4096]"
    # t1692 = prims.mul(t1681, t1691)  # t1692: "cuda:0 f32[1, 512, 4096]"
  t1693 = prims.convert_element_type(t1692, dtypes.bfloat16)  # t1693: "cuda:0 bf16[1, 512, 4096]"
  t1694 = prims.broadcast_in_dim(t_transformer_h_11_norm_2_weight, (1, 512, 4096), (2,))  # t1694: "cuda:0 bf16[1, 512, 4096]"
  t1695 = prims.convert_element_type(t1693, dtypes.float32)  # t1695: "cuda:0 f32[1, 512, 4096]"
  t1696 = prims.convert_element_type(t1694, dtypes.float32)  # t1696: "cuda:0 f32[1, 512, 4096]"
  t1697 = ltorch.mul(t1695, t1696)  # t1697: "cuda:0 f32[1, 512, 4096]"
    # t1697 = prims.mul(t1695, t1696)  # t1697: "cuda:0 f32[1, 512, 4096]"
  t1698 = prims.convert_element_type(t1697, dtypes.bfloat16)  # t1698: "cuda:0 bf16[1, 512, 4096]"
  t1699 = prims.linear(t1698, t_transformer_h_11_mlp_fc_1_weight, None)  # t1699: "cuda:0 bf16[1, 512, 11008]"
  t1700 = prims.linear(t1698, t_transformer_h_11_mlp_fc_2_weight, None)  # t1700: "cuda:0 bf16[1, 512, 11008]"
  t1701 = prims.convert_element_type(t1699, dtypes.float32)  # t1701: "cuda:0 f32[1, 512, 11008]"
  t1702 = prims.neg(t1701)  # t1702: "cuda:0 f32[1, 512, 11008]"
  t1703 = prims.exp(t1702)  # t1703: "cuda:0 f32[1, 512, 11008]"
  t1704 = ltorch.add(1.0, t1703, alpha=None)  # t1704: "cuda:0 f32[1, 512, 11008]"
    # t1704 = prims.add(1.0, t1703)  # t1704: "cuda:0 f32[1, 512, 11008]"
  t1705 = prims.reciprocal(t1704)  # t1705: "cuda:0 f32[1, 512, 11008]"
  t1706 = prims.convert_element_type(t1705, dtypes.bfloat16)  # t1706: "cuda:0 bf16[1, 512, 11008]"
  t1707 = prims.convert_element_type(t1699, dtypes.float32)  # t1707: "cuda:0 f32[1, 512, 11008]"
  t1708 = prims.convert_element_type(t1706, dtypes.float32)  # t1708: "cuda:0 f32[1, 512, 11008]"
  t1709 = ltorch.mul(t1707, t1708)  # t1709: "cuda:0 f32[1, 512, 11008]"
    # t1709 = prims.mul(t1707, t1708)  # t1709: "cuda:0 f32[1, 512, 11008]"
  t1710 = prims.convert_element_type(t1709, dtypes.bfloat16)  # t1710: "cuda:0 bf16[1, 512, 11008]"
  t1711 = prims.convert_element_type(t1710, dtypes.float32)  # t1711: "cuda:0 f32[1, 512, 11008]"
  t1712 = prims.convert_element_type(t1700, dtypes.float32)  # t1712: "cuda:0 f32[1, 512, 11008]"
  t1713 = ltorch.mul(t1711, t1712)  # t1713: "cuda:0 f32[1, 512, 11008]"
    # t1713 = prims.mul(t1711, t1712)  # t1713: "cuda:0 f32[1, 512, 11008]"
  t1714 = prims.convert_element_type(t1713, dtypes.bfloat16)  # t1714: "cuda:0 bf16[1, 512, 11008]"
  t1715 = prims.linear(t1714, t_transformer_h_11_mlp_proj_weight, None)  # t1715: "cuda:0 bf16[1, 512, 4096]"
  t1716 = prims.convert_element_type(t1715, dtypes.float32)  # t1716: "cuda:0 f32[1, 512, 4096]"
  t1717 = prims.convert_element_type(t1680, dtypes.float32)  # t1717: "cuda:0 f32[1, 512, 4096]"
  t1718 = ltorch.add(t1716, t1717, alpha=None)  # t1718: "cuda:0 f32[1, 512, 4096]"
    # t1718 = prims.add(t1716, t1717)  # t1718: "cuda:0 f32[1, 512, 4096]"
  t1719 = prims.convert_element_type(t1718, dtypes.bfloat16)  # t1719: "cuda:0 bf16[1, 512, 4096]"
  t1720 = prims.convert_element_type(t1719, dtypes.float32)  # t1720: "cuda:0 f32[1, 512, 4096]"
  t1721 = ltorch.mul(t1720, t1720)  # t1721: "cuda:0 f32[1, 512, 4096]"
    # t1721 = prims.mul(t1720, t1720)  # t1721: "cuda:0 f32[1, 512, 4096]"
  t1723 = prims.sum(t1721, (2,))  # t1723: "cuda:0 f32[1, 512]"
  t1724 = prims.broadcast_in_dim(t1723, [1, 512, 1], [0, 1])  # t1724: "cuda:0 f32[1, 512, 1]"
  t1726 = ltorch.true_divide(t1724, 4096.0)  # t1726: "cuda:0 f32[1, 512, 1]"
    # t1726 = prims.div(t1724, 4096.0)  # t1726: "cuda:0 f32[1, 512, 1]"
  t1728 = ltorch.add(t1726, 1e-05, alpha=None)  # t1728: "cuda:0 f32[1, 512, 1]"
    # t1728 = prims.add(t1726, 1e-05)  # t1728: "cuda:0 f32[1, 512, 1]"
  t1729 = prims.rsqrt(t1728)  # t1729: "cuda:0 f32[1, 512, 1]"
  t1730 = prims.broadcast_in_dim(t1729, (1, 512, 4096), (0, 1, 2))  # t1730: "cuda:0 f32[1, 512, 4096]"
  t1731 = ltorch.mul(t1720, t1730)  # t1731: "cuda:0 f32[1, 512, 4096]"
    # t1731 = prims.mul(t1720, t1730)  # t1731: "cuda:0 f32[1, 512, 4096]"
  t1732 = prims.convert_element_type(t1731, dtypes.bfloat16)  # t1732: "cuda:0 bf16[1, 512, 4096]"
  t1733 = prims.broadcast_in_dim(t_transformer_h_12_norm_1_weight, (1, 512, 4096), (2,))  # t1733: "cuda:0 bf16[1, 512, 4096]"
  t1734 = prims.convert_element_type(t1732, dtypes.float32)  # t1734: "cuda:0 f32[1, 512, 4096]"
  t1735 = prims.convert_element_type(t1733, dtypes.float32)  # t1735: "cuda:0 f32[1, 512, 4096]"
  t1736 = ltorch.mul(t1734, t1735)  # t1736: "cuda:0 f32[1, 512, 4096]"
    # t1736 = prims.mul(t1734, t1735)  # t1736: "cuda:0 f32[1, 512, 4096]"
  t1737 = prims.convert_element_type(t1736, dtypes.bfloat16)  # t1737: "cuda:0 bf16[1, 512, 4096]"
  t1738 = prims.linear(t1737, t_transformer_h_12_attn_attn_weight, None)  # t1738: "cuda:0 bf16[1, 512, 12288]"
  t1744 = prims.reshape(t1738, (1, 512, 32, 3, 128))  # t1744: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1750 = prims.transpose(t1744, (0, 2, 3, 1, 4))  # t1750: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1751, t1752, t1753) = ltorch.split(t1750, (1, 1, 1), 2)
    # t1751 = prims.slice_prim(t1750, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1751: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1752 = prims.slice_prim(t1750, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1752: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1753 = prims.slice_prim(t1750, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1753: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1759 = prims.reshape(t1751, (1, 32, 512, 128))  # t1759: "cuda:0 bf16[1, 32, 512, 128]"
  t1765 = prims.reshape(t1752, (1, 32, 512, 128))  # t1765: "cuda:0 bf16[1, 32, 512, 128]"
  t1771 = prims.reshape(t1753, (1, 32, 512, 128))  # t1771: "cuda:0 bf16[1, 32, 512, 128]"
  t1772 = prims.slice_prim(t1759, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1772: "cuda:0 bf16[1, 32, 512, 128]"
  t1773 = prims.slice_prim(t1772, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1773: "cuda:0 bf16[1, 32, 512, 64]"
  t1774 = prims.slice_prim(t1772, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1774: "cuda:0 bf16[1, 32, 512, 64]"
  t1775 = prims.convert_element_type(t1774, dtypes.float32)  # t1775: "cuda:0 f32[1, 32, 512, 64]"
  t1776 = prims.neg(t1775)  # t1776: "cuda:0 f32[1, 32, 512, 64]"
  t1777 = prims.convert_element_type(t1776, dtypes.bfloat16)  # t1777: "cuda:0 bf16[1, 32, 512, 64]"
  t1779 = prims.cat((t1777, t1773), -1)  # t1779: "cuda:0 bf16[1, 32, 512, 128]"
  t1780 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1780: "cuda:0 f32[1, 32, 512, 128]"
  t1781 = prims.convert_element_type(t1772, dtypes.float32)  # t1781: "cuda:0 f32[1, 32, 512, 128]"
  t1782 = ltorch.mul(t1781, t1780)  # t1782: "cuda:0 f32[1, 32, 512, 128]"
    # t1782 = prims.mul(t1781, t1780)  # t1782: "cuda:0 f32[1, 32, 512, 128]"
  t1783 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1783: "cuda:0 f32[1, 32, 512, 128]"
  t1784 = prims.convert_element_type(t1779, dtypes.float32)  # t1784: "cuda:0 f32[1, 32, 512, 128]"
  t1785 = ltorch.mul(t1784, t1783)  # t1785: "cuda:0 f32[1, 32, 512, 128]"
    # t1785 = prims.mul(t1784, t1783)  # t1785: "cuda:0 f32[1, 32, 512, 128]"
  t1786 = ltorch.add(t1782, t1785, alpha=None)  # t1786: "cuda:0 f32[1, 32, 512, 128]"
    # t1786 = prims.add(t1782, t1785)  # t1786: "cuda:0 f32[1, 32, 512, 128]"
  t1787 = prims.convert_element_type(t1786, dtypes.bfloat16)  # t1787: "cuda:0 bf16[1, 32, 512, 128]"
  t1788 = prims.slice_prim(t1765, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1788: "cuda:0 bf16[1, 32, 512, 128]"
  t1789 = prims.slice_prim(t1788, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1789: "cuda:0 bf16[1, 32, 512, 64]"
  t1790 = prims.slice_prim(t1788, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1790: "cuda:0 bf16[1, 32, 512, 64]"
  t1791 = prims.convert_element_type(t1790, dtypes.float32)  # t1791: "cuda:0 f32[1, 32, 512, 64]"
  t1792 = prims.neg(t1791)  # t1792: "cuda:0 f32[1, 32, 512, 64]"
  t1793 = prims.convert_element_type(t1792, dtypes.bfloat16)  # t1793: "cuda:0 bf16[1, 32, 512, 64]"
  t1795 = prims.cat((t1793, t1789), -1)  # t1795: "cuda:0 bf16[1, 32, 512, 128]"
  t1796 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1796: "cuda:0 f32[1, 32, 512, 128]"
  t1797 = prims.convert_element_type(t1788, dtypes.float32)  # t1797: "cuda:0 f32[1, 32, 512, 128]"
  t1798 = ltorch.mul(t1797, t1796)  # t1798: "cuda:0 f32[1, 32, 512, 128]"
    # t1798 = prims.mul(t1797, t1796)  # t1798: "cuda:0 f32[1, 32, 512, 128]"
  t1799 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1799: "cuda:0 f32[1, 32, 512, 128]"
  t1800 = prims.convert_element_type(t1795, dtypes.float32)  # t1800: "cuda:0 f32[1, 32, 512, 128]"
  t1801 = ltorch.mul(t1800, t1799)  # t1801: "cuda:0 f32[1, 32, 512, 128]"
    # t1801 = prims.mul(t1800, t1799)  # t1801: "cuda:0 f32[1, 32, 512, 128]"
  t1802 = ltorch.add(t1798, t1801, alpha=None)  # t1802: "cuda:0 f32[1, 32, 512, 128]"
    # t1802 = prims.add(t1798, t1801)  # t1802: "cuda:0 f32[1, 32, 512, 128]"
  t1803 = prims.convert_element_type(t1802, dtypes.bfloat16)  # t1803: "cuda:0 bf16[1, 32, 512, 128]"
  t1804 = prims.slice_prim(t1759, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1804: "cuda:0 bf16[1, 32, 512, 0]"
  t1806 = prims.cat((t1787, t1804), -1)  # t1806: "cuda:0 bf16[1, 32, 512, 128]"
  t1807 = prims.slice_prim(t1765, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1807: "cuda:0 bf16[1, 32, 512, 0]"
  t1809 = prims.cat((t1803, t1807), -1)  # t1809: "cuda:0 bf16[1, 32, 512, 128]"
  (t1810, t1811, t1812, t1813) = cudnn_sdpa_fwd(t1806, t1809, t1771, None, 0.0, True, scale=0.08838834764831843)
  t1816 = prims.transpose(t1810, (0, 2, 1, 3))  # t1816: "cuda:0 bf16[1, 512, 32, 128]"
  t1820 = prims.reshape(t1816, (1, 512, 4096))  # t1820: "cuda:0 bf16[1, 512, 4096]"
  t1821 = prims.linear(t1820, t_transformer_h_12_attn_proj_weight, None)  # t1821: "cuda:0 bf16[1, 512, 4096]"
  t1822 = prims.convert_element_type(t1821, dtypes.float32)  # t1822: "cuda:0 f32[1, 512, 4096]"
  t1823 = prims.convert_element_type(t1719, dtypes.float32)  # t1823: "cuda:0 f32[1, 512, 4096]"
  t1824 = ltorch.add(t1822, t1823, alpha=None)  # t1824: "cuda:0 f32[1, 512, 4096]"
    # t1824 = prims.add(t1822, t1823)  # t1824: "cuda:0 f32[1, 512, 4096]"
  t1825 = prims.convert_element_type(t1824, dtypes.bfloat16)  # t1825: "cuda:0 bf16[1, 512, 4096]"
  t1826 = prims.convert_element_type(t1825, dtypes.float32)  # t1826: "cuda:0 f32[1, 512, 4096]"
  t1827 = ltorch.mul(t1826, t1826)  # t1827: "cuda:0 f32[1, 512, 4096]"
    # t1827 = prims.mul(t1826, t1826)  # t1827: "cuda:0 f32[1, 512, 4096]"
  t1829 = prims.sum(t1827, (2,))  # t1829: "cuda:0 f32[1, 512]"
  t1830 = prims.broadcast_in_dim(t1829, [1, 512, 1], [0, 1])  # t1830: "cuda:0 f32[1, 512, 1]"
  t1832 = ltorch.true_divide(t1830, 4096.0)  # t1832: "cuda:0 f32[1, 512, 1]"
    # t1832 = prims.div(t1830, 4096.0)  # t1832: "cuda:0 f32[1, 512, 1]"
  t1834 = ltorch.add(t1832, 1e-05, alpha=None)  # t1834: "cuda:0 f32[1, 512, 1]"
    # t1834 = prims.add(t1832, 1e-05)  # t1834: "cuda:0 f32[1, 512, 1]"
  t1835 = prims.rsqrt(t1834)  # t1835: "cuda:0 f32[1, 512, 1]"
  t1836 = prims.broadcast_in_dim(t1835, (1, 512, 4096), (0, 1, 2))  # t1836: "cuda:0 f32[1, 512, 4096]"
  t1837 = ltorch.mul(t1826, t1836)  # t1837: "cuda:0 f32[1, 512, 4096]"
    # t1837 = prims.mul(t1826, t1836)  # t1837: "cuda:0 f32[1, 512, 4096]"
  t1838 = prims.convert_element_type(t1837, dtypes.bfloat16)  # t1838: "cuda:0 bf16[1, 512, 4096]"
  t1839 = prims.broadcast_in_dim(t_transformer_h_12_norm_2_weight, (1, 512, 4096), (2,))  # t1839: "cuda:0 bf16[1, 512, 4096]"
  t1840 = prims.convert_element_type(t1838, dtypes.float32)  # t1840: "cuda:0 f32[1, 512, 4096]"
  t1841 = prims.convert_element_type(t1839, dtypes.float32)  # t1841: "cuda:0 f32[1, 512, 4096]"
  t1842 = ltorch.mul(t1840, t1841)  # t1842: "cuda:0 f32[1, 512, 4096]"
    # t1842 = prims.mul(t1840, t1841)  # t1842: "cuda:0 f32[1, 512, 4096]"
  t1843 = prims.convert_element_type(t1842, dtypes.bfloat16)  # t1843: "cuda:0 bf16[1, 512, 4096]"
  t1844 = prims.linear(t1843, t_transformer_h_12_mlp_fc_1_weight, None)  # t1844: "cuda:0 bf16[1, 512, 11008]"
  t1845 = prims.linear(t1843, t_transformer_h_12_mlp_fc_2_weight, None)  # t1845: "cuda:0 bf16[1, 512, 11008]"
  t1846 = prims.convert_element_type(t1844, dtypes.float32)  # t1846: "cuda:0 f32[1, 512, 11008]"
  t1847 = prims.neg(t1846)  # t1847: "cuda:0 f32[1, 512, 11008]"
  t1848 = prims.exp(t1847)  # t1848: "cuda:0 f32[1, 512, 11008]"
  t1849 = ltorch.add(1.0, t1848, alpha=None)  # t1849: "cuda:0 f32[1, 512, 11008]"
    # t1849 = prims.add(1.0, t1848)  # t1849: "cuda:0 f32[1, 512, 11008]"
  t1850 = prims.reciprocal(t1849)  # t1850: "cuda:0 f32[1, 512, 11008]"
  t1851 = prims.convert_element_type(t1850, dtypes.bfloat16)  # t1851: "cuda:0 bf16[1, 512, 11008]"
  t1852 = prims.convert_element_type(t1844, dtypes.float32)  # t1852: "cuda:0 f32[1, 512, 11008]"
  t1853 = prims.convert_element_type(t1851, dtypes.float32)  # t1853: "cuda:0 f32[1, 512, 11008]"
  t1854 = ltorch.mul(t1852, t1853)  # t1854: "cuda:0 f32[1, 512, 11008]"
    # t1854 = prims.mul(t1852, t1853)  # t1854: "cuda:0 f32[1, 512, 11008]"
  t1855 = prims.convert_element_type(t1854, dtypes.bfloat16)  # t1855: "cuda:0 bf16[1, 512, 11008]"
  t1856 = prims.convert_element_type(t1855, dtypes.float32)  # t1856: "cuda:0 f32[1, 512, 11008]"
  t1857 = prims.convert_element_type(t1845, dtypes.float32)  # t1857: "cuda:0 f32[1, 512, 11008]"
  t1858 = ltorch.mul(t1856, t1857)  # t1858: "cuda:0 f32[1, 512, 11008]"
    # t1858 = prims.mul(t1856, t1857)  # t1858: "cuda:0 f32[1, 512, 11008]"
  t1859 = prims.convert_element_type(t1858, dtypes.bfloat16)  # t1859: "cuda:0 bf16[1, 512, 11008]"
  t1860 = prims.linear(t1859, t_transformer_h_12_mlp_proj_weight, None)  # t1860: "cuda:0 bf16[1, 512, 4096]"
  t1861 = prims.convert_element_type(t1860, dtypes.float32)  # t1861: "cuda:0 f32[1, 512, 4096]"
  t1862 = prims.convert_element_type(t1825, dtypes.float32)  # t1862: "cuda:0 f32[1, 512, 4096]"
  t1863 = ltorch.add(t1861, t1862, alpha=None)  # t1863: "cuda:0 f32[1, 512, 4096]"
    # t1863 = prims.add(t1861, t1862)  # t1863: "cuda:0 f32[1, 512, 4096]"
  t1864 = prims.convert_element_type(t1863, dtypes.bfloat16)  # t1864: "cuda:0 bf16[1, 512, 4096]"
  t1865 = prims.convert_element_type(t1864, dtypes.float32)  # t1865: "cuda:0 f32[1, 512, 4096]"
  t1866 = ltorch.mul(t1865, t1865)  # t1866: "cuda:0 f32[1, 512, 4096]"
    # t1866 = prims.mul(t1865, t1865)  # t1866: "cuda:0 f32[1, 512, 4096]"
  t1868 = prims.sum(t1866, (2,))  # t1868: "cuda:0 f32[1, 512]"
  t1869 = prims.broadcast_in_dim(t1868, [1, 512, 1], [0, 1])  # t1869: "cuda:0 f32[1, 512, 1]"
  t1871 = ltorch.true_divide(t1869, 4096.0)  # t1871: "cuda:0 f32[1, 512, 1]"
    # t1871 = prims.div(t1869, 4096.0)  # t1871: "cuda:0 f32[1, 512, 1]"
  t1873 = ltorch.add(t1871, 1e-05, alpha=None)  # t1873: "cuda:0 f32[1, 512, 1]"
    # t1873 = prims.add(t1871, 1e-05)  # t1873: "cuda:0 f32[1, 512, 1]"
  t1874 = prims.rsqrt(t1873)  # t1874: "cuda:0 f32[1, 512, 1]"
  t1875 = prims.broadcast_in_dim(t1874, (1, 512, 4096), (0, 1, 2))  # t1875: "cuda:0 f32[1, 512, 4096]"
  t1876 = ltorch.mul(t1865, t1875)  # t1876: "cuda:0 f32[1, 512, 4096]"
    # t1876 = prims.mul(t1865, t1875)  # t1876: "cuda:0 f32[1, 512, 4096]"
  t1877 = prims.convert_element_type(t1876, dtypes.bfloat16)  # t1877: "cuda:0 bf16[1, 512, 4096]"
  t1878 = prims.broadcast_in_dim(t_transformer_h_13_norm_1_weight, (1, 512, 4096), (2,))  # t1878: "cuda:0 bf16[1, 512, 4096]"
  t1879 = prims.convert_element_type(t1877, dtypes.float32)  # t1879: "cuda:0 f32[1, 512, 4096]"
  t1880 = prims.convert_element_type(t1878, dtypes.float32)  # t1880: "cuda:0 f32[1, 512, 4096]"
  t1881 = ltorch.mul(t1879, t1880)  # t1881: "cuda:0 f32[1, 512, 4096]"
    # t1881 = prims.mul(t1879, t1880)  # t1881: "cuda:0 f32[1, 512, 4096]"
  t1882 = prims.convert_element_type(t1881, dtypes.bfloat16)  # t1882: "cuda:0 bf16[1, 512, 4096]"
  t1883 = prims.linear(t1882, t_transformer_h_13_attn_attn_weight, None)  # t1883: "cuda:0 bf16[1, 512, 12288]"
  t1889 = prims.reshape(t1883, (1, 512, 32, 3, 128))  # t1889: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1895 = prims.transpose(t1889, (0, 2, 3, 1, 4))  # t1895: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1896, t1897, t1898) = ltorch.split(t1895, (1, 1, 1), 2)
    # t1896 = prims.slice_prim(t1895, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1896: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1897 = prims.slice_prim(t1895, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1897: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1898 = prims.slice_prim(t1895, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1898: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1904 = prims.reshape(t1896, (1, 32, 512, 128))  # t1904: "cuda:0 bf16[1, 32, 512, 128]"
  t1910 = prims.reshape(t1897, (1, 32, 512, 128))  # t1910: "cuda:0 bf16[1, 32, 512, 128]"
  t1916 = prims.reshape(t1898, (1, 32, 512, 128))  # t1916: "cuda:0 bf16[1, 32, 512, 128]"
  t1917 = prims.slice_prim(t1904, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1917: "cuda:0 bf16[1, 32, 512, 128]"
  t1918 = prims.slice_prim(t1917, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1918: "cuda:0 bf16[1, 32, 512, 64]"
  t1919 = prims.slice_prim(t1917, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1919: "cuda:0 bf16[1, 32, 512, 64]"
  t1920 = prims.convert_element_type(t1919, dtypes.float32)  # t1920: "cuda:0 f32[1, 32, 512, 64]"
  t1921 = prims.neg(t1920)  # t1921: "cuda:0 f32[1, 32, 512, 64]"
  t1922 = prims.convert_element_type(t1921, dtypes.bfloat16)  # t1922: "cuda:0 bf16[1, 32, 512, 64]"
  t1924 = prims.cat((t1922, t1918), -1)  # t1924: "cuda:0 bf16[1, 32, 512, 128]"
  t1925 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1925: "cuda:0 f32[1, 32, 512, 128]"
  t1926 = prims.convert_element_type(t1917, dtypes.float32)  # t1926: "cuda:0 f32[1, 32, 512, 128]"
  t1927 = ltorch.mul(t1926, t1925)  # t1927: "cuda:0 f32[1, 32, 512, 128]"
    # t1927 = prims.mul(t1926, t1925)  # t1927: "cuda:0 f32[1, 32, 512, 128]"
  t1928 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1928: "cuda:0 f32[1, 32, 512, 128]"
  t1929 = prims.convert_element_type(t1924, dtypes.float32)  # t1929: "cuda:0 f32[1, 32, 512, 128]"
  t1930 = ltorch.mul(t1929, t1928)  # t1930: "cuda:0 f32[1, 32, 512, 128]"
    # t1930 = prims.mul(t1929, t1928)  # t1930: "cuda:0 f32[1, 32, 512, 128]"
  t1931 = ltorch.add(t1927, t1930, alpha=None)  # t1931: "cuda:0 f32[1, 32, 512, 128]"
    # t1931 = prims.add(t1927, t1930)  # t1931: "cuda:0 f32[1, 32, 512, 128]"
  t1932 = prims.convert_element_type(t1931, dtypes.bfloat16)  # t1932: "cuda:0 bf16[1, 32, 512, 128]"
  t1933 = prims.slice_prim(t1910, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1933: "cuda:0 bf16[1, 32, 512, 128]"
  t1934 = prims.slice_prim(t1933, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1934: "cuda:0 bf16[1, 32, 512, 64]"
  t1935 = prims.slice_prim(t1933, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1935: "cuda:0 bf16[1, 32, 512, 64]"
  t1936 = prims.convert_element_type(t1935, dtypes.float32)  # t1936: "cuda:0 f32[1, 32, 512, 64]"
  t1937 = prims.neg(t1936)  # t1937: "cuda:0 f32[1, 32, 512, 64]"
  t1938 = prims.convert_element_type(t1937, dtypes.bfloat16)  # t1938: "cuda:0 bf16[1, 32, 512, 64]"
  t1940 = prims.cat((t1938, t1934), -1)  # t1940: "cuda:0 bf16[1, 32, 512, 128]"
  t1941 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1941: "cuda:0 f32[1, 32, 512, 128]"
  t1942 = prims.convert_element_type(t1933, dtypes.float32)  # t1942: "cuda:0 f32[1, 32, 512, 128]"
  t1943 = ltorch.mul(t1942, t1941)  # t1943: "cuda:0 f32[1, 32, 512, 128]"
    # t1943 = prims.mul(t1942, t1941)  # t1943: "cuda:0 f32[1, 32, 512, 128]"
  t1944 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1944: "cuda:0 f32[1, 32, 512, 128]"
  t1945 = prims.convert_element_type(t1940, dtypes.float32)  # t1945: "cuda:0 f32[1, 32, 512, 128]"
  t1946 = ltorch.mul(t1945, t1944)  # t1946: "cuda:0 f32[1, 32, 512, 128]"
    # t1946 = prims.mul(t1945, t1944)  # t1946: "cuda:0 f32[1, 32, 512, 128]"
  t1947 = ltorch.add(t1943, t1946, alpha=None)  # t1947: "cuda:0 f32[1, 32, 512, 128]"
    # t1947 = prims.add(t1943, t1946)  # t1947: "cuda:0 f32[1, 32, 512, 128]"
  t1948 = prims.convert_element_type(t1947, dtypes.bfloat16)  # t1948: "cuda:0 bf16[1, 32, 512, 128]"
  t1949 = prims.slice_prim(t1904, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1949: "cuda:0 bf16[1, 32, 512, 0]"
  t1951 = prims.cat((t1932, t1949), -1)  # t1951: "cuda:0 bf16[1, 32, 512, 128]"
  t1952 = prims.slice_prim(t1910, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1952: "cuda:0 bf16[1, 32, 512, 0]"
  t1954 = prims.cat((t1948, t1952), -1)  # t1954: "cuda:0 bf16[1, 32, 512, 128]"
  (t1955, t1956, t1957, t1958) = cudnn_sdpa_fwd(t1951, t1954, t1916, None, 0.0, True, scale=0.08838834764831843)
  t1961 = prims.transpose(t1955, (0, 2, 1, 3))  # t1961: "cuda:0 bf16[1, 512, 32, 128]"
  t1965 = prims.reshape(t1961, (1, 512, 4096))  # t1965: "cuda:0 bf16[1, 512, 4096]"
  t1966 = prims.linear(t1965, t_transformer_h_13_attn_proj_weight, None)  # t1966: "cuda:0 bf16[1, 512, 4096]"
  t1967 = prims.convert_element_type(t1966, dtypes.float32)  # t1967: "cuda:0 f32[1, 512, 4096]"
  t1968 = prims.convert_element_type(t1864, dtypes.float32)  # t1968: "cuda:0 f32[1, 512, 4096]"
  t1969 = ltorch.add(t1967, t1968, alpha=None)  # t1969: "cuda:0 f32[1, 512, 4096]"
    # t1969 = prims.add(t1967, t1968)  # t1969: "cuda:0 f32[1, 512, 4096]"
  t1970 = prims.convert_element_type(t1969, dtypes.bfloat16)  # t1970: "cuda:0 bf16[1, 512, 4096]"
  t1971 = prims.convert_element_type(t1970, dtypes.float32)  # t1971: "cuda:0 f32[1, 512, 4096]"
  t1972 = ltorch.mul(t1971, t1971)  # t1972: "cuda:0 f32[1, 512, 4096]"
    # t1972 = prims.mul(t1971, t1971)  # t1972: "cuda:0 f32[1, 512, 4096]"
  t1974 = prims.sum(t1972, (2,))  # t1974: "cuda:0 f32[1, 512]"
  t1975 = prims.broadcast_in_dim(t1974, [1, 512, 1], [0, 1])  # t1975: "cuda:0 f32[1, 512, 1]"
  t1977 = ltorch.true_divide(t1975, 4096.0)  # t1977: "cuda:0 f32[1, 512, 1]"
    # t1977 = prims.div(t1975, 4096.0)  # t1977: "cuda:0 f32[1, 512, 1]"
  t1979 = ltorch.add(t1977, 1e-05, alpha=None)  # t1979: "cuda:0 f32[1, 512, 1]"
    # t1979 = prims.add(t1977, 1e-05)  # t1979: "cuda:0 f32[1, 512, 1]"
  t1980 = prims.rsqrt(t1979)  # t1980: "cuda:0 f32[1, 512, 1]"
  t1981 = prims.broadcast_in_dim(t1980, (1, 512, 4096), (0, 1, 2))  # t1981: "cuda:0 f32[1, 512, 4096]"
  t1982 = ltorch.mul(t1971, t1981)  # t1982: "cuda:0 f32[1, 512, 4096]"
    # t1982 = prims.mul(t1971, t1981)  # t1982: "cuda:0 f32[1, 512, 4096]"
  t1983 = prims.convert_element_type(t1982, dtypes.bfloat16)  # t1983: "cuda:0 bf16[1, 512, 4096]"
  t1984 = prims.broadcast_in_dim(t_transformer_h_13_norm_2_weight, (1, 512, 4096), (2,))  # t1984: "cuda:0 bf16[1, 512, 4096]"
  t1985 = prims.convert_element_type(t1983, dtypes.float32)  # t1985: "cuda:0 f32[1, 512, 4096]"
  t1986 = prims.convert_element_type(t1984, dtypes.float32)  # t1986: "cuda:0 f32[1, 512, 4096]"
  t1987 = ltorch.mul(t1985, t1986)  # t1987: "cuda:0 f32[1, 512, 4096]"
    # t1987 = prims.mul(t1985, t1986)  # t1987: "cuda:0 f32[1, 512, 4096]"
  t1988 = prims.convert_element_type(t1987, dtypes.bfloat16)  # t1988: "cuda:0 bf16[1, 512, 4096]"
  t1989 = prims.linear(t1988, t_transformer_h_13_mlp_fc_1_weight, None)  # t1989: "cuda:0 bf16[1, 512, 11008]"
  t1990 = prims.linear(t1988, t_transformer_h_13_mlp_fc_2_weight, None)  # t1990: "cuda:0 bf16[1, 512, 11008]"
  t1991 = prims.convert_element_type(t1989, dtypes.float32)  # t1991: "cuda:0 f32[1, 512, 11008]"
  t1992 = prims.neg(t1991)  # t1992: "cuda:0 f32[1, 512, 11008]"
  t1993 = prims.exp(t1992)  # t1993: "cuda:0 f32[1, 512, 11008]"
  t1994 = ltorch.add(1.0, t1993, alpha=None)  # t1994: "cuda:0 f32[1, 512, 11008]"
    # t1994 = prims.add(1.0, t1993)  # t1994: "cuda:0 f32[1, 512, 11008]"
  t1995 = prims.reciprocal(t1994)  # t1995: "cuda:0 f32[1, 512, 11008]"
  t1996 = prims.convert_element_type(t1995, dtypes.bfloat16)  # t1996: "cuda:0 bf16[1, 512, 11008]"
  t1997 = prims.convert_element_type(t1989, dtypes.float32)  # t1997: "cuda:0 f32[1, 512, 11008]"
  t1998 = prims.convert_element_type(t1996, dtypes.float32)  # t1998: "cuda:0 f32[1, 512, 11008]"
  t1999 = ltorch.mul(t1997, t1998)  # t1999: "cuda:0 f32[1, 512, 11008]"
    # t1999 = prims.mul(t1997, t1998)  # t1999: "cuda:0 f32[1, 512, 11008]"
  t2000 = prims.convert_element_type(t1999, dtypes.bfloat16)  # t2000: "cuda:0 bf16[1, 512, 11008]"
  t2001 = prims.convert_element_type(t2000, dtypes.float32)  # t2001: "cuda:0 f32[1, 512, 11008]"
  t2002 = prims.convert_element_type(t1990, dtypes.float32)  # t2002: "cuda:0 f32[1, 512, 11008]"
  t2003 = ltorch.mul(t2001, t2002)  # t2003: "cuda:0 f32[1, 512, 11008]"
    # t2003 = prims.mul(t2001, t2002)  # t2003: "cuda:0 f32[1, 512, 11008]"
  t2004 = prims.convert_element_type(t2003, dtypes.bfloat16)  # t2004: "cuda:0 bf16[1, 512, 11008]"
  t2005 = prims.linear(t2004, t_transformer_h_13_mlp_proj_weight, None)  # t2005: "cuda:0 bf16[1, 512, 4096]"
  t2006 = prims.convert_element_type(t2005, dtypes.float32)  # t2006: "cuda:0 f32[1, 512, 4096]"
  t2007 = prims.convert_element_type(t1970, dtypes.float32)  # t2007: "cuda:0 f32[1, 512, 4096]"
  t2008 = ltorch.add(t2006, t2007, alpha=None)  # t2008: "cuda:0 f32[1, 512, 4096]"
    # t2008 = prims.add(t2006, t2007)  # t2008: "cuda:0 f32[1, 512, 4096]"
  t2009 = prims.convert_element_type(t2008, dtypes.bfloat16)  # t2009: "cuda:0 bf16[1, 512, 4096]"
  t2010 = prims.convert_element_type(t2009, dtypes.float32)  # t2010: "cuda:0 f32[1, 512, 4096]"
  t2011 = ltorch.mul(t2010, t2010)  # t2011: "cuda:0 f32[1, 512, 4096]"
    # t2011 = prims.mul(t2010, t2010)  # t2011: "cuda:0 f32[1, 512, 4096]"
  t2013 = prims.sum(t2011, (2,))  # t2013: "cuda:0 f32[1, 512]"
  t2014 = prims.broadcast_in_dim(t2013, [1, 512, 1], [0, 1])  # t2014: "cuda:0 f32[1, 512, 1]"
  t2016 = ltorch.true_divide(t2014, 4096.0)  # t2016: "cuda:0 f32[1, 512, 1]"
    # t2016 = prims.div(t2014, 4096.0)  # t2016: "cuda:0 f32[1, 512, 1]"
  t2018 = ltorch.add(t2016, 1e-05, alpha=None)  # t2018: "cuda:0 f32[1, 512, 1]"
    # t2018 = prims.add(t2016, 1e-05)  # t2018: "cuda:0 f32[1, 512, 1]"
  t2019 = prims.rsqrt(t2018)  # t2019: "cuda:0 f32[1, 512, 1]"
  t2020 = prims.broadcast_in_dim(t2019, (1, 512, 4096), (0, 1, 2))  # t2020: "cuda:0 f32[1, 512, 4096]"
  t2021 = ltorch.mul(t2010, t2020)  # t2021: "cuda:0 f32[1, 512, 4096]"
    # t2021 = prims.mul(t2010, t2020)  # t2021: "cuda:0 f32[1, 512, 4096]"
  t2022 = prims.convert_element_type(t2021, dtypes.bfloat16)  # t2022: "cuda:0 bf16[1, 512, 4096]"
  t2023 = prims.broadcast_in_dim(t_transformer_h_14_norm_1_weight, (1, 512, 4096), (2,))  # t2023: "cuda:0 bf16[1, 512, 4096]"
  t2024 = prims.convert_element_type(t2022, dtypes.float32)  # t2024: "cuda:0 f32[1, 512, 4096]"
  t2025 = prims.convert_element_type(t2023, dtypes.float32)  # t2025: "cuda:0 f32[1, 512, 4096]"
  t2026 = ltorch.mul(t2024, t2025)  # t2026: "cuda:0 f32[1, 512, 4096]"
    # t2026 = prims.mul(t2024, t2025)  # t2026: "cuda:0 f32[1, 512, 4096]"
  t2027 = prims.convert_element_type(t2026, dtypes.bfloat16)  # t2027: "cuda:0 bf16[1, 512, 4096]"
  t2028 = prims.linear(t2027, t_transformer_h_14_attn_attn_weight, None)  # t2028: "cuda:0 bf16[1, 512, 12288]"
  t2034 = prims.reshape(t2028, (1, 512, 32, 3, 128))  # t2034: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t2040 = prims.transpose(t2034, (0, 2, 3, 1, 4))  # t2040: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t2041, t2042, t2043) = ltorch.split(t2040, (1, 1, 1), 2)
    # t2041 = prims.slice_prim(t2040, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2041: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2042 = prims.slice_prim(t2040, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2042: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2043 = prims.slice_prim(t2040, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2043: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t2049 = prims.reshape(t2041, (1, 32, 512, 128))  # t2049: "cuda:0 bf16[1, 32, 512, 128]"
  t2055 = prims.reshape(t2042, (1, 32, 512, 128))  # t2055: "cuda:0 bf16[1, 32, 512, 128]"
  t2061 = prims.reshape(t2043, (1, 32, 512, 128))  # t2061: "cuda:0 bf16[1, 32, 512, 128]"
  t2062 = prims.slice_prim(t2049, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2062: "cuda:0 bf16[1, 32, 512, 128]"
  t2063 = prims.slice_prim(t2062, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2063: "cuda:0 bf16[1, 32, 512, 64]"
  t2064 = prims.slice_prim(t2062, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2064: "cuda:0 bf16[1, 32, 512, 64]"
  t2065 = prims.convert_element_type(t2064, dtypes.float32)  # t2065: "cuda:0 f32[1, 32, 512, 64]"
  t2066 = prims.neg(t2065)  # t2066: "cuda:0 f32[1, 32, 512, 64]"
  t2067 = prims.convert_element_type(t2066, dtypes.bfloat16)  # t2067: "cuda:0 bf16[1, 32, 512, 64]"
  t2069 = prims.cat((t2067, t2063), -1)  # t2069: "cuda:0 bf16[1, 32, 512, 128]"
  t2070 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2070: "cuda:0 f32[1, 32, 512, 128]"
  t2071 = prims.convert_element_type(t2062, dtypes.float32)  # t2071: "cuda:0 f32[1, 32, 512, 128]"
  t2072 = ltorch.mul(t2071, t2070)  # t2072: "cuda:0 f32[1, 32, 512, 128]"
    # t2072 = prims.mul(t2071, t2070)  # t2072: "cuda:0 f32[1, 32, 512, 128]"
  t2073 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2073: "cuda:0 f32[1, 32, 512, 128]"
  t2074 = prims.convert_element_type(t2069, dtypes.float32)  # t2074: "cuda:0 f32[1, 32, 512, 128]"
  t2075 = ltorch.mul(t2074, t2073)  # t2075: "cuda:0 f32[1, 32, 512, 128]"
    # t2075 = prims.mul(t2074, t2073)  # t2075: "cuda:0 f32[1, 32, 512, 128]"
  t2076 = ltorch.add(t2072, t2075, alpha=None)  # t2076: "cuda:0 f32[1, 32, 512, 128]"
    # t2076 = prims.add(t2072, t2075)  # t2076: "cuda:0 f32[1, 32, 512, 128]"
  t2077 = prims.convert_element_type(t2076, dtypes.bfloat16)  # t2077: "cuda:0 bf16[1, 32, 512, 128]"
  t2078 = prims.slice_prim(t2055, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2078: "cuda:0 bf16[1, 32, 512, 128]"
  t2079 = prims.slice_prim(t2078, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2079: "cuda:0 bf16[1, 32, 512, 64]"
  t2080 = prims.slice_prim(t2078, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2080: "cuda:0 bf16[1, 32, 512, 64]"
  t2081 = prims.convert_element_type(t2080, dtypes.float32)  # t2081: "cuda:0 f32[1, 32, 512, 64]"
  t2082 = prims.neg(t2081)  # t2082: "cuda:0 f32[1, 32, 512, 64]"
  t2083 = prims.convert_element_type(t2082, dtypes.bfloat16)  # t2083: "cuda:0 bf16[1, 32, 512, 64]"
  t2085 = prims.cat((t2083, t2079), -1)  # t2085: "cuda:0 bf16[1, 32, 512, 128]"
  t2086 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2086: "cuda:0 f32[1, 32, 512, 128]"
  t2087 = prims.convert_element_type(t2078, dtypes.float32)  # t2087: "cuda:0 f32[1, 32, 512, 128]"
  t2088 = ltorch.mul(t2087, t2086)  # t2088: "cuda:0 f32[1, 32, 512, 128]"
    # t2088 = prims.mul(t2087, t2086)  # t2088: "cuda:0 f32[1, 32, 512, 128]"
  t2089 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2089: "cuda:0 f32[1, 32, 512, 128]"
  t2090 = prims.convert_element_type(t2085, dtypes.float32)  # t2090: "cuda:0 f32[1, 32, 512, 128]"
  t2091 = ltorch.mul(t2090, t2089)  # t2091: "cuda:0 f32[1, 32, 512, 128]"
    # t2091 = prims.mul(t2090, t2089)  # t2091: "cuda:0 f32[1, 32, 512, 128]"
  t2092 = ltorch.add(t2088, t2091, alpha=None)  # t2092: "cuda:0 f32[1, 32, 512, 128]"
    # t2092 = prims.add(t2088, t2091)  # t2092: "cuda:0 f32[1, 32, 512, 128]"
  t2093 = prims.convert_element_type(t2092, dtypes.bfloat16)  # t2093: "cuda:0 bf16[1, 32, 512, 128]"
  t2094 = prims.slice_prim(t2049, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2094: "cuda:0 bf16[1, 32, 512, 0]"
  t2096 = prims.cat((t2077, t2094), -1)  # t2096: "cuda:0 bf16[1, 32, 512, 128]"
  t2097 = prims.slice_prim(t2055, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2097: "cuda:0 bf16[1, 32, 512, 0]"
  t2099 = prims.cat((t2093, t2097), -1)  # t2099: "cuda:0 bf16[1, 32, 512, 128]"
  (t2100, t2101, t2102, t2103) = cudnn_sdpa_fwd(t2096, t2099, t2061, None, 0.0, True, scale=0.08838834764831843)
  t2106 = prims.transpose(t2100, (0, 2, 1, 3))  # t2106: "cuda:0 bf16[1, 512, 32, 128]"
  t2110 = prims.reshape(t2106, (1, 512, 4096))  # t2110: "cuda:0 bf16[1, 512, 4096]"
  t2111 = prims.linear(t2110, t_transformer_h_14_attn_proj_weight, None)  # t2111: "cuda:0 bf16[1, 512, 4096]"
  t2112 = prims.convert_element_type(t2111, dtypes.float32)  # t2112: "cuda:0 f32[1, 512, 4096]"
  t2113 = prims.convert_element_type(t2009, dtypes.float32)  # t2113: "cuda:0 f32[1, 512, 4096]"
  t2114 = ltorch.add(t2112, t2113, alpha=None)  # t2114: "cuda:0 f32[1, 512, 4096]"
    # t2114 = prims.add(t2112, t2113)  # t2114: "cuda:0 f32[1, 512, 4096]"
  t2115 = prims.convert_element_type(t2114, dtypes.bfloat16)  # t2115: "cuda:0 bf16[1, 512, 4096]"
  t2116 = prims.convert_element_type(t2115, dtypes.float32)  # t2116: "cuda:0 f32[1, 512, 4096]"
  t2117 = ltorch.mul(t2116, t2116)  # t2117: "cuda:0 f32[1, 512, 4096]"
    # t2117 = prims.mul(t2116, t2116)  # t2117: "cuda:0 f32[1, 512, 4096]"
  t2119 = prims.sum(t2117, (2,))  # t2119: "cuda:0 f32[1, 512]"
  t2120 = prims.broadcast_in_dim(t2119, [1, 512, 1], [0, 1])  # t2120: "cuda:0 f32[1, 512, 1]"
  t2122 = ltorch.true_divide(t2120, 4096.0)  # t2122: "cuda:0 f32[1, 512, 1]"
    # t2122 = prims.div(t2120, 4096.0)  # t2122: "cuda:0 f32[1, 512, 1]"
  t2124 = ltorch.add(t2122, 1e-05, alpha=None)  # t2124: "cuda:0 f32[1, 512, 1]"
    # t2124 = prims.add(t2122, 1e-05)  # t2124: "cuda:0 f32[1, 512, 1]"
  t2125 = prims.rsqrt(t2124)  # t2125: "cuda:0 f32[1, 512, 1]"
  t2126 = prims.broadcast_in_dim(t2125, (1, 512, 4096), (0, 1, 2))  # t2126: "cuda:0 f32[1, 512, 4096]"
  t2127 = ltorch.mul(t2116, t2126)  # t2127: "cuda:0 f32[1, 512, 4096]"
    # t2127 = prims.mul(t2116, t2126)  # t2127: "cuda:0 f32[1, 512, 4096]"
  t2128 = prims.convert_element_type(t2127, dtypes.bfloat16)  # t2128: "cuda:0 bf16[1, 512, 4096]"
  t2129 = prims.broadcast_in_dim(t_transformer_h_14_norm_2_weight, (1, 512, 4096), (2,))  # t2129: "cuda:0 bf16[1, 512, 4096]"
  t2130 = prims.convert_element_type(t2128, dtypes.float32)  # t2130: "cuda:0 f32[1, 512, 4096]"
  t2131 = prims.convert_element_type(t2129, dtypes.float32)  # t2131: "cuda:0 f32[1, 512, 4096]"
  t2132 = ltorch.mul(t2130, t2131)  # t2132: "cuda:0 f32[1, 512, 4096]"
    # t2132 = prims.mul(t2130, t2131)  # t2132: "cuda:0 f32[1, 512, 4096]"
  t2133 = prims.convert_element_type(t2132, dtypes.bfloat16)  # t2133: "cuda:0 bf16[1, 512, 4096]"
  t2134 = prims.linear(t2133, t_transformer_h_14_mlp_fc_1_weight, None)  # t2134: "cuda:0 bf16[1, 512, 11008]"
  t2135 = prims.linear(t2133, t_transformer_h_14_mlp_fc_2_weight, None)  # t2135: "cuda:0 bf16[1, 512, 11008]"
  t2136 = prims.convert_element_type(t2134, dtypes.float32)  # t2136: "cuda:0 f32[1, 512, 11008]"
  t2137 = prims.neg(t2136)  # t2137: "cuda:0 f32[1, 512, 11008]"
  t2138 = prims.exp(t2137)  # t2138: "cuda:0 f32[1, 512, 11008]"
  t2139 = ltorch.add(1.0, t2138, alpha=None)  # t2139: "cuda:0 f32[1, 512, 11008]"
    # t2139 = prims.add(1.0, t2138)  # t2139: "cuda:0 f32[1, 512, 11008]"
  t2140 = prims.reciprocal(t2139)  # t2140: "cuda:0 f32[1, 512, 11008]"
  t2141 = prims.convert_element_type(t2140, dtypes.bfloat16)  # t2141: "cuda:0 bf16[1, 512, 11008]"
  t2142 = prims.convert_element_type(t2134, dtypes.float32)  # t2142: "cuda:0 f32[1, 512, 11008]"
  t2143 = prims.convert_element_type(t2141, dtypes.float32)  # t2143: "cuda:0 f32[1, 512, 11008]"
  t2144 = ltorch.mul(t2142, t2143)  # t2144: "cuda:0 f32[1, 512, 11008]"
    # t2144 = prims.mul(t2142, t2143)  # t2144: "cuda:0 f32[1, 512, 11008]"
  t2145 = prims.convert_element_type(t2144, dtypes.bfloat16)  # t2145: "cuda:0 bf16[1, 512, 11008]"
  t2146 = prims.convert_element_type(t2145, dtypes.float32)  # t2146: "cuda:0 f32[1, 512, 11008]"
  t2147 = prims.convert_element_type(t2135, dtypes.float32)  # t2147: "cuda:0 f32[1, 512, 11008]"
  t2148 = ltorch.mul(t2146, t2147)  # t2148: "cuda:0 f32[1, 512, 11008]"
    # t2148 = prims.mul(t2146, t2147)  # t2148: "cuda:0 f32[1, 512, 11008]"
  t2149 = prims.convert_element_type(t2148, dtypes.bfloat16)  # t2149: "cuda:0 bf16[1, 512, 11008]"
  t2150 = prims.linear(t2149, t_transformer_h_14_mlp_proj_weight, None)  # t2150: "cuda:0 bf16[1, 512, 4096]"
  t2151 = prims.convert_element_type(t2150, dtypes.float32)  # t2151: "cuda:0 f32[1, 512, 4096]"
  t2152 = prims.convert_element_type(t2115, dtypes.float32)  # t2152: "cuda:0 f32[1, 512, 4096]"
  t2153 = ltorch.add(t2151, t2152, alpha=None)  # t2153: "cuda:0 f32[1, 512, 4096]"
    # t2153 = prims.add(t2151, t2152)  # t2153: "cuda:0 f32[1, 512, 4096]"
  t2154 = prims.convert_element_type(t2153, dtypes.bfloat16)  # t2154: "cuda:0 bf16[1, 512, 4096]"
  t2155 = prims.convert_element_type(t2154, dtypes.float32)  # t2155: "cuda:0 f32[1, 512, 4096]"
  t2156 = ltorch.mul(t2155, t2155)  # t2156: "cuda:0 f32[1, 512, 4096]"
    # t2156 = prims.mul(t2155, t2155)  # t2156: "cuda:0 f32[1, 512, 4096]"
  t2158 = prims.sum(t2156, (2,))  # t2158: "cuda:0 f32[1, 512]"
  t2159 = prims.broadcast_in_dim(t2158, [1, 512, 1], [0, 1])  # t2159: "cuda:0 f32[1, 512, 1]"
  t2161 = ltorch.true_divide(t2159, 4096.0)  # t2161: "cuda:0 f32[1, 512, 1]"
    # t2161 = prims.div(t2159, 4096.0)  # t2161: "cuda:0 f32[1, 512, 1]"
  t2163 = ltorch.add(t2161, 1e-05, alpha=None)  # t2163: "cuda:0 f32[1, 512, 1]"
    # t2163 = prims.add(t2161, 1e-05)  # t2163: "cuda:0 f32[1, 512, 1]"
  t2164 = prims.rsqrt(t2163)  # t2164: "cuda:0 f32[1, 512, 1]"
  t2165 = prims.broadcast_in_dim(t2164, (1, 512, 4096), (0, 1, 2))  # t2165: "cuda:0 f32[1, 512, 4096]"
  t2166 = ltorch.mul(t2155, t2165)  # t2166: "cuda:0 f32[1, 512, 4096]"
    # t2166 = prims.mul(t2155, t2165)  # t2166: "cuda:0 f32[1, 512, 4096]"
  t2167 = prims.convert_element_type(t2166, dtypes.bfloat16)  # t2167: "cuda:0 bf16[1, 512, 4096]"
  t2168 = prims.broadcast_in_dim(t_transformer_h_15_norm_1_weight, (1, 512, 4096), (2,))  # t2168: "cuda:0 bf16[1, 512, 4096]"
  t2169 = prims.convert_element_type(t2167, dtypes.float32)  # t2169: "cuda:0 f32[1, 512, 4096]"
  t2170 = prims.convert_element_type(t2168, dtypes.float32)  # t2170: "cuda:0 f32[1, 512, 4096]"
  t2171 = ltorch.mul(t2169, t2170)  # t2171: "cuda:0 f32[1, 512, 4096]"
    # t2171 = prims.mul(t2169, t2170)  # t2171: "cuda:0 f32[1, 512, 4096]"
  t2172 = prims.convert_element_type(t2171, dtypes.bfloat16)  # t2172: "cuda:0 bf16[1, 512, 4096]"
  t2173 = prims.linear(t2172, t_transformer_h_15_attn_attn_weight, None)  # t2173: "cuda:0 bf16[1, 512, 12288]"
  t2179 = prims.reshape(t2173, (1, 512, 32, 3, 128))  # t2179: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t2185 = prims.transpose(t2179, (0, 2, 3, 1, 4))  # t2185: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t2186, t2187, t2188) = ltorch.split(t2185, (1, 1, 1), 2)
    # t2186 = prims.slice_prim(t2185, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2186: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2187 = prims.slice_prim(t2185, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2187: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2188 = prims.slice_prim(t2185, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2188: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t2194 = prims.reshape(t2186, (1, 32, 512, 128))  # t2194: "cuda:0 bf16[1, 32, 512, 128]"
  t2200 = prims.reshape(t2187, (1, 32, 512, 128))  # t2200: "cuda:0 bf16[1, 32, 512, 128]"
  t2206 = prims.reshape(t2188, (1, 32, 512, 128))  # t2206: "cuda:0 bf16[1, 32, 512, 128]"
  t2207 = prims.slice_prim(t2194, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2207: "cuda:0 bf16[1, 32, 512, 128]"
  t2208 = prims.slice_prim(t2207, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2208: "cuda:0 bf16[1, 32, 512, 64]"
  t2209 = prims.slice_prim(t2207, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2209: "cuda:0 bf16[1, 32, 512, 64]"
  t2210 = prims.convert_element_type(t2209, dtypes.float32)  # t2210: "cuda:0 f32[1, 32, 512, 64]"
  t2211 = prims.neg(t2210)  # t2211: "cuda:0 f32[1, 32, 512, 64]"
  t2212 = prims.convert_element_type(t2211, dtypes.bfloat16)  # t2212: "cuda:0 bf16[1, 32, 512, 64]"
  t2214 = prims.cat((t2212, t2208), -1)  # t2214: "cuda:0 bf16[1, 32, 512, 128]"
  t2215 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2215: "cuda:0 f32[1, 32, 512, 128]"
  t2216 = prims.convert_element_type(t2207, dtypes.float32)  # t2216: "cuda:0 f32[1, 32, 512, 128]"
  t2217 = ltorch.mul(t2216, t2215)  # t2217: "cuda:0 f32[1, 32, 512, 128]"
    # t2217 = prims.mul(t2216, t2215)  # t2217: "cuda:0 f32[1, 32, 512, 128]"
  t2218 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2218: "cuda:0 f32[1, 32, 512, 128]"
  t2219 = prims.convert_element_type(t2214, dtypes.float32)  # t2219: "cuda:0 f32[1, 32, 512, 128]"
  t2220 = ltorch.mul(t2219, t2218)  # t2220: "cuda:0 f32[1, 32, 512, 128]"
    # t2220 = prims.mul(t2219, t2218)  # t2220: "cuda:0 f32[1, 32, 512, 128]"
  t2221 = ltorch.add(t2217, t2220, alpha=None)  # t2221: "cuda:0 f32[1, 32, 512, 128]"
    # t2221 = prims.add(t2217, t2220)  # t2221: "cuda:0 f32[1, 32, 512, 128]"
  t2222 = prims.convert_element_type(t2221, dtypes.bfloat16)  # t2222: "cuda:0 bf16[1, 32, 512, 128]"
  t2223 = prims.slice_prim(t2200, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2223: "cuda:0 bf16[1, 32, 512, 128]"
  t2224 = prims.slice_prim(t2223, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2224: "cuda:0 bf16[1, 32, 512, 64]"
  t2225 = prims.slice_prim(t2223, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2225: "cuda:0 bf16[1, 32, 512, 64]"
  t2226 = prims.convert_element_type(t2225, dtypes.float32)  # t2226: "cuda:0 f32[1, 32, 512, 64]"
  t2227 = prims.neg(t2226)  # t2227: "cuda:0 f32[1, 32, 512, 64]"
  t2228 = prims.convert_element_type(t2227, dtypes.bfloat16)  # t2228: "cuda:0 bf16[1, 32, 512, 64]"
  t2230 = prims.cat((t2228, t2224), -1)  # t2230: "cuda:0 bf16[1, 32, 512, 128]"
  t2231 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2231: "cuda:0 f32[1, 32, 512, 128]"
  t2232 = prims.convert_element_type(t2223, dtypes.float32)  # t2232: "cuda:0 f32[1, 32, 512, 128]"
  t2233 = ltorch.mul(t2232, t2231)  # t2233: "cuda:0 f32[1, 32, 512, 128]"
    # t2233 = prims.mul(t2232, t2231)  # t2233: "cuda:0 f32[1, 32, 512, 128]"
  t2234 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2234: "cuda:0 f32[1, 32, 512, 128]"
  t2235 = prims.convert_element_type(t2230, dtypes.float32)  # t2235: "cuda:0 f32[1, 32, 512, 128]"
  t2236 = ltorch.mul(t2235, t2234)  # t2236: "cuda:0 f32[1, 32, 512, 128]"
    # t2236 = prims.mul(t2235, t2234)  # t2236: "cuda:0 f32[1, 32, 512, 128]"
  t2237 = ltorch.add(t2233, t2236, alpha=None)  # t2237: "cuda:0 f32[1, 32, 512, 128]"
    # t2237 = prims.add(t2233, t2236)  # t2237: "cuda:0 f32[1, 32, 512, 128]"
  t2238 = prims.convert_element_type(t2237, dtypes.bfloat16)  # t2238: "cuda:0 bf16[1, 32, 512, 128]"
  t2239 = prims.slice_prim(t2194, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2239: "cuda:0 bf16[1, 32, 512, 0]"
  t2241 = prims.cat((t2222, t2239), -1)  # t2241: "cuda:0 bf16[1, 32, 512, 128]"
  t2242 = prims.slice_prim(t2200, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2242: "cuda:0 bf16[1, 32, 512, 0]"
  t2244 = prims.cat((t2238, t2242), -1)  # t2244: "cuda:0 bf16[1, 32, 512, 128]"
  (t2245, t2246, t2247, t2248) = cudnn_sdpa_fwd(t2241, t2244, t2206, None, 0.0, True, scale=0.08838834764831843)
  t2251 = prims.transpose(t2245, (0, 2, 1, 3))  # t2251: "cuda:0 bf16[1, 512, 32, 128]"
  t2255 = prims.reshape(t2251, (1, 512, 4096))  # t2255: "cuda:0 bf16[1, 512, 4096]"
  t2256 = prims.linear(t2255, t_transformer_h_15_attn_proj_weight, None)  # t2256: "cuda:0 bf16[1, 512, 4096]"
  t2257 = prims.convert_element_type(t2256, dtypes.float32)  # t2257: "cuda:0 f32[1, 512, 4096]"
  t2258 = prims.convert_element_type(t2154, dtypes.float32)  # t2258: "cuda:0 f32[1, 512, 4096]"
  t2259 = ltorch.add(t2257, t2258, alpha=None)  # t2259: "cuda:0 f32[1, 512, 4096]"
    # t2259 = prims.add(t2257, t2258)  # t2259: "cuda:0 f32[1, 512, 4096]"
  t2260 = prims.convert_element_type(t2259, dtypes.bfloat16)  # t2260: "cuda:0 bf16[1, 512, 4096]"
  t2261 = prims.convert_element_type(t2260, dtypes.float32)  # t2261: "cuda:0 f32[1, 512, 4096]"
  t2262 = ltorch.mul(t2261, t2261)  # t2262: "cuda:0 f32[1, 512, 4096]"
    # t2262 = prims.mul(t2261, t2261)  # t2262: "cuda:0 f32[1, 512, 4096]"
  t2264 = prims.sum(t2262, (2,))  # t2264: "cuda:0 f32[1, 512]"
  t2265 = prims.broadcast_in_dim(t2264, [1, 512, 1], [0, 1])  # t2265: "cuda:0 f32[1, 512, 1]"
  t2267 = ltorch.true_divide(t2265, 4096.0)  # t2267: "cuda:0 f32[1, 512, 1]"
    # t2267 = prims.div(t2265, 4096.0)  # t2267: "cuda:0 f32[1, 512, 1]"
  t2269 = ltorch.add(t2267, 1e-05, alpha=None)  # t2269: "cuda:0 f32[1, 512, 1]"
    # t2269 = prims.add(t2267, 1e-05)  # t2269: "cuda:0 f32[1, 512, 1]"
  t2270 = prims.rsqrt(t2269)  # t2270: "cuda:0 f32[1, 512, 1]"
  t2271 = prims.broadcast_in_dim(t2270, (1, 512, 4096), (0, 1, 2))  # t2271: "cuda:0 f32[1, 512, 4096]"
  t2272 = ltorch.mul(t2261, t2271)  # t2272: "cuda:0 f32[1, 512, 4096]"
    # t2272 = prims.mul(t2261, t2271)  # t2272: "cuda:0 f32[1, 512, 4096]"
  t2273 = prims.convert_element_type(t2272, dtypes.bfloat16)  # t2273: "cuda:0 bf16[1, 512, 4096]"
  t2274 = prims.broadcast_in_dim(t_transformer_h_15_norm_2_weight, (1, 512, 4096), (2,))  # t2274: "cuda:0 bf16[1, 512, 4096]"
  t2275 = prims.convert_element_type(t2273, dtypes.float32)  # t2275: "cuda:0 f32[1, 512, 4096]"
  t2276 = prims.convert_element_type(t2274, dtypes.float32)  # t2276: "cuda:0 f32[1, 512, 4096]"
  t2277 = ltorch.mul(t2275, t2276)  # t2277: "cuda:0 f32[1, 512, 4096]"
    # t2277 = prims.mul(t2275, t2276)  # t2277: "cuda:0 f32[1, 512, 4096]"
  t2278 = prims.convert_element_type(t2277, dtypes.bfloat16)  # t2278: "cuda:0 bf16[1, 512, 4096]"
  t2279 = prims.linear(t2278, t_transformer_h_15_mlp_fc_1_weight, None)  # t2279: "cuda:0 bf16[1, 512, 11008]"
  t2280 = prims.linear(t2278, t_transformer_h_15_mlp_fc_2_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
  t2281 = prims.convert_element_type(t2279, dtypes.float32)  # t2281: "cuda:0 f32[1, 512, 11008]"
  t2282 = prims.neg(t2281)  # t2282: "cuda:0 f32[1, 512, 11008]"
  t2283 = prims.exp(t2282)  # t2283: "cuda:0 f32[1, 512, 11008]"
  t2284 = ltorch.add(1.0, t2283, alpha=None)  # t2284: "cuda:0 f32[1, 512, 11008]"
    # t2284 = prims.add(1.0, t2283)  # t2284: "cuda:0 f32[1, 512, 11008]"
  t2285 = prims.reciprocal(t2284)  # t2285: "cuda:0 f32[1, 512, 11008]"
  t2286 = prims.convert_element_type(t2285, dtypes.bfloat16)  # t2286: "cuda:0 bf16[1, 512, 11008]"
  t2287 = prims.convert_element_type(t2279, dtypes.float32)  # t2287: "cuda:0 f32[1, 512, 11008]"
  t2288 = prims.convert_element_type(t2286, dtypes.float32)  # t2288: "cuda:0 f32[1, 512, 11008]"
  t2289 = ltorch.mul(t2287, t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
    # t2289 = prims.mul(t2287, t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
  t2290 = prims.convert_element_type(t2289, dtypes.bfloat16)  # t2290: "cuda:0 bf16[1, 512, 11008]"
  t2291 = prims.convert_element_type(t2290, dtypes.float32)  # t2291: "cuda:0 f32[1, 512, 11008]"
  t2292 = prims.convert_element_type(t2280, dtypes.float32)  # t2292: "cuda:0 f32[1, 512, 11008]"
  t2293 = ltorch.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
    # t2293 = prims.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
  t2294 = prims.convert_element_type(t2293, dtypes.bfloat16)  # t2294: "cuda:0 bf16[1, 512, 11008]"
  t2295 = prims.linear(t2294, t_transformer_h_15_mlp_proj_weight, None)  # t2295: "cuda:0 bf16[1, 512, 4096]"
  t2296 = prims.convert_element_type(t2295, dtypes.float32)  # t2296: "cuda:0 f32[1, 512, 4096]"
  t2297 = prims.convert_element_type(t2260, dtypes.float32)  # t2297: "cuda:0 f32[1, 512, 4096]"
  t2298 = ltorch.add(t2296, t2297, alpha=None)  # t2298: "cuda:0 f32[1, 512, 4096]"
    # t2298 = prims.add(t2296, t2297)  # t2298: "cuda:0 f32[1, 512, 4096]"
  t2299 = prims.convert_element_type(t2298, dtypes.bfloat16)  # t2299: "cuda:0 bf16[1, 512, 4096]"
  t2300 = prims.convert_element_type(t2299, dtypes.float32)  # t2300: "cuda:0 f32[1, 512, 4096]"
  t2301 = ltorch.mul(t2300, t2300)  # t2301: "cuda:0 f32[1, 512, 4096]"
    # t2301 = prims.mul(t2300, t2300)  # t2301: "cuda:0 f32[1, 512, 4096]"
  t2303 = prims.sum(t2301, (2,))  # t2303: "cuda:0 f32[1, 512]"
  t2304 = prims.broadcast_in_dim(t2303, [1, 512, 1], [0, 1])  # t2304: "cuda:0 f32[1, 512, 1]"
  t2306 = ltorch.true_divide(t2304, 4096.0)  # t2306: "cuda:0 f32[1, 512, 1]"
    # t2306 = prims.div(t2304, 4096.0)  # t2306: "cuda:0 f32[1, 512, 1]"
  t2308 = ltorch.add(t2306, 1e-05, alpha=None)  # t2308: "cuda:0 f32[1, 512, 1]"
    # t2308 = prims.add(t2306, 1e-05)  # t2308: "cuda:0 f32[1, 512, 1]"
  t2309 = prims.rsqrt(t2308)  # t2309: "cuda:0 f32[1, 512, 1]"
  t2310 = prims.broadcast_in_dim(t2309, (1, 512, 4096), (0, 1, 2))  # t2310: "cuda:0 f32[1, 512, 4096]"
  t2311 = ltorch.mul(t2300, t2310)  # t2311: "cuda:0 f32[1, 512, 4096]"
    # t2311 = prims.mul(t2300, t2310)  # t2311: "cuda:0 f32[1, 512, 4096]"
  t2312 = prims.convert_element_type(t2311, dtypes.bfloat16)  # t2312: "cuda:0 bf16[1, 512, 4096]"
  t2313 = prims.broadcast_in_dim(t_transformer_ln_f_weight, (1, 512, 4096), (2,))  # t2313: "cuda:0 bf16[1, 512, 4096]"
  t2314 = prims.convert_element_type(t2312, dtypes.float32)  # t2314: "cuda:0 f32[1, 512, 4096]"
  t2315 = prims.convert_element_type(t2313, dtypes.float32)  # t2315: "cuda:0 f32[1, 512, 4096]"
  t2316 = ltorch.mul(t2314, t2315)  # t2316: "cuda:0 f32[1, 512, 4096]"
    # t2316 = prims.mul(t2314, t2315)  # t2316: "cuda:0 f32[1, 512, 4096]"
  t2317 = prims.convert_element_type(t2316, dtypes.bfloat16)  # t2317: "cuda:0 bf16[1, 512, 4096]"
  t2318 = prims.linear(t2317, t_lm_head_weight, None)  # t2318: "cuda:0 bf16[1, 512, 32000]"
  return {'output': t2318, 'flat_args': [idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight], 'flat_output': (t2318,)}, ((idx, t5, t11, t12, t17, t16, t19, t_transformer_h_0_attn_attn_weight, t46, t47, t49, t50, t62, t63, t65, t66, t71, t74, t38, t75, t76, t77, t78, t80, t_transformer_h_0_attn_proj_weight, t86, t95, t96, t101, t100, t103, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t108, t110, t113, t112, t117, t116, t119, t_transformer_h_0_mlp_proj_weight, t125, t134, t135, t140, t139, t142, t_transformer_h_1_attn_attn_weight, t185, t186, t188, t189, t201, t202, t204, t205, t211, t214, t176, t215, t216, t217, t218, t225, t_transformer_h_1_attn_proj_weight, t231, t240, t241, t246, t245, t248, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t253, t255, t258, t257, t262, t261, t264, t_transformer_h_1_mlp_proj_weight, t270, t279, t280, t285, t284, t287, t_transformer_h_2_attn_attn_weight, t330, t331, t333, t334, t346, t347, t349, t350, t356, t359, t321, t360, t361, t362, t363, t370, t_transformer_h_2_attn_proj_weight, t376, t385, t386, t391, t390, t393, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t398, t400, t403, t402, t407, t406, t409, t_transformer_h_2_mlp_proj_weight, t415, t424, t425, t430, t429, t432, t_transformer_h_3_attn_attn_weight, t475, t476, t478, t479, t491, t492, t494, t495, t501, t504, t466, t505, t506, t507, t508, t515, t_transformer_h_3_attn_proj_weight, t521, t530, t531, t536, t535, t538, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t543, t545, t548, t547, t552, t551, t554, t_transformer_h_3_mlp_proj_weight, t560, t569, t570, t575, t574, t577, t_transformer_h_4_attn_attn_weight, t620, t621, t623, t624, t636, t637, t639, t640, t646, t649, t611, t650, t651, t652, t653, t660, t_transformer_h_4_attn_proj_weight, t666, t675, t676, t681, t680, t683, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t688, t690, t693, t692, t697, t696, t699, t_transformer_h_4_mlp_proj_weight, t705, t714, t715, t720, t719, t722, t_transformer_h_5_attn_attn_weight, t765, t766, t768, t769, t781, t782, t784, t785, t791, t794, t756, t795, t796, t797, t798, t805, t_transformer_h_5_attn_proj_weight, t811, t820, t821, t826, t825, t828, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t833, t835, t838, t837, t842, t841, t844, t_transformer_h_5_mlp_proj_weight, t850, t859, t860, t865, t864, t867, t_transformer_h_6_attn_attn_weight, t910, t911, t913, t914, t926, t927, t929, t930, t936, t939, t901, t940, t941, t942, t943, t950, t_transformer_h_6_attn_proj_weight, t956, t965, t966, t971, t970, t973, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t978, t980, t983, t982, t987, t986, t989, t_transformer_h_6_mlp_proj_weight, t995, t1004, t1005, t1010, t1009, t1012, t_transformer_h_7_attn_attn_weight, t1055, t1056, t1058, t1059, t1071, t1072, t1074, t1075, t1081, t1084, t1046, t1085, t1086, t1087, t1088, t1095, t_transformer_h_7_attn_proj_weight, t1101, t1110, t1111, t1116, t1115, t1118, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t1123, t1125, t1128, t1127, t1132, t1131, t1134, t_transformer_h_7_mlp_proj_weight, t1140, t1149, t1150, t1155, t1154, t1157, t_transformer_h_8_attn_attn_weight, t1200, t1201, t1203, t1204, t1216, t1217, t1219, t1220, t1226, t1229, t1191, t1230, t1231, t1232, t1233, t1240, t_transformer_h_8_attn_proj_weight, t1246, t1255, t1256, t1261, t1260, t1263, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t1268, t1270, t1273, t1272, t1277, t1276, t1279, t_transformer_h_8_mlp_proj_weight, t1285, t1294, t1295, t1300, t1299, t1302, t_transformer_h_9_attn_attn_weight, t1345, t1346, t1348, t1349, t1361, t1362, t1364, t1365, t1371, t1374, t1336, t1375, t1376, t1377, t1378, t1385, t_transformer_h_9_attn_proj_weight, t1391, t1400, t1401, t1406, t1405, t1408, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t1413, t1415, t1418, t1417, t1422, t1421, t1424, t_transformer_h_9_mlp_proj_weight, t1430, t1439, t1440, t1445, t1444, t1447, t_transformer_h_10_attn_attn_weight, t1490, t1491, t1493, t1494, t1506, t1507, t1509, t1510, t1516, t1519, t1481, t1520, t1521, t1522, t1523, t1530, t_transformer_h_10_attn_proj_weight, t1536, t1545, t1546, t1551, t1550, t1553, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t1558, t1560, t1563, t1562, t1567, t1566, t1569, t_transformer_h_10_mlp_proj_weight, t1575, t1584, t1585, t1590, t1589, t1592, t_transformer_h_11_attn_attn_weight, t1635, t1636, t1638, t1639, t1651, t1652, t1654, t1655, t1661, t1664, t1626, t1665, t1666, t1667, t1668, t1675, t_transformer_h_11_attn_proj_weight, t1681, t1690, t1691, t1696, t1695, t1698, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t1703, t1705, t1708, t1707, t1712, t1711, t1714, t_transformer_h_11_mlp_proj_weight, t1720, t1729, t1730, t1735, t1734, t1737, t_transformer_h_12_attn_attn_weight, t1780, t1781, t1783, t1784, t1796, t1797, t1799, t1800, t1806, t1809, t1771, t1810, t1811, t1812, t1813, t1820, t_transformer_h_12_attn_proj_weight, t1826, t1835, t1836, t1841, t1840, t1843, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t1848, t1850, t1853, t1852, t1857, t1856, t1859, t_transformer_h_12_mlp_proj_weight, t1865, t1874, t1875, t1880, t1879, t1882, t_transformer_h_13_attn_attn_weight, t1925, t1926, t1928, t1929, t1941, t1942, t1944, t1945, t1951, t1954, t1916, t1955, t1956, t1957, t1958, t1965, t_transformer_h_13_attn_proj_weight, t1971, t1980, t1981, t1986, t1985, t1988, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t1993, t1995, t1998, t1997, t2002, t2001, t2004, t_transformer_h_13_mlp_proj_weight, t2010, t2019, t2020, t2025, t2024, t2027, t_transformer_h_14_attn_attn_weight, t2070, t2071, t2073, t2074, t2086, t2087, t2089, t2090, t2096, t2099, t2061, t2100, t2101, t2102, t2103, t2110, t_transformer_h_14_attn_proj_weight, t2116, t2125, t2126, t2131, t2130, t2133, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t2138, t2140, t2143, t2142, t2147, t2146, t2149, t_transformer_h_14_mlp_proj_weight, t2155, t2164, t2165, t2170, t2169, t2172, t_transformer_h_15_attn_attn_weight, t2215, t2216, t2218, t2219, t2231, t2232, t2234, t2235, t2241, t2244, t2206, t2245, t2246, t2247, t2248, t2255, t_transformer_h_15_attn_proj_weight, t2261, t2270, t2271, t2276, t2275, t2278, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t2283, t2285, t2288, t2287, t2292, t2291, t2294, t_transformer_h_15_mlp_proj_weight, t2300, t2309, t2310, t2315, t2314, t2317, t_lm_head_weight), (32000, False, False, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0))
============================================ END: before _transform_for_operator_executor_execution
============================================ START: after _transform_for_operator_executor_execution
# Constructed by Transform for operator executor execution (took 52 milliseconds)
import thunder
import thunder.core.dtypes as dtypes
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight):
  # idx: "cuda:0 i64[1, 512]"
  # tos1: "cuda:0 f32[4096, 128]"
  # t_lm_head_weight: "cuda:0 bf16[32000, 4096]"
  # t_sin: "cuda:0 f32[4096, 128]"
  # t_transformer_h_0_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_0_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_0_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_0_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_0_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_0_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_1_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_1_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_1_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_1_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_1_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_2_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_2_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_2_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_2_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_2_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_3_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_3_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_3_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_3_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_3_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_4_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_4_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_4_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_4_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_4_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_5_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_5_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_5_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_5_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_5_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_6_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_6_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_6_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_6_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_6_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_7_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_7_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_7_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_7_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_7_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_8_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_8_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_8_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_8_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_8_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_9_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_9_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_9_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_9_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_9_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_10_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_10_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_10_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_10_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_10_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_11_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_11_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_11_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_11_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_11_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_12_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_12_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_12_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_12_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_12_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_13_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_13_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_13_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_13_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_13_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_14_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_14_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_14_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_14_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_14_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_attn_attn_weight: "cuda:0 bf16[12288, 4096]"
  # t_transformer_h_15_attn_proj_weight: "cuda:0 bf16[4096, 4096]"
  # t_transformer_h_15_mlp_fc_1_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_fc_2_weight: "cuda:0 bf16[11008, 4096]"
  # t_transformer_h_15_mlp_proj_weight: "cuda:0 bf16[4096, 11008]"
  # t_transformer_h_15_norm_1_weight: "cuda:0 bf16[4096]"
  # t_transformer_h_15_norm_2_weight: "cuda:0 bf16[4096]"
  # t_transformer_ln_f_weight: "cuda:0 bf16[4096]"
  # t_transformer_wte_weight: "cuda:0 bf16[32000, 4096]"
  t0 = prims.slice_prim(tos1, [0, 0], [512, 128], [1, 1])  # t0: "cuda:0 f32[512, 128]"
  t1 = prims.slice_prim(t_sin, [0, 0], [512, 128], [1, 1])  # t1: "cuda:0 f32[512, 128]"
  t4 = torch.nn.functional.embedding(idx, t_transformer_wte_weight, None, None, 2.0, False, False)  # t4: "cuda:0 bf16[1, 512, 4096]"
    # t4 = ltorch.embedding(idx, t_transformer_wte_weight, None, None, 2.0, False, False)  # t4: "cuda:0 bf16[1, 512, 4096]"
      # _ = ltorch.numel(idx)
      # t2319 = ltorch.reshape(idx, [512])  # t2319: "cuda:0 i64[512]"
        # t2319 = prims.reshape(idx, (512,))  # t2319: "cuda:0 i64[512]"
      # t2320 = prims.take(t_transformer_wte_weight, t2319, 0)  # t2320: "cuda:0 bf16[512, 4096]"
      # t4 = ltorch.reshape(t2320, [1, 512, 4096])  # t4: "cuda:0 bf16[1, 512, 4096]"
        # t4 = prims.reshape(t2320, (1, 512, 4096))  # t4: "cuda:0 bf16[1, 512, 4096]"
  t5 = prims.convert_element_type(t4, dtypes.float32)  # t5: "cuda:0 f32[1, 512, 4096]"
  t6 = ltorch.mul(t5, t5)  # t6: "cuda:0 f32[1, 512, 4096]"
    # t6 = prims.mul(t5, t5)  # t6: "cuda:0 f32[1, 512, 4096]"
  t7 = prims.sum(t6, (2,))  # t7: "cuda:0 f32[1, 512]"
  t8 = prims.broadcast_in_dim(t7, [1, 512, 1], [0, 1])  # t8: "cuda:0 f32[1, 512, 1]"
  t9 = ltorch.true_divide(t8, 4096.0)  # t9: "cuda:0 f32[1, 512, 1]"
    # t9 = prims.div(t8, 4096.0)  # t9: "cuda:0 f32[1, 512, 1]"
  t10 = ltorch.add(t9, 1e-05, alpha=None)  # t10: "cuda:0 f32[1, 512, 1]"
    # t10 = prims.add(t9, 1e-05)  # t10: "cuda:0 f32[1, 512, 1]"
  t11 = prims.rsqrt(t10)  # t11: "cuda:0 f32[1, 512, 1]"
  t12 = prims.broadcast_in_dim(t11, (1, 512, 4096), (0, 1, 2))  # t12: "cuda:0 f32[1, 512, 4096]"
  t13 = ltorch.mul(t5, t12)  # t13: "cuda:0 f32[1, 512, 4096]"
    # t13 = prims.mul(t5, t12)  # t13: "cuda:0 f32[1, 512, 4096]"
  t14 = prims.convert_element_type(t13, dtypes.bfloat16)  # t14: "cuda:0 bf16[1, 512, 4096]"
  t15 = prims.broadcast_in_dim(t_transformer_h_0_norm_1_weight, (1, 512, 4096), (2,))  # t15: "cuda:0 bf16[1, 512, 4096]"
  t16 = prims.convert_element_type(t14, dtypes.float32)  # t16: "cuda:0 f32[1, 512, 4096]"
  t17 = prims.convert_element_type(t15, dtypes.float32)  # t17: "cuda:0 f32[1, 512, 4096]"
  t18 = ltorch.mul(t16, t17)  # t18: "cuda:0 f32[1, 512, 4096]"
    # t18 = prims.mul(t16, t17)  # t18: "cuda:0 f32[1, 512, 4096]"
  t19 = prims.convert_element_type(t18, dtypes.bfloat16)  # t19: "cuda:0 bf16[1, 512, 4096]"
  t20 = torch.nn.functional.linear(t19, t_transformer_h_0_attn_attn_weight, None)  # t20: "cuda:0 bf16[1, 512, 12288]"
    # t20 = ltorch.linear(t19, t_transformer_h_0_attn_attn_weight, None)  # t20: "cuda:0 bf16[1, 512, 12288]"
      # t20 = prims.linear(t19, t_transformer_h_0_attn_attn_weight, None)  # t20: "cuda:0 bf16[1, 512, 12288]"
  t21 = prims.reshape(t20, (1, 512, 32, 3, 128))  # t21: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t22 = prims.transpose(t21, (0, 2, 3, 1, 4))  # t22: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t23, t24, t25) = ltorch.split(t22, (1, 1, 1), 2)
    # t23 = prims.slice_prim(t22, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t23: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t24 = prims.slice_prim(t22, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t24: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t25 = prims.slice_prim(t22, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t25: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t26 = prims.reshape(t23, (1, 32, 512, 128))  # t26: "cuda:0 bf16[1, 32, 512, 128]"
  t32 = prims.reshape(t24, (1, 32, 512, 128))  # t32: "cuda:0 bf16[1, 32, 512, 128]"
  t38 = prims.reshape(t25, (1, 32, 512, 128))  # t38: "cuda:0 bf16[1, 32, 512, 128]"
  t39 = prims.slice_prim(t26, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t39: "cuda:0 bf16[1, 32, 512, 128]"
  t40 = prims.slice_prim(t39, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t40: "cuda:0 bf16[1, 32, 512, 64]"
  t41 = prims.slice_prim(t39, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t41: "cuda:0 bf16[1, 32, 512, 64]"
  t42 = prims.convert_element_type(t41, dtypes.float32)  # t42: "cuda:0 f32[1, 32, 512, 64]"
  t43 = prims.neg(t42)  # t43: "cuda:0 f32[1, 32, 512, 64]"
  t44 = prims.convert_element_type(t43, dtypes.bfloat16)  # t44: "cuda:0 bf16[1, 32, 512, 64]"
  t45 = prims.cat((t44, t40), -1)  # t45: "cuda:0 bf16[1, 32, 512, 128]"
  t46 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t46: "cuda:0 f32[1, 32, 512, 128]"
  t47 = prims.convert_element_type(t39, dtypes.float32)  # t47: "cuda:0 f32[1, 32, 512, 128]"
  t48 = ltorch.mul(t47, t46)  # t48: "cuda:0 f32[1, 32, 512, 128]"
    # t48 = prims.mul(t47, t46)  # t48: "cuda:0 f32[1, 32, 512, 128]"
  t49 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t49: "cuda:0 f32[1, 32, 512, 128]"
  t50 = prims.convert_element_type(t45, dtypes.float32)  # t50: "cuda:0 f32[1, 32, 512, 128]"
  t51 = ltorch.mul(t50, t49)  # t51: "cuda:0 f32[1, 32, 512, 128]"
    # t51 = prims.mul(t50, t49)  # t51: "cuda:0 f32[1, 32, 512, 128]"
  t52 = ltorch.add(t48, t51, alpha=None)  # t52: "cuda:0 f32[1, 32, 512, 128]"
    # t52 = prims.add(t48, t51)  # t52: "cuda:0 f32[1, 32, 512, 128]"
  t53 = prims.convert_element_type(t52, dtypes.bfloat16)  # t53: "cuda:0 bf16[1, 32, 512, 128]"
  t54 = prims.slice_prim(t32, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t54: "cuda:0 bf16[1, 32, 512, 128]"
  t55 = prims.slice_prim(t54, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t55: "cuda:0 bf16[1, 32, 512, 64]"
  t56 = prims.slice_prim(t54, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t56: "cuda:0 bf16[1, 32, 512, 64]"
  t57 = prims.convert_element_type(t56, dtypes.float32)  # t57: "cuda:0 f32[1, 32, 512, 64]"
  t58 = prims.neg(t57)  # t58: "cuda:0 f32[1, 32, 512, 64]"
  t59 = prims.convert_element_type(t58, dtypes.bfloat16)  # t59: "cuda:0 bf16[1, 32, 512, 64]"
  t61 = prims.cat((t59, t55), -1)  # t61: "cuda:0 bf16[1, 32, 512, 128]"
  t62 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t62: "cuda:0 f32[1, 32, 512, 128]"
  t63 = prims.convert_element_type(t54, dtypes.float32)  # t63: "cuda:0 f32[1, 32, 512, 128]"
  t64 = ltorch.mul(t63, t62)  # t64: "cuda:0 f32[1, 32, 512, 128]"
    # t64 = prims.mul(t63, t62)  # t64: "cuda:0 f32[1, 32, 512, 128]"
  t65 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t65: "cuda:0 f32[1, 32, 512, 128]"
  t66 = prims.convert_element_type(t61, dtypes.float32)  # t66: "cuda:0 f32[1, 32, 512, 128]"
  t67 = ltorch.mul(t66, t65)  # t67: "cuda:0 f32[1, 32, 512, 128]"
    # t67 = prims.mul(t66, t65)  # t67: "cuda:0 f32[1, 32, 512, 128]"
  t68 = ltorch.add(t64, t67, alpha=None)  # t68: "cuda:0 f32[1, 32, 512, 128]"
    # t68 = prims.add(t64, t67)  # t68: "cuda:0 f32[1, 32, 512, 128]"
  t69 = prims.convert_element_type(t68, dtypes.bfloat16)  # t69: "cuda:0 bf16[1, 32, 512, 128]"
  t70 = prims.slice_prim(t26, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t70: "cuda:0 bf16[1, 32, 512, 0]"
  t71 = prims.cat((t53, t70), -1)  # t71: "cuda:0 bf16[1, 32, 512, 128]"
  t72 = prims.slice_prim(t32, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t72: "cuda:0 bf16[1, 32, 512, 0]"
  t74 = prims.cat((t69, t72), -1)  # t74: "cuda:0 bf16[1, 32, 512, 128]"
  (t75, t76, t77, t78) = cudnn_sdpa_fwd(t71, t74, t38, None, 0.0, True, scale=0.08838834764831843)
  t79 = prims.transpose(t75, (0, 2, 1, 3))  # t79: "cuda:0 bf16[1, 512, 32, 128]"
  t80 = prims.reshape(t79, (1, 512, 4096))  # t80: "cuda:0 bf16[1, 512, 4096]"
  t81 = torch.nn.functional.linear(t80, t_transformer_h_0_attn_proj_weight, None)  # t81: "cuda:0 bf16[1, 512, 4096]"
    # t81 = ltorch.linear(t80, t_transformer_h_0_attn_proj_weight, None)  # t81: "cuda:0 bf16[1, 512, 4096]"
      # t81 = prims.linear(t80, t_transformer_h_0_attn_proj_weight, None)  # t81: "cuda:0 bf16[1, 512, 4096]"
  t82 = prims.convert_element_type(t81, dtypes.float32)  # t82: "cuda:0 f32[1, 512, 4096]"
  t83 = prims.convert_element_type(t4, dtypes.float32)  # t83: "cuda:0 f32[1, 512, 4096]"
  t84 = ltorch.add(t82, t83, alpha=None)  # t84: "cuda:0 f32[1, 512, 4096]"
    # t84 = prims.add(t82, t83)  # t84: "cuda:0 f32[1, 512, 4096]"
  t85 = prims.convert_element_type(t84, dtypes.bfloat16)  # t85: "cuda:0 bf16[1, 512, 4096]"
  t86 = prims.convert_element_type(t85, dtypes.float32)  # t86: "cuda:0 f32[1, 512, 4096]"
  t87 = ltorch.mul(t86, t86)  # t87: "cuda:0 f32[1, 512, 4096]"
    # t87 = prims.mul(t86, t86)  # t87: "cuda:0 f32[1, 512, 4096]"
  t89 = prims.sum(t87, (2,))  # t89: "cuda:0 f32[1, 512]"
  t90 = prims.broadcast_in_dim(t89, [1, 512, 1], [0, 1])  # t90: "cuda:0 f32[1, 512, 1]"
  t92 = ltorch.true_divide(t90, 4096.0)  # t92: "cuda:0 f32[1, 512, 1]"
    # t92 = prims.div(t90, 4096.0)  # t92: "cuda:0 f32[1, 512, 1]"
  t94 = ltorch.add(t92, 1e-05, alpha=None)  # t94: "cuda:0 f32[1, 512, 1]"
    # t94 = prims.add(t92, 1e-05)  # t94: "cuda:0 f32[1, 512, 1]"
  t95 = prims.rsqrt(t94)  # t95: "cuda:0 f32[1, 512, 1]"
  t96 = prims.broadcast_in_dim(t95, (1, 512, 4096), (0, 1, 2))  # t96: "cuda:0 f32[1, 512, 4096]"
  t97 = ltorch.mul(t86, t96)  # t97: "cuda:0 f32[1, 512, 4096]"
    # t97 = prims.mul(t86, t96)  # t97: "cuda:0 f32[1, 512, 4096]"
  t98 = prims.convert_element_type(t97, dtypes.bfloat16)  # t98: "cuda:0 bf16[1, 512, 4096]"
  t99 = prims.broadcast_in_dim(t_transformer_h_0_norm_2_weight, (1, 512, 4096), (2,))  # t99: "cuda:0 bf16[1, 512, 4096]"
  t100 = prims.convert_element_type(t98, dtypes.float32)  # t100: "cuda:0 f32[1, 512, 4096]"
  t101 = prims.convert_element_type(t99, dtypes.float32)  # t101: "cuda:0 f32[1, 512, 4096]"
  t102 = ltorch.mul(t100, t101)  # t102: "cuda:0 f32[1, 512, 4096]"
    # t102 = prims.mul(t100, t101)  # t102: "cuda:0 f32[1, 512, 4096]"
  t103 = prims.convert_element_type(t102, dtypes.bfloat16)  # t103: "cuda:0 bf16[1, 512, 4096]"
  t104 = torch.nn.functional.linear(t103, t_transformer_h_0_mlp_fc_1_weight, None)  # t104: "cuda:0 bf16[1, 512, 11008]"
    # t104 = ltorch.linear(t103, t_transformer_h_0_mlp_fc_1_weight, None)  # t104: "cuda:0 bf16[1, 512, 11008]"
      # t104 = prims.linear(t103, t_transformer_h_0_mlp_fc_1_weight, None)  # t104: "cuda:0 bf16[1, 512, 11008]"
  t105 = torch.nn.functional.linear(t103, t_transformer_h_0_mlp_fc_2_weight, None)  # t105: "cuda:0 bf16[1, 512, 11008]"
    # t105 = ltorch.linear(t103, t_transformer_h_0_mlp_fc_2_weight, None)  # t105: "cuda:0 bf16[1, 512, 11008]"
      # t105 = prims.linear(t103, t_transformer_h_0_mlp_fc_2_weight, None)  # t105: "cuda:0 bf16[1, 512, 11008]"
  t106 = prims.convert_element_type(t104, dtypes.float32)  # t106: "cuda:0 f32[1, 512, 11008]"
  t107 = prims.neg(t106)  # t107: "cuda:0 f32[1, 512, 11008]"
  t108 = prims.exp(t107)  # t108: "cuda:0 f32[1, 512, 11008]"
  t109 = ltorch.add(1.0, t108, alpha=None)  # t109: "cuda:0 f32[1, 512, 11008]"
    # t109 = prims.add(1.0, t108)  # t109: "cuda:0 f32[1, 512, 11008]"
  t110 = prims.reciprocal(t109)  # t110: "cuda:0 f32[1, 512, 11008]"
  t111 = prims.convert_element_type(t110, dtypes.bfloat16)  # t111: "cuda:0 bf16[1, 512, 11008]"
  t112 = prims.convert_element_type(t104, dtypes.float32)  # t112: "cuda:0 f32[1, 512, 11008]"
  t113 = prims.convert_element_type(t111, dtypes.float32)  # t113: "cuda:0 f32[1, 512, 11008]"
  t114 = ltorch.mul(t112, t113)  # t114: "cuda:0 f32[1, 512, 11008]"
    # t114 = prims.mul(t112, t113)  # t114: "cuda:0 f32[1, 512, 11008]"
  t115 = prims.convert_element_type(t114, dtypes.bfloat16)  # t115: "cuda:0 bf16[1, 512, 11008]"
  t116 = prims.convert_element_type(t115, dtypes.float32)  # t116: "cuda:0 f32[1, 512, 11008]"
  t117 = prims.convert_element_type(t105, dtypes.float32)  # t117: "cuda:0 f32[1, 512, 11008]"
  t118 = ltorch.mul(t116, t117)  # t118: "cuda:0 f32[1, 512, 11008]"
    # t118 = prims.mul(t116, t117)  # t118: "cuda:0 f32[1, 512, 11008]"
  t119 = prims.convert_element_type(t118, dtypes.bfloat16)  # t119: "cuda:0 bf16[1, 512, 11008]"
  t120 = torch.nn.functional.linear(t119, t_transformer_h_0_mlp_proj_weight, None)  # t120: "cuda:0 bf16[1, 512, 4096]"
    # t120 = ltorch.linear(t119, t_transformer_h_0_mlp_proj_weight, None)  # t120: "cuda:0 bf16[1, 512, 4096]"
      # t120 = prims.linear(t119, t_transformer_h_0_mlp_proj_weight, None)  # t120: "cuda:0 bf16[1, 512, 4096]"
  t121 = prims.convert_element_type(t120, dtypes.float32)  # t121: "cuda:0 f32[1, 512, 4096]"
  t122 = prims.convert_element_type(t85, dtypes.float32)  # t122: "cuda:0 f32[1, 512, 4096]"
  t123 = ltorch.add(t121, t122, alpha=None)  # t123: "cuda:0 f32[1, 512, 4096]"
    # t123 = prims.add(t121, t122)  # t123: "cuda:0 f32[1, 512, 4096]"
  t124 = prims.convert_element_type(t123, dtypes.bfloat16)  # t124: "cuda:0 bf16[1, 512, 4096]"
  t125 = prims.convert_element_type(t124, dtypes.float32)  # t125: "cuda:0 f32[1, 512, 4096]"
  t126 = ltorch.mul(t125, t125)  # t126: "cuda:0 f32[1, 512, 4096]"
    # t126 = prims.mul(t125, t125)  # t126: "cuda:0 f32[1, 512, 4096]"
  t128 = prims.sum(t126, (2,))  # t128: "cuda:0 f32[1, 512]"
  t129 = prims.broadcast_in_dim(t128, [1, 512, 1], [0, 1])  # t129: "cuda:0 f32[1, 512, 1]"
  t131 = ltorch.true_divide(t129, 4096.0)  # t131: "cuda:0 f32[1, 512, 1]"
    # t131 = prims.div(t129, 4096.0)  # t131: "cuda:0 f32[1, 512, 1]"
  t133 = ltorch.add(t131, 1e-05, alpha=None)  # t133: "cuda:0 f32[1, 512, 1]"
    # t133 = prims.add(t131, 1e-05)  # t133: "cuda:0 f32[1, 512, 1]"
  t134 = prims.rsqrt(t133)  # t134: "cuda:0 f32[1, 512, 1]"
  t135 = prims.broadcast_in_dim(t134, (1, 512, 4096), (0, 1, 2))  # t135: "cuda:0 f32[1, 512, 4096]"
  t136 = ltorch.mul(t125, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
    # t136 = prims.mul(t125, t135)  # t136: "cuda:0 f32[1, 512, 4096]"
  t137 = prims.convert_element_type(t136, dtypes.bfloat16)  # t137: "cuda:0 bf16[1, 512, 4096]"
  t138 = prims.broadcast_in_dim(t_transformer_h_1_norm_1_weight, (1, 512, 4096), (2,))  # t138: "cuda:0 bf16[1, 512, 4096]"
  t139 = prims.convert_element_type(t137, dtypes.float32)  # t139: "cuda:0 f32[1, 512, 4096]"
  t140 = prims.convert_element_type(t138, dtypes.float32)  # t140: "cuda:0 f32[1, 512, 4096]"
  t141 = ltorch.mul(t139, t140)  # t141: "cuda:0 f32[1, 512, 4096]"
    # t141 = prims.mul(t139, t140)  # t141: "cuda:0 f32[1, 512, 4096]"
  t142 = prims.convert_element_type(t141, dtypes.bfloat16)  # t142: "cuda:0 bf16[1, 512, 4096]"
  t143 = torch.nn.functional.linear(t142, t_transformer_h_1_attn_attn_weight, None)  # t143: "cuda:0 bf16[1, 512, 12288]"
    # t143 = ltorch.linear(t142, t_transformer_h_1_attn_attn_weight, None)  # t143: "cuda:0 bf16[1, 512, 12288]"
      # t143 = prims.linear(t142, t_transformer_h_1_attn_attn_weight, None)  # t143: "cuda:0 bf16[1, 512, 12288]"
  t149 = prims.reshape(t143, (1, 512, 32, 3, 128))  # t149: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t155 = prims.transpose(t149, (0, 2, 3, 1, 4))  # t155: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t156, t157, t158) = ltorch.split(t155, (1, 1, 1), 2)
    # t156 = prims.slice_prim(t155, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t156: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t157 = prims.slice_prim(t155, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t157: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t158 = prims.slice_prim(t155, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t158: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t164 = prims.reshape(t156, (1, 32, 512, 128))  # t164: "cuda:0 bf16[1, 32, 512, 128]"
  t170 = prims.reshape(t157, (1, 32, 512, 128))  # t170: "cuda:0 bf16[1, 32, 512, 128]"
  t176 = prims.reshape(t158, (1, 32, 512, 128))  # t176: "cuda:0 bf16[1, 32, 512, 128]"
  t177 = prims.slice_prim(t164, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t177: "cuda:0 bf16[1, 32, 512, 128]"
  t178 = prims.slice_prim(t177, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t178: "cuda:0 bf16[1, 32, 512, 64]"
  t179 = prims.slice_prim(t177, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t179: "cuda:0 bf16[1, 32, 512, 64]"
  t180 = prims.convert_element_type(t179, dtypes.float32)  # t180: "cuda:0 f32[1, 32, 512, 64]"
  t181 = prims.neg(t180)  # t181: "cuda:0 f32[1, 32, 512, 64]"
  t182 = prims.convert_element_type(t181, dtypes.bfloat16)  # t182: "cuda:0 bf16[1, 32, 512, 64]"
  t184 = prims.cat((t182, t178), -1)  # t184: "cuda:0 bf16[1, 32, 512, 128]"
  t185 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t185: "cuda:0 f32[1, 32, 512, 128]"
  t186 = prims.convert_element_type(t177, dtypes.float32)  # t186: "cuda:0 f32[1, 32, 512, 128]"
  t187 = ltorch.mul(t186, t185)  # t187: "cuda:0 f32[1, 32, 512, 128]"
    # t187 = prims.mul(t186, t185)  # t187: "cuda:0 f32[1, 32, 512, 128]"
  t188 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t188: "cuda:0 f32[1, 32, 512, 128]"
  t189 = prims.convert_element_type(t184, dtypes.float32)  # t189: "cuda:0 f32[1, 32, 512, 128]"
  t190 = ltorch.mul(t189, t188)  # t190: "cuda:0 f32[1, 32, 512, 128]"
    # t190 = prims.mul(t189, t188)  # t190: "cuda:0 f32[1, 32, 512, 128]"
  t191 = ltorch.add(t187, t190, alpha=None)  # t191: "cuda:0 f32[1, 32, 512, 128]"
    # t191 = prims.add(t187, t190)  # t191: "cuda:0 f32[1, 32, 512, 128]"
  t192 = prims.convert_element_type(t191, dtypes.bfloat16)  # t192: "cuda:0 bf16[1, 32, 512, 128]"
  t193 = prims.slice_prim(t170, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t193: "cuda:0 bf16[1, 32, 512, 128]"
  t194 = prims.slice_prim(t193, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t194: "cuda:0 bf16[1, 32, 512, 64]"
  t195 = prims.slice_prim(t193, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t195: "cuda:0 bf16[1, 32, 512, 64]"
  t196 = prims.convert_element_type(t195, dtypes.float32)  # t196: "cuda:0 f32[1, 32, 512, 64]"
  t197 = prims.neg(t196)  # t197: "cuda:0 f32[1, 32, 512, 64]"
  t198 = prims.convert_element_type(t197, dtypes.bfloat16)  # t198: "cuda:0 bf16[1, 32, 512, 64]"
  t200 = prims.cat((t198, t194), -1)  # t200: "cuda:0 bf16[1, 32, 512, 128]"
  t201 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t201: "cuda:0 f32[1, 32, 512, 128]"
  t202 = prims.convert_element_type(t193, dtypes.float32)  # t202: "cuda:0 f32[1, 32, 512, 128]"
  t203 = ltorch.mul(t202, t201)  # t203: "cuda:0 f32[1, 32, 512, 128]"
    # t203 = prims.mul(t202, t201)  # t203: "cuda:0 f32[1, 32, 512, 128]"
  t204 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t204: "cuda:0 f32[1, 32, 512, 128]"
  t205 = prims.convert_element_type(t200, dtypes.float32)  # t205: "cuda:0 f32[1, 32, 512, 128]"
  t206 = ltorch.mul(t205, t204)  # t206: "cuda:0 f32[1, 32, 512, 128]"
    # t206 = prims.mul(t205, t204)  # t206: "cuda:0 f32[1, 32, 512, 128]"
  t207 = ltorch.add(t203, t206, alpha=None)  # t207: "cuda:0 f32[1, 32, 512, 128]"
    # t207 = prims.add(t203, t206)  # t207: "cuda:0 f32[1, 32, 512, 128]"
  t208 = prims.convert_element_type(t207, dtypes.bfloat16)  # t208: "cuda:0 bf16[1, 32, 512, 128]"
  t209 = prims.slice_prim(t164, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t209: "cuda:0 bf16[1, 32, 512, 0]"
  t211 = prims.cat((t192, t209), -1)  # t211: "cuda:0 bf16[1, 32, 512, 128]"
  t212 = prims.slice_prim(t170, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t212: "cuda:0 bf16[1, 32, 512, 0]"
  t214 = prims.cat((t208, t212), -1)  # t214: "cuda:0 bf16[1, 32, 512, 128]"
  (t215, t216, t217, t218) = cudnn_sdpa_fwd(t211, t214, t176, None, 0.0, True, scale=0.08838834764831843)
  t221 = prims.transpose(t215, (0, 2, 1, 3))  # t221: "cuda:0 bf16[1, 512, 32, 128]"
  t225 = prims.reshape(t221, (1, 512, 4096))  # t225: "cuda:0 bf16[1, 512, 4096]"
  t226 = torch.nn.functional.linear(t225, t_transformer_h_1_attn_proj_weight, None)  # t226: "cuda:0 bf16[1, 512, 4096]"
    # t226 = ltorch.linear(t225, t_transformer_h_1_attn_proj_weight, None)  # t226: "cuda:0 bf16[1, 512, 4096]"
      # t226 = prims.linear(t225, t_transformer_h_1_attn_proj_weight, None)  # t226: "cuda:0 bf16[1, 512, 4096]"
  t227 = prims.convert_element_type(t226, dtypes.float32)  # t227: "cuda:0 f32[1, 512, 4096]"
  t228 = prims.convert_element_type(t124, dtypes.float32)  # t228: "cuda:0 f32[1, 512, 4096]"
  t229 = ltorch.add(t227, t228, alpha=None)  # t229: "cuda:0 f32[1, 512, 4096]"
    # t229 = prims.add(t227, t228)  # t229: "cuda:0 f32[1, 512, 4096]"
  t230 = prims.convert_element_type(t229, dtypes.bfloat16)  # t230: "cuda:0 bf16[1, 512, 4096]"
  t231 = prims.convert_element_type(t230, dtypes.float32)  # t231: "cuda:0 f32[1, 512, 4096]"
  t232 = ltorch.mul(t231, t231)  # t232: "cuda:0 f32[1, 512, 4096]"
    # t232 = prims.mul(t231, t231)  # t232: "cuda:0 f32[1, 512, 4096]"
  t234 = prims.sum(t232, (2,))  # t234: "cuda:0 f32[1, 512]"
  t235 = prims.broadcast_in_dim(t234, [1, 512, 1], [0, 1])  # t235: "cuda:0 f32[1, 512, 1]"
  t237 = ltorch.true_divide(t235, 4096.0)  # t237: "cuda:0 f32[1, 512, 1]"
    # t237 = prims.div(t235, 4096.0)  # t237: "cuda:0 f32[1, 512, 1]"
  t239 = ltorch.add(t237, 1e-05, alpha=None)  # t239: "cuda:0 f32[1, 512, 1]"
    # t239 = prims.add(t237, 1e-05)  # t239: "cuda:0 f32[1, 512, 1]"
  t240 = prims.rsqrt(t239)  # t240: "cuda:0 f32[1, 512, 1]"
  t241 = prims.broadcast_in_dim(t240, (1, 512, 4096), (0, 1, 2))  # t241: "cuda:0 f32[1, 512, 4096]"
  t242 = ltorch.mul(t231, t241)  # t242: "cuda:0 f32[1, 512, 4096]"
    # t242 = prims.mul(t231, t241)  # t242: "cuda:0 f32[1, 512, 4096]"
  t243 = prims.convert_element_type(t242, dtypes.bfloat16)  # t243: "cuda:0 bf16[1, 512, 4096]"
  t244 = prims.broadcast_in_dim(t_transformer_h_1_norm_2_weight, (1, 512, 4096), (2,))  # t244: "cuda:0 bf16[1, 512, 4096]"
  t245 = prims.convert_element_type(t243, dtypes.float32)  # t245: "cuda:0 f32[1, 512, 4096]"
  t246 = prims.convert_element_type(t244, dtypes.float32)  # t246: "cuda:0 f32[1, 512, 4096]"
  t247 = ltorch.mul(t245, t246)  # t247: "cuda:0 f32[1, 512, 4096]"
    # t247 = prims.mul(t245, t246)  # t247: "cuda:0 f32[1, 512, 4096]"
  t248 = prims.convert_element_type(t247, dtypes.bfloat16)  # t248: "cuda:0 bf16[1, 512, 4096]"
  t249 = torch.nn.functional.linear(t248, t_transformer_h_1_mlp_fc_1_weight, None)  # t249: "cuda:0 bf16[1, 512, 11008]"
    # t249 = ltorch.linear(t248, t_transformer_h_1_mlp_fc_1_weight, None)  # t249: "cuda:0 bf16[1, 512, 11008]"
      # t249 = prims.linear(t248, t_transformer_h_1_mlp_fc_1_weight, None)  # t249: "cuda:0 bf16[1, 512, 11008]"
  t250 = torch.nn.functional.linear(t248, t_transformer_h_1_mlp_fc_2_weight, None)  # t250: "cuda:0 bf16[1, 512, 11008]"
    # t250 = ltorch.linear(t248, t_transformer_h_1_mlp_fc_2_weight, None)  # t250: "cuda:0 bf16[1, 512, 11008]"
      # t250 = prims.linear(t248, t_transformer_h_1_mlp_fc_2_weight, None)  # t250: "cuda:0 bf16[1, 512, 11008]"
  t251 = prims.convert_element_type(t249, dtypes.float32)  # t251: "cuda:0 f32[1, 512, 11008]"
  t252 = prims.neg(t251)  # t252: "cuda:0 f32[1, 512, 11008]"
  t253 = prims.exp(t252)  # t253: "cuda:0 f32[1, 512, 11008]"
  t254 = ltorch.add(1.0, t253, alpha=None)  # t254: "cuda:0 f32[1, 512, 11008]"
    # t254 = prims.add(1.0, t253)  # t254: "cuda:0 f32[1, 512, 11008]"
  t255 = prims.reciprocal(t254)  # t255: "cuda:0 f32[1, 512, 11008]"
  t256 = prims.convert_element_type(t255, dtypes.bfloat16)  # t256: "cuda:0 bf16[1, 512, 11008]"
  t257 = prims.convert_element_type(t249, dtypes.float32)  # t257: "cuda:0 f32[1, 512, 11008]"
  t258 = prims.convert_element_type(t256, dtypes.float32)  # t258: "cuda:0 f32[1, 512, 11008]"
  t259 = ltorch.mul(t257, t258)  # t259: "cuda:0 f32[1, 512, 11008]"
    # t259 = prims.mul(t257, t258)  # t259: "cuda:0 f32[1, 512, 11008]"
  t260 = prims.convert_element_type(t259, dtypes.bfloat16)  # t260: "cuda:0 bf16[1, 512, 11008]"
  t261 = prims.convert_element_type(t260, dtypes.float32)  # t261: "cuda:0 f32[1, 512, 11008]"
  t262 = prims.convert_element_type(t250, dtypes.float32)  # t262: "cuda:0 f32[1, 512, 11008]"
  t263 = ltorch.mul(t261, t262)  # t263: "cuda:0 f32[1, 512, 11008]"
    # t263 = prims.mul(t261, t262)  # t263: "cuda:0 f32[1, 512, 11008]"
  t264 = prims.convert_element_type(t263, dtypes.bfloat16)  # t264: "cuda:0 bf16[1, 512, 11008]"
  t265 = torch.nn.functional.linear(t264, t_transformer_h_1_mlp_proj_weight, None)  # t265: "cuda:0 bf16[1, 512, 4096]"
    # t265 = ltorch.linear(t264, t_transformer_h_1_mlp_proj_weight, None)  # t265: "cuda:0 bf16[1, 512, 4096]"
      # t265 = prims.linear(t264, t_transformer_h_1_mlp_proj_weight, None)  # t265: "cuda:0 bf16[1, 512, 4096]"
  t266 = prims.convert_element_type(t265, dtypes.float32)  # t266: "cuda:0 f32[1, 512, 4096]"
  t267 = prims.convert_element_type(t230, dtypes.float32)  # t267: "cuda:0 f32[1, 512, 4096]"
  t268 = ltorch.add(t266, t267, alpha=None)  # t268: "cuda:0 f32[1, 512, 4096]"
    # t268 = prims.add(t266, t267)  # t268: "cuda:0 f32[1, 512, 4096]"
  t269 = prims.convert_element_type(t268, dtypes.bfloat16)  # t269: "cuda:0 bf16[1, 512, 4096]"
  t270 = prims.convert_element_type(t269, dtypes.float32)  # t270: "cuda:0 f32[1, 512, 4096]"
  t271 = ltorch.mul(t270, t270)  # t271: "cuda:0 f32[1, 512, 4096]"
    # t271 = prims.mul(t270, t270)  # t271: "cuda:0 f32[1, 512, 4096]"
  t273 = prims.sum(t271, (2,))  # t273: "cuda:0 f32[1, 512]"
  t274 = prims.broadcast_in_dim(t273, [1, 512, 1], [0, 1])  # t274: "cuda:0 f32[1, 512, 1]"
  t276 = ltorch.true_divide(t274, 4096.0)  # t276: "cuda:0 f32[1, 512, 1]"
    # t276 = prims.div(t274, 4096.0)  # t276: "cuda:0 f32[1, 512, 1]"
  t278 = ltorch.add(t276, 1e-05, alpha=None)  # t278: "cuda:0 f32[1, 512, 1]"
    # t278 = prims.add(t276, 1e-05)  # t278: "cuda:0 f32[1, 512, 1]"
  t279 = prims.rsqrt(t278)  # t279: "cuda:0 f32[1, 512, 1]"
  t280 = prims.broadcast_in_dim(t279, (1, 512, 4096), (0, 1, 2))  # t280: "cuda:0 f32[1, 512, 4096]"
  t281 = ltorch.mul(t270, t280)  # t281: "cuda:0 f32[1, 512, 4096]"
    # t281 = prims.mul(t270, t280)  # t281: "cuda:0 f32[1, 512, 4096]"
  t282 = prims.convert_element_type(t281, dtypes.bfloat16)  # t282: "cuda:0 bf16[1, 512, 4096]"
  t283 = prims.broadcast_in_dim(t_transformer_h_2_norm_1_weight, (1, 512, 4096), (2,))  # t283: "cuda:0 bf16[1, 512, 4096]"
  t284 = prims.convert_element_type(t282, dtypes.float32)  # t284: "cuda:0 f32[1, 512, 4096]"
  t285 = prims.convert_element_type(t283, dtypes.float32)  # t285: "cuda:0 f32[1, 512, 4096]"
  t286 = ltorch.mul(t284, t285)  # t286: "cuda:0 f32[1, 512, 4096]"
    # t286 = prims.mul(t284, t285)  # t286: "cuda:0 f32[1, 512, 4096]"
  t287 = prims.convert_element_type(t286, dtypes.bfloat16)  # t287: "cuda:0 bf16[1, 512, 4096]"
  t288 = torch.nn.functional.linear(t287, t_transformer_h_2_attn_attn_weight, None)  # t288: "cuda:0 bf16[1, 512, 12288]"
    # t288 = ltorch.linear(t287, t_transformer_h_2_attn_attn_weight, None)  # t288: "cuda:0 bf16[1, 512, 12288]"
      # t288 = prims.linear(t287, t_transformer_h_2_attn_attn_weight, None)  # t288: "cuda:0 bf16[1, 512, 12288]"
  t294 = prims.reshape(t288, (1, 512, 32, 3, 128))  # t294: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t300 = prims.transpose(t294, (0, 2, 3, 1, 4))  # t300: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t301, t302, t303) = ltorch.split(t300, (1, 1, 1), 2)
    # t301 = prims.slice_prim(t300, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t301: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t302 = prims.slice_prim(t300, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t302: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t303 = prims.slice_prim(t300, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t303: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t309 = prims.reshape(t301, (1, 32, 512, 128))  # t309: "cuda:0 bf16[1, 32, 512, 128]"
  t315 = prims.reshape(t302, (1, 32, 512, 128))  # t315: "cuda:0 bf16[1, 32, 512, 128]"
  t321 = prims.reshape(t303, (1, 32, 512, 128))  # t321: "cuda:0 bf16[1, 32, 512, 128]"
  t322 = prims.slice_prim(t309, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t322: "cuda:0 bf16[1, 32, 512, 128]"
  t323 = prims.slice_prim(t322, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t323: "cuda:0 bf16[1, 32, 512, 64]"
  t324 = prims.slice_prim(t322, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t324: "cuda:0 bf16[1, 32, 512, 64]"
  t325 = prims.convert_element_type(t324, dtypes.float32)  # t325: "cuda:0 f32[1, 32, 512, 64]"
  t326 = prims.neg(t325)  # t326: "cuda:0 f32[1, 32, 512, 64]"
  t327 = prims.convert_element_type(t326, dtypes.bfloat16)  # t327: "cuda:0 bf16[1, 32, 512, 64]"
  t329 = prims.cat((t327, t323), -1)  # t329: "cuda:0 bf16[1, 32, 512, 128]"
  t330 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t330: "cuda:0 f32[1, 32, 512, 128]"
  t331 = prims.convert_element_type(t322, dtypes.float32)  # t331: "cuda:0 f32[1, 32, 512, 128]"
  t332 = ltorch.mul(t331, t330)  # t332: "cuda:0 f32[1, 32, 512, 128]"
    # t332 = prims.mul(t331, t330)  # t332: "cuda:0 f32[1, 32, 512, 128]"
  t333 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t333: "cuda:0 f32[1, 32, 512, 128]"
  t334 = prims.convert_element_type(t329, dtypes.float32)  # t334: "cuda:0 f32[1, 32, 512, 128]"
  t335 = ltorch.mul(t334, t333)  # t335: "cuda:0 f32[1, 32, 512, 128]"
    # t335 = prims.mul(t334, t333)  # t335: "cuda:0 f32[1, 32, 512, 128]"
  t336 = ltorch.add(t332, t335, alpha=None)  # t336: "cuda:0 f32[1, 32, 512, 128]"
    # t336 = prims.add(t332, t335)  # t336: "cuda:0 f32[1, 32, 512, 128]"
  t337 = prims.convert_element_type(t336, dtypes.bfloat16)  # t337: "cuda:0 bf16[1, 32, 512, 128]"
  t338 = prims.slice_prim(t315, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t338: "cuda:0 bf16[1, 32, 512, 128]"
  t339 = prims.slice_prim(t338, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t339: "cuda:0 bf16[1, 32, 512, 64]"
  t340 = prims.slice_prim(t338, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t340: "cuda:0 bf16[1, 32, 512, 64]"
  t341 = prims.convert_element_type(t340, dtypes.float32)  # t341: "cuda:0 f32[1, 32, 512, 64]"
  t342 = prims.neg(t341)  # t342: "cuda:0 f32[1, 32, 512, 64]"
  t343 = prims.convert_element_type(t342, dtypes.bfloat16)  # t343: "cuda:0 bf16[1, 32, 512, 64]"
  t345 = prims.cat((t343, t339), -1)  # t345: "cuda:0 bf16[1, 32, 512, 128]"
  t346 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t346: "cuda:0 f32[1, 32, 512, 128]"
  t347 = prims.convert_element_type(t338, dtypes.float32)  # t347: "cuda:0 f32[1, 32, 512, 128]"
  t348 = ltorch.mul(t347, t346)  # t348: "cuda:0 f32[1, 32, 512, 128]"
    # t348 = prims.mul(t347, t346)  # t348: "cuda:0 f32[1, 32, 512, 128]"
  t349 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t349: "cuda:0 f32[1, 32, 512, 128]"
  t350 = prims.convert_element_type(t345, dtypes.float32)  # t350: "cuda:0 f32[1, 32, 512, 128]"
  t351 = ltorch.mul(t350, t349)  # t351: "cuda:0 f32[1, 32, 512, 128]"
    # t351 = prims.mul(t350, t349)  # t351: "cuda:0 f32[1, 32, 512, 128]"
  t352 = ltorch.add(t348, t351, alpha=None)  # t352: "cuda:0 f32[1, 32, 512, 128]"
    # t352 = prims.add(t348, t351)  # t352: "cuda:0 f32[1, 32, 512, 128]"
  t353 = prims.convert_element_type(t352, dtypes.bfloat16)  # t353: "cuda:0 bf16[1, 32, 512, 128]"
  t354 = prims.slice_prim(t309, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t354: "cuda:0 bf16[1, 32, 512, 0]"
  t356 = prims.cat((t337, t354), -1)  # t356: "cuda:0 bf16[1, 32, 512, 128]"
  t357 = prims.slice_prim(t315, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t357: "cuda:0 bf16[1, 32, 512, 0]"
  t359 = prims.cat((t353, t357), -1)  # t359: "cuda:0 bf16[1, 32, 512, 128]"
  (t360, t361, t362, t363) = cudnn_sdpa_fwd(t356, t359, t321, None, 0.0, True, scale=0.08838834764831843)
  t366 = prims.transpose(t360, (0, 2, 1, 3))  # t366: "cuda:0 bf16[1, 512, 32, 128]"
  t370 = prims.reshape(t366, (1, 512, 4096))  # t370: "cuda:0 bf16[1, 512, 4096]"
  t371 = torch.nn.functional.linear(t370, t_transformer_h_2_attn_proj_weight, None)  # t371: "cuda:0 bf16[1, 512, 4096]"
    # t371 = ltorch.linear(t370, t_transformer_h_2_attn_proj_weight, None)  # t371: "cuda:0 bf16[1, 512, 4096]"
      # t371 = prims.linear(t370, t_transformer_h_2_attn_proj_weight, None)  # t371: "cuda:0 bf16[1, 512, 4096]"
  t372 = prims.convert_element_type(t371, dtypes.float32)  # t372: "cuda:0 f32[1, 512, 4096]"
  t373 = prims.convert_element_type(t269, dtypes.float32)  # t373: "cuda:0 f32[1, 512, 4096]"
  t374 = ltorch.add(t372, t373, alpha=None)  # t374: "cuda:0 f32[1, 512, 4096]"
    # t374 = prims.add(t372, t373)  # t374: "cuda:0 f32[1, 512, 4096]"
  t375 = prims.convert_element_type(t374, dtypes.bfloat16)  # t375: "cuda:0 bf16[1, 512, 4096]"
  t376 = prims.convert_element_type(t375, dtypes.float32)  # t376: "cuda:0 f32[1, 512, 4096]"
  t377 = ltorch.mul(t376, t376)  # t377: "cuda:0 f32[1, 512, 4096]"
    # t377 = prims.mul(t376, t376)  # t377: "cuda:0 f32[1, 512, 4096]"
  t379 = prims.sum(t377, (2,))  # t379: "cuda:0 f32[1, 512]"
  t380 = prims.broadcast_in_dim(t379, [1, 512, 1], [0, 1])  # t380: "cuda:0 f32[1, 512, 1]"
  t382 = ltorch.true_divide(t380, 4096.0)  # t382: "cuda:0 f32[1, 512, 1]"
    # t382 = prims.div(t380, 4096.0)  # t382: "cuda:0 f32[1, 512, 1]"
  t384 = ltorch.add(t382, 1e-05, alpha=None)  # t384: "cuda:0 f32[1, 512, 1]"
    # t384 = prims.add(t382, 1e-05)  # t384: "cuda:0 f32[1, 512, 1]"
  t385 = prims.rsqrt(t384)  # t385: "cuda:0 f32[1, 512, 1]"
  t386 = prims.broadcast_in_dim(t385, (1, 512, 4096), (0, 1, 2))  # t386: "cuda:0 f32[1, 512, 4096]"
  t387 = ltorch.mul(t376, t386)  # t387: "cuda:0 f32[1, 512, 4096]"
    # t387 = prims.mul(t376, t386)  # t387: "cuda:0 f32[1, 512, 4096]"
  t388 = prims.convert_element_type(t387, dtypes.bfloat16)  # t388: "cuda:0 bf16[1, 512, 4096]"
  t389 = prims.broadcast_in_dim(t_transformer_h_2_norm_2_weight, (1, 512, 4096), (2,))  # t389: "cuda:0 bf16[1, 512, 4096]"
  t390 = prims.convert_element_type(t388, dtypes.float32)  # t390: "cuda:0 f32[1, 512, 4096]"
  t391 = prims.convert_element_type(t389, dtypes.float32)  # t391: "cuda:0 f32[1, 512, 4096]"
  t392 = ltorch.mul(t390, t391)  # t392: "cuda:0 f32[1, 512, 4096]"
    # t392 = prims.mul(t390, t391)  # t392: "cuda:0 f32[1, 512, 4096]"
  t393 = prims.convert_element_type(t392, dtypes.bfloat16)  # t393: "cuda:0 bf16[1, 512, 4096]"
  t394 = torch.nn.functional.linear(t393, t_transformer_h_2_mlp_fc_1_weight, None)  # t394: "cuda:0 bf16[1, 512, 11008]"
    # t394 = ltorch.linear(t393, t_transformer_h_2_mlp_fc_1_weight, None)  # t394: "cuda:0 bf16[1, 512, 11008]"
      # t394 = prims.linear(t393, t_transformer_h_2_mlp_fc_1_weight, None)  # t394: "cuda:0 bf16[1, 512, 11008]"
  t395 = torch.nn.functional.linear(t393, t_transformer_h_2_mlp_fc_2_weight, None)  # t395: "cuda:0 bf16[1, 512, 11008]"
    # t395 = ltorch.linear(t393, t_transformer_h_2_mlp_fc_2_weight, None)  # t395: "cuda:0 bf16[1, 512, 11008]"
      # t395 = prims.linear(t393, t_transformer_h_2_mlp_fc_2_weight, None)  # t395: "cuda:0 bf16[1, 512, 11008]"
  t396 = prims.convert_element_type(t394, dtypes.float32)  # t396: "cuda:0 f32[1, 512, 11008]"
  t397 = prims.neg(t396)  # t397: "cuda:0 f32[1, 512, 11008]"
  t398 = prims.exp(t397)  # t398: "cuda:0 f32[1, 512, 11008]"
  t399 = ltorch.add(1.0, t398, alpha=None)  # t399: "cuda:0 f32[1, 512, 11008]"
    # t399 = prims.add(1.0, t398)  # t399: "cuda:0 f32[1, 512, 11008]"
  t400 = prims.reciprocal(t399)  # t400: "cuda:0 f32[1, 512, 11008]"
  t401 = prims.convert_element_type(t400, dtypes.bfloat16)  # t401: "cuda:0 bf16[1, 512, 11008]"
  t402 = prims.convert_element_type(t394, dtypes.float32)  # t402: "cuda:0 f32[1, 512, 11008]"
  t403 = prims.convert_element_type(t401, dtypes.float32)  # t403: "cuda:0 f32[1, 512, 11008]"
  t404 = ltorch.mul(t402, t403)  # t404: "cuda:0 f32[1, 512, 11008]"
    # t404 = prims.mul(t402, t403)  # t404: "cuda:0 f32[1, 512, 11008]"
  t405 = prims.convert_element_type(t404, dtypes.bfloat16)  # t405: "cuda:0 bf16[1, 512, 11008]"
  t406 = prims.convert_element_type(t405, dtypes.float32)  # t406: "cuda:0 f32[1, 512, 11008]"
  t407 = prims.convert_element_type(t395, dtypes.float32)  # t407: "cuda:0 f32[1, 512, 11008]"
  t408 = ltorch.mul(t406, t407)  # t408: "cuda:0 f32[1, 512, 11008]"
    # t408 = prims.mul(t406, t407)  # t408: "cuda:0 f32[1, 512, 11008]"
  t409 = prims.convert_element_type(t408, dtypes.bfloat16)  # t409: "cuda:0 bf16[1, 512, 11008]"
  t410 = torch.nn.functional.linear(t409, t_transformer_h_2_mlp_proj_weight, None)  # t410: "cuda:0 bf16[1, 512, 4096]"
    # t410 = ltorch.linear(t409, t_transformer_h_2_mlp_proj_weight, None)  # t410: "cuda:0 bf16[1, 512, 4096]"
      # t410 = prims.linear(t409, t_transformer_h_2_mlp_proj_weight, None)  # t410: "cuda:0 bf16[1, 512, 4096]"
  t411 = prims.convert_element_type(t410, dtypes.float32)  # t411: "cuda:0 f32[1, 512, 4096]"
  t412 = prims.convert_element_type(t375, dtypes.float32)  # t412: "cuda:0 f32[1, 512, 4096]"
  t413 = ltorch.add(t411, t412, alpha=None)  # t413: "cuda:0 f32[1, 512, 4096]"
    # t413 = prims.add(t411, t412)  # t413: "cuda:0 f32[1, 512, 4096]"
  t414 = prims.convert_element_type(t413, dtypes.bfloat16)  # t414: "cuda:0 bf16[1, 512, 4096]"
  t415 = prims.convert_element_type(t414, dtypes.float32)  # t415: "cuda:0 f32[1, 512, 4096]"
  t416 = ltorch.mul(t415, t415)  # t416: "cuda:0 f32[1, 512, 4096]"
    # t416 = prims.mul(t415, t415)  # t416: "cuda:0 f32[1, 512, 4096]"
  t418 = prims.sum(t416, (2,))  # t418: "cuda:0 f32[1, 512]"
  t419 = prims.broadcast_in_dim(t418, [1, 512, 1], [0, 1])  # t419: "cuda:0 f32[1, 512, 1]"
  t421 = ltorch.true_divide(t419, 4096.0)  # t421: "cuda:0 f32[1, 512, 1]"
    # t421 = prims.div(t419, 4096.0)  # t421: "cuda:0 f32[1, 512, 1]"
  t423 = ltorch.add(t421, 1e-05, alpha=None)  # t423: "cuda:0 f32[1, 512, 1]"
    # t423 = prims.add(t421, 1e-05)  # t423: "cuda:0 f32[1, 512, 1]"
  t424 = prims.rsqrt(t423)  # t424: "cuda:0 f32[1, 512, 1]"
  t425 = prims.broadcast_in_dim(t424, (1, 512, 4096), (0, 1, 2))  # t425: "cuda:0 f32[1, 512, 4096]"
  t426 = ltorch.mul(t415, t425)  # t426: "cuda:0 f32[1, 512, 4096]"
    # t426 = prims.mul(t415, t425)  # t426: "cuda:0 f32[1, 512, 4096]"
  t427 = prims.convert_element_type(t426, dtypes.bfloat16)  # t427: "cuda:0 bf16[1, 512, 4096]"
  t428 = prims.broadcast_in_dim(t_transformer_h_3_norm_1_weight, (1, 512, 4096), (2,))  # t428: "cuda:0 bf16[1, 512, 4096]"
  t429 = prims.convert_element_type(t427, dtypes.float32)  # t429: "cuda:0 f32[1, 512, 4096]"
  t430 = prims.convert_element_type(t428, dtypes.float32)  # t430: "cuda:0 f32[1, 512, 4096]"
  t431 = ltorch.mul(t429, t430)  # t431: "cuda:0 f32[1, 512, 4096]"
    # t431 = prims.mul(t429, t430)  # t431: "cuda:0 f32[1, 512, 4096]"
  t432 = prims.convert_element_type(t431, dtypes.bfloat16)  # t432: "cuda:0 bf16[1, 512, 4096]"
  t433 = torch.nn.functional.linear(t432, t_transformer_h_3_attn_attn_weight, None)  # t433: "cuda:0 bf16[1, 512, 12288]"
    # t433 = ltorch.linear(t432, t_transformer_h_3_attn_attn_weight, None)  # t433: "cuda:0 bf16[1, 512, 12288]"
      # t433 = prims.linear(t432, t_transformer_h_3_attn_attn_weight, None)  # t433: "cuda:0 bf16[1, 512, 12288]"
  t439 = prims.reshape(t433, (1, 512, 32, 3, 128))  # t439: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t445 = prims.transpose(t439, (0, 2, 3, 1, 4))  # t445: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t446, t447, t448) = ltorch.split(t445, (1, 1, 1), 2)
    # t446 = prims.slice_prim(t445, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t446: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t447 = prims.slice_prim(t445, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t447: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t448 = prims.slice_prim(t445, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t448: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t454 = prims.reshape(t446, (1, 32, 512, 128))  # t454: "cuda:0 bf16[1, 32, 512, 128]"
  t460 = prims.reshape(t447, (1, 32, 512, 128))  # t460: "cuda:0 bf16[1, 32, 512, 128]"
  t466 = prims.reshape(t448, (1, 32, 512, 128))  # t466: "cuda:0 bf16[1, 32, 512, 128]"
  t467 = prims.slice_prim(t454, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t467: "cuda:0 bf16[1, 32, 512, 128]"
  t468 = prims.slice_prim(t467, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t468: "cuda:0 bf16[1, 32, 512, 64]"
  t469 = prims.slice_prim(t467, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t469: "cuda:0 bf16[1, 32, 512, 64]"
  t470 = prims.convert_element_type(t469, dtypes.float32)  # t470: "cuda:0 f32[1, 32, 512, 64]"
  t471 = prims.neg(t470)  # t471: "cuda:0 f32[1, 32, 512, 64]"
  t472 = prims.convert_element_type(t471, dtypes.bfloat16)  # t472: "cuda:0 bf16[1, 32, 512, 64]"
  t474 = prims.cat((t472, t468), -1)  # t474: "cuda:0 bf16[1, 32, 512, 128]"
  t475 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t475: "cuda:0 f32[1, 32, 512, 128]"
  t476 = prims.convert_element_type(t467, dtypes.float32)  # t476: "cuda:0 f32[1, 32, 512, 128]"
  t477 = ltorch.mul(t476, t475)  # t477: "cuda:0 f32[1, 32, 512, 128]"
    # t477 = prims.mul(t476, t475)  # t477: "cuda:0 f32[1, 32, 512, 128]"
  t478 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t478: "cuda:0 f32[1, 32, 512, 128]"
  t479 = prims.convert_element_type(t474, dtypes.float32)  # t479: "cuda:0 f32[1, 32, 512, 128]"
  t480 = ltorch.mul(t479, t478)  # t480: "cuda:0 f32[1, 32, 512, 128]"
    # t480 = prims.mul(t479, t478)  # t480: "cuda:0 f32[1, 32, 512, 128]"
  t481 = ltorch.add(t477, t480, alpha=None)  # t481: "cuda:0 f32[1, 32, 512, 128]"
    # t481 = prims.add(t477, t480)  # t481: "cuda:0 f32[1, 32, 512, 128]"
  t482 = prims.convert_element_type(t481, dtypes.bfloat16)  # t482: "cuda:0 bf16[1, 32, 512, 128]"
  t483 = prims.slice_prim(t460, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t483: "cuda:0 bf16[1, 32, 512, 128]"
  t484 = prims.slice_prim(t483, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t484: "cuda:0 bf16[1, 32, 512, 64]"
  t485 = prims.slice_prim(t483, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t485: "cuda:0 bf16[1, 32, 512, 64]"
  t486 = prims.convert_element_type(t485, dtypes.float32)  # t486: "cuda:0 f32[1, 32, 512, 64]"
  t487 = prims.neg(t486)  # t487: "cuda:0 f32[1, 32, 512, 64]"
  t488 = prims.convert_element_type(t487, dtypes.bfloat16)  # t488: "cuda:0 bf16[1, 32, 512, 64]"
  t490 = prims.cat((t488, t484), -1)  # t490: "cuda:0 bf16[1, 32, 512, 128]"
  t491 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t491: "cuda:0 f32[1, 32, 512, 128]"
  t492 = prims.convert_element_type(t483, dtypes.float32)  # t492: "cuda:0 f32[1, 32, 512, 128]"
  t493 = ltorch.mul(t492, t491)  # t493: "cuda:0 f32[1, 32, 512, 128]"
    # t493 = prims.mul(t492, t491)  # t493: "cuda:0 f32[1, 32, 512, 128]"
  t494 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t494: "cuda:0 f32[1, 32, 512, 128]"
  t495 = prims.convert_element_type(t490, dtypes.float32)  # t495: "cuda:0 f32[1, 32, 512, 128]"
  t496 = ltorch.mul(t495, t494)  # t496: "cuda:0 f32[1, 32, 512, 128]"
    # t496 = prims.mul(t495, t494)  # t496: "cuda:0 f32[1, 32, 512, 128]"
  t497 = ltorch.add(t493, t496, alpha=None)  # t497: "cuda:0 f32[1, 32, 512, 128]"
    # t497 = prims.add(t493, t496)  # t497: "cuda:0 f32[1, 32, 512, 128]"
  t498 = prims.convert_element_type(t497, dtypes.bfloat16)  # t498: "cuda:0 bf16[1, 32, 512, 128]"
  t499 = prims.slice_prim(t454, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t499: "cuda:0 bf16[1, 32, 512, 0]"
  t501 = prims.cat((t482, t499), -1)  # t501: "cuda:0 bf16[1, 32, 512, 128]"
  t502 = prims.slice_prim(t460, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t502: "cuda:0 bf16[1, 32, 512, 0]"
  t504 = prims.cat((t498, t502), -1)  # t504: "cuda:0 bf16[1, 32, 512, 128]"
  (t505, t506, t507, t508) = cudnn_sdpa_fwd(t501, t504, t466, None, 0.0, True, scale=0.08838834764831843)
  t511 = prims.transpose(t505, (0, 2, 1, 3))  # t511: "cuda:0 bf16[1, 512, 32, 128]"
  t515 = prims.reshape(t511, (1, 512, 4096))  # t515: "cuda:0 bf16[1, 512, 4096]"
  t516 = torch.nn.functional.linear(t515, t_transformer_h_3_attn_proj_weight, None)  # t516: "cuda:0 bf16[1, 512, 4096]"
    # t516 = ltorch.linear(t515, t_transformer_h_3_attn_proj_weight, None)  # t516: "cuda:0 bf16[1, 512, 4096]"
      # t516 = prims.linear(t515, t_transformer_h_3_attn_proj_weight, None)  # t516: "cuda:0 bf16[1, 512, 4096]"
  t517 = prims.convert_element_type(t516, dtypes.float32)  # t517: "cuda:0 f32[1, 512, 4096]"
  t518 = prims.convert_element_type(t414, dtypes.float32)  # t518: "cuda:0 f32[1, 512, 4096]"
  t519 = ltorch.add(t517, t518, alpha=None)  # t519: "cuda:0 f32[1, 512, 4096]"
    # t519 = prims.add(t517, t518)  # t519: "cuda:0 f32[1, 512, 4096]"
  t520 = prims.convert_element_type(t519, dtypes.bfloat16)  # t520: "cuda:0 bf16[1, 512, 4096]"
  t521 = prims.convert_element_type(t520, dtypes.float32)  # t521: "cuda:0 f32[1, 512, 4096]"
  t522 = ltorch.mul(t521, t521)  # t522: "cuda:0 f32[1, 512, 4096]"
    # t522 = prims.mul(t521, t521)  # t522: "cuda:0 f32[1, 512, 4096]"
  t524 = prims.sum(t522, (2,))  # t524: "cuda:0 f32[1, 512]"
  t525 = prims.broadcast_in_dim(t524, [1, 512, 1], [0, 1])  # t525: "cuda:0 f32[1, 512, 1]"
  t527 = ltorch.true_divide(t525, 4096.0)  # t527: "cuda:0 f32[1, 512, 1]"
    # t527 = prims.div(t525, 4096.0)  # t527: "cuda:0 f32[1, 512, 1]"
  t529 = ltorch.add(t527, 1e-05, alpha=None)  # t529: "cuda:0 f32[1, 512, 1]"
    # t529 = prims.add(t527, 1e-05)  # t529: "cuda:0 f32[1, 512, 1]"
  t530 = prims.rsqrt(t529)  # t530: "cuda:0 f32[1, 512, 1]"
  t531 = prims.broadcast_in_dim(t530, (1, 512, 4096), (0, 1, 2))  # t531: "cuda:0 f32[1, 512, 4096]"
  t532 = ltorch.mul(t521, t531)  # t532: "cuda:0 f32[1, 512, 4096]"
    # t532 = prims.mul(t521, t531)  # t532: "cuda:0 f32[1, 512, 4096]"
  t533 = prims.convert_element_type(t532, dtypes.bfloat16)  # t533: "cuda:0 bf16[1, 512, 4096]"
  t534 = prims.broadcast_in_dim(t_transformer_h_3_norm_2_weight, (1, 512, 4096), (2,))  # t534: "cuda:0 bf16[1, 512, 4096]"
  t535 = prims.convert_element_type(t533, dtypes.float32)  # t535: "cuda:0 f32[1, 512, 4096]"
  t536 = prims.convert_element_type(t534, dtypes.float32)  # t536: "cuda:0 f32[1, 512, 4096]"
  t537 = ltorch.mul(t535, t536)  # t537: "cuda:0 f32[1, 512, 4096]"
    # t537 = prims.mul(t535, t536)  # t537: "cuda:0 f32[1, 512, 4096]"
  t538 = prims.convert_element_type(t537, dtypes.bfloat16)  # t538: "cuda:0 bf16[1, 512, 4096]"
  t539 = torch.nn.functional.linear(t538, t_transformer_h_3_mlp_fc_1_weight, None)  # t539: "cuda:0 bf16[1, 512, 11008]"
    # t539 = ltorch.linear(t538, t_transformer_h_3_mlp_fc_1_weight, None)  # t539: "cuda:0 bf16[1, 512, 11008]"
      # t539 = prims.linear(t538, t_transformer_h_3_mlp_fc_1_weight, None)  # t539: "cuda:0 bf16[1, 512, 11008]"
  t540 = torch.nn.functional.linear(t538, t_transformer_h_3_mlp_fc_2_weight, None)  # t540: "cuda:0 bf16[1, 512, 11008]"
    # t540 = ltorch.linear(t538, t_transformer_h_3_mlp_fc_2_weight, None)  # t540: "cuda:0 bf16[1, 512, 11008]"
      # t540 = prims.linear(t538, t_transformer_h_3_mlp_fc_2_weight, None)  # t540: "cuda:0 bf16[1, 512, 11008]"
  t541 = prims.convert_element_type(t539, dtypes.float32)  # t541: "cuda:0 f32[1, 512, 11008]"
  t542 = prims.neg(t541)  # t542: "cuda:0 f32[1, 512, 11008]"
  t543 = prims.exp(t542)  # t543: "cuda:0 f32[1, 512, 11008]"
  t544 = ltorch.add(1.0, t543, alpha=None)  # t544: "cuda:0 f32[1, 512, 11008]"
    # t544 = prims.add(1.0, t543)  # t544: "cuda:0 f32[1, 512, 11008]"
  t545 = prims.reciprocal(t544)  # t545: "cuda:0 f32[1, 512, 11008]"
  t546 = prims.convert_element_type(t545, dtypes.bfloat16)  # t546: "cuda:0 bf16[1, 512, 11008]"
  t547 = prims.convert_element_type(t539, dtypes.float32)  # t547: "cuda:0 f32[1, 512, 11008]"
  t548 = prims.convert_element_type(t546, dtypes.float32)  # t548: "cuda:0 f32[1, 512, 11008]"
  t549 = ltorch.mul(t547, t548)  # t549: "cuda:0 f32[1, 512, 11008]"
    # t549 = prims.mul(t547, t548)  # t549: "cuda:0 f32[1, 512, 11008]"
  t550 = prims.convert_element_type(t549, dtypes.bfloat16)  # t550: "cuda:0 bf16[1, 512, 11008]"
  t551 = prims.convert_element_type(t550, dtypes.float32)  # t551: "cuda:0 f32[1, 512, 11008]"
  t552 = prims.convert_element_type(t540, dtypes.float32)  # t552: "cuda:0 f32[1, 512, 11008]"
  t553 = ltorch.mul(t551, t552)  # t553: "cuda:0 f32[1, 512, 11008]"
    # t553 = prims.mul(t551, t552)  # t553: "cuda:0 f32[1, 512, 11008]"
  t554 = prims.convert_element_type(t553, dtypes.bfloat16)  # t554: "cuda:0 bf16[1, 512, 11008]"
  t555 = torch.nn.functional.linear(t554, t_transformer_h_3_mlp_proj_weight, None)  # t555: "cuda:0 bf16[1, 512, 4096]"
    # t555 = ltorch.linear(t554, t_transformer_h_3_mlp_proj_weight, None)  # t555: "cuda:0 bf16[1, 512, 4096]"
      # t555 = prims.linear(t554, t_transformer_h_3_mlp_proj_weight, None)  # t555: "cuda:0 bf16[1, 512, 4096]"
  t556 = prims.convert_element_type(t555, dtypes.float32)  # t556: "cuda:0 f32[1, 512, 4096]"
  t557 = prims.convert_element_type(t520, dtypes.float32)  # t557: "cuda:0 f32[1, 512, 4096]"
  t558 = ltorch.add(t556, t557, alpha=None)  # t558: "cuda:0 f32[1, 512, 4096]"
    # t558 = prims.add(t556, t557)  # t558: "cuda:0 f32[1, 512, 4096]"
  t559 = prims.convert_element_type(t558, dtypes.bfloat16)  # t559: "cuda:0 bf16[1, 512, 4096]"
  t560 = prims.convert_element_type(t559, dtypes.float32)  # t560: "cuda:0 f32[1, 512, 4096]"
  t561 = ltorch.mul(t560, t560)  # t561: "cuda:0 f32[1, 512, 4096]"
    # t561 = prims.mul(t560, t560)  # t561: "cuda:0 f32[1, 512, 4096]"
  t563 = prims.sum(t561, (2,))  # t563: "cuda:0 f32[1, 512]"
  t564 = prims.broadcast_in_dim(t563, [1, 512, 1], [0, 1])  # t564: "cuda:0 f32[1, 512, 1]"
  t566 = ltorch.true_divide(t564, 4096.0)  # t566: "cuda:0 f32[1, 512, 1]"
    # t566 = prims.div(t564, 4096.0)  # t566: "cuda:0 f32[1, 512, 1]"
  t568 = ltorch.add(t566, 1e-05, alpha=None)  # t568: "cuda:0 f32[1, 512, 1]"
    # t568 = prims.add(t566, 1e-05)  # t568: "cuda:0 f32[1, 512, 1]"
  t569 = prims.rsqrt(t568)  # t569: "cuda:0 f32[1, 512, 1]"
  t570 = prims.broadcast_in_dim(t569, (1, 512, 4096), (0, 1, 2))  # t570: "cuda:0 f32[1, 512, 4096]"
  t571 = ltorch.mul(t560, t570)  # t571: "cuda:0 f32[1, 512, 4096]"
    # t571 = prims.mul(t560, t570)  # t571: "cuda:0 f32[1, 512, 4096]"
  t572 = prims.convert_element_type(t571, dtypes.bfloat16)  # t572: "cuda:0 bf16[1, 512, 4096]"
  t573 = prims.broadcast_in_dim(t_transformer_h_4_norm_1_weight, (1, 512, 4096), (2,))  # t573: "cuda:0 bf16[1, 512, 4096]"
  t574 = prims.convert_element_type(t572, dtypes.float32)  # t574: "cuda:0 f32[1, 512, 4096]"
  t575 = prims.convert_element_type(t573, dtypes.float32)  # t575: "cuda:0 f32[1, 512, 4096]"
  t576 = ltorch.mul(t574, t575)  # t576: "cuda:0 f32[1, 512, 4096]"
    # t576 = prims.mul(t574, t575)  # t576: "cuda:0 f32[1, 512, 4096]"
  t577 = prims.convert_element_type(t576, dtypes.bfloat16)  # t577: "cuda:0 bf16[1, 512, 4096]"
  t578 = torch.nn.functional.linear(t577, t_transformer_h_4_attn_attn_weight, None)  # t578: "cuda:0 bf16[1, 512, 12288]"
    # t578 = ltorch.linear(t577, t_transformer_h_4_attn_attn_weight, None)  # t578: "cuda:0 bf16[1, 512, 12288]"
      # t578 = prims.linear(t577, t_transformer_h_4_attn_attn_weight, None)  # t578: "cuda:0 bf16[1, 512, 12288]"
  t584 = prims.reshape(t578, (1, 512, 32, 3, 128))  # t584: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t590 = prims.transpose(t584, (0, 2, 3, 1, 4))  # t590: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t591, t592, t593) = ltorch.split(t590, (1, 1, 1), 2)
    # t591 = prims.slice_prim(t590, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t591: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t592 = prims.slice_prim(t590, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t592: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t593 = prims.slice_prim(t590, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t593: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t599 = prims.reshape(t591, (1, 32, 512, 128))  # t599: "cuda:0 bf16[1, 32, 512, 128]"
  t605 = prims.reshape(t592, (1, 32, 512, 128))  # t605: "cuda:0 bf16[1, 32, 512, 128]"
  t611 = prims.reshape(t593, (1, 32, 512, 128))  # t611: "cuda:0 bf16[1, 32, 512, 128]"
  t612 = prims.slice_prim(t599, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t612: "cuda:0 bf16[1, 32, 512, 128]"
  t613 = prims.slice_prim(t612, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t613: "cuda:0 bf16[1, 32, 512, 64]"
  t614 = prims.slice_prim(t612, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t614: "cuda:0 bf16[1, 32, 512, 64]"
  t615 = prims.convert_element_type(t614, dtypes.float32)  # t615: "cuda:0 f32[1, 32, 512, 64]"
  t616 = prims.neg(t615)  # t616: "cuda:0 f32[1, 32, 512, 64]"
  t617 = prims.convert_element_type(t616, dtypes.bfloat16)  # t617: "cuda:0 bf16[1, 32, 512, 64]"
  t619 = prims.cat((t617, t613), -1)  # t619: "cuda:0 bf16[1, 32, 512, 128]"
  t620 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t620: "cuda:0 f32[1, 32, 512, 128]"
  t621 = prims.convert_element_type(t612, dtypes.float32)  # t621: "cuda:0 f32[1, 32, 512, 128]"
  t622 = ltorch.mul(t621, t620)  # t622: "cuda:0 f32[1, 32, 512, 128]"
    # t622 = prims.mul(t621, t620)  # t622: "cuda:0 f32[1, 32, 512, 128]"
  t623 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t623: "cuda:0 f32[1, 32, 512, 128]"
  t624 = prims.convert_element_type(t619, dtypes.float32)  # t624: "cuda:0 f32[1, 32, 512, 128]"
  t625 = ltorch.mul(t624, t623)  # t625: "cuda:0 f32[1, 32, 512, 128]"
    # t625 = prims.mul(t624, t623)  # t625: "cuda:0 f32[1, 32, 512, 128]"
  t626 = ltorch.add(t622, t625, alpha=None)  # t626: "cuda:0 f32[1, 32, 512, 128]"
    # t626 = prims.add(t622, t625)  # t626: "cuda:0 f32[1, 32, 512, 128]"
  t627 = prims.convert_element_type(t626, dtypes.bfloat16)  # t627: "cuda:0 bf16[1, 32, 512, 128]"
  t628 = prims.slice_prim(t605, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t628: "cuda:0 bf16[1, 32, 512, 128]"
  t629 = prims.slice_prim(t628, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t629: "cuda:0 bf16[1, 32, 512, 64]"
  t630 = prims.slice_prim(t628, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t630: "cuda:0 bf16[1, 32, 512, 64]"
  t631 = prims.convert_element_type(t630, dtypes.float32)  # t631: "cuda:0 f32[1, 32, 512, 64]"
  t632 = prims.neg(t631)  # t632: "cuda:0 f32[1, 32, 512, 64]"
  t633 = prims.convert_element_type(t632, dtypes.bfloat16)  # t633: "cuda:0 bf16[1, 32, 512, 64]"
  t635 = prims.cat((t633, t629), -1)  # t635: "cuda:0 bf16[1, 32, 512, 128]"
  t636 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t636: "cuda:0 f32[1, 32, 512, 128]"
  t637 = prims.convert_element_type(t628, dtypes.float32)  # t637: "cuda:0 f32[1, 32, 512, 128]"
  t638 = ltorch.mul(t637, t636)  # t638: "cuda:0 f32[1, 32, 512, 128]"
    # t638 = prims.mul(t637, t636)  # t638: "cuda:0 f32[1, 32, 512, 128]"
  t639 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t639: "cuda:0 f32[1, 32, 512, 128]"
  t640 = prims.convert_element_type(t635, dtypes.float32)  # t640: "cuda:0 f32[1, 32, 512, 128]"
  t641 = ltorch.mul(t640, t639)  # t641: "cuda:0 f32[1, 32, 512, 128]"
    # t641 = prims.mul(t640, t639)  # t641: "cuda:0 f32[1, 32, 512, 128]"
  t642 = ltorch.add(t638, t641, alpha=None)  # t642: "cuda:0 f32[1, 32, 512, 128]"
    # t642 = prims.add(t638, t641)  # t642: "cuda:0 f32[1, 32, 512, 128]"
  t643 = prims.convert_element_type(t642, dtypes.bfloat16)  # t643: "cuda:0 bf16[1, 32, 512, 128]"
  t644 = prims.slice_prim(t599, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t644: "cuda:0 bf16[1, 32, 512, 0]"
  t646 = prims.cat((t627, t644), -1)  # t646: "cuda:0 bf16[1, 32, 512, 128]"
  t647 = prims.slice_prim(t605, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t647: "cuda:0 bf16[1, 32, 512, 0]"
  t649 = prims.cat((t643, t647), -1)  # t649: "cuda:0 bf16[1, 32, 512, 128]"
  (t650, t651, t652, t653) = cudnn_sdpa_fwd(t646, t649, t611, None, 0.0, True, scale=0.08838834764831843)
  t656 = prims.transpose(t650, (0, 2, 1, 3))  # t656: "cuda:0 bf16[1, 512, 32, 128]"
  t660 = prims.reshape(t656, (1, 512, 4096))  # t660: "cuda:0 bf16[1, 512, 4096]"
  t661 = torch.nn.functional.linear(t660, t_transformer_h_4_attn_proj_weight, None)  # t661: "cuda:0 bf16[1, 512, 4096]"
    # t661 = ltorch.linear(t660, t_transformer_h_4_attn_proj_weight, None)  # t661: "cuda:0 bf16[1, 512, 4096]"
      # t661 = prims.linear(t660, t_transformer_h_4_attn_proj_weight, None)  # t661: "cuda:0 bf16[1, 512, 4096]"
  t662 = prims.convert_element_type(t661, dtypes.float32)  # t662: "cuda:0 f32[1, 512, 4096]"
  t663 = prims.convert_element_type(t559, dtypes.float32)  # t663: "cuda:0 f32[1, 512, 4096]"
  t664 = ltorch.add(t662, t663, alpha=None)  # t664: "cuda:0 f32[1, 512, 4096]"
    # t664 = prims.add(t662, t663)  # t664: "cuda:0 f32[1, 512, 4096]"
  t665 = prims.convert_element_type(t664, dtypes.bfloat16)  # t665: "cuda:0 bf16[1, 512, 4096]"
  t666 = prims.convert_element_type(t665, dtypes.float32)  # t666: "cuda:0 f32[1, 512, 4096]"
  t667 = ltorch.mul(t666, t666)  # t667: "cuda:0 f32[1, 512, 4096]"
    # t667 = prims.mul(t666, t666)  # t667: "cuda:0 f32[1, 512, 4096]"
  t669 = prims.sum(t667, (2,))  # t669: "cuda:0 f32[1, 512]"
  t670 = prims.broadcast_in_dim(t669, [1, 512, 1], [0, 1])  # t670: "cuda:0 f32[1, 512, 1]"
  t672 = ltorch.true_divide(t670, 4096.0)  # t672: "cuda:0 f32[1, 512, 1]"
    # t672 = prims.div(t670, 4096.0)  # t672: "cuda:0 f32[1, 512, 1]"
  t674 = ltorch.add(t672, 1e-05, alpha=None)  # t674: "cuda:0 f32[1, 512, 1]"
    # t674 = prims.add(t672, 1e-05)  # t674: "cuda:0 f32[1, 512, 1]"
  t675 = prims.rsqrt(t674)  # t675: "cuda:0 f32[1, 512, 1]"
  t676 = prims.broadcast_in_dim(t675, (1, 512, 4096), (0, 1, 2))  # t676: "cuda:0 f32[1, 512, 4096]"
  t677 = ltorch.mul(t666, t676)  # t677: "cuda:0 f32[1, 512, 4096]"
    # t677 = prims.mul(t666, t676)  # t677: "cuda:0 f32[1, 512, 4096]"
  t678 = prims.convert_element_type(t677, dtypes.bfloat16)  # t678: "cuda:0 bf16[1, 512, 4096]"
  t679 = prims.broadcast_in_dim(t_transformer_h_4_norm_2_weight, (1, 512, 4096), (2,))  # t679: "cuda:0 bf16[1, 512, 4096]"
  t680 = prims.convert_element_type(t678, dtypes.float32)  # t680: "cuda:0 f32[1, 512, 4096]"
  t681 = prims.convert_element_type(t679, dtypes.float32)  # t681: "cuda:0 f32[1, 512, 4096]"
  t682 = ltorch.mul(t680, t681)  # t682: "cuda:0 f32[1, 512, 4096]"
    # t682 = prims.mul(t680, t681)  # t682: "cuda:0 f32[1, 512, 4096]"
  t683 = prims.convert_element_type(t682, dtypes.bfloat16)  # t683: "cuda:0 bf16[1, 512, 4096]"
  t684 = torch.nn.functional.linear(t683, t_transformer_h_4_mlp_fc_1_weight, None)  # t684: "cuda:0 bf16[1, 512, 11008]"
    # t684 = ltorch.linear(t683, t_transformer_h_4_mlp_fc_1_weight, None)  # t684: "cuda:0 bf16[1, 512, 11008]"
      # t684 = prims.linear(t683, t_transformer_h_4_mlp_fc_1_weight, None)  # t684: "cuda:0 bf16[1, 512, 11008]"
  t685 = torch.nn.functional.linear(t683, t_transformer_h_4_mlp_fc_2_weight, None)  # t685: "cuda:0 bf16[1, 512, 11008]"
    # t685 = ltorch.linear(t683, t_transformer_h_4_mlp_fc_2_weight, None)  # t685: "cuda:0 bf16[1, 512, 11008]"
      # t685 = prims.linear(t683, t_transformer_h_4_mlp_fc_2_weight, None)  # t685: "cuda:0 bf16[1, 512, 11008]"
  t686 = prims.convert_element_type(t684, dtypes.float32)  # t686: "cuda:0 f32[1, 512, 11008]"
  t687 = prims.neg(t686)  # t687: "cuda:0 f32[1, 512, 11008]"
  t688 = prims.exp(t687)  # t688: "cuda:0 f32[1, 512, 11008]"
  t689 = ltorch.add(1.0, t688, alpha=None)  # t689: "cuda:0 f32[1, 512, 11008]"
    # t689 = prims.add(1.0, t688)  # t689: "cuda:0 f32[1, 512, 11008]"
  t690 = prims.reciprocal(t689)  # t690: "cuda:0 f32[1, 512, 11008]"
  t691 = prims.convert_element_type(t690, dtypes.bfloat16)  # t691: "cuda:0 bf16[1, 512, 11008]"
  t692 = prims.convert_element_type(t684, dtypes.float32)  # t692: "cuda:0 f32[1, 512, 11008]"
  t693 = prims.convert_element_type(t691, dtypes.float32)  # t693: "cuda:0 f32[1, 512, 11008]"
  t694 = ltorch.mul(t692, t693)  # t694: "cuda:0 f32[1, 512, 11008]"
    # t694 = prims.mul(t692, t693)  # t694: "cuda:0 f32[1, 512, 11008]"
  t695 = prims.convert_element_type(t694, dtypes.bfloat16)  # t695: "cuda:0 bf16[1, 512, 11008]"
  t696 = prims.convert_element_type(t695, dtypes.float32)  # t696: "cuda:0 f32[1, 512, 11008]"
  t697 = prims.convert_element_type(t685, dtypes.float32)  # t697: "cuda:0 f32[1, 512, 11008]"
  t698 = ltorch.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 11008]"
    # t698 = prims.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 11008]"
  t699 = prims.convert_element_type(t698, dtypes.bfloat16)  # t699: "cuda:0 bf16[1, 512, 11008]"
  t700 = torch.nn.functional.linear(t699, t_transformer_h_4_mlp_proj_weight, None)  # t700: "cuda:0 bf16[1, 512, 4096]"
    # t700 = ltorch.linear(t699, t_transformer_h_4_mlp_proj_weight, None)  # t700: "cuda:0 bf16[1, 512, 4096]"
      # t700 = prims.linear(t699, t_transformer_h_4_mlp_proj_weight, None)  # t700: "cuda:0 bf16[1, 512, 4096]"
  t701 = prims.convert_element_type(t700, dtypes.float32)  # t701: "cuda:0 f32[1, 512, 4096]"
  t702 = prims.convert_element_type(t665, dtypes.float32)  # t702: "cuda:0 f32[1, 512, 4096]"
  t703 = ltorch.add(t701, t702, alpha=None)  # t703: "cuda:0 f32[1, 512, 4096]"
    # t703 = prims.add(t701, t702)  # t703: "cuda:0 f32[1, 512, 4096]"
  t704 = prims.convert_element_type(t703, dtypes.bfloat16)  # t704: "cuda:0 bf16[1, 512, 4096]"
  t705 = prims.convert_element_type(t704, dtypes.float32)  # t705: "cuda:0 f32[1, 512, 4096]"
  t706 = ltorch.mul(t705, t705)  # t706: "cuda:0 f32[1, 512, 4096]"
    # t706 = prims.mul(t705, t705)  # t706: "cuda:0 f32[1, 512, 4096]"
  t708 = prims.sum(t706, (2,))  # t708: "cuda:0 f32[1, 512]"
  t709 = prims.broadcast_in_dim(t708, [1, 512, 1], [0, 1])  # t709: "cuda:0 f32[1, 512, 1]"
  t711 = ltorch.true_divide(t709, 4096.0)  # t711: "cuda:0 f32[1, 512, 1]"
    # t711 = prims.div(t709, 4096.0)  # t711: "cuda:0 f32[1, 512, 1]"
  t713 = ltorch.add(t711, 1e-05, alpha=None)  # t713: "cuda:0 f32[1, 512, 1]"
    # t713 = prims.add(t711, 1e-05)  # t713: "cuda:0 f32[1, 512, 1]"
  t714 = prims.rsqrt(t713)  # t714: "cuda:0 f32[1, 512, 1]"
  t715 = prims.broadcast_in_dim(t714, (1, 512, 4096), (0, 1, 2))  # t715: "cuda:0 f32[1, 512, 4096]"
  t716 = ltorch.mul(t705, t715)  # t716: "cuda:0 f32[1, 512, 4096]"
    # t716 = prims.mul(t705, t715)  # t716: "cuda:0 f32[1, 512, 4096]"
  t717 = prims.convert_element_type(t716, dtypes.bfloat16)  # t717: "cuda:0 bf16[1, 512, 4096]"
  t718 = prims.broadcast_in_dim(t_transformer_h_5_norm_1_weight, (1, 512, 4096), (2,))  # t718: "cuda:0 bf16[1, 512, 4096]"
  t719 = prims.convert_element_type(t717, dtypes.float32)  # t719: "cuda:0 f32[1, 512, 4096]"
  t720 = prims.convert_element_type(t718, dtypes.float32)  # t720: "cuda:0 f32[1, 512, 4096]"
  t721 = ltorch.mul(t719, t720)  # t721: "cuda:0 f32[1, 512, 4096]"
    # t721 = prims.mul(t719, t720)  # t721: "cuda:0 f32[1, 512, 4096]"
  t722 = prims.convert_element_type(t721, dtypes.bfloat16)  # t722: "cuda:0 bf16[1, 512, 4096]"
  t723 = torch.nn.functional.linear(t722, t_transformer_h_5_attn_attn_weight, None)  # t723: "cuda:0 bf16[1, 512, 12288]"
    # t723 = ltorch.linear(t722, t_transformer_h_5_attn_attn_weight, None)  # t723: "cuda:0 bf16[1, 512, 12288]"
      # t723 = prims.linear(t722, t_transformer_h_5_attn_attn_weight, None)  # t723: "cuda:0 bf16[1, 512, 12288]"
  t729 = prims.reshape(t723, (1, 512, 32, 3, 128))  # t729: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t735 = prims.transpose(t729, (0, 2, 3, 1, 4))  # t735: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t736, t737, t738) = ltorch.split(t735, (1, 1, 1), 2)
    # t736 = prims.slice_prim(t735, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t736: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t737 = prims.slice_prim(t735, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t737: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t738 = prims.slice_prim(t735, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t738: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t744 = prims.reshape(t736, (1, 32, 512, 128))  # t744: "cuda:0 bf16[1, 32, 512, 128]"
  t750 = prims.reshape(t737, (1, 32, 512, 128))  # t750: "cuda:0 bf16[1, 32, 512, 128]"
  t756 = prims.reshape(t738, (1, 32, 512, 128))  # t756: "cuda:0 bf16[1, 32, 512, 128]"
  t757 = prims.slice_prim(t744, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t757: "cuda:0 bf16[1, 32, 512, 128]"
  t758 = prims.slice_prim(t757, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t758: "cuda:0 bf16[1, 32, 512, 64]"
  t759 = prims.slice_prim(t757, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t759: "cuda:0 bf16[1, 32, 512, 64]"
  t760 = prims.convert_element_type(t759, dtypes.float32)  # t760: "cuda:0 f32[1, 32, 512, 64]"
  t761 = prims.neg(t760)  # t761: "cuda:0 f32[1, 32, 512, 64]"
  t762 = prims.convert_element_type(t761, dtypes.bfloat16)  # t762: "cuda:0 bf16[1, 32, 512, 64]"
  t764 = prims.cat((t762, t758), -1)  # t764: "cuda:0 bf16[1, 32, 512, 128]"
  t765 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t765: "cuda:0 f32[1, 32, 512, 128]"
  t766 = prims.convert_element_type(t757, dtypes.float32)  # t766: "cuda:0 f32[1, 32, 512, 128]"
  t767 = ltorch.mul(t766, t765)  # t767: "cuda:0 f32[1, 32, 512, 128]"
    # t767 = prims.mul(t766, t765)  # t767: "cuda:0 f32[1, 32, 512, 128]"
  t768 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t768: "cuda:0 f32[1, 32, 512, 128]"
  t769 = prims.convert_element_type(t764, dtypes.float32)  # t769: "cuda:0 f32[1, 32, 512, 128]"
  t770 = ltorch.mul(t769, t768)  # t770: "cuda:0 f32[1, 32, 512, 128]"
    # t770 = prims.mul(t769, t768)  # t770: "cuda:0 f32[1, 32, 512, 128]"
  t771 = ltorch.add(t767, t770, alpha=None)  # t771: "cuda:0 f32[1, 32, 512, 128]"
    # t771 = prims.add(t767, t770)  # t771: "cuda:0 f32[1, 32, 512, 128]"
  t772 = prims.convert_element_type(t771, dtypes.bfloat16)  # t772: "cuda:0 bf16[1, 32, 512, 128]"
  t773 = prims.slice_prim(t750, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t773: "cuda:0 bf16[1, 32, 512, 128]"
  t774 = prims.slice_prim(t773, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t774: "cuda:0 bf16[1, 32, 512, 64]"
  t775 = prims.slice_prim(t773, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t775: "cuda:0 bf16[1, 32, 512, 64]"
  t776 = prims.convert_element_type(t775, dtypes.float32)  # t776: "cuda:0 f32[1, 32, 512, 64]"
  t777 = prims.neg(t776)  # t777: "cuda:0 f32[1, 32, 512, 64]"
  t778 = prims.convert_element_type(t777, dtypes.bfloat16)  # t778: "cuda:0 bf16[1, 32, 512, 64]"
  t780 = prims.cat((t778, t774), -1)  # t780: "cuda:0 bf16[1, 32, 512, 128]"
  t781 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t781: "cuda:0 f32[1, 32, 512, 128]"
  t782 = prims.convert_element_type(t773, dtypes.float32)  # t782: "cuda:0 f32[1, 32, 512, 128]"
  t783 = ltorch.mul(t782, t781)  # t783: "cuda:0 f32[1, 32, 512, 128]"
    # t783 = prims.mul(t782, t781)  # t783: "cuda:0 f32[1, 32, 512, 128]"
  t784 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t784: "cuda:0 f32[1, 32, 512, 128]"
  t785 = prims.convert_element_type(t780, dtypes.float32)  # t785: "cuda:0 f32[1, 32, 512, 128]"
  t786 = ltorch.mul(t785, t784)  # t786: "cuda:0 f32[1, 32, 512, 128]"
    # t786 = prims.mul(t785, t784)  # t786: "cuda:0 f32[1, 32, 512, 128]"
  t787 = ltorch.add(t783, t786, alpha=None)  # t787: "cuda:0 f32[1, 32, 512, 128]"
    # t787 = prims.add(t783, t786)  # t787: "cuda:0 f32[1, 32, 512, 128]"
  t788 = prims.convert_element_type(t787, dtypes.bfloat16)  # t788: "cuda:0 bf16[1, 32, 512, 128]"
  t789 = prims.slice_prim(t744, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t789: "cuda:0 bf16[1, 32, 512, 0]"
  t791 = prims.cat((t772, t789), -1)  # t791: "cuda:0 bf16[1, 32, 512, 128]"
  t792 = prims.slice_prim(t750, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t792: "cuda:0 bf16[1, 32, 512, 0]"
  t794 = prims.cat((t788, t792), -1)  # t794: "cuda:0 bf16[1, 32, 512, 128]"
  (t795, t796, t797, t798) = cudnn_sdpa_fwd(t791, t794, t756, None, 0.0, True, scale=0.08838834764831843)
  t801 = prims.transpose(t795, (0, 2, 1, 3))  # t801: "cuda:0 bf16[1, 512, 32, 128]"
  t805 = prims.reshape(t801, (1, 512, 4096))  # t805: "cuda:0 bf16[1, 512, 4096]"
  t806 = torch.nn.functional.linear(t805, t_transformer_h_5_attn_proj_weight, None)  # t806: "cuda:0 bf16[1, 512, 4096]"
    # t806 = ltorch.linear(t805, t_transformer_h_5_attn_proj_weight, None)  # t806: "cuda:0 bf16[1, 512, 4096]"
      # t806 = prims.linear(t805, t_transformer_h_5_attn_proj_weight, None)  # t806: "cuda:0 bf16[1, 512, 4096]"
  t807 = prims.convert_element_type(t806, dtypes.float32)  # t807: "cuda:0 f32[1, 512, 4096]"
  t808 = prims.convert_element_type(t704, dtypes.float32)  # t808: "cuda:0 f32[1, 512, 4096]"
  t809 = ltorch.add(t807, t808, alpha=None)  # t809: "cuda:0 f32[1, 512, 4096]"
    # t809 = prims.add(t807, t808)  # t809: "cuda:0 f32[1, 512, 4096]"
  t810 = prims.convert_element_type(t809, dtypes.bfloat16)  # t810: "cuda:0 bf16[1, 512, 4096]"
  t811 = prims.convert_element_type(t810, dtypes.float32)  # t811: "cuda:0 f32[1, 512, 4096]"
  t812 = ltorch.mul(t811, t811)  # t812: "cuda:0 f32[1, 512, 4096]"
    # t812 = prims.mul(t811, t811)  # t812: "cuda:0 f32[1, 512, 4096]"
  t814 = prims.sum(t812, (2,))  # t814: "cuda:0 f32[1, 512]"
  t815 = prims.broadcast_in_dim(t814, [1, 512, 1], [0, 1])  # t815: "cuda:0 f32[1, 512, 1]"
  t817 = ltorch.true_divide(t815, 4096.0)  # t817: "cuda:0 f32[1, 512, 1]"
    # t817 = prims.div(t815, 4096.0)  # t817: "cuda:0 f32[1, 512, 1]"
  t819 = ltorch.add(t817, 1e-05, alpha=None)  # t819: "cuda:0 f32[1, 512, 1]"
    # t819 = prims.add(t817, 1e-05)  # t819: "cuda:0 f32[1, 512, 1]"
  t820 = prims.rsqrt(t819)  # t820: "cuda:0 f32[1, 512, 1]"
  t821 = prims.broadcast_in_dim(t820, (1, 512, 4096), (0, 1, 2))  # t821: "cuda:0 f32[1, 512, 4096]"
  t822 = ltorch.mul(t811, t821)  # t822: "cuda:0 f32[1, 512, 4096]"
    # t822 = prims.mul(t811, t821)  # t822: "cuda:0 f32[1, 512, 4096]"
  t823 = prims.convert_element_type(t822, dtypes.bfloat16)  # t823: "cuda:0 bf16[1, 512, 4096]"
  t824 = prims.broadcast_in_dim(t_transformer_h_5_norm_2_weight, (1, 512, 4096), (2,))  # t824: "cuda:0 bf16[1, 512, 4096]"
  t825 = prims.convert_element_type(t823, dtypes.float32)  # t825: "cuda:0 f32[1, 512, 4096]"
  t826 = prims.convert_element_type(t824, dtypes.float32)  # t826: "cuda:0 f32[1, 512, 4096]"
  t827 = ltorch.mul(t825, t826)  # t827: "cuda:0 f32[1, 512, 4096]"
    # t827 = prims.mul(t825, t826)  # t827: "cuda:0 f32[1, 512, 4096]"
  t828 = prims.convert_element_type(t827, dtypes.bfloat16)  # t828: "cuda:0 bf16[1, 512, 4096]"
  t829 = torch.nn.functional.linear(t828, t_transformer_h_5_mlp_fc_1_weight, None)  # t829: "cuda:0 bf16[1, 512, 11008]"
    # t829 = ltorch.linear(t828, t_transformer_h_5_mlp_fc_1_weight, None)  # t829: "cuda:0 bf16[1, 512, 11008]"
      # t829 = prims.linear(t828, t_transformer_h_5_mlp_fc_1_weight, None)  # t829: "cuda:0 bf16[1, 512, 11008]"
  t830 = torch.nn.functional.linear(t828, t_transformer_h_5_mlp_fc_2_weight, None)  # t830: "cuda:0 bf16[1, 512, 11008]"
    # t830 = ltorch.linear(t828, t_transformer_h_5_mlp_fc_2_weight, None)  # t830: "cuda:0 bf16[1, 512, 11008]"
      # t830 = prims.linear(t828, t_transformer_h_5_mlp_fc_2_weight, None)  # t830: "cuda:0 bf16[1, 512, 11008]"
  t831 = prims.convert_element_type(t829, dtypes.float32)  # t831: "cuda:0 f32[1, 512, 11008]"
  t832 = prims.neg(t831)  # t832: "cuda:0 f32[1, 512, 11008]"
  t833 = prims.exp(t832)  # t833: "cuda:0 f32[1, 512, 11008]"
  t834 = ltorch.add(1.0, t833, alpha=None)  # t834: "cuda:0 f32[1, 512, 11008]"
    # t834 = prims.add(1.0, t833)  # t834: "cuda:0 f32[1, 512, 11008]"
  t835 = prims.reciprocal(t834)  # t835: "cuda:0 f32[1, 512, 11008]"
  t836 = prims.convert_element_type(t835, dtypes.bfloat16)  # t836: "cuda:0 bf16[1, 512, 11008]"
  t837 = prims.convert_element_type(t829, dtypes.float32)  # t837: "cuda:0 f32[1, 512, 11008]"
  t838 = prims.convert_element_type(t836, dtypes.float32)  # t838: "cuda:0 f32[1, 512, 11008]"
  t839 = ltorch.mul(t837, t838)  # t839: "cuda:0 f32[1, 512, 11008]"
    # t839 = prims.mul(t837, t838)  # t839: "cuda:0 f32[1, 512, 11008]"
  t840 = prims.convert_element_type(t839, dtypes.bfloat16)  # t840: "cuda:0 bf16[1, 512, 11008]"
  t841 = prims.convert_element_type(t840, dtypes.float32)  # t841: "cuda:0 f32[1, 512, 11008]"
  t842 = prims.convert_element_type(t830, dtypes.float32)  # t842: "cuda:0 f32[1, 512, 11008]"
  t843 = ltorch.mul(t841, t842)  # t843: "cuda:0 f32[1, 512, 11008]"
    # t843 = prims.mul(t841, t842)  # t843: "cuda:0 f32[1, 512, 11008]"
  t844 = prims.convert_element_type(t843, dtypes.bfloat16)  # t844: "cuda:0 bf16[1, 512, 11008]"
  t845 = torch.nn.functional.linear(t844, t_transformer_h_5_mlp_proj_weight, None)  # t845: "cuda:0 bf16[1, 512, 4096]"
    # t845 = ltorch.linear(t844, t_transformer_h_5_mlp_proj_weight, None)  # t845: "cuda:0 bf16[1, 512, 4096]"
      # t845 = prims.linear(t844, t_transformer_h_5_mlp_proj_weight, None)  # t845: "cuda:0 bf16[1, 512, 4096]"
  t846 = prims.convert_element_type(t845, dtypes.float32)  # t846: "cuda:0 f32[1, 512, 4096]"
  t847 = prims.convert_element_type(t810, dtypes.float32)  # t847: "cuda:0 f32[1, 512, 4096]"
  t848 = ltorch.add(t846, t847, alpha=None)  # t848: "cuda:0 f32[1, 512, 4096]"
    # t848 = prims.add(t846, t847)  # t848: "cuda:0 f32[1, 512, 4096]"
  t849 = prims.convert_element_type(t848, dtypes.bfloat16)  # t849: "cuda:0 bf16[1, 512, 4096]"
  t850 = prims.convert_element_type(t849, dtypes.float32)  # t850: "cuda:0 f32[1, 512, 4096]"
  t851 = ltorch.mul(t850, t850)  # t851: "cuda:0 f32[1, 512, 4096]"
    # t851 = prims.mul(t850, t850)  # t851: "cuda:0 f32[1, 512, 4096]"
  t853 = prims.sum(t851, (2,))  # t853: "cuda:0 f32[1, 512]"
  t854 = prims.broadcast_in_dim(t853, [1, 512, 1], [0, 1])  # t854: "cuda:0 f32[1, 512, 1]"
  t856 = ltorch.true_divide(t854, 4096.0)  # t856: "cuda:0 f32[1, 512, 1]"
    # t856 = prims.div(t854, 4096.0)  # t856: "cuda:0 f32[1, 512, 1]"
  t858 = ltorch.add(t856, 1e-05, alpha=None)  # t858: "cuda:0 f32[1, 512, 1]"
    # t858 = prims.add(t856, 1e-05)  # t858: "cuda:0 f32[1, 512, 1]"
  t859 = prims.rsqrt(t858)  # t859: "cuda:0 f32[1, 512, 1]"
  t860 = prims.broadcast_in_dim(t859, (1, 512, 4096), (0, 1, 2))  # t860: "cuda:0 f32[1, 512, 4096]"
  t861 = ltorch.mul(t850, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
    # t861 = prims.mul(t850, t860)  # t861: "cuda:0 f32[1, 512, 4096]"
  t862 = prims.convert_element_type(t861, dtypes.bfloat16)  # t862: "cuda:0 bf16[1, 512, 4096]"
  t863 = prims.broadcast_in_dim(t_transformer_h_6_norm_1_weight, (1, 512, 4096), (2,))  # t863: "cuda:0 bf16[1, 512, 4096]"
  t864 = prims.convert_element_type(t862, dtypes.float32)  # t864: "cuda:0 f32[1, 512, 4096]"
  t865 = prims.convert_element_type(t863, dtypes.float32)  # t865: "cuda:0 f32[1, 512, 4096]"
  t866 = ltorch.mul(t864, t865)  # t866: "cuda:0 f32[1, 512, 4096]"
    # t866 = prims.mul(t864, t865)  # t866: "cuda:0 f32[1, 512, 4096]"
  t867 = prims.convert_element_type(t866, dtypes.bfloat16)  # t867: "cuda:0 bf16[1, 512, 4096]"
  t868 = torch.nn.functional.linear(t867, t_transformer_h_6_attn_attn_weight, None)  # t868: "cuda:0 bf16[1, 512, 12288]"
    # t868 = ltorch.linear(t867, t_transformer_h_6_attn_attn_weight, None)  # t868: "cuda:0 bf16[1, 512, 12288]"
      # t868 = prims.linear(t867, t_transformer_h_6_attn_attn_weight, None)  # t868: "cuda:0 bf16[1, 512, 12288]"
  t874 = prims.reshape(t868, (1, 512, 32, 3, 128))  # t874: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t880 = prims.transpose(t874, (0, 2, 3, 1, 4))  # t880: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t881, t882, t883) = ltorch.split(t880, (1, 1, 1), 2)
    # t881 = prims.slice_prim(t880, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t881: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t882 = prims.slice_prim(t880, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t882: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t883 = prims.slice_prim(t880, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t883: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t889 = prims.reshape(t881, (1, 32, 512, 128))  # t889: "cuda:0 bf16[1, 32, 512, 128]"
  t895 = prims.reshape(t882, (1, 32, 512, 128))  # t895: "cuda:0 bf16[1, 32, 512, 128]"
  t901 = prims.reshape(t883, (1, 32, 512, 128))  # t901: "cuda:0 bf16[1, 32, 512, 128]"
  t902 = prims.slice_prim(t889, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t902: "cuda:0 bf16[1, 32, 512, 128]"
  t903 = prims.slice_prim(t902, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t903: "cuda:0 bf16[1, 32, 512, 64]"
  t904 = prims.slice_prim(t902, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t904: "cuda:0 bf16[1, 32, 512, 64]"
  t905 = prims.convert_element_type(t904, dtypes.float32)  # t905: "cuda:0 f32[1, 32, 512, 64]"
  t906 = prims.neg(t905)  # t906: "cuda:0 f32[1, 32, 512, 64]"
  t907 = prims.convert_element_type(t906, dtypes.bfloat16)  # t907: "cuda:0 bf16[1, 32, 512, 64]"
  t909 = prims.cat((t907, t903), -1)  # t909: "cuda:0 bf16[1, 32, 512, 128]"
  t910 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t910: "cuda:0 f32[1, 32, 512, 128]"
  t911 = prims.convert_element_type(t902, dtypes.float32)  # t911: "cuda:0 f32[1, 32, 512, 128]"
  t912 = ltorch.mul(t911, t910)  # t912: "cuda:0 f32[1, 32, 512, 128]"
    # t912 = prims.mul(t911, t910)  # t912: "cuda:0 f32[1, 32, 512, 128]"
  t913 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t913: "cuda:0 f32[1, 32, 512, 128]"
  t914 = prims.convert_element_type(t909, dtypes.float32)  # t914: "cuda:0 f32[1, 32, 512, 128]"
  t915 = ltorch.mul(t914, t913)  # t915: "cuda:0 f32[1, 32, 512, 128]"
    # t915 = prims.mul(t914, t913)  # t915: "cuda:0 f32[1, 32, 512, 128]"
  t916 = ltorch.add(t912, t915, alpha=None)  # t916: "cuda:0 f32[1, 32, 512, 128]"
    # t916 = prims.add(t912, t915)  # t916: "cuda:0 f32[1, 32, 512, 128]"
  t917 = prims.convert_element_type(t916, dtypes.bfloat16)  # t917: "cuda:0 bf16[1, 32, 512, 128]"
  t918 = prims.slice_prim(t895, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t918: "cuda:0 bf16[1, 32, 512, 128]"
  t919 = prims.slice_prim(t918, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t919: "cuda:0 bf16[1, 32, 512, 64]"
  t920 = prims.slice_prim(t918, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t920: "cuda:0 bf16[1, 32, 512, 64]"
  t921 = prims.convert_element_type(t920, dtypes.float32)  # t921: "cuda:0 f32[1, 32, 512, 64]"
  t922 = prims.neg(t921)  # t922: "cuda:0 f32[1, 32, 512, 64]"
  t923 = prims.convert_element_type(t922, dtypes.bfloat16)  # t923: "cuda:0 bf16[1, 32, 512, 64]"
  t925 = prims.cat((t923, t919), -1)  # t925: "cuda:0 bf16[1, 32, 512, 128]"
  t926 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t926: "cuda:0 f32[1, 32, 512, 128]"
  t927 = prims.convert_element_type(t918, dtypes.float32)  # t927: "cuda:0 f32[1, 32, 512, 128]"
  t928 = ltorch.mul(t927, t926)  # t928: "cuda:0 f32[1, 32, 512, 128]"
    # t928 = prims.mul(t927, t926)  # t928: "cuda:0 f32[1, 32, 512, 128]"
  t929 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t929: "cuda:0 f32[1, 32, 512, 128]"
  t930 = prims.convert_element_type(t925, dtypes.float32)  # t930: "cuda:0 f32[1, 32, 512, 128]"
  t931 = ltorch.mul(t930, t929)  # t931: "cuda:0 f32[1, 32, 512, 128]"
    # t931 = prims.mul(t930, t929)  # t931: "cuda:0 f32[1, 32, 512, 128]"
  t932 = ltorch.add(t928, t931, alpha=None)  # t932: "cuda:0 f32[1, 32, 512, 128]"
    # t932 = prims.add(t928, t931)  # t932: "cuda:0 f32[1, 32, 512, 128]"
  t933 = prims.convert_element_type(t932, dtypes.bfloat16)  # t933: "cuda:0 bf16[1, 32, 512, 128]"
  t934 = prims.slice_prim(t889, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t934: "cuda:0 bf16[1, 32, 512, 0]"
  t936 = prims.cat((t917, t934), -1)  # t936: "cuda:0 bf16[1, 32, 512, 128]"
  t937 = prims.slice_prim(t895, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t937: "cuda:0 bf16[1, 32, 512, 0]"
  t939 = prims.cat((t933, t937), -1)  # t939: "cuda:0 bf16[1, 32, 512, 128]"
  (t940, t941, t942, t943) = cudnn_sdpa_fwd(t936, t939, t901, None, 0.0, True, scale=0.08838834764831843)
  t946 = prims.transpose(t940, (0, 2, 1, 3))  # t946: "cuda:0 bf16[1, 512, 32, 128]"
  t950 = prims.reshape(t946, (1, 512, 4096))  # t950: "cuda:0 bf16[1, 512, 4096]"
  t951 = torch.nn.functional.linear(t950, t_transformer_h_6_attn_proj_weight, None)  # t951: "cuda:0 bf16[1, 512, 4096]"
    # t951 = ltorch.linear(t950, t_transformer_h_6_attn_proj_weight, None)  # t951: "cuda:0 bf16[1, 512, 4096]"
      # t951 = prims.linear(t950, t_transformer_h_6_attn_proj_weight, None)  # t951: "cuda:0 bf16[1, 512, 4096]"
  t952 = prims.convert_element_type(t951, dtypes.float32)  # t952: "cuda:0 f32[1, 512, 4096]"
  t953 = prims.convert_element_type(t849, dtypes.float32)  # t953: "cuda:0 f32[1, 512, 4096]"
  t954 = ltorch.add(t952, t953, alpha=None)  # t954: "cuda:0 f32[1, 512, 4096]"
    # t954 = prims.add(t952, t953)  # t954: "cuda:0 f32[1, 512, 4096]"
  t955 = prims.convert_element_type(t954, dtypes.bfloat16)  # t955: "cuda:0 bf16[1, 512, 4096]"
  t956 = prims.convert_element_type(t955, dtypes.float32)  # t956: "cuda:0 f32[1, 512, 4096]"
  t957 = ltorch.mul(t956, t956)  # t957: "cuda:0 f32[1, 512, 4096]"
    # t957 = prims.mul(t956, t956)  # t957: "cuda:0 f32[1, 512, 4096]"
  t959 = prims.sum(t957, (2,))  # t959: "cuda:0 f32[1, 512]"
  t960 = prims.broadcast_in_dim(t959, [1, 512, 1], [0, 1])  # t960: "cuda:0 f32[1, 512, 1]"
  t962 = ltorch.true_divide(t960, 4096.0)  # t962: "cuda:0 f32[1, 512, 1]"
    # t962 = prims.div(t960, 4096.0)  # t962: "cuda:0 f32[1, 512, 1]"
  t964 = ltorch.add(t962, 1e-05, alpha=None)  # t964: "cuda:0 f32[1, 512, 1]"
    # t964 = prims.add(t962, 1e-05)  # t964: "cuda:0 f32[1, 512, 1]"
  t965 = prims.rsqrt(t964)  # t965: "cuda:0 f32[1, 512, 1]"
  t966 = prims.broadcast_in_dim(t965, (1, 512, 4096), (0, 1, 2))  # t966: "cuda:0 f32[1, 512, 4096]"
  t967 = ltorch.mul(t956, t966)  # t967: "cuda:0 f32[1, 512, 4096]"
    # t967 = prims.mul(t956, t966)  # t967: "cuda:0 f32[1, 512, 4096]"
  t968 = prims.convert_element_type(t967, dtypes.bfloat16)  # t968: "cuda:0 bf16[1, 512, 4096]"
  t969 = prims.broadcast_in_dim(t_transformer_h_6_norm_2_weight, (1, 512, 4096), (2,))  # t969: "cuda:0 bf16[1, 512, 4096]"
  t970 = prims.convert_element_type(t968, dtypes.float32)  # t970: "cuda:0 f32[1, 512, 4096]"
  t971 = prims.convert_element_type(t969, dtypes.float32)  # t971: "cuda:0 f32[1, 512, 4096]"
  t972 = ltorch.mul(t970, t971)  # t972: "cuda:0 f32[1, 512, 4096]"
    # t972 = prims.mul(t970, t971)  # t972: "cuda:0 f32[1, 512, 4096]"
  t973 = prims.convert_element_type(t972, dtypes.bfloat16)  # t973: "cuda:0 bf16[1, 512, 4096]"
  t974 = torch.nn.functional.linear(t973, t_transformer_h_6_mlp_fc_1_weight, None)  # t974: "cuda:0 bf16[1, 512, 11008]"
    # t974 = ltorch.linear(t973, t_transformer_h_6_mlp_fc_1_weight, None)  # t974: "cuda:0 bf16[1, 512, 11008]"
      # t974 = prims.linear(t973, t_transformer_h_6_mlp_fc_1_weight, None)  # t974: "cuda:0 bf16[1, 512, 11008]"
  t975 = torch.nn.functional.linear(t973, t_transformer_h_6_mlp_fc_2_weight, None)  # t975: "cuda:0 bf16[1, 512, 11008]"
    # t975 = ltorch.linear(t973, t_transformer_h_6_mlp_fc_2_weight, None)  # t975: "cuda:0 bf16[1, 512, 11008]"
      # t975 = prims.linear(t973, t_transformer_h_6_mlp_fc_2_weight, None)  # t975: "cuda:0 bf16[1, 512, 11008]"
  t976 = prims.convert_element_type(t974, dtypes.float32)  # t976: "cuda:0 f32[1, 512, 11008]"
  t977 = prims.neg(t976)  # t977: "cuda:0 f32[1, 512, 11008]"
  t978 = prims.exp(t977)  # t978: "cuda:0 f32[1, 512, 11008]"
  t979 = ltorch.add(1.0, t978, alpha=None)  # t979: "cuda:0 f32[1, 512, 11008]"
    # t979 = prims.add(1.0, t978)  # t979: "cuda:0 f32[1, 512, 11008]"
  t980 = prims.reciprocal(t979)  # t980: "cuda:0 f32[1, 512, 11008]"
  t981 = prims.convert_element_type(t980, dtypes.bfloat16)  # t981: "cuda:0 bf16[1, 512, 11008]"
  t982 = prims.convert_element_type(t974, dtypes.float32)  # t982: "cuda:0 f32[1, 512, 11008]"
  t983 = prims.convert_element_type(t981, dtypes.float32)  # t983: "cuda:0 f32[1, 512, 11008]"
  t984 = ltorch.mul(t982, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
    # t984 = prims.mul(t982, t983)  # t984: "cuda:0 f32[1, 512, 11008]"
  t985 = prims.convert_element_type(t984, dtypes.bfloat16)  # t985: "cuda:0 bf16[1, 512, 11008]"
  t986 = prims.convert_element_type(t985, dtypes.float32)  # t986: "cuda:0 f32[1, 512, 11008]"
  t987 = prims.convert_element_type(t975, dtypes.float32)  # t987: "cuda:0 f32[1, 512, 11008]"
  t988 = ltorch.mul(t986, t987)  # t988: "cuda:0 f32[1, 512, 11008]"
    # t988 = prims.mul(t986, t987)  # t988: "cuda:0 f32[1, 512, 11008]"
  t989 = prims.convert_element_type(t988, dtypes.bfloat16)  # t989: "cuda:0 bf16[1, 512, 11008]"
  t990 = torch.nn.functional.linear(t989, t_transformer_h_6_mlp_proj_weight, None)  # t990: "cuda:0 bf16[1, 512, 4096]"
    # t990 = ltorch.linear(t989, t_transformer_h_6_mlp_proj_weight, None)  # t990: "cuda:0 bf16[1, 512, 4096]"
      # t990 = prims.linear(t989, t_transformer_h_6_mlp_proj_weight, None)  # t990: "cuda:0 bf16[1, 512, 4096]"
  t991 = prims.convert_element_type(t990, dtypes.float32)  # t991: "cuda:0 f32[1, 512, 4096]"
  t992 = prims.convert_element_type(t955, dtypes.float32)  # t992: "cuda:0 f32[1, 512, 4096]"
  t993 = ltorch.add(t991, t992, alpha=None)  # t993: "cuda:0 f32[1, 512, 4096]"
    # t993 = prims.add(t991, t992)  # t993: "cuda:0 f32[1, 512, 4096]"
  t994 = prims.convert_element_type(t993, dtypes.bfloat16)  # t994: "cuda:0 bf16[1, 512, 4096]"
  t995 = prims.convert_element_type(t994, dtypes.float32)  # t995: "cuda:0 f32[1, 512, 4096]"
  t996 = ltorch.mul(t995, t995)  # t996: "cuda:0 f32[1, 512, 4096]"
    # t996 = prims.mul(t995, t995)  # t996: "cuda:0 f32[1, 512, 4096]"
  t998 = prims.sum(t996, (2,))  # t998: "cuda:0 f32[1, 512]"
  t999 = prims.broadcast_in_dim(t998, [1, 512, 1], [0, 1])  # t999: "cuda:0 f32[1, 512, 1]"
  t1001 = ltorch.true_divide(t999, 4096.0)  # t1001: "cuda:0 f32[1, 512, 1]"
    # t1001 = prims.div(t999, 4096.0)  # t1001: "cuda:0 f32[1, 512, 1]"
  t1003 = ltorch.add(t1001, 1e-05, alpha=None)  # t1003: "cuda:0 f32[1, 512, 1]"
    # t1003 = prims.add(t1001, 1e-05)  # t1003: "cuda:0 f32[1, 512, 1]"
  t1004 = prims.rsqrt(t1003)  # t1004: "cuda:0 f32[1, 512, 1]"
  t1005 = prims.broadcast_in_dim(t1004, (1, 512, 4096), (0, 1, 2))  # t1005: "cuda:0 f32[1, 512, 4096]"
  t1006 = ltorch.mul(t995, t1005)  # t1006: "cuda:0 f32[1, 512, 4096]"
    # t1006 = prims.mul(t995, t1005)  # t1006: "cuda:0 f32[1, 512, 4096]"
  t1007 = prims.convert_element_type(t1006, dtypes.bfloat16)  # t1007: "cuda:0 bf16[1, 512, 4096]"
  t1008 = prims.broadcast_in_dim(t_transformer_h_7_norm_1_weight, (1, 512, 4096), (2,))  # t1008: "cuda:0 bf16[1, 512, 4096]"
  t1009 = prims.convert_element_type(t1007, dtypes.float32)  # t1009: "cuda:0 f32[1, 512, 4096]"
  t1010 = prims.convert_element_type(t1008, dtypes.float32)  # t1010: "cuda:0 f32[1, 512, 4096]"
  t1011 = ltorch.mul(t1009, t1010)  # t1011: "cuda:0 f32[1, 512, 4096]"
    # t1011 = prims.mul(t1009, t1010)  # t1011: "cuda:0 f32[1, 512, 4096]"
  t1012 = prims.convert_element_type(t1011, dtypes.bfloat16)  # t1012: "cuda:0 bf16[1, 512, 4096]"
  t1013 = torch.nn.functional.linear(t1012, t_transformer_h_7_attn_attn_weight, None)  # t1013: "cuda:0 bf16[1, 512, 12288]"
    # t1013 = ltorch.linear(t1012, t_transformer_h_7_attn_attn_weight, None)  # t1013: "cuda:0 bf16[1, 512, 12288]"
      # t1013 = prims.linear(t1012, t_transformer_h_7_attn_attn_weight, None)  # t1013: "cuda:0 bf16[1, 512, 12288]"
  t1019 = prims.reshape(t1013, (1, 512, 32, 3, 128))  # t1019: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1025 = prims.transpose(t1019, (0, 2, 3, 1, 4))  # t1025: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1026, t1027, t1028) = ltorch.split(t1025, (1, 1, 1), 2)
    # t1026 = prims.slice_prim(t1025, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1026: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1027 = prims.slice_prim(t1025, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1027: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1028 = prims.slice_prim(t1025, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1028: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1034 = prims.reshape(t1026, (1, 32, 512, 128))  # t1034: "cuda:0 bf16[1, 32, 512, 128]"
  t1040 = prims.reshape(t1027, (1, 32, 512, 128))  # t1040: "cuda:0 bf16[1, 32, 512, 128]"
  t1046 = prims.reshape(t1028, (1, 32, 512, 128))  # t1046: "cuda:0 bf16[1, 32, 512, 128]"
  t1047 = prims.slice_prim(t1034, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1047: "cuda:0 bf16[1, 32, 512, 128]"
  t1048 = prims.slice_prim(t1047, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1048: "cuda:0 bf16[1, 32, 512, 64]"
  t1049 = prims.slice_prim(t1047, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1049: "cuda:0 bf16[1, 32, 512, 64]"
  t1050 = prims.convert_element_type(t1049, dtypes.float32)  # t1050: "cuda:0 f32[1, 32, 512, 64]"
  t1051 = prims.neg(t1050)  # t1051: "cuda:0 f32[1, 32, 512, 64]"
  t1052 = prims.convert_element_type(t1051, dtypes.bfloat16)  # t1052: "cuda:0 bf16[1, 32, 512, 64]"
  t1054 = prims.cat((t1052, t1048), -1)  # t1054: "cuda:0 bf16[1, 32, 512, 128]"
  t1055 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1055: "cuda:0 f32[1, 32, 512, 128]"
  t1056 = prims.convert_element_type(t1047, dtypes.float32)  # t1056: "cuda:0 f32[1, 32, 512, 128]"
  t1057 = ltorch.mul(t1056, t1055)  # t1057: "cuda:0 f32[1, 32, 512, 128]"
    # t1057 = prims.mul(t1056, t1055)  # t1057: "cuda:0 f32[1, 32, 512, 128]"
  t1058 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1058: "cuda:0 f32[1, 32, 512, 128]"
  t1059 = prims.convert_element_type(t1054, dtypes.float32)  # t1059: "cuda:0 f32[1, 32, 512, 128]"
  t1060 = ltorch.mul(t1059, t1058)  # t1060: "cuda:0 f32[1, 32, 512, 128]"
    # t1060 = prims.mul(t1059, t1058)  # t1060: "cuda:0 f32[1, 32, 512, 128]"
  t1061 = ltorch.add(t1057, t1060, alpha=None)  # t1061: "cuda:0 f32[1, 32, 512, 128]"
    # t1061 = prims.add(t1057, t1060)  # t1061: "cuda:0 f32[1, 32, 512, 128]"
  t1062 = prims.convert_element_type(t1061, dtypes.bfloat16)  # t1062: "cuda:0 bf16[1, 32, 512, 128]"
  t1063 = prims.slice_prim(t1040, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1063: "cuda:0 bf16[1, 32, 512, 128]"
  t1064 = prims.slice_prim(t1063, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1064: "cuda:0 bf16[1, 32, 512, 64]"
  t1065 = prims.slice_prim(t1063, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1065: "cuda:0 bf16[1, 32, 512, 64]"
  t1066 = prims.convert_element_type(t1065, dtypes.float32)  # t1066: "cuda:0 f32[1, 32, 512, 64]"
  t1067 = prims.neg(t1066)  # t1067: "cuda:0 f32[1, 32, 512, 64]"
  t1068 = prims.convert_element_type(t1067, dtypes.bfloat16)  # t1068: "cuda:0 bf16[1, 32, 512, 64]"
  t1070 = prims.cat((t1068, t1064), -1)  # t1070: "cuda:0 bf16[1, 32, 512, 128]"
  t1071 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1071: "cuda:0 f32[1, 32, 512, 128]"
  t1072 = prims.convert_element_type(t1063, dtypes.float32)  # t1072: "cuda:0 f32[1, 32, 512, 128]"
  t1073 = ltorch.mul(t1072, t1071)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
    # t1073 = prims.mul(t1072, t1071)  # t1073: "cuda:0 f32[1, 32, 512, 128]"
  t1074 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1074: "cuda:0 f32[1, 32, 512, 128]"
  t1075 = prims.convert_element_type(t1070, dtypes.float32)  # t1075: "cuda:0 f32[1, 32, 512, 128]"
  t1076 = ltorch.mul(t1075, t1074)  # t1076: "cuda:0 f32[1, 32, 512, 128]"
    # t1076 = prims.mul(t1075, t1074)  # t1076: "cuda:0 f32[1, 32, 512, 128]"
  t1077 = ltorch.add(t1073, t1076, alpha=None)  # t1077: "cuda:0 f32[1, 32, 512, 128]"
    # t1077 = prims.add(t1073, t1076)  # t1077: "cuda:0 f32[1, 32, 512, 128]"
  t1078 = prims.convert_element_type(t1077, dtypes.bfloat16)  # t1078: "cuda:0 bf16[1, 32, 512, 128]"
  t1079 = prims.slice_prim(t1034, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1079: "cuda:0 bf16[1, 32, 512, 0]"
  t1081 = prims.cat((t1062, t1079), -1)  # t1081: "cuda:0 bf16[1, 32, 512, 128]"
  t1082 = prims.slice_prim(t1040, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1082: "cuda:0 bf16[1, 32, 512, 0]"
  t1084 = prims.cat((t1078, t1082), -1)  # t1084: "cuda:0 bf16[1, 32, 512, 128]"
  (t1085, t1086, t1087, t1088) = cudnn_sdpa_fwd(t1081, t1084, t1046, None, 0.0, True, scale=0.08838834764831843)
  t1091 = prims.transpose(t1085, (0, 2, 1, 3))  # t1091: "cuda:0 bf16[1, 512, 32, 128]"
  t1095 = prims.reshape(t1091, (1, 512, 4096))  # t1095: "cuda:0 bf16[1, 512, 4096]"
  t1096 = torch.nn.functional.linear(t1095, t_transformer_h_7_attn_proj_weight, None)  # t1096: "cuda:0 bf16[1, 512, 4096]"
    # t1096 = ltorch.linear(t1095, t_transformer_h_7_attn_proj_weight, None)  # t1096: "cuda:0 bf16[1, 512, 4096]"
      # t1096 = prims.linear(t1095, t_transformer_h_7_attn_proj_weight, None)  # t1096: "cuda:0 bf16[1, 512, 4096]"
  t1097 = prims.convert_element_type(t1096, dtypes.float32)  # t1097: "cuda:0 f32[1, 512, 4096]"
  t1098 = prims.convert_element_type(t994, dtypes.float32)  # t1098: "cuda:0 f32[1, 512, 4096]"
  t1099 = ltorch.add(t1097, t1098, alpha=None)  # t1099: "cuda:0 f32[1, 512, 4096]"
    # t1099 = prims.add(t1097, t1098)  # t1099: "cuda:0 f32[1, 512, 4096]"
  t1100 = prims.convert_element_type(t1099, dtypes.bfloat16)  # t1100: "cuda:0 bf16[1, 512, 4096]"
  t1101 = prims.convert_element_type(t1100, dtypes.float32)  # t1101: "cuda:0 f32[1, 512, 4096]"
  t1102 = ltorch.mul(t1101, t1101)  # t1102: "cuda:0 f32[1, 512, 4096]"
    # t1102 = prims.mul(t1101, t1101)  # t1102: "cuda:0 f32[1, 512, 4096]"
  t1104 = prims.sum(t1102, (2,))  # t1104: "cuda:0 f32[1, 512]"
  t1105 = prims.broadcast_in_dim(t1104, [1, 512, 1], [0, 1])  # t1105: "cuda:0 f32[1, 512, 1]"
  t1107 = ltorch.true_divide(t1105, 4096.0)  # t1107: "cuda:0 f32[1, 512, 1]"
    # t1107 = prims.div(t1105, 4096.0)  # t1107: "cuda:0 f32[1, 512, 1]"
  t1109 = ltorch.add(t1107, 1e-05, alpha=None)  # t1109: "cuda:0 f32[1, 512, 1]"
    # t1109 = prims.add(t1107, 1e-05)  # t1109: "cuda:0 f32[1, 512, 1]"
  t1110 = prims.rsqrt(t1109)  # t1110: "cuda:0 f32[1, 512, 1]"
  t1111 = prims.broadcast_in_dim(t1110, (1, 512, 4096), (0, 1, 2))  # t1111: "cuda:0 f32[1, 512, 4096]"
  t1112 = ltorch.mul(t1101, t1111)  # t1112: "cuda:0 f32[1, 512, 4096]"
    # t1112 = prims.mul(t1101, t1111)  # t1112: "cuda:0 f32[1, 512, 4096]"
  t1113 = prims.convert_element_type(t1112, dtypes.bfloat16)  # t1113: "cuda:0 bf16[1, 512, 4096]"
  t1114 = prims.broadcast_in_dim(t_transformer_h_7_norm_2_weight, (1, 512, 4096), (2,))  # t1114: "cuda:0 bf16[1, 512, 4096]"
  t1115 = prims.convert_element_type(t1113, dtypes.float32)  # t1115: "cuda:0 f32[1, 512, 4096]"
  t1116 = prims.convert_element_type(t1114, dtypes.float32)  # t1116: "cuda:0 f32[1, 512, 4096]"
  t1117 = ltorch.mul(t1115, t1116)  # t1117: "cuda:0 f32[1, 512, 4096]"
    # t1117 = prims.mul(t1115, t1116)  # t1117: "cuda:0 f32[1, 512, 4096]"
  t1118 = prims.convert_element_type(t1117, dtypes.bfloat16)  # t1118: "cuda:0 bf16[1, 512, 4096]"
  t1119 = torch.nn.functional.linear(t1118, t_transformer_h_7_mlp_fc_1_weight, None)  # t1119: "cuda:0 bf16[1, 512, 11008]"
    # t1119 = ltorch.linear(t1118, t_transformer_h_7_mlp_fc_1_weight, None)  # t1119: "cuda:0 bf16[1, 512, 11008]"
      # t1119 = prims.linear(t1118, t_transformer_h_7_mlp_fc_1_weight, None)  # t1119: "cuda:0 bf16[1, 512, 11008]"
  t1120 = torch.nn.functional.linear(t1118, t_transformer_h_7_mlp_fc_2_weight, None)  # t1120: "cuda:0 bf16[1, 512, 11008]"
    # t1120 = ltorch.linear(t1118, t_transformer_h_7_mlp_fc_2_weight, None)  # t1120: "cuda:0 bf16[1, 512, 11008]"
      # t1120 = prims.linear(t1118, t_transformer_h_7_mlp_fc_2_weight, None)  # t1120: "cuda:0 bf16[1, 512, 11008]"
  t1121 = prims.convert_element_type(t1119, dtypes.float32)  # t1121: "cuda:0 f32[1, 512, 11008]"
  t1122 = prims.neg(t1121)  # t1122: "cuda:0 f32[1, 512, 11008]"
  t1123 = prims.exp(t1122)  # t1123: "cuda:0 f32[1, 512, 11008]"
  t1124 = ltorch.add(1.0, t1123, alpha=None)  # t1124: "cuda:0 f32[1, 512, 11008]"
    # t1124 = prims.add(1.0, t1123)  # t1124: "cuda:0 f32[1, 512, 11008]"
  t1125 = prims.reciprocal(t1124)  # t1125: "cuda:0 f32[1, 512, 11008]"
  t1126 = prims.convert_element_type(t1125, dtypes.bfloat16)  # t1126: "cuda:0 bf16[1, 512, 11008]"
  t1127 = prims.convert_element_type(t1119, dtypes.float32)  # t1127: "cuda:0 f32[1, 512, 11008]"
  t1128 = prims.convert_element_type(t1126, dtypes.float32)  # t1128: "cuda:0 f32[1, 512, 11008]"
  t1129 = ltorch.mul(t1127, t1128)  # t1129: "cuda:0 f32[1, 512, 11008]"
    # t1129 = prims.mul(t1127, t1128)  # t1129: "cuda:0 f32[1, 512, 11008]"
  t1130 = prims.convert_element_type(t1129, dtypes.bfloat16)  # t1130: "cuda:0 bf16[1, 512, 11008]"
  t1131 = prims.convert_element_type(t1130, dtypes.float32)  # t1131: "cuda:0 f32[1, 512, 11008]"
  t1132 = prims.convert_element_type(t1120, dtypes.float32)  # t1132: "cuda:0 f32[1, 512, 11008]"
  t1133 = ltorch.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 11008]"
    # t1133 = prims.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 11008]"
  t1134 = prims.convert_element_type(t1133, dtypes.bfloat16)  # t1134: "cuda:0 bf16[1, 512, 11008]"
  t1135 = torch.nn.functional.linear(t1134, t_transformer_h_7_mlp_proj_weight, None)  # t1135: "cuda:0 bf16[1, 512, 4096]"
    # t1135 = ltorch.linear(t1134, t_transformer_h_7_mlp_proj_weight, None)  # t1135: "cuda:0 bf16[1, 512, 4096]"
      # t1135 = prims.linear(t1134, t_transformer_h_7_mlp_proj_weight, None)  # t1135: "cuda:0 bf16[1, 512, 4096]"
  t1136 = prims.convert_element_type(t1135, dtypes.float32)  # t1136: "cuda:0 f32[1, 512, 4096]"
  t1137 = prims.convert_element_type(t1100, dtypes.float32)  # t1137: "cuda:0 f32[1, 512, 4096]"
  t1138 = ltorch.add(t1136, t1137, alpha=None)  # t1138: "cuda:0 f32[1, 512, 4096]"
    # t1138 = prims.add(t1136, t1137)  # t1138: "cuda:0 f32[1, 512, 4096]"
  t1139 = prims.convert_element_type(t1138, dtypes.bfloat16)  # t1139: "cuda:0 bf16[1, 512, 4096]"
  t1140 = prims.convert_element_type(t1139, dtypes.float32)  # t1140: "cuda:0 f32[1, 512, 4096]"
  t1141 = ltorch.mul(t1140, t1140)  # t1141: "cuda:0 f32[1, 512, 4096]"
    # t1141 = prims.mul(t1140, t1140)  # t1141: "cuda:0 f32[1, 512, 4096]"
  t1143 = prims.sum(t1141, (2,))  # t1143: "cuda:0 f32[1, 512]"
  t1144 = prims.broadcast_in_dim(t1143, [1, 512, 1], [0, 1])  # t1144: "cuda:0 f32[1, 512, 1]"
  t1146 = ltorch.true_divide(t1144, 4096.0)  # t1146: "cuda:0 f32[1, 512, 1]"
    # t1146 = prims.div(t1144, 4096.0)  # t1146: "cuda:0 f32[1, 512, 1]"
  t1148 = ltorch.add(t1146, 1e-05, alpha=None)  # t1148: "cuda:0 f32[1, 512, 1]"
    # t1148 = prims.add(t1146, 1e-05)  # t1148: "cuda:0 f32[1, 512, 1]"
  t1149 = prims.rsqrt(t1148)  # t1149: "cuda:0 f32[1, 512, 1]"
  t1150 = prims.broadcast_in_dim(t1149, (1, 512, 4096), (0, 1, 2))  # t1150: "cuda:0 f32[1, 512, 4096]"
  t1151 = ltorch.mul(t1140, t1150)  # t1151: "cuda:0 f32[1, 512, 4096]"
    # t1151 = prims.mul(t1140, t1150)  # t1151: "cuda:0 f32[1, 512, 4096]"
  t1152 = prims.convert_element_type(t1151, dtypes.bfloat16)  # t1152: "cuda:0 bf16[1, 512, 4096]"
  t1153 = prims.broadcast_in_dim(t_transformer_h_8_norm_1_weight, (1, 512, 4096), (2,))  # t1153: "cuda:0 bf16[1, 512, 4096]"
  t1154 = prims.convert_element_type(t1152, dtypes.float32)  # t1154: "cuda:0 f32[1, 512, 4096]"
  t1155 = prims.convert_element_type(t1153, dtypes.float32)  # t1155: "cuda:0 f32[1, 512, 4096]"
  t1156 = ltorch.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 4096]"
    # t1156 = prims.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 4096]"
  t1157 = prims.convert_element_type(t1156, dtypes.bfloat16)  # t1157: "cuda:0 bf16[1, 512, 4096]"
  t1158 = torch.nn.functional.linear(t1157, t_transformer_h_8_attn_attn_weight, None)  # t1158: "cuda:0 bf16[1, 512, 12288]"
    # t1158 = ltorch.linear(t1157, t_transformer_h_8_attn_attn_weight, None)  # t1158: "cuda:0 bf16[1, 512, 12288]"
      # t1158 = prims.linear(t1157, t_transformer_h_8_attn_attn_weight, None)  # t1158: "cuda:0 bf16[1, 512, 12288]"
  t1164 = prims.reshape(t1158, (1, 512, 32, 3, 128))  # t1164: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1170 = prims.transpose(t1164, (0, 2, 3, 1, 4))  # t1170: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1171, t1172, t1173) = ltorch.split(t1170, (1, 1, 1), 2)
    # t1171 = prims.slice_prim(t1170, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1171: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1172 = prims.slice_prim(t1170, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1172: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1173 = prims.slice_prim(t1170, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1173: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1179 = prims.reshape(t1171, (1, 32, 512, 128))  # t1179: "cuda:0 bf16[1, 32, 512, 128]"
  t1185 = prims.reshape(t1172, (1, 32, 512, 128))  # t1185: "cuda:0 bf16[1, 32, 512, 128]"
  t1191 = prims.reshape(t1173, (1, 32, 512, 128))  # t1191: "cuda:0 bf16[1, 32, 512, 128]"
  t1192 = prims.slice_prim(t1179, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1192: "cuda:0 bf16[1, 32, 512, 128]"
  t1193 = prims.slice_prim(t1192, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1193: "cuda:0 bf16[1, 32, 512, 64]"
  t1194 = prims.slice_prim(t1192, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1194: "cuda:0 bf16[1, 32, 512, 64]"
  t1195 = prims.convert_element_type(t1194, dtypes.float32)  # t1195: "cuda:0 f32[1, 32, 512, 64]"
  t1196 = prims.neg(t1195)  # t1196: "cuda:0 f32[1, 32, 512, 64]"
  t1197 = prims.convert_element_type(t1196, dtypes.bfloat16)  # t1197: "cuda:0 bf16[1, 32, 512, 64]"
  t1199 = prims.cat((t1197, t1193), -1)  # t1199: "cuda:0 bf16[1, 32, 512, 128]"
  t1200 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1200: "cuda:0 f32[1, 32, 512, 128]"
  t1201 = prims.convert_element_type(t1192, dtypes.float32)  # t1201: "cuda:0 f32[1, 32, 512, 128]"
  t1202 = ltorch.mul(t1201, t1200)  # t1202: "cuda:0 f32[1, 32, 512, 128]"
    # t1202 = prims.mul(t1201, t1200)  # t1202: "cuda:0 f32[1, 32, 512, 128]"
  t1203 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1203: "cuda:0 f32[1, 32, 512, 128]"
  t1204 = prims.convert_element_type(t1199, dtypes.float32)  # t1204: "cuda:0 f32[1, 32, 512, 128]"
  t1205 = ltorch.mul(t1204, t1203)  # t1205: "cuda:0 f32[1, 32, 512, 128]"
    # t1205 = prims.mul(t1204, t1203)  # t1205: "cuda:0 f32[1, 32, 512, 128]"
  t1206 = ltorch.add(t1202, t1205, alpha=None)  # t1206: "cuda:0 f32[1, 32, 512, 128]"
    # t1206 = prims.add(t1202, t1205)  # t1206: "cuda:0 f32[1, 32, 512, 128]"
  t1207 = prims.convert_element_type(t1206, dtypes.bfloat16)  # t1207: "cuda:0 bf16[1, 32, 512, 128]"
  t1208 = prims.slice_prim(t1185, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1208: "cuda:0 bf16[1, 32, 512, 128]"
  t1209 = prims.slice_prim(t1208, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1209: "cuda:0 bf16[1, 32, 512, 64]"
  t1210 = prims.slice_prim(t1208, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1210: "cuda:0 bf16[1, 32, 512, 64]"
  t1211 = prims.convert_element_type(t1210, dtypes.float32)  # t1211: "cuda:0 f32[1, 32, 512, 64]"
  t1212 = prims.neg(t1211)  # t1212: "cuda:0 f32[1, 32, 512, 64]"
  t1213 = prims.convert_element_type(t1212, dtypes.bfloat16)  # t1213: "cuda:0 bf16[1, 32, 512, 64]"
  t1215 = prims.cat((t1213, t1209), -1)  # t1215: "cuda:0 bf16[1, 32, 512, 128]"
  t1216 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1216: "cuda:0 f32[1, 32, 512, 128]"
  t1217 = prims.convert_element_type(t1208, dtypes.float32)  # t1217: "cuda:0 f32[1, 32, 512, 128]"
  t1218 = ltorch.mul(t1217, t1216)  # t1218: "cuda:0 f32[1, 32, 512, 128]"
    # t1218 = prims.mul(t1217, t1216)  # t1218: "cuda:0 f32[1, 32, 512, 128]"
  t1219 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1219: "cuda:0 f32[1, 32, 512, 128]"
  t1220 = prims.convert_element_type(t1215, dtypes.float32)  # t1220: "cuda:0 f32[1, 32, 512, 128]"
  t1221 = ltorch.mul(t1220, t1219)  # t1221: "cuda:0 f32[1, 32, 512, 128]"
    # t1221 = prims.mul(t1220, t1219)  # t1221: "cuda:0 f32[1, 32, 512, 128]"
  t1222 = ltorch.add(t1218, t1221, alpha=None)  # t1222: "cuda:0 f32[1, 32, 512, 128]"
    # t1222 = prims.add(t1218, t1221)  # t1222: "cuda:0 f32[1, 32, 512, 128]"
  t1223 = prims.convert_element_type(t1222, dtypes.bfloat16)  # t1223: "cuda:0 bf16[1, 32, 512, 128]"
  t1224 = prims.slice_prim(t1179, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1224: "cuda:0 bf16[1, 32, 512, 0]"
  t1226 = prims.cat((t1207, t1224), -1)  # t1226: "cuda:0 bf16[1, 32, 512, 128]"
  t1227 = prims.slice_prim(t1185, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1227: "cuda:0 bf16[1, 32, 512, 0]"
  t1229 = prims.cat((t1223, t1227), -1)  # t1229: "cuda:0 bf16[1, 32, 512, 128]"
  (t1230, t1231, t1232, t1233) = cudnn_sdpa_fwd(t1226, t1229, t1191, None, 0.0, True, scale=0.08838834764831843)
  t1236 = prims.transpose(t1230, (0, 2, 1, 3))  # t1236: "cuda:0 bf16[1, 512, 32, 128]"
  t1240 = prims.reshape(t1236, (1, 512, 4096))  # t1240: "cuda:0 bf16[1, 512, 4096]"
  t1241 = torch.nn.functional.linear(t1240, t_transformer_h_8_attn_proj_weight, None)  # t1241: "cuda:0 bf16[1, 512, 4096]"
    # t1241 = ltorch.linear(t1240, t_transformer_h_8_attn_proj_weight, None)  # t1241: "cuda:0 bf16[1, 512, 4096]"
      # t1241 = prims.linear(t1240, t_transformer_h_8_attn_proj_weight, None)  # t1241: "cuda:0 bf16[1, 512, 4096]"
  t1242 = prims.convert_element_type(t1241, dtypes.float32)  # t1242: "cuda:0 f32[1, 512, 4096]"
  t1243 = prims.convert_element_type(t1139, dtypes.float32)  # t1243: "cuda:0 f32[1, 512, 4096]"
  t1244 = ltorch.add(t1242, t1243, alpha=None)  # t1244: "cuda:0 f32[1, 512, 4096]"
    # t1244 = prims.add(t1242, t1243)  # t1244: "cuda:0 f32[1, 512, 4096]"
  t1245 = prims.convert_element_type(t1244, dtypes.bfloat16)  # t1245: "cuda:0 bf16[1, 512, 4096]"
  t1246 = prims.convert_element_type(t1245, dtypes.float32)  # t1246: "cuda:0 f32[1, 512, 4096]"
  t1247 = ltorch.mul(t1246, t1246)  # t1247: "cuda:0 f32[1, 512, 4096]"
    # t1247 = prims.mul(t1246, t1246)  # t1247: "cuda:0 f32[1, 512, 4096]"
  t1249 = prims.sum(t1247, (2,))  # t1249: "cuda:0 f32[1, 512]"
  t1250 = prims.broadcast_in_dim(t1249, [1, 512, 1], [0, 1])  # t1250: "cuda:0 f32[1, 512, 1]"
  t1252 = ltorch.true_divide(t1250, 4096.0)  # t1252: "cuda:0 f32[1, 512, 1]"
    # t1252 = prims.div(t1250, 4096.0)  # t1252: "cuda:0 f32[1, 512, 1]"
  t1254 = ltorch.add(t1252, 1e-05, alpha=None)  # t1254: "cuda:0 f32[1, 512, 1]"
    # t1254 = prims.add(t1252, 1e-05)  # t1254: "cuda:0 f32[1, 512, 1]"
  t1255 = prims.rsqrt(t1254)  # t1255: "cuda:0 f32[1, 512, 1]"
  t1256 = prims.broadcast_in_dim(t1255, (1, 512, 4096), (0, 1, 2))  # t1256: "cuda:0 f32[1, 512, 4096]"
  t1257 = ltorch.mul(t1246, t1256)  # t1257: "cuda:0 f32[1, 512, 4096]"
    # t1257 = prims.mul(t1246, t1256)  # t1257: "cuda:0 f32[1, 512, 4096]"
  t1258 = prims.convert_element_type(t1257, dtypes.bfloat16)  # t1258: "cuda:0 bf16[1, 512, 4096]"
  t1259 = prims.broadcast_in_dim(t_transformer_h_8_norm_2_weight, (1, 512, 4096), (2,))  # t1259: "cuda:0 bf16[1, 512, 4096]"
  t1260 = prims.convert_element_type(t1258, dtypes.float32)  # t1260: "cuda:0 f32[1, 512, 4096]"
  t1261 = prims.convert_element_type(t1259, dtypes.float32)  # t1261: "cuda:0 f32[1, 512, 4096]"
  t1262 = ltorch.mul(t1260, t1261)  # t1262: "cuda:0 f32[1, 512, 4096]"
    # t1262 = prims.mul(t1260, t1261)  # t1262: "cuda:0 f32[1, 512, 4096]"
  t1263 = prims.convert_element_type(t1262, dtypes.bfloat16)  # t1263: "cuda:0 bf16[1, 512, 4096]"
  t1264 = torch.nn.functional.linear(t1263, t_transformer_h_8_mlp_fc_1_weight, None)  # t1264: "cuda:0 bf16[1, 512, 11008]"
    # t1264 = ltorch.linear(t1263, t_transformer_h_8_mlp_fc_1_weight, None)  # t1264: "cuda:0 bf16[1, 512, 11008]"
      # t1264 = prims.linear(t1263, t_transformer_h_8_mlp_fc_1_weight, None)  # t1264: "cuda:0 bf16[1, 512, 11008]"
  t1265 = torch.nn.functional.linear(t1263, t_transformer_h_8_mlp_fc_2_weight, None)  # t1265: "cuda:0 bf16[1, 512, 11008]"
    # t1265 = ltorch.linear(t1263, t_transformer_h_8_mlp_fc_2_weight, None)  # t1265: "cuda:0 bf16[1, 512, 11008]"
      # t1265 = prims.linear(t1263, t_transformer_h_8_mlp_fc_2_weight, None)  # t1265: "cuda:0 bf16[1, 512, 11008]"
  t1266 = prims.convert_element_type(t1264, dtypes.float32)  # t1266: "cuda:0 f32[1, 512, 11008]"
  t1267 = prims.neg(t1266)  # t1267: "cuda:0 f32[1, 512, 11008]"
  t1268 = prims.exp(t1267)  # t1268: "cuda:0 f32[1, 512, 11008]"
  t1269 = ltorch.add(1.0, t1268, alpha=None)  # t1269: "cuda:0 f32[1, 512, 11008]"
    # t1269 = prims.add(1.0, t1268)  # t1269: "cuda:0 f32[1, 512, 11008]"
  t1270 = prims.reciprocal(t1269)  # t1270: "cuda:0 f32[1, 512, 11008]"
  t1271 = prims.convert_element_type(t1270, dtypes.bfloat16)  # t1271: "cuda:0 bf16[1, 512, 11008]"
  t1272 = prims.convert_element_type(t1264, dtypes.float32)  # t1272: "cuda:0 f32[1, 512, 11008]"
  t1273 = prims.convert_element_type(t1271, dtypes.float32)  # t1273: "cuda:0 f32[1, 512, 11008]"
  t1274 = ltorch.mul(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 11008]"
    # t1274 = prims.mul(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 11008]"
  t1275 = prims.convert_element_type(t1274, dtypes.bfloat16)  # t1275: "cuda:0 bf16[1, 512, 11008]"
  t1276 = prims.convert_element_type(t1275, dtypes.float32)  # t1276: "cuda:0 f32[1, 512, 11008]"
  t1277 = prims.convert_element_type(t1265, dtypes.float32)  # t1277: "cuda:0 f32[1, 512, 11008]"
  t1278 = ltorch.mul(t1276, t1277)  # t1278: "cuda:0 f32[1, 512, 11008]"
    # t1278 = prims.mul(t1276, t1277)  # t1278: "cuda:0 f32[1, 512, 11008]"
  t1279 = prims.convert_element_type(t1278, dtypes.bfloat16)  # t1279: "cuda:0 bf16[1, 512, 11008]"
  t1280 = torch.nn.functional.linear(t1279, t_transformer_h_8_mlp_proj_weight, None)  # t1280: "cuda:0 bf16[1, 512, 4096]"
    # t1280 = ltorch.linear(t1279, t_transformer_h_8_mlp_proj_weight, None)  # t1280: "cuda:0 bf16[1, 512, 4096]"
      # t1280 = prims.linear(t1279, t_transformer_h_8_mlp_proj_weight, None)  # t1280: "cuda:0 bf16[1, 512, 4096]"
  t1281 = prims.convert_element_type(t1280, dtypes.float32)  # t1281: "cuda:0 f32[1, 512, 4096]"
  t1282 = prims.convert_element_type(t1245, dtypes.float32)  # t1282: "cuda:0 f32[1, 512, 4096]"
  t1283 = ltorch.add(t1281, t1282, alpha=None)  # t1283: "cuda:0 f32[1, 512, 4096]"
    # t1283 = prims.add(t1281, t1282)  # t1283: "cuda:0 f32[1, 512, 4096]"
  t1284 = prims.convert_element_type(t1283, dtypes.bfloat16)  # t1284: "cuda:0 bf16[1, 512, 4096]"
  t1285 = prims.convert_element_type(t1284, dtypes.float32)  # t1285: "cuda:0 f32[1, 512, 4096]"
  t1286 = ltorch.mul(t1285, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"
    # t1286 = prims.mul(t1285, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"
  t1288 = prims.sum(t1286, (2,))  # t1288: "cuda:0 f32[1, 512]"
  t1289 = prims.broadcast_in_dim(t1288, [1, 512, 1], [0, 1])  # t1289: "cuda:0 f32[1, 512, 1]"
  t1291 = ltorch.true_divide(t1289, 4096.0)  # t1291: "cuda:0 f32[1, 512, 1]"
    # t1291 = prims.div(t1289, 4096.0)  # t1291: "cuda:0 f32[1, 512, 1]"
  t1293 = ltorch.add(t1291, 1e-05, alpha=None)  # t1293: "cuda:0 f32[1, 512, 1]"
    # t1293 = prims.add(t1291, 1e-05)  # t1293: "cuda:0 f32[1, 512, 1]"
  t1294 = prims.rsqrt(t1293)  # t1294: "cuda:0 f32[1, 512, 1]"
  t1295 = prims.broadcast_in_dim(t1294, (1, 512, 4096), (0, 1, 2))  # t1295: "cuda:0 f32[1, 512, 4096]"
  t1296 = ltorch.mul(t1285, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
    # t1296 = prims.mul(t1285, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"
  t1297 = prims.convert_element_type(t1296, dtypes.bfloat16)  # t1297: "cuda:0 bf16[1, 512, 4096]"
  t1298 = prims.broadcast_in_dim(t_transformer_h_9_norm_1_weight, (1, 512, 4096), (2,))  # t1298: "cuda:0 bf16[1, 512, 4096]"
  t1299 = prims.convert_element_type(t1297, dtypes.float32)  # t1299: "cuda:0 f32[1, 512, 4096]"
  t1300 = prims.convert_element_type(t1298, dtypes.float32)  # t1300: "cuda:0 f32[1, 512, 4096]"
  t1301 = ltorch.mul(t1299, t1300)  # t1301: "cuda:0 f32[1, 512, 4096]"
    # t1301 = prims.mul(t1299, t1300)  # t1301: "cuda:0 f32[1, 512, 4096]"
  t1302 = prims.convert_element_type(t1301, dtypes.bfloat16)  # t1302: "cuda:0 bf16[1, 512, 4096]"
  t1303 = torch.nn.functional.linear(t1302, t_transformer_h_9_attn_attn_weight, None)  # t1303: "cuda:0 bf16[1, 512, 12288]"
    # t1303 = ltorch.linear(t1302, t_transformer_h_9_attn_attn_weight, None)  # t1303: "cuda:0 bf16[1, 512, 12288]"
      # t1303 = prims.linear(t1302, t_transformer_h_9_attn_attn_weight, None)  # t1303: "cuda:0 bf16[1, 512, 12288]"
  t1309 = prims.reshape(t1303, (1, 512, 32, 3, 128))  # t1309: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1315 = prims.transpose(t1309, (0, 2, 3, 1, 4))  # t1315: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1316, t1317, t1318) = ltorch.split(t1315, (1, 1, 1), 2)
    # t1316 = prims.slice_prim(t1315, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1316: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1317 = prims.slice_prim(t1315, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1317: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1318 = prims.slice_prim(t1315, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1318: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1324 = prims.reshape(t1316, (1, 32, 512, 128))  # t1324: "cuda:0 bf16[1, 32, 512, 128]"
  t1330 = prims.reshape(t1317, (1, 32, 512, 128))  # t1330: "cuda:0 bf16[1, 32, 512, 128]"
  t1336 = prims.reshape(t1318, (1, 32, 512, 128))  # t1336: "cuda:0 bf16[1, 32, 512, 128]"
  t1337 = prims.slice_prim(t1324, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1337: "cuda:0 bf16[1, 32, 512, 128]"
  t1338 = prims.slice_prim(t1337, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1338: "cuda:0 bf16[1, 32, 512, 64]"
  t1339 = prims.slice_prim(t1337, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1339: "cuda:0 bf16[1, 32, 512, 64]"
  t1340 = prims.convert_element_type(t1339, dtypes.float32)  # t1340: "cuda:0 f32[1, 32, 512, 64]"
  t1341 = prims.neg(t1340)  # t1341: "cuda:0 f32[1, 32, 512, 64]"
  t1342 = prims.convert_element_type(t1341, dtypes.bfloat16)  # t1342: "cuda:0 bf16[1, 32, 512, 64]"
  t1344 = prims.cat((t1342, t1338), -1)  # t1344: "cuda:0 bf16[1, 32, 512, 128]"
  t1345 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1345: "cuda:0 f32[1, 32, 512, 128]"
  t1346 = prims.convert_element_type(t1337, dtypes.float32)  # t1346: "cuda:0 f32[1, 32, 512, 128]"
  t1347 = ltorch.mul(t1346, t1345)  # t1347: "cuda:0 f32[1, 32, 512, 128]"
    # t1347 = prims.mul(t1346, t1345)  # t1347: "cuda:0 f32[1, 32, 512, 128]"
  t1348 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1348: "cuda:0 f32[1, 32, 512, 128]"
  t1349 = prims.convert_element_type(t1344, dtypes.float32)  # t1349: "cuda:0 f32[1, 32, 512, 128]"
  t1350 = ltorch.mul(t1349, t1348)  # t1350: "cuda:0 f32[1, 32, 512, 128]"
    # t1350 = prims.mul(t1349, t1348)  # t1350: "cuda:0 f32[1, 32, 512, 128]"
  t1351 = ltorch.add(t1347, t1350, alpha=None)  # t1351: "cuda:0 f32[1, 32, 512, 128]"
    # t1351 = prims.add(t1347, t1350)  # t1351: "cuda:0 f32[1, 32, 512, 128]"
  t1352 = prims.convert_element_type(t1351, dtypes.bfloat16)  # t1352: "cuda:0 bf16[1, 32, 512, 128]"
  t1353 = prims.slice_prim(t1330, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1353: "cuda:0 bf16[1, 32, 512, 128]"
  t1354 = prims.slice_prim(t1353, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1354: "cuda:0 bf16[1, 32, 512, 64]"
  t1355 = prims.slice_prim(t1353, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1355: "cuda:0 bf16[1, 32, 512, 64]"
  t1356 = prims.convert_element_type(t1355, dtypes.float32)  # t1356: "cuda:0 f32[1, 32, 512, 64]"
  t1357 = prims.neg(t1356)  # t1357: "cuda:0 f32[1, 32, 512, 64]"
  t1358 = prims.convert_element_type(t1357, dtypes.bfloat16)  # t1358: "cuda:0 bf16[1, 32, 512, 64]"
  t1360 = prims.cat((t1358, t1354), -1)  # t1360: "cuda:0 bf16[1, 32, 512, 128]"
  t1361 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1361: "cuda:0 f32[1, 32, 512, 128]"
  t1362 = prims.convert_element_type(t1353, dtypes.float32)  # t1362: "cuda:0 f32[1, 32, 512, 128]"
  t1363 = ltorch.mul(t1362, t1361)  # t1363: "cuda:0 f32[1, 32, 512, 128]"
    # t1363 = prims.mul(t1362, t1361)  # t1363: "cuda:0 f32[1, 32, 512, 128]"
  t1364 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1364: "cuda:0 f32[1, 32, 512, 128]"
  t1365 = prims.convert_element_type(t1360, dtypes.float32)  # t1365: "cuda:0 f32[1, 32, 512, 128]"
  t1366 = ltorch.mul(t1365, t1364)  # t1366: "cuda:0 f32[1, 32, 512, 128]"
    # t1366 = prims.mul(t1365, t1364)  # t1366: "cuda:0 f32[1, 32, 512, 128]"
  t1367 = ltorch.add(t1363, t1366, alpha=None)  # t1367: "cuda:0 f32[1, 32, 512, 128]"
    # t1367 = prims.add(t1363, t1366)  # t1367: "cuda:0 f32[1, 32, 512, 128]"
  t1368 = prims.convert_element_type(t1367, dtypes.bfloat16)  # t1368: "cuda:0 bf16[1, 32, 512, 128]"
  t1369 = prims.slice_prim(t1324, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1369: "cuda:0 bf16[1, 32, 512, 0]"
  t1371 = prims.cat((t1352, t1369), -1)  # t1371: "cuda:0 bf16[1, 32, 512, 128]"
  t1372 = prims.slice_prim(t1330, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1372: "cuda:0 bf16[1, 32, 512, 0]"
  t1374 = prims.cat((t1368, t1372), -1)  # t1374: "cuda:0 bf16[1, 32, 512, 128]"
  (t1375, t1376, t1377, t1378) = cudnn_sdpa_fwd(t1371, t1374, t1336, None, 0.0, True, scale=0.08838834764831843)
  t1381 = prims.transpose(t1375, (0, 2, 1, 3))  # t1381: "cuda:0 bf16[1, 512, 32, 128]"
  t1385 = prims.reshape(t1381, (1, 512, 4096))  # t1385: "cuda:0 bf16[1, 512, 4096]"
  t1386 = torch.nn.functional.linear(t1385, t_transformer_h_9_attn_proj_weight, None)  # t1386: "cuda:0 bf16[1, 512, 4096]"
    # t1386 = ltorch.linear(t1385, t_transformer_h_9_attn_proj_weight, None)  # t1386: "cuda:0 bf16[1, 512, 4096]"
      # t1386 = prims.linear(t1385, t_transformer_h_9_attn_proj_weight, None)  # t1386: "cuda:0 bf16[1, 512, 4096]"
  t1387 = prims.convert_element_type(t1386, dtypes.float32)  # t1387: "cuda:0 f32[1, 512, 4096]"
  t1388 = prims.convert_element_type(t1284, dtypes.float32)  # t1388: "cuda:0 f32[1, 512, 4096]"
  t1389 = ltorch.add(t1387, t1388, alpha=None)  # t1389: "cuda:0 f32[1, 512, 4096]"
    # t1389 = prims.add(t1387, t1388)  # t1389: "cuda:0 f32[1, 512, 4096]"
  t1390 = prims.convert_element_type(t1389, dtypes.bfloat16)  # t1390: "cuda:0 bf16[1, 512, 4096]"
  t1391 = prims.convert_element_type(t1390, dtypes.float32)  # t1391: "cuda:0 f32[1, 512, 4096]"
  t1392 = ltorch.mul(t1391, t1391)  # t1392: "cuda:0 f32[1, 512, 4096]"
    # t1392 = prims.mul(t1391, t1391)  # t1392: "cuda:0 f32[1, 512, 4096]"
  t1394 = prims.sum(t1392, (2,))  # t1394: "cuda:0 f32[1, 512]"
  t1395 = prims.broadcast_in_dim(t1394, [1, 512, 1], [0, 1])  # t1395: "cuda:0 f32[1, 512, 1]"
  t1397 = ltorch.true_divide(t1395, 4096.0)  # t1397: "cuda:0 f32[1, 512, 1]"
    # t1397 = prims.div(t1395, 4096.0)  # t1397: "cuda:0 f32[1, 512, 1]"
  t1399 = ltorch.add(t1397, 1e-05, alpha=None)  # t1399: "cuda:0 f32[1, 512, 1]"
    # t1399 = prims.add(t1397, 1e-05)  # t1399: "cuda:0 f32[1, 512, 1]"
  t1400 = prims.rsqrt(t1399)  # t1400: "cuda:0 f32[1, 512, 1]"
  t1401 = prims.broadcast_in_dim(t1400, (1, 512, 4096), (0, 1, 2))  # t1401: "cuda:0 f32[1, 512, 4096]"
  t1402 = ltorch.mul(t1391, t1401)  # t1402: "cuda:0 f32[1, 512, 4096]"
    # t1402 = prims.mul(t1391, t1401)  # t1402: "cuda:0 f32[1, 512, 4096]"
  t1403 = prims.convert_element_type(t1402, dtypes.bfloat16)  # t1403: "cuda:0 bf16[1, 512, 4096]"
  t1404 = prims.broadcast_in_dim(t_transformer_h_9_norm_2_weight, (1, 512, 4096), (2,))  # t1404: "cuda:0 bf16[1, 512, 4096]"
  t1405 = prims.convert_element_type(t1403, dtypes.float32)  # t1405: "cuda:0 f32[1, 512, 4096]"
  t1406 = prims.convert_element_type(t1404, dtypes.float32)  # t1406: "cuda:0 f32[1, 512, 4096]"
  t1407 = ltorch.mul(t1405, t1406)  # t1407: "cuda:0 f32[1, 512, 4096]"
    # t1407 = prims.mul(t1405, t1406)  # t1407: "cuda:0 f32[1, 512, 4096]"
  t1408 = prims.convert_element_type(t1407, dtypes.bfloat16)  # t1408: "cuda:0 bf16[1, 512, 4096]"
  t1409 = torch.nn.functional.linear(t1408, t_transformer_h_9_mlp_fc_1_weight, None)  # t1409: "cuda:0 bf16[1, 512, 11008]"
    # t1409 = ltorch.linear(t1408, t_transformer_h_9_mlp_fc_1_weight, None)  # t1409: "cuda:0 bf16[1, 512, 11008]"
      # t1409 = prims.linear(t1408, t_transformer_h_9_mlp_fc_1_weight, None)  # t1409: "cuda:0 bf16[1, 512, 11008]"
  t1410 = torch.nn.functional.linear(t1408, t_transformer_h_9_mlp_fc_2_weight, None)  # t1410: "cuda:0 bf16[1, 512, 11008]"
    # t1410 = ltorch.linear(t1408, t_transformer_h_9_mlp_fc_2_weight, None)  # t1410: "cuda:0 bf16[1, 512, 11008]"
      # t1410 = prims.linear(t1408, t_transformer_h_9_mlp_fc_2_weight, None)  # t1410: "cuda:0 bf16[1, 512, 11008]"
  t1411 = prims.convert_element_type(t1409, dtypes.float32)  # t1411: "cuda:0 f32[1, 512, 11008]"
  t1412 = prims.neg(t1411)  # t1412: "cuda:0 f32[1, 512, 11008]"
  t1413 = prims.exp(t1412)  # t1413: "cuda:0 f32[1, 512, 11008]"
  t1414 = ltorch.add(1.0, t1413, alpha=None)  # t1414: "cuda:0 f32[1, 512, 11008]"
    # t1414 = prims.add(1.0, t1413)  # t1414: "cuda:0 f32[1, 512, 11008]"
  t1415 = prims.reciprocal(t1414)  # t1415: "cuda:0 f32[1, 512, 11008]"
  t1416 = prims.convert_element_type(t1415, dtypes.bfloat16)  # t1416: "cuda:0 bf16[1, 512, 11008]"
  t1417 = prims.convert_element_type(t1409, dtypes.float32)  # t1417: "cuda:0 f32[1, 512, 11008]"
  t1418 = prims.convert_element_type(t1416, dtypes.float32)  # t1418: "cuda:0 f32[1, 512, 11008]"
  t1419 = ltorch.mul(t1417, t1418)  # t1419: "cuda:0 f32[1, 512, 11008]"
    # t1419 = prims.mul(t1417, t1418)  # t1419: "cuda:0 f32[1, 512, 11008]"
  t1420 = prims.convert_element_type(t1419, dtypes.bfloat16)  # t1420: "cuda:0 bf16[1, 512, 11008]"
  t1421 = prims.convert_element_type(t1420, dtypes.float32)  # t1421: "cuda:0 f32[1, 512, 11008]"
  t1422 = prims.convert_element_type(t1410, dtypes.float32)  # t1422: "cuda:0 f32[1, 512, 11008]"
  t1423 = ltorch.mul(t1421, t1422)  # t1423: "cuda:0 f32[1, 512, 11008]"
    # t1423 = prims.mul(t1421, t1422)  # t1423: "cuda:0 f32[1, 512, 11008]"
  t1424 = prims.convert_element_type(t1423, dtypes.bfloat16)  # t1424: "cuda:0 bf16[1, 512, 11008]"
  t1425 = torch.nn.functional.linear(t1424, t_transformer_h_9_mlp_proj_weight, None)  # t1425: "cuda:0 bf16[1, 512, 4096]"
    # t1425 = ltorch.linear(t1424, t_transformer_h_9_mlp_proj_weight, None)  # t1425: "cuda:0 bf16[1, 512, 4096]"
      # t1425 = prims.linear(t1424, t_transformer_h_9_mlp_proj_weight, None)  # t1425: "cuda:0 bf16[1, 512, 4096]"
  t1426 = prims.convert_element_type(t1425, dtypes.float32)  # t1426: "cuda:0 f32[1, 512, 4096]"
  t1427 = prims.convert_element_type(t1390, dtypes.float32)  # t1427: "cuda:0 f32[1, 512, 4096]"
  t1428 = ltorch.add(t1426, t1427, alpha=None)  # t1428: "cuda:0 f32[1, 512, 4096]"
    # t1428 = prims.add(t1426, t1427)  # t1428: "cuda:0 f32[1, 512, 4096]"
  t1429 = prims.convert_element_type(t1428, dtypes.bfloat16)  # t1429: "cuda:0 bf16[1, 512, 4096]"
  t1430 = prims.convert_element_type(t1429, dtypes.float32)  # t1430: "cuda:0 f32[1, 512, 4096]"
  t1431 = ltorch.mul(t1430, t1430)  # t1431: "cuda:0 f32[1, 512, 4096]"
    # t1431 = prims.mul(t1430, t1430)  # t1431: "cuda:0 f32[1, 512, 4096]"
  t1433 = prims.sum(t1431, (2,))  # t1433: "cuda:0 f32[1, 512]"
  t1434 = prims.broadcast_in_dim(t1433, [1, 512, 1], [0, 1])  # t1434: "cuda:0 f32[1, 512, 1]"
  t1436 = ltorch.true_divide(t1434, 4096.0)  # t1436: "cuda:0 f32[1, 512, 1]"
    # t1436 = prims.div(t1434, 4096.0)  # t1436: "cuda:0 f32[1, 512, 1]"
  t1438 = ltorch.add(t1436, 1e-05, alpha=None)  # t1438: "cuda:0 f32[1, 512, 1]"
    # t1438 = prims.add(t1436, 1e-05)  # t1438: "cuda:0 f32[1, 512, 1]"
  t1439 = prims.rsqrt(t1438)  # t1439: "cuda:0 f32[1, 512, 1]"
  t1440 = prims.broadcast_in_dim(t1439, (1, 512, 4096), (0, 1, 2))  # t1440: "cuda:0 f32[1, 512, 4096]"
  t1441 = ltorch.mul(t1430, t1440)  # t1441: "cuda:0 f32[1, 512, 4096]"
    # t1441 = prims.mul(t1430, t1440)  # t1441: "cuda:0 f32[1, 512, 4096]"
  t1442 = prims.convert_element_type(t1441, dtypes.bfloat16)  # t1442: "cuda:0 bf16[1, 512, 4096]"
  t1443 = prims.broadcast_in_dim(t_transformer_h_10_norm_1_weight, (1, 512, 4096), (2,))  # t1443: "cuda:0 bf16[1, 512, 4096]"
  t1444 = prims.convert_element_type(t1442, dtypes.float32)  # t1444: "cuda:0 f32[1, 512, 4096]"
  t1445 = prims.convert_element_type(t1443, dtypes.float32)  # t1445: "cuda:0 f32[1, 512, 4096]"
  t1446 = ltorch.mul(t1444, t1445)  # t1446: "cuda:0 f32[1, 512, 4096]"
    # t1446 = prims.mul(t1444, t1445)  # t1446: "cuda:0 f32[1, 512, 4096]"
  t1447 = prims.convert_element_type(t1446, dtypes.bfloat16)  # t1447: "cuda:0 bf16[1, 512, 4096]"
  t1448 = torch.nn.functional.linear(t1447, t_transformer_h_10_attn_attn_weight, None)  # t1448: "cuda:0 bf16[1, 512, 12288]"
    # t1448 = ltorch.linear(t1447, t_transformer_h_10_attn_attn_weight, None)  # t1448: "cuda:0 bf16[1, 512, 12288]"
      # t1448 = prims.linear(t1447, t_transformer_h_10_attn_attn_weight, None)  # t1448: "cuda:0 bf16[1, 512, 12288]"
  t1454 = prims.reshape(t1448, (1, 512, 32, 3, 128))  # t1454: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1460 = prims.transpose(t1454, (0, 2, 3, 1, 4))  # t1460: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1461, t1462, t1463) = ltorch.split(t1460, (1, 1, 1), 2)
    # t1461 = prims.slice_prim(t1460, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1461: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1462 = prims.slice_prim(t1460, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1462: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1463 = prims.slice_prim(t1460, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1463: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1469 = prims.reshape(t1461, (1, 32, 512, 128))  # t1469: "cuda:0 bf16[1, 32, 512, 128]"
  t1475 = prims.reshape(t1462, (1, 32, 512, 128))  # t1475: "cuda:0 bf16[1, 32, 512, 128]"
  t1481 = prims.reshape(t1463, (1, 32, 512, 128))  # t1481: "cuda:0 bf16[1, 32, 512, 128]"
  t1482 = prims.slice_prim(t1469, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1482: "cuda:0 bf16[1, 32, 512, 128]"
  t1483 = prims.slice_prim(t1482, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1483: "cuda:0 bf16[1, 32, 512, 64]"
  t1484 = prims.slice_prim(t1482, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1484: "cuda:0 bf16[1, 32, 512, 64]"
  t1485 = prims.convert_element_type(t1484, dtypes.float32)  # t1485: "cuda:0 f32[1, 32, 512, 64]"
  t1486 = prims.neg(t1485)  # t1486: "cuda:0 f32[1, 32, 512, 64]"
  t1487 = prims.convert_element_type(t1486, dtypes.bfloat16)  # t1487: "cuda:0 bf16[1, 32, 512, 64]"
  t1489 = prims.cat((t1487, t1483), -1)  # t1489: "cuda:0 bf16[1, 32, 512, 128]"
  t1490 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1490: "cuda:0 f32[1, 32, 512, 128]"
  t1491 = prims.convert_element_type(t1482, dtypes.float32)  # t1491: "cuda:0 f32[1, 32, 512, 128]"
  t1492 = ltorch.mul(t1491, t1490)  # t1492: "cuda:0 f32[1, 32, 512, 128]"
    # t1492 = prims.mul(t1491, t1490)  # t1492: "cuda:0 f32[1, 32, 512, 128]"
  t1493 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1493: "cuda:0 f32[1, 32, 512, 128]"
  t1494 = prims.convert_element_type(t1489, dtypes.float32)  # t1494: "cuda:0 f32[1, 32, 512, 128]"
  t1495 = ltorch.mul(t1494, t1493)  # t1495: "cuda:0 f32[1, 32, 512, 128]"
    # t1495 = prims.mul(t1494, t1493)  # t1495: "cuda:0 f32[1, 32, 512, 128]"
  t1496 = ltorch.add(t1492, t1495, alpha=None)  # t1496: "cuda:0 f32[1, 32, 512, 128]"
    # t1496 = prims.add(t1492, t1495)  # t1496: "cuda:0 f32[1, 32, 512, 128]"
  t1497 = prims.convert_element_type(t1496, dtypes.bfloat16)  # t1497: "cuda:0 bf16[1, 32, 512, 128]"
  t1498 = prims.slice_prim(t1475, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1498: "cuda:0 bf16[1, 32, 512, 128]"
  t1499 = prims.slice_prim(t1498, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1499: "cuda:0 bf16[1, 32, 512, 64]"
  t1500 = prims.slice_prim(t1498, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1500: "cuda:0 bf16[1, 32, 512, 64]"
  t1501 = prims.convert_element_type(t1500, dtypes.float32)  # t1501: "cuda:0 f32[1, 32, 512, 64]"
  t1502 = prims.neg(t1501)  # t1502: "cuda:0 f32[1, 32, 512, 64]"
  t1503 = prims.convert_element_type(t1502, dtypes.bfloat16)  # t1503: "cuda:0 bf16[1, 32, 512, 64]"
  t1505 = prims.cat((t1503, t1499), -1)  # t1505: "cuda:0 bf16[1, 32, 512, 128]"
  t1506 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1506: "cuda:0 f32[1, 32, 512, 128]"
  t1507 = prims.convert_element_type(t1498, dtypes.float32)  # t1507: "cuda:0 f32[1, 32, 512, 128]"
  t1508 = ltorch.mul(t1507, t1506)  # t1508: "cuda:0 f32[1, 32, 512, 128]"
    # t1508 = prims.mul(t1507, t1506)  # t1508: "cuda:0 f32[1, 32, 512, 128]"
  t1509 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1509: "cuda:0 f32[1, 32, 512, 128]"
  t1510 = prims.convert_element_type(t1505, dtypes.float32)  # t1510: "cuda:0 f32[1, 32, 512, 128]"
  t1511 = ltorch.mul(t1510, t1509)  # t1511: "cuda:0 f32[1, 32, 512, 128]"
    # t1511 = prims.mul(t1510, t1509)  # t1511: "cuda:0 f32[1, 32, 512, 128]"
  t1512 = ltorch.add(t1508, t1511, alpha=None)  # t1512: "cuda:0 f32[1, 32, 512, 128]"
    # t1512 = prims.add(t1508, t1511)  # t1512: "cuda:0 f32[1, 32, 512, 128]"
  t1513 = prims.convert_element_type(t1512, dtypes.bfloat16)  # t1513: "cuda:0 bf16[1, 32, 512, 128]"
  t1514 = prims.slice_prim(t1469, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1514: "cuda:0 bf16[1, 32, 512, 0]"
  t1516 = prims.cat((t1497, t1514), -1)  # t1516: "cuda:0 bf16[1, 32, 512, 128]"
  t1517 = prims.slice_prim(t1475, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1517: "cuda:0 bf16[1, 32, 512, 0]"
  t1519 = prims.cat((t1513, t1517), -1)  # t1519: "cuda:0 bf16[1, 32, 512, 128]"
  (t1520, t1521, t1522, t1523) = cudnn_sdpa_fwd(t1516, t1519, t1481, None, 0.0, True, scale=0.08838834764831843)
  t1526 = prims.transpose(t1520, (0, 2, 1, 3))  # t1526: "cuda:0 bf16[1, 512, 32, 128]"
  t1530 = prims.reshape(t1526, (1, 512, 4096))  # t1530: "cuda:0 bf16[1, 512, 4096]"
  t1531 = torch.nn.functional.linear(t1530, t_transformer_h_10_attn_proj_weight, None)  # t1531: "cuda:0 bf16[1, 512, 4096]"
    # t1531 = ltorch.linear(t1530, t_transformer_h_10_attn_proj_weight, None)  # t1531: "cuda:0 bf16[1, 512, 4096]"
      # t1531 = prims.linear(t1530, t_transformer_h_10_attn_proj_weight, None)  # t1531: "cuda:0 bf16[1, 512, 4096]"
  t1532 = prims.convert_element_type(t1531, dtypes.float32)  # t1532: "cuda:0 f32[1, 512, 4096]"
  t1533 = prims.convert_element_type(t1429, dtypes.float32)  # t1533: "cuda:0 f32[1, 512, 4096]"
  t1534 = ltorch.add(t1532, t1533, alpha=None)  # t1534: "cuda:0 f32[1, 512, 4096]"
    # t1534 = prims.add(t1532, t1533)  # t1534: "cuda:0 f32[1, 512, 4096]"
  t1535 = prims.convert_element_type(t1534, dtypes.bfloat16)  # t1535: "cuda:0 bf16[1, 512, 4096]"
  t1536 = prims.convert_element_type(t1535, dtypes.float32)  # t1536: "cuda:0 f32[1, 512, 4096]"
  t1537 = ltorch.mul(t1536, t1536)  # t1537: "cuda:0 f32[1, 512, 4096]"
    # t1537 = prims.mul(t1536, t1536)  # t1537: "cuda:0 f32[1, 512, 4096]"
  t1539 = prims.sum(t1537, (2,))  # t1539: "cuda:0 f32[1, 512]"
  t1540 = prims.broadcast_in_dim(t1539, [1, 512, 1], [0, 1])  # t1540: "cuda:0 f32[1, 512, 1]"
  t1542 = ltorch.true_divide(t1540, 4096.0)  # t1542: "cuda:0 f32[1, 512, 1]"
    # t1542 = prims.div(t1540, 4096.0)  # t1542: "cuda:0 f32[1, 512, 1]"
  t1544 = ltorch.add(t1542, 1e-05, alpha=None)  # t1544: "cuda:0 f32[1, 512, 1]"
    # t1544 = prims.add(t1542, 1e-05)  # t1544: "cuda:0 f32[1, 512, 1]"
  t1545 = prims.rsqrt(t1544)  # t1545: "cuda:0 f32[1, 512, 1]"
  t1546 = prims.broadcast_in_dim(t1545, (1, 512, 4096), (0, 1, 2))  # t1546: "cuda:0 f32[1, 512, 4096]"
  t1547 = ltorch.mul(t1536, t1546)  # t1547: "cuda:0 f32[1, 512, 4096]"
    # t1547 = prims.mul(t1536, t1546)  # t1547: "cuda:0 f32[1, 512, 4096]"
  t1548 = prims.convert_element_type(t1547, dtypes.bfloat16)  # t1548: "cuda:0 bf16[1, 512, 4096]"
  t1549 = prims.broadcast_in_dim(t_transformer_h_10_norm_2_weight, (1, 512, 4096), (2,))  # t1549: "cuda:0 bf16[1, 512, 4096]"
  t1550 = prims.convert_element_type(t1548, dtypes.float32)  # t1550: "cuda:0 f32[1, 512, 4096]"
  t1551 = prims.convert_element_type(t1549, dtypes.float32)  # t1551: "cuda:0 f32[1, 512, 4096]"
  t1552 = ltorch.mul(t1550, t1551)  # t1552: "cuda:0 f32[1, 512, 4096]"
    # t1552 = prims.mul(t1550, t1551)  # t1552: "cuda:0 f32[1, 512, 4096]"
  t1553 = prims.convert_element_type(t1552, dtypes.bfloat16)  # t1553: "cuda:0 bf16[1, 512, 4096]"
  t1554 = torch.nn.functional.linear(t1553, t_transformer_h_10_mlp_fc_1_weight, None)  # t1554: "cuda:0 bf16[1, 512, 11008]"
    # t1554 = ltorch.linear(t1553, t_transformer_h_10_mlp_fc_1_weight, None)  # t1554: "cuda:0 bf16[1, 512, 11008]"
      # t1554 = prims.linear(t1553, t_transformer_h_10_mlp_fc_1_weight, None)  # t1554: "cuda:0 bf16[1, 512, 11008]"
  t1555 = torch.nn.functional.linear(t1553, t_transformer_h_10_mlp_fc_2_weight, None)  # t1555: "cuda:0 bf16[1, 512, 11008]"
    # t1555 = ltorch.linear(t1553, t_transformer_h_10_mlp_fc_2_weight, None)  # t1555: "cuda:0 bf16[1, 512, 11008]"
      # t1555 = prims.linear(t1553, t_transformer_h_10_mlp_fc_2_weight, None)  # t1555: "cuda:0 bf16[1, 512, 11008]"
  t1556 = prims.convert_element_type(t1554, dtypes.float32)  # t1556: "cuda:0 f32[1, 512, 11008]"
  t1557 = prims.neg(t1556)  # t1557: "cuda:0 f32[1, 512, 11008]"
  t1558 = prims.exp(t1557)  # t1558: "cuda:0 f32[1, 512, 11008]"
  t1559 = ltorch.add(1.0, t1558, alpha=None)  # t1559: "cuda:0 f32[1, 512, 11008]"
    # t1559 = prims.add(1.0, t1558)  # t1559: "cuda:0 f32[1, 512, 11008]"
  t1560 = prims.reciprocal(t1559)  # t1560: "cuda:0 f32[1, 512, 11008]"
  t1561 = prims.convert_element_type(t1560, dtypes.bfloat16)  # t1561: "cuda:0 bf16[1, 512, 11008]"
  t1562 = prims.convert_element_type(t1554, dtypes.float32)  # t1562: "cuda:0 f32[1, 512, 11008]"
  t1563 = prims.convert_element_type(t1561, dtypes.float32)  # t1563: "cuda:0 f32[1, 512, 11008]"
  t1564 = ltorch.mul(t1562, t1563)  # t1564: "cuda:0 f32[1, 512, 11008]"
    # t1564 = prims.mul(t1562, t1563)  # t1564: "cuda:0 f32[1, 512, 11008]"
  t1565 = prims.convert_element_type(t1564, dtypes.bfloat16)  # t1565: "cuda:0 bf16[1, 512, 11008]"
  t1566 = prims.convert_element_type(t1565, dtypes.float32)  # t1566: "cuda:0 f32[1, 512, 11008]"
  t1567 = prims.convert_element_type(t1555, dtypes.float32)  # t1567: "cuda:0 f32[1, 512, 11008]"
  t1568 = ltorch.mul(t1566, t1567)  # t1568: "cuda:0 f32[1, 512, 11008]"
    # t1568 = prims.mul(t1566, t1567)  # t1568: "cuda:0 f32[1, 512, 11008]"
  t1569 = prims.convert_element_type(t1568, dtypes.bfloat16)  # t1569: "cuda:0 bf16[1, 512, 11008]"
  t1570 = torch.nn.functional.linear(t1569, t_transformer_h_10_mlp_proj_weight, None)  # t1570: "cuda:0 bf16[1, 512, 4096]"
    # t1570 = ltorch.linear(t1569, t_transformer_h_10_mlp_proj_weight, None)  # t1570: "cuda:0 bf16[1, 512, 4096]"
      # t1570 = prims.linear(t1569, t_transformer_h_10_mlp_proj_weight, None)  # t1570: "cuda:0 bf16[1, 512, 4096]"
  t1571 = prims.convert_element_type(t1570, dtypes.float32)  # t1571: "cuda:0 f32[1, 512, 4096]"
  t1572 = prims.convert_element_type(t1535, dtypes.float32)  # t1572: "cuda:0 f32[1, 512, 4096]"
  t1573 = ltorch.add(t1571, t1572, alpha=None)  # t1573: "cuda:0 f32[1, 512, 4096]"
    # t1573 = prims.add(t1571, t1572)  # t1573: "cuda:0 f32[1, 512, 4096]"
  t1574 = prims.convert_element_type(t1573, dtypes.bfloat16)  # t1574: "cuda:0 bf16[1, 512, 4096]"
  t1575 = prims.convert_element_type(t1574, dtypes.float32)  # t1575: "cuda:0 f32[1, 512, 4096]"
  t1576 = ltorch.mul(t1575, t1575)  # t1576: "cuda:0 f32[1, 512, 4096]"
    # t1576 = prims.mul(t1575, t1575)  # t1576: "cuda:0 f32[1, 512, 4096]"
  t1578 = prims.sum(t1576, (2,))  # t1578: "cuda:0 f32[1, 512]"
  t1579 = prims.broadcast_in_dim(t1578, [1, 512, 1], [0, 1])  # t1579: "cuda:0 f32[1, 512, 1]"
  t1581 = ltorch.true_divide(t1579, 4096.0)  # t1581: "cuda:0 f32[1, 512, 1]"
    # t1581 = prims.div(t1579, 4096.0)  # t1581: "cuda:0 f32[1, 512, 1]"
  t1583 = ltorch.add(t1581, 1e-05, alpha=None)  # t1583: "cuda:0 f32[1, 512, 1]"
    # t1583 = prims.add(t1581, 1e-05)  # t1583: "cuda:0 f32[1, 512, 1]"
  t1584 = prims.rsqrt(t1583)  # t1584: "cuda:0 f32[1, 512, 1]"
  t1585 = prims.broadcast_in_dim(t1584, (1, 512, 4096), (0, 1, 2))  # t1585: "cuda:0 f32[1, 512, 4096]"
  t1586 = ltorch.mul(t1575, t1585)  # t1586: "cuda:0 f32[1, 512, 4096]"
    # t1586 = prims.mul(t1575, t1585)  # t1586: "cuda:0 f32[1, 512, 4096]"
  t1587 = prims.convert_element_type(t1586, dtypes.bfloat16)  # t1587: "cuda:0 bf16[1, 512, 4096]"
  t1588 = prims.broadcast_in_dim(t_transformer_h_11_norm_1_weight, (1, 512, 4096), (2,))  # t1588: "cuda:0 bf16[1, 512, 4096]"
  t1589 = prims.convert_element_type(t1587, dtypes.float32)  # t1589: "cuda:0 f32[1, 512, 4096]"
  t1590 = prims.convert_element_type(t1588, dtypes.float32)  # t1590: "cuda:0 f32[1, 512, 4096]"
  t1591 = ltorch.mul(t1589, t1590)  # t1591: "cuda:0 f32[1, 512, 4096]"
    # t1591 = prims.mul(t1589, t1590)  # t1591: "cuda:0 f32[1, 512, 4096]"
  t1592 = prims.convert_element_type(t1591, dtypes.bfloat16)  # t1592: "cuda:0 bf16[1, 512, 4096]"
  t1593 = torch.nn.functional.linear(t1592, t_transformer_h_11_attn_attn_weight, None)  # t1593: "cuda:0 bf16[1, 512, 12288]"
    # t1593 = ltorch.linear(t1592, t_transformer_h_11_attn_attn_weight, None)  # t1593: "cuda:0 bf16[1, 512, 12288]"
      # t1593 = prims.linear(t1592, t_transformer_h_11_attn_attn_weight, None)  # t1593: "cuda:0 bf16[1, 512, 12288]"
  t1599 = prims.reshape(t1593, (1, 512, 32, 3, 128))  # t1599: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1605 = prims.transpose(t1599, (0, 2, 3, 1, 4))  # t1605: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1606, t1607, t1608) = ltorch.split(t1605, (1, 1, 1), 2)
    # t1606 = prims.slice_prim(t1605, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1606: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1607 = prims.slice_prim(t1605, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1607: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1608 = prims.slice_prim(t1605, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1608: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1614 = prims.reshape(t1606, (1, 32, 512, 128))  # t1614: "cuda:0 bf16[1, 32, 512, 128]"
  t1620 = prims.reshape(t1607, (1, 32, 512, 128))  # t1620: "cuda:0 bf16[1, 32, 512, 128]"
  t1626 = prims.reshape(t1608, (1, 32, 512, 128))  # t1626: "cuda:0 bf16[1, 32, 512, 128]"
  t1627 = prims.slice_prim(t1614, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1627: "cuda:0 bf16[1, 32, 512, 128]"
  t1628 = prims.slice_prim(t1627, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1628: "cuda:0 bf16[1, 32, 512, 64]"
  t1629 = prims.slice_prim(t1627, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1629: "cuda:0 bf16[1, 32, 512, 64]"
  t1630 = prims.convert_element_type(t1629, dtypes.float32)  # t1630: "cuda:0 f32[1, 32, 512, 64]"
  t1631 = prims.neg(t1630)  # t1631: "cuda:0 f32[1, 32, 512, 64]"
  t1632 = prims.convert_element_type(t1631, dtypes.bfloat16)  # t1632: "cuda:0 bf16[1, 32, 512, 64]"
  t1634 = prims.cat((t1632, t1628), -1)  # t1634: "cuda:0 bf16[1, 32, 512, 128]"
  t1635 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1635: "cuda:0 f32[1, 32, 512, 128]"
  t1636 = prims.convert_element_type(t1627, dtypes.float32)  # t1636: "cuda:0 f32[1, 32, 512, 128]"
  t1637 = ltorch.mul(t1636, t1635)  # t1637: "cuda:0 f32[1, 32, 512, 128]"
    # t1637 = prims.mul(t1636, t1635)  # t1637: "cuda:0 f32[1, 32, 512, 128]"
  t1638 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1638: "cuda:0 f32[1, 32, 512, 128]"
  t1639 = prims.convert_element_type(t1634, dtypes.float32)  # t1639: "cuda:0 f32[1, 32, 512, 128]"
  t1640 = ltorch.mul(t1639, t1638)  # t1640: "cuda:0 f32[1, 32, 512, 128]"
    # t1640 = prims.mul(t1639, t1638)  # t1640: "cuda:0 f32[1, 32, 512, 128]"
  t1641 = ltorch.add(t1637, t1640, alpha=None)  # t1641: "cuda:0 f32[1, 32, 512, 128]"
    # t1641 = prims.add(t1637, t1640)  # t1641: "cuda:0 f32[1, 32, 512, 128]"
  t1642 = prims.convert_element_type(t1641, dtypes.bfloat16)  # t1642: "cuda:0 bf16[1, 32, 512, 128]"
  t1643 = prims.slice_prim(t1620, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1643: "cuda:0 bf16[1, 32, 512, 128]"
  t1644 = prims.slice_prim(t1643, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1644: "cuda:0 bf16[1, 32, 512, 64]"
  t1645 = prims.slice_prim(t1643, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1645: "cuda:0 bf16[1, 32, 512, 64]"
  t1646 = prims.convert_element_type(t1645, dtypes.float32)  # t1646: "cuda:0 f32[1, 32, 512, 64]"
  t1647 = prims.neg(t1646)  # t1647: "cuda:0 f32[1, 32, 512, 64]"
  t1648 = prims.convert_element_type(t1647, dtypes.bfloat16)  # t1648: "cuda:0 bf16[1, 32, 512, 64]"
  t1650 = prims.cat((t1648, t1644), -1)  # t1650: "cuda:0 bf16[1, 32, 512, 128]"
  t1651 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1651: "cuda:0 f32[1, 32, 512, 128]"
  t1652 = prims.convert_element_type(t1643, dtypes.float32)  # t1652: "cuda:0 f32[1, 32, 512, 128]"
  t1653 = ltorch.mul(t1652, t1651)  # t1653: "cuda:0 f32[1, 32, 512, 128]"
    # t1653 = prims.mul(t1652, t1651)  # t1653: "cuda:0 f32[1, 32, 512, 128]"
  t1654 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1654: "cuda:0 f32[1, 32, 512, 128]"
  t1655 = prims.convert_element_type(t1650, dtypes.float32)  # t1655: "cuda:0 f32[1, 32, 512, 128]"
  t1656 = ltorch.mul(t1655, t1654)  # t1656: "cuda:0 f32[1, 32, 512, 128]"
    # t1656 = prims.mul(t1655, t1654)  # t1656: "cuda:0 f32[1, 32, 512, 128]"
  t1657 = ltorch.add(t1653, t1656, alpha=None)  # t1657: "cuda:0 f32[1, 32, 512, 128]"
    # t1657 = prims.add(t1653, t1656)  # t1657: "cuda:0 f32[1, 32, 512, 128]"
  t1658 = prims.convert_element_type(t1657, dtypes.bfloat16)  # t1658: "cuda:0 bf16[1, 32, 512, 128]"
  t1659 = prims.slice_prim(t1614, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1659: "cuda:0 bf16[1, 32, 512, 0]"
  t1661 = prims.cat((t1642, t1659), -1)  # t1661: "cuda:0 bf16[1, 32, 512, 128]"
  t1662 = prims.slice_prim(t1620, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1662: "cuda:0 bf16[1, 32, 512, 0]"
  t1664 = prims.cat((t1658, t1662), -1)  # t1664: "cuda:0 bf16[1, 32, 512, 128]"
  (t1665, t1666, t1667, t1668) = cudnn_sdpa_fwd(t1661, t1664, t1626, None, 0.0, True, scale=0.08838834764831843)
  t1671 = prims.transpose(t1665, (0, 2, 1, 3))  # t1671: "cuda:0 bf16[1, 512, 32, 128]"
  t1675 = prims.reshape(t1671, (1, 512, 4096))  # t1675: "cuda:0 bf16[1, 512, 4096]"
  t1676 = torch.nn.functional.linear(t1675, t_transformer_h_11_attn_proj_weight, None)  # t1676: "cuda:0 bf16[1, 512, 4096]"
    # t1676 = ltorch.linear(t1675, t_transformer_h_11_attn_proj_weight, None)  # t1676: "cuda:0 bf16[1, 512, 4096]"
      # t1676 = prims.linear(t1675, t_transformer_h_11_attn_proj_weight, None)  # t1676: "cuda:0 bf16[1, 512, 4096]"
  t1677 = prims.convert_element_type(t1676, dtypes.float32)  # t1677: "cuda:0 f32[1, 512, 4096]"
  t1678 = prims.convert_element_type(t1574, dtypes.float32)  # t1678: "cuda:0 f32[1, 512, 4096]"
  t1679 = ltorch.add(t1677, t1678, alpha=None)  # t1679: "cuda:0 f32[1, 512, 4096]"
    # t1679 = prims.add(t1677, t1678)  # t1679: "cuda:0 f32[1, 512, 4096]"
  t1680 = prims.convert_element_type(t1679, dtypes.bfloat16)  # t1680: "cuda:0 bf16[1, 512, 4096]"
  t1681 = prims.convert_element_type(t1680, dtypes.float32)  # t1681: "cuda:0 f32[1, 512, 4096]"
  t1682 = ltorch.mul(t1681, t1681)  # t1682: "cuda:0 f32[1, 512, 4096]"
    # t1682 = prims.mul(t1681, t1681)  # t1682: "cuda:0 f32[1, 512, 4096]"
  t1684 = prims.sum(t1682, (2,))  # t1684: "cuda:0 f32[1, 512]"
  t1685 = prims.broadcast_in_dim(t1684, [1, 512, 1], [0, 1])  # t1685: "cuda:0 f32[1, 512, 1]"
  t1687 = ltorch.true_divide(t1685, 4096.0)  # t1687: "cuda:0 f32[1, 512, 1]"
    # t1687 = prims.div(t1685, 4096.0)  # t1687: "cuda:0 f32[1, 512, 1]"
  t1689 = ltorch.add(t1687, 1e-05, alpha=None)  # t1689: "cuda:0 f32[1, 512, 1]"
    # t1689 = prims.add(t1687, 1e-05)  # t1689: "cuda:0 f32[1, 512, 1]"
  t1690 = prims.rsqrt(t1689)  # t1690: "cuda:0 f32[1, 512, 1]"
  t1691 = prims.broadcast_in_dim(t1690, (1, 512, 4096), (0, 1, 2))  # t1691: "cuda:0 f32[1, 512, 4096]"
  t1692 = ltorch.mul(t1681, t1691)  # t1692: "cuda:0 f32[1, 512, 4096]"
    # t1692 = prims.mul(t1681, t1691)  # t1692: "cuda:0 f32[1, 512, 4096]"
  t1693 = prims.convert_element_type(t1692, dtypes.bfloat16)  # t1693: "cuda:0 bf16[1, 512, 4096]"
  t1694 = prims.broadcast_in_dim(t_transformer_h_11_norm_2_weight, (1, 512, 4096), (2,))  # t1694: "cuda:0 bf16[1, 512, 4096]"
  t1695 = prims.convert_element_type(t1693, dtypes.float32)  # t1695: "cuda:0 f32[1, 512, 4096]"
  t1696 = prims.convert_element_type(t1694, dtypes.float32)  # t1696: "cuda:0 f32[1, 512, 4096]"
  t1697 = ltorch.mul(t1695, t1696)  # t1697: "cuda:0 f32[1, 512, 4096]"
    # t1697 = prims.mul(t1695, t1696)  # t1697: "cuda:0 f32[1, 512, 4096]"
  t1698 = prims.convert_element_type(t1697, dtypes.bfloat16)  # t1698: "cuda:0 bf16[1, 512, 4096]"
  t1699 = torch.nn.functional.linear(t1698, t_transformer_h_11_mlp_fc_1_weight, None)  # t1699: "cuda:0 bf16[1, 512, 11008]"
    # t1699 = ltorch.linear(t1698, t_transformer_h_11_mlp_fc_1_weight, None)  # t1699: "cuda:0 bf16[1, 512, 11008]"
      # t1699 = prims.linear(t1698, t_transformer_h_11_mlp_fc_1_weight, None)  # t1699: "cuda:0 bf16[1, 512, 11008]"
  t1700 = torch.nn.functional.linear(t1698, t_transformer_h_11_mlp_fc_2_weight, None)  # t1700: "cuda:0 bf16[1, 512, 11008]"
    # t1700 = ltorch.linear(t1698, t_transformer_h_11_mlp_fc_2_weight, None)  # t1700: "cuda:0 bf16[1, 512, 11008]"
      # t1700 = prims.linear(t1698, t_transformer_h_11_mlp_fc_2_weight, None)  # t1700: "cuda:0 bf16[1, 512, 11008]"
  t1701 = prims.convert_element_type(t1699, dtypes.float32)  # t1701: "cuda:0 f32[1, 512, 11008]"
  t1702 = prims.neg(t1701)  # t1702: "cuda:0 f32[1, 512, 11008]"
  t1703 = prims.exp(t1702)  # t1703: "cuda:0 f32[1, 512, 11008]"
  t1704 = ltorch.add(1.0, t1703, alpha=None)  # t1704: "cuda:0 f32[1, 512, 11008]"
    # t1704 = prims.add(1.0, t1703)  # t1704: "cuda:0 f32[1, 512, 11008]"
  t1705 = prims.reciprocal(t1704)  # t1705: "cuda:0 f32[1, 512, 11008]"
  t1706 = prims.convert_element_type(t1705, dtypes.bfloat16)  # t1706: "cuda:0 bf16[1, 512, 11008]"
  t1707 = prims.convert_element_type(t1699, dtypes.float32)  # t1707: "cuda:0 f32[1, 512, 11008]"
  t1708 = prims.convert_element_type(t1706, dtypes.float32)  # t1708: "cuda:0 f32[1, 512, 11008]"
  t1709 = ltorch.mul(t1707, t1708)  # t1709: "cuda:0 f32[1, 512, 11008]"
    # t1709 = prims.mul(t1707, t1708)  # t1709: "cuda:0 f32[1, 512, 11008]"
  t1710 = prims.convert_element_type(t1709, dtypes.bfloat16)  # t1710: "cuda:0 bf16[1, 512, 11008]"
  t1711 = prims.convert_element_type(t1710, dtypes.float32)  # t1711: "cuda:0 f32[1, 512, 11008]"
  t1712 = prims.convert_element_type(t1700, dtypes.float32)  # t1712: "cuda:0 f32[1, 512, 11008]"
  t1713 = ltorch.mul(t1711, t1712)  # t1713: "cuda:0 f32[1, 512, 11008]"
    # t1713 = prims.mul(t1711, t1712)  # t1713: "cuda:0 f32[1, 512, 11008]"
  t1714 = prims.convert_element_type(t1713, dtypes.bfloat16)  # t1714: "cuda:0 bf16[1, 512, 11008]"
  t1715 = torch.nn.functional.linear(t1714, t_transformer_h_11_mlp_proj_weight, None)  # t1715: "cuda:0 bf16[1, 512, 4096]"
    # t1715 = ltorch.linear(t1714, t_transformer_h_11_mlp_proj_weight, None)  # t1715: "cuda:0 bf16[1, 512, 4096]"
      # t1715 = prims.linear(t1714, t_transformer_h_11_mlp_proj_weight, None)  # t1715: "cuda:0 bf16[1, 512, 4096]"
  t1716 = prims.convert_element_type(t1715, dtypes.float32)  # t1716: "cuda:0 f32[1, 512, 4096]"
  t1717 = prims.convert_element_type(t1680, dtypes.float32)  # t1717: "cuda:0 f32[1, 512, 4096]"
  t1718 = ltorch.add(t1716, t1717, alpha=None)  # t1718: "cuda:0 f32[1, 512, 4096]"
    # t1718 = prims.add(t1716, t1717)  # t1718: "cuda:0 f32[1, 512, 4096]"
  t1719 = prims.convert_element_type(t1718, dtypes.bfloat16)  # t1719: "cuda:0 bf16[1, 512, 4096]"
  t1720 = prims.convert_element_type(t1719, dtypes.float32)  # t1720: "cuda:0 f32[1, 512, 4096]"
  t1721 = ltorch.mul(t1720, t1720)  # t1721: "cuda:0 f32[1, 512, 4096]"
    # t1721 = prims.mul(t1720, t1720)  # t1721: "cuda:0 f32[1, 512, 4096]"
  t1723 = prims.sum(t1721, (2,))  # t1723: "cuda:0 f32[1, 512]"
  t1724 = prims.broadcast_in_dim(t1723, [1, 512, 1], [0, 1])  # t1724: "cuda:0 f32[1, 512, 1]"
  t1726 = ltorch.true_divide(t1724, 4096.0)  # t1726: "cuda:0 f32[1, 512, 1]"
    # t1726 = prims.div(t1724, 4096.0)  # t1726: "cuda:0 f32[1, 512, 1]"
  t1728 = ltorch.add(t1726, 1e-05, alpha=None)  # t1728: "cuda:0 f32[1, 512, 1]"
    # t1728 = prims.add(t1726, 1e-05)  # t1728: "cuda:0 f32[1, 512, 1]"
  t1729 = prims.rsqrt(t1728)  # t1729: "cuda:0 f32[1, 512, 1]"
  t1730 = prims.broadcast_in_dim(t1729, (1, 512, 4096), (0, 1, 2))  # t1730: "cuda:0 f32[1, 512, 4096]"
  t1731 = ltorch.mul(t1720, t1730)  # t1731: "cuda:0 f32[1, 512, 4096]"
    # t1731 = prims.mul(t1720, t1730)  # t1731: "cuda:0 f32[1, 512, 4096]"
  t1732 = prims.convert_element_type(t1731, dtypes.bfloat16)  # t1732: "cuda:0 bf16[1, 512, 4096]"
  t1733 = prims.broadcast_in_dim(t_transformer_h_12_norm_1_weight, (1, 512, 4096), (2,))  # t1733: "cuda:0 bf16[1, 512, 4096]"
  t1734 = prims.convert_element_type(t1732, dtypes.float32)  # t1734: "cuda:0 f32[1, 512, 4096]"
  t1735 = prims.convert_element_type(t1733, dtypes.float32)  # t1735: "cuda:0 f32[1, 512, 4096]"
  t1736 = ltorch.mul(t1734, t1735)  # t1736: "cuda:0 f32[1, 512, 4096]"
    # t1736 = prims.mul(t1734, t1735)  # t1736: "cuda:0 f32[1, 512, 4096]"
  t1737 = prims.convert_element_type(t1736, dtypes.bfloat16)  # t1737: "cuda:0 bf16[1, 512, 4096]"
  t1738 = torch.nn.functional.linear(t1737, t_transformer_h_12_attn_attn_weight, None)  # t1738: "cuda:0 bf16[1, 512, 12288]"
    # t1738 = ltorch.linear(t1737, t_transformer_h_12_attn_attn_weight, None)  # t1738: "cuda:0 bf16[1, 512, 12288]"
      # t1738 = prims.linear(t1737, t_transformer_h_12_attn_attn_weight, None)  # t1738: "cuda:0 bf16[1, 512, 12288]"
  t1744 = prims.reshape(t1738, (1, 512, 32, 3, 128))  # t1744: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1750 = prims.transpose(t1744, (0, 2, 3, 1, 4))  # t1750: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1751, t1752, t1753) = ltorch.split(t1750, (1, 1, 1), 2)
    # t1751 = prims.slice_prim(t1750, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1751: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1752 = prims.slice_prim(t1750, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1752: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1753 = prims.slice_prim(t1750, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1753: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1759 = prims.reshape(t1751, (1, 32, 512, 128))  # t1759: "cuda:0 bf16[1, 32, 512, 128]"
  t1765 = prims.reshape(t1752, (1, 32, 512, 128))  # t1765: "cuda:0 bf16[1, 32, 512, 128]"
  t1771 = prims.reshape(t1753, (1, 32, 512, 128))  # t1771: "cuda:0 bf16[1, 32, 512, 128]"
  t1772 = prims.slice_prim(t1759, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1772: "cuda:0 bf16[1, 32, 512, 128]"
  t1773 = prims.slice_prim(t1772, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1773: "cuda:0 bf16[1, 32, 512, 64]"
  t1774 = prims.slice_prim(t1772, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1774: "cuda:0 bf16[1, 32, 512, 64]"
  t1775 = prims.convert_element_type(t1774, dtypes.float32)  # t1775: "cuda:0 f32[1, 32, 512, 64]"
  t1776 = prims.neg(t1775)  # t1776: "cuda:0 f32[1, 32, 512, 64]"
  t1777 = prims.convert_element_type(t1776, dtypes.bfloat16)  # t1777: "cuda:0 bf16[1, 32, 512, 64]"
  t1779 = prims.cat((t1777, t1773), -1)  # t1779: "cuda:0 bf16[1, 32, 512, 128]"
  t1780 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1780: "cuda:0 f32[1, 32, 512, 128]"
  t1781 = prims.convert_element_type(t1772, dtypes.float32)  # t1781: "cuda:0 f32[1, 32, 512, 128]"
  t1782 = ltorch.mul(t1781, t1780)  # t1782: "cuda:0 f32[1, 32, 512, 128]"
    # t1782 = prims.mul(t1781, t1780)  # t1782: "cuda:0 f32[1, 32, 512, 128]"
  t1783 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1783: "cuda:0 f32[1, 32, 512, 128]"
  t1784 = prims.convert_element_type(t1779, dtypes.float32)  # t1784: "cuda:0 f32[1, 32, 512, 128]"
  t1785 = ltorch.mul(t1784, t1783)  # t1785: "cuda:0 f32[1, 32, 512, 128]"
    # t1785 = prims.mul(t1784, t1783)  # t1785: "cuda:0 f32[1, 32, 512, 128]"
  t1786 = ltorch.add(t1782, t1785, alpha=None)  # t1786: "cuda:0 f32[1, 32, 512, 128]"
    # t1786 = prims.add(t1782, t1785)  # t1786: "cuda:0 f32[1, 32, 512, 128]"
  t1787 = prims.convert_element_type(t1786, dtypes.bfloat16)  # t1787: "cuda:0 bf16[1, 32, 512, 128]"
  t1788 = prims.slice_prim(t1765, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1788: "cuda:0 bf16[1, 32, 512, 128]"
  t1789 = prims.slice_prim(t1788, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1789: "cuda:0 bf16[1, 32, 512, 64]"
  t1790 = prims.slice_prim(t1788, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1790: "cuda:0 bf16[1, 32, 512, 64]"
  t1791 = prims.convert_element_type(t1790, dtypes.float32)  # t1791: "cuda:0 f32[1, 32, 512, 64]"
  t1792 = prims.neg(t1791)  # t1792: "cuda:0 f32[1, 32, 512, 64]"
  t1793 = prims.convert_element_type(t1792, dtypes.bfloat16)  # t1793: "cuda:0 bf16[1, 32, 512, 64]"
  t1795 = prims.cat((t1793, t1789), -1)  # t1795: "cuda:0 bf16[1, 32, 512, 128]"
  t1796 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1796: "cuda:0 f32[1, 32, 512, 128]"
  t1797 = prims.convert_element_type(t1788, dtypes.float32)  # t1797: "cuda:0 f32[1, 32, 512, 128]"
  t1798 = ltorch.mul(t1797, t1796)  # t1798: "cuda:0 f32[1, 32, 512, 128]"
    # t1798 = prims.mul(t1797, t1796)  # t1798: "cuda:0 f32[1, 32, 512, 128]"
  t1799 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1799: "cuda:0 f32[1, 32, 512, 128]"
  t1800 = prims.convert_element_type(t1795, dtypes.float32)  # t1800: "cuda:0 f32[1, 32, 512, 128]"
  t1801 = ltorch.mul(t1800, t1799)  # t1801: "cuda:0 f32[1, 32, 512, 128]"
    # t1801 = prims.mul(t1800, t1799)  # t1801: "cuda:0 f32[1, 32, 512, 128]"
  t1802 = ltorch.add(t1798, t1801, alpha=None)  # t1802: "cuda:0 f32[1, 32, 512, 128]"
    # t1802 = prims.add(t1798, t1801)  # t1802: "cuda:0 f32[1, 32, 512, 128]"
  t1803 = prims.convert_element_type(t1802, dtypes.bfloat16)  # t1803: "cuda:0 bf16[1, 32, 512, 128]"
  t1804 = prims.slice_prim(t1759, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1804: "cuda:0 bf16[1, 32, 512, 0]"
  t1806 = prims.cat((t1787, t1804), -1)  # t1806: "cuda:0 bf16[1, 32, 512, 128]"
  t1807 = prims.slice_prim(t1765, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1807: "cuda:0 bf16[1, 32, 512, 0]"
  t1809 = prims.cat((t1803, t1807), -1)  # t1809: "cuda:0 bf16[1, 32, 512, 128]"
  (t1810, t1811, t1812, t1813) = cudnn_sdpa_fwd(t1806, t1809, t1771, None, 0.0, True, scale=0.08838834764831843)
  t1816 = prims.transpose(t1810, (0, 2, 1, 3))  # t1816: "cuda:0 bf16[1, 512, 32, 128]"
  t1820 = prims.reshape(t1816, (1, 512, 4096))  # t1820: "cuda:0 bf16[1, 512, 4096]"
  t1821 = torch.nn.functional.linear(t1820, t_transformer_h_12_attn_proj_weight, None)  # t1821: "cuda:0 bf16[1, 512, 4096]"
    # t1821 = ltorch.linear(t1820, t_transformer_h_12_attn_proj_weight, None)  # t1821: "cuda:0 bf16[1, 512, 4096]"
      # t1821 = prims.linear(t1820, t_transformer_h_12_attn_proj_weight, None)  # t1821: "cuda:0 bf16[1, 512, 4096]"
  t1822 = prims.convert_element_type(t1821, dtypes.float32)  # t1822: "cuda:0 f32[1, 512, 4096]"
  t1823 = prims.convert_element_type(t1719, dtypes.float32)  # t1823: "cuda:0 f32[1, 512, 4096]"
  t1824 = ltorch.add(t1822, t1823, alpha=None)  # t1824: "cuda:0 f32[1, 512, 4096]"
    # t1824 = prims.add(t1822, t1823)  # t1824: "cuda:0 f32[1, 512, 4096]"
  t1825 = prims.convert_element_type(t1824, dtypes.bfloat16)  # t1825: "cuda:0 bf16[1, 512, 4096]"
  t1826 = prims.convert_element_type(t1825, dtypes.float32)  # t1826: "cuda:0 f32[1, 512, 4096]"
  t1827 = ltorch.mul(t1826, t1826)  # t1827: "cuda:0 f32[1, 512, 4096]"
    # t1827 = prims.mul(t1826, t1826)  # t1827: "cuda:0 f32[1, 512, 4096]"
  t1829 = prims.sum(t1827, (2,))  # t1829: "cuda:0 f32[1, 512]"
  t1830 = prims.broadcast_in_dim(t1829, [1, 512, 1], [0, 1])  # t1830: "cuda:0 f32[1, 512, 1]"
  t1832 = ltorch.true_divide(t1830, 4096.0)  # t1832: "cuda:0 f32[1, 512, 1]"
    # t1832 = prims.div(t1830, 4096.0)  # t1832: "cuda:0 f32[1, 512, 1]"
  t1834 = ltorch.add(t1832, 1e-05, alpha=None)  # t1834: "cuda:0 f32[1, 512, 1]"
    # t1834 = prims.add(t1832, 1e-05)  # t1834: "cuda:0 f32[1, 512, 1]"
  t1835 = prims.rsqrt(t1834)  # t1835: "cuda:0 f32[1, 512, 1]"
  t1836 = prims.broadcast_in_dim(t1835, (1, 512, 4096), (0, 1, 2))  # t1836: "cuda:0 f32[1, 512, 4096]"
  t1837 = ltorch.mul(t1826, t1836)  # t1837: "cuda:0 f32[1, 512, 4096]"
    # t1837 = prims.mul(t1826, t1836)  # t1837: "cuda:0 f32[1, 512, 4096]"
  t1838 = prims.convert_element_type(t1837, dtypes.bfloat16)  # t1838: "cuda:0 bf16[1, 512, 4096]"
  t1839 = prims.broadcast_in_dim(t_transformer_h_12_norm_2_weight, (1, 512, 4096), (2,))  # t1839: "cuda:0 bf16[1, 512, 4096]"
  t1840 = prims.convert_element_type(t1838, dtypes.float32)  # t1840: "cuda:0 f32[1, 512, 4096]"
  t1841 = prims.convert_element_type(t1839, dtypes.float32)  # t1841: "cuda:0 f32[1, 512, 4096]"
  t1842 = ltorch.mul(t1840, t1841)  # t1842: "cuda:0 f32[1, 512, 4096]"
    # t1842 = prims.mul(t1840, t1841)  # t1842: "cuda:0 f32[1, 512, 4096]"
  t1843 = prims.convert_element_type(t1842, dtypes.bfloat16)  # t1843: "cuda:0 bf16[1, 512, 4096]"
  t1844 = torch.nn.functional.linear(t1843, t_transformer_h_12_mlp_fc_1_weight, None)  # t1844: "cuda:0 bf16[1, 512, 11008]"
    # t1844 = ltorch.linear(t1843, t_transformer_h_12_mlp_fc_1_weight, None)  # t1844: "cuda:0 bf16[1, 512, 11008]"
      # t1844 = prims.linear(t1843, t_transformer_h_12_mlp_fc_1_weight, None)  # t1844: "cuda:0 bf16[1, 512, 11008]"
  t1845 = torch.nn.functional.linear(t1843, t_transformer_h_12_mlp_fc_2_weight, None)  # t1845: "cuda:0 bf16[1, 512, 11008]"
    # t1845 = ltorch.linear(t1843, t_transformer_h_12_mlp_fc_2_weight, None)  # t1845: "cuda:0 bf16[1, 512, 11008]"
      # t1845 = prims.linear(t1843, t_transformer_h_12_mlp_fc_2_weight, None)  # t1845: "cuda:0 bf16[1, 512, 11008]"
  t1846 = prims.convert_element_type(t1844, dtypes.float32)  # t1846: "cuda:0 f32[1, 512, 11008]"
  t1847 = prims.neg(t1846)  # t1847: "cuda:0 f32[1, 512, 11008]"
  t1848 = prims.exp(t1847)  # t1848: "cuda:0 f32[1, 512, 11008]"
  t1849 = ltorch.add(1.0, t1848, alpha=None)  # t1849: "cuda:0 f32[1, 512, 11008]"
    # t1849 = prims.add(1.0, t1848)  # t1849: "cuda:0 f32[1, 512, 11008]"
  t1850 = prims.reciprocal(t1849)  # t1850: "cuda:0 f32[1, 512, 11008]"
  t1851 = prims.convert_element_type(t1850, dtypes.bfloat16)  # t1851: "cuda:0 bf16[1, 512, 11008]"
  t1852 = prims.convert_element_type(t1844, dtypes.float32)  # t1852: "cuda:0 f32[1, 512, 11008]"
  t1853 = prims.convert_element_type(t1851, dtypes.float32)  # t1853: "cuda:0 f32[1, 512, 11008]"
  t1854 = ltorch.mul(t1852, t1853)  # t1854: "cuda:0 f32[1, 512, 11008]"
    # t1854 = prims.mul(t1852, t1853)  # t1854: "cuda:0 f32[1, 512, 11008]"
  t1855 = prims.convert_element_type(t1854, dtypes.bfloat16)  # t1855: "cuda:0 bf16[1, 512, 11008]"
  t1856 = prims.convert_element_type(t1855, dtypes.float32)  # t1856: "cuda:0 f32[1, 512, 11008]"
  t1857 = prims.convert_element_type(t1845, dtypes.float32)  # t1857: "cuda:0 f32[1, 512, 11008]"
  t1858 = ltorch.mul(t1856, t1857)  # t1858: "cuda:0 f32[1, 512, 11008]"
    # t1858 = prims.mul(t1856, t1857)  # t1858: "cuda:0 f32[1, 512, 11008]"
  t1859 = prims.convert_element_type(t1858, dtypes.bfloat16)  # t1859: "cuda:0 bf16[1, 512, 11008]"
  t1860 = torch.nn.functional.linear(t1859, t_transformer_h_12_mlp_proj_weight, None)  # t1860: "cuda:0 bf16[1, 512, 4096]"
    # t1860 = ltorch.linear(t1859, t_transformer_h_12_mlp_proj_weight, None)  # t1860: "cuda:0 bf16[1, 512, 4096]"
      # t1860 = prims.linear(t1859, t_transformer_h_12_mlp_proj_weight, None)  # t1860: "cuda:0 bf16[1, 512, 4096]"
  t1861 = prims.convert_element_type(t1860, dtypes.float32)  # t1861: "cuda:0 f32[1, 512, 4096]"
  t1862 = prims.convert_element_type(t1825, dtypes.float32)  # t1862: "cuda:0 f32[1, 512, 4096]"
  t1863 = ltorch.add(t1861, t1862, alpha=None)  # t1863: "cuda:0 f32[1, 512, 4096]"
    # t1863 = prims.add(t1861, t1862)  # t1863: "cuda:0 f32[1, 512, 4096]"
  t1864 = prims.convert_element_type(t1863, dtypes.bfloat16)  # t1864: "cuda:0 bf16[1, 512, 4096]"
  t1865 = prims.convert_element_type(t1864, dtypes.float32)  # t1865: "cuda:0 f32[1, 512, 4096]"
  t1866 = ltorch.mul(t1865, t1865)  # t1866: "cuda:0 f32[1, 512, 4096]"
    # t1866 = prims.mul(t1865, t1865)  # t1866: "cuda:0 f32[1, 512, 4096]"
  t1868 = prims.sum(t1866, (2,))  # t1868: "cuda:0 f32[1, 512]"
  t1869 = prims.broadcast_in_dim(t1868, [1, 512, 1], [0, 1])  # t1869: "cuda:0 f32[1, 512, 1]"
  t1871 = ltorch.true_divide(t1869, 4096.0)  # t1871: "cuda:0 f32[1, 512, 1]"
    # t1871 = prims.div(t1869, 4096.0)  # t1871: "cuda:0 f32[1, 512, 1]"
  t1873 = ltorch.add(t1871, 1e-05, alpha=None)  # t1873: "cuda:0 f32[1, 512, 1]"
    # t1873 = prims.add(t1871, 1e-05)  # t1873: "cuda:0 f32[1, 512, 1]"
  t1874 = prims.rsqrt(t1873)  # t1874: "cuda:0 f32[1, 512, 1]"
  t1875 = prims.broadcast_in_dim(t1874, (1, 512, 4096), (0, 1, 2))  # t1875: "cuda:0 f32[1, 512, 4096]"
  t1876 = ltorch.mul(t1865, t1875)  # t1876: "cuda:0 f32[1, 512, 4096]"
    # t1876 = prims.mul(t1865, t1875)  # t1876: "cuda:0 f32[1, 512, 4096]"
  t1877 = prims.convert_element_type(t1876, dtypes.bfloat16)  # t1877: "cuda:0 bf16[1, 512, 4096]"
  t1878 = prims.broadcast_in_dim(t_transformer_h_13_norm_1_weight, (1, 512, 4096), (2,))  # t1878: "cuda:0 bf16[1, 512, 4096]"
  t1879 = prims.convert_element_type(t1877, dtypes.float32)  # t1879: "cuda:0 f32[1, 512, 4096]"
  t1880 = prims.convert_element_type(t1878, dtypes.float32)  # t1880: "cuda:0 f32[1, 512, 4096]"
  t1881 = ltorch.mul(t1879, t1880)  # t1881: "cuda:0 f32[1, 512, 4096]"
    # t1881 = prims.mul(t1879, t1880)  # t1881: "cuda:0 f32[1, 512, 4096]"
  t1882 = prims.convert_element_type(t1881, dtypes.bfloat16)  # t1882: "cuda:0 bf16[1, 512, 4096]"
  t1883 = torch.nn.functional.linear(t1882, t_transformer_h_13_attn_attn_weight, None)  # t1883: "cuda:0 bf16[1, 512, 12288]"
    # t1883 = ltorch.linear(t1882, t_transformer_h_13_attn_attn_weight, None)  # t1883: "cuda:0 bf16[1, 512, 12288]"
      # t1883 = prims.linear(t1882, t_transformer_h_13_attn_attn_weight, None)  # t1883: "cuda:0 bf16[1, 512, 12288]"
  t1889 = prims.reshape(t1883, (1, 512, 32, 3, 128))  # t1889: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t1895 = prims.transpose(t1889, (0, 2, 3, 1, 4))  # t1895: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t1896, t1897, t1898) = ltorch.split(t1895, (1, 1, 1), 2)
    # t1896 = prims.slice_prim(t1895, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1896: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1897 = prims.slice_prim(t1895, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1897: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t1898 = prims.slice_prim(t1895, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1898: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t1904 = prims.reshape(t1896, (1, 32, 512, 128))  # t1904: "cuda:0 bf16[1, 32, 512, 128]"
  t1910 = prims.reshape(t1897, (1, 32, 512, 128))  # t1910: "cuda:0 bf16[1, 32, 512, 128]"
  t1916 = prims.reshape(t1898, (1, 32, 512, 128))  # t1916: "cuda:0 bf16[1, 32, 512, 128]"
  t1917 = prims.slice_prim(t1904, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1917: "cuda:0 bf16[1, 32, 512, 128]"
  t1918 = prims.slice_prim(t1917, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1918: "cuda:0 bf16[1, 32, 512, 64]"
  t1919 = prims.slice_prim(t1917, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1919: "cuda:0 bf16[1, 32, 512, 64]"
  t1920 = prims.convert_element_type(t1919, dtypes.float32)  # t1920: "cuda:0 f32[1, 32, 512, 64]"
  t1921 = prims.neg(t1920)  # t1921: "cuda:0 f32[1, 32, 512, 64]"
  t1922 = prims.convert_element_type(t1921, dtypes.bfloat16)  # t1922: "cuda:0 bf16[1, 32, 512, 64]"
  t1924 = prims.cat((t1922, t1918), -1)  # t1924: "cuda:0 bf16[1, 32, 512, 128]"
  t1925 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1925: "cuda:0 f32[1, 32, 512, 128]"
  t1926 = prims.convert_element_type(t1917, dtypes.float32)  # t1926: "cuda:0 f32[1, 32, 512, 128]"
  t1927 = ltorch.mul(t1926, t1925)  # t1927: "cuda:0 f32[1, 32, 512, 128]"
    # t1927 = prims.mul(t1926, t1925)  # t1927: "cuda:0 f32[1, 32, 512, 128]"
  t1928 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1928: "cuda:0 f32[1, 32, 512, 128]"
  t1929 = prims.convert_element_type(t1924, dtypes.float32)  # t1929: "cuda:0 f32[1, 32, 512, 128]"
  t1930 = ltorch.mul(t1929, t1928)  # t1930: "cuda:0 f32[1, 32, 512, 128]"
    # t1930 = prims.mul(t1929, t1928)  # t1930: "cuda:0 f32[1, 32, 512, 128]"
  t1931 = ltorch.add(t1927, t1930, alpha=None)  # t1931: "cuda:0 f32[1, 32, 512, 128]"
    # t1931 = prims.add(t1927, t1930)  # t1931: "cuda:0 f32[1, 32, 512, 128]"
  t1932 = prims.convert_element_type(t1931, dtypes.bfloat16)  # t1932: "cuda:0 bf16[1, 32, 512, 128]"
  t1933 = prims.slice_prim(t1910, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1933: "cuda:0 bf16[1, 32, 512, 128]"
  t1934 = prims.slice_prim(t1933, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1934: "cuda:0 bf16[1, 32, 512, 64]"
  t1935 = prims.slice_prim(t1933, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1935: "cuda:0 bf16[1, 32, 512, 64]"
  t1936 = prims.convert_element_type(t1935, dtypes.float32)  # t1936: "cuda:0 f32[1, 32, 512, 64]"
  t1937 = prims.neg(t1936)  # t1937: "cuda:0 f32[1, 32, 512, 64]"
  t1938 = prims.convert_element_type(t1937, dtypes.bfloat16)  # t1938: "cuda:0 bf16[1, 32, 512, 64]"
  t1940 = prims.cat((t1938, t1934), -1)  # t1940: "cuda:0 bf16[1, 32, 512, 128]"
  t1941 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t1941: "cuda:0 f32[1, 32, 512, 128]"
  t1942 = prims.convert_element_type(t1933, dtypes.float32)  # t1942: "cuda:0 f32[1, 32, 512, 128]"
  t1943 = ltorch.mul(t1942, t1941)  # t1943: "cuda:0 f32[1, 32, 512, 128]"
    # t1943 = prims.mul(t1942, t1941)  # t1943: "cuda:0 f32[1, 32, 512, 128]"
  t1944 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t1944: "cuda:0 f32[1, 32, 512, 128]"
  t1945 = prims.convert_element_type(t1940, dtypes.float32)  # t1945: "cuda:0 f32[1, 32, 512, 128]"
  t1946 = ltorch.mul(t1945, t1944)  # t1946: "cuda:0 f32[1, 32, 512, 128]"
    # t1946 = prims.mul(t1945, t1944)  # t1946: "cuda:0 f32[1, 32, 512, 128]"
  t1947 = ltorch.add(t1943, t1946, alpha=None)  # t1947: "cuda:0 f32[1, 32, 512, 128]"
    # t1947 = prims.add(t1943, t1946)  # t1947: "cuda:0 f32[1, 32, 512, 128]"
  t1948 = prims.convert_element_type(t1947, dtypes.bfloat16)  # t1948: "cuda:0 bf16[1, 32, 512, 128]"
  t1949 = prims.slice_prim(t1904, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1949: "cuda:0 bf16[1, 32, 512, 0]"
  t1951 = prims.cat((t1932, t1949), -1)  # t1951: "cuda:0 bf16[1, 32, 512, 128]"
  t1952 = prims.slice_prim(t1910, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1952: "cuda:0 bf16[1, 32, 512, 0]"
  t1954 = prims.cat((t1948, t1952), -1)  # t1954: "cuda:0 bf16[1, 32, 512, 128]"
  (t1955, t1956, t1957, t1958) = cudnn_sdpa_fwd(t1951, t1954, t1916, None, 0.0, True, scale=0.08838834764831843)
  t1961 = prims.transpose(t1955, (0, 2, 1, 3))  # t1961: "cuda:0 bf16[1, 512, 32, 128]"
  t1965 = prims.reshape(t1961, (1, 512, 4096))  # t1965: "cuda:0 bf16[1, 512, 4096]"
  t1966 = torch.nn.functional.linear(t1965, t_transformer_h_13_attn_proj_weight, None)  # t1966: "cuda:0 bf16[1, 512, 4096]"
    # t1966 = ltorch.linear(t1965, t_transformer_h_13_attn_proj_weight, None)  # t1966: "cuda:0 bf16[1, 512, 4096]"
      # t1966 = prims.linear(t1965, t_transformer_h_13_attn_proj_weight, None)  # t1966: "cuda:0 bf16[1, 512, 4096]"
  t1967 = prims.convert_element_type(t1966, dtypes.float32)  # t1967: "cuda:0 f32[1, 512, 4096]"
  t1968 = prims.convert_element_type(t1864, dtypes.float32)  # t1968: "cuda:0 f32[1, 512, 4096]"
  t1969 = ltorch.add(t1967, t1968, alpha=None)  # t1969: "cuda:0 f32[1, 512, 4096]"
    # t1969 = prims.add(t1967, t1968)  # t1969: "cuda:0 f32[1, 512, 4096]"
  t1970 = prims.convert_element_type(t1969, dtypes.bfloat16)  # t1970: "cuda:0 bf16[1, 512, 4096]"
  t1971 = prims.convert_element_type(t1970, dtypes.float32)  # t1971: "cuda:0 f32[1, 512, 4096]"
  t1972 = ltorch.mul(t1971, t1971)  # t1972: "cuda:0 f32[1, 512, 4096]"
    # t1972 = prims.mul(t1971, t1971)  # t1972: "cuda:0 f32[1, 512, 4096]"
  t1974 = prims.sum(t1972, (2,))  # t1974: "cuda:0 f32[1, 512]"
  t1975 = prims.broadcast_in_dim(t1974, [1, 512, 1], [0, 1])  # t1975: "cuda:0 f32[1, 512, 1]"
  t1977 = ltorch.true_divide(t1975, 4096.0)  # t1977: "cuda:0 f32[1, 512, 1]"
    # t1977 = prims.div(t1975, 4096.0)  # t1977: "cuda:0 f32[1, 512, 1]"
  t1979 = ltorch.add(t1977, 1e-05, alpha=None)  # t1979: "cuda:0 f32[1, 512, 1]"
    # t1979 = prims.add(t1977, 1e-05)  # t1979: "cuda:0 f32[1, 512, 1]"
  t1980 = prims.rsqrt(t1979)  # t1980: "cuda:0 f32[1, 512, 1]"
  t1981 = prims.broadcast_in_dim(t1980, (1, 512, 4096), (0, 1, 2))  # t1981: "cuda:0 f32[1, 512, 4096]"
  t1982 = ltorch.mul(t1971, t1981)  # t1982: "cuda:0 f32[1, 512, 4096]"
    # t1982 = prims.mul(t1971, t1981)  # t1982: "cuda:0 f32[1, 512, 4096]"
  t1983 = prims.convert_element_type(t1982, dtypes.bfloat16)  # t1983: "cuda:0 bf16[1, 512, 4096]"
  t1984 = prims.broadcast_in_dim(t_transformer_h_13_norm_2_weight, (1, 512, 4096), (2,))  # t1984: "cuda:0 bf16[1, 512, 4096]"
  t1985 = prims.convert_element_type(t1983, dtypes.float32)  # t1985: "cuda:0 f32[1, 512, 4096]"
  t1986 = prims.convert_element_type(t1984, dtypes.float32)  # t1986: "cuda:0 f32[1, 512, 4096]"
  t1987 = ltorch.mul(t1985, t1986)  # t1987: "cuda:0 f32[1, 512, 4096]"
    # t1987 = prims.mul(t1985, t1986)  # t1987: "cuda:0 f32[1, 512, 4096]"
  t1988 = prims.convert_element_type(t1987, dtypes.bfloat16)  # t1988: "cuda:0 bf16[1, 512, 4096]"
  t1989 = torch.nn.functional.linear(t1988, t_transformer_h_13_mlp_fc_1_weight, None)  # t1989: "cuda:0 bf16[1, 512, 11008]"
    # t1989 = ltorch.linear(t1988, t_transformer_h_13_mlp_fc_1_weight, None)  # t1989: "cuda:0 bf16[1, 512, 11008]"
      # t1989 = prims.linear(t1988, t_transformer_h_13_mlp_fc_1_weight, None)  # t1989: "cuda:0 bf16[1, 512, 11008]"
  t1990 = torch.nn.functional.linear(t1988, t_transformer_h_13_mlp_fc_2_weight, None)  # t1990: "cuda:0 bf16[1, 512, 11008]"
    # t1990 = ltorch.linear(t1988, t_transformer_h_13_mlp_fc_2_weight, None)  # t1990: "cuda:0 bf16[1, 512, 11008]"
      # t1990 = prims.linear(t1988, t_transformer_h_13_mlp_fc_2_weight, None)  # t1990: "cuda:0 bf16[1, 512, 11008]"
  t1991 = prims.convert_element_type(t1989, dtypes.float32)  # t1991: "cuda:0 f32[1, 512, 11008]"
  t1992 = prims.neg(t1991)  # t1992: "cuda:0 f32[1, 512, 11008]"
  t1993 = prims.exp(t1992)  # t1993: "cuda:0 f32[1, 512, 11008]"
  t1994 = ltorch.add(1.0, t1993, alpha=None)  # t1994: "cuda:0 f32[1, 512, 11008]"
    # t1994 = prims.add(1.0, t1993)  # t1994: "cuda:0 f32[1, 512, 11008]"
  t1995 = prims.reciprocal(t1994)  # t1995: "cuda:0 f32[1, 512, 11008]"
  t1996 = prims.convert_element_type(t1995, dtypes.bfloat16)  # t1996: "cuda:0 bf16[1, 512, 11008]"
  t1997 = prims.convert_element_type(t1989, dtypes.float32)  # t1997: "cuda:0 f32[1, 512, 11008]"
  t1998 = prims.convert_element_type(t1996, dtypes.float32)  # t1998: "cuda:0 f32[1, 512, 11008]"
  t1999 = ltorch.mul(t1997, t1998)  # t1999: "cuda:0 f32[1, 512, 11008]"
    # t1999 = prims.mul(t1997, t1998)  # t1999: "cuda:0 f32[1, 512, 11008]"
  t2000 = prims.convert_element_type(t1999, dtypes.bfloat16)  # t2000: "cuda:0 bf16[1, 512, 11008]"
  t2001 = prims.convert_element_type(t2000, dtypes.float32)  # t2001: "cuda:0 f32[1, 512, 11008]"
  t2002 = prims.convert_element_type(t1990, dtypes.float32)  # t2002: "cuda:0 f32[1, 512, 11008]"
  t2003 = ltorch.mul(t2001, t2002)  # t2003: "cuda:0 f32[1, 512, 11008]"
    # t2003 = prims.mul(t2001, t2002)  # t2003: "cuda:0 f32[1, 512, 11008]"
  t2004 = prims.convert_element_type(t2003, dtypes.bfloat16)  # t2004: "cuda:0 bf16[1, 512, 11008]"
  t2005 = torch.nn.functional.linear(t2004, t_transformer_h_13_mlp_proj_weight, None)  # t2005: "cuda:0 bf16[1, 512, 4096]"
    # t2005 = ltorch.linear(t2004, t_transformer_h_13_mlp_proj_weight, None)  # t2005: "cuda:0 bf16[1, 512, 4096]"
      # t2005 = prims.linear(t2004, t_transformer_h_13_mlp_proj_weight, None)  # t2005: "cuda:0 bf16[1, 512, 4096]"
  t2006 = prims.convert_element_type(t2005, dtypes.float32)  # t2006: "cuda:0 f32[1, 512, 4096]"
  t2007 = prims.convert_element_type(t1970, dtypes.float32)  # t2007: "cuda:0 f32[1, 512, 4096]"
  t2008 = ltorch.add(t2006, t2007, alpha=None)  # t2008: "cuda:0 f32[1, 512, 4096]"
    # t2008 = prims.add(t2006, t2007)  # t2008: "cuda:0 f32[1, 512, 4096]"
  t2009 = prims.convert_element_type(t2008, dtypes.bfloat16)  # t2009: "cuda:0 bf16[1, 512, 4096]"
  t2010 = prims.convert_element_type(t2009, dtypes.float32)  # t2010: "cuda:0 f32[1, 512, 4096]"
  t2011 = ltorch.mul(t2010, t2010)  # t2011: "cuda:0 f32[1, 512, 4096]"
    # t2011 = prims.mul(t2010, t2010)  # t2011: "cuda:0 f32[1, 512, 4096]"
  t2013 = prims.sum(t2011, (2,))  # t2013: "cuda:0 f32[1, 512]"
  t2014 = prims.broadcast_in_dim(t2013, [1, 512, 1], [0, 1])  # t2014: "cuda:0 f32[1, 512, 1]"
  t2016 = ltorch.true_divide(t2014, 4096.0)  # t2016: "cuda:0 f32[1, 512, 1]"
    # t2016 = prims.div(t2014, 4096.0)  # t2016: "cuda:0 f32[1, 512, 1]"
  t2018 = ltorch.add(t2016, 1e-05, alpha=None)  # t2018: "cuda:0 f32[1, 512, 1]"
    # t2018 = prims.add(t2016, 1e-05)  # t2018: "cuda:0 f32[1, 512, 1]"
  t2019 = prims.rsqrt(t2018)  # t2019: "cuda:0 f32[1, 512, 1]"
  t2020 = prims.broadcast_in_dim(t2019, (1, 512, 4096), (0, 1, 2))  # t2020: "cuda:0 f32[1, 512, 4096]"
  t2021 = ltorch.mul(t2010, t2020)  # t2021: "cuda:0 f32[1, 512, 4096]"
    # t2021 = prims.mul(t2010, t2020)  # t2021: "cuda:0 f32[1, 512, 4096]"
  t2022 = prims.convert_element_type(t2021, dtypes.bfloat16)  # t2022: "cuda:0 bf16[1, 512, 4096]"
  t2023 = prims.broadcast_in_dim(t_transformer_h_14_norm_1_weight, (1, 512, 4096), (2,))  # t2023: "cuda:0 bf16[1, 512, 4096]"
  t2024 = prims.convert_element_type(t2022, dtypes.float32)  # t2024: "cuda:0 f32[1, 512, 4096]"
  t2025 = prims.convert_element_type(t2023, dtypes.float32)  # t2025: "cuda:0 f32[1, 512, 4096]"
  t2026 = ltorch.mul(t2024, t2025)  # t2026: "cuda:0 f32[1, 512, 4096]"
    # t2026 = prims.mul(t2024, t2025)  # t2026: "cuda:0 f32[1, 512, 4096]"
  t2027 = prims.convert_element_type(t2026, dtypes.bfloat16)  # t2027: "cuda:0 bf16[1, 512, 4096]"
  t2028 = torch.nn.functional.linear(t2027, t_transformer_h_14_attn_attn_weight, None)  # t2028: "cuda:0 bf16[1, 512, 12288]"
    # t2028 = ltorch.linear(t2027, t_transformer_h_14_attn_attn_weight, None)  # t2028: "cuda:0 bf16[1, 512, 12288]"
      # t2028 = prims.linear(t2027, t_transformer_h_14_attn_attn_weight, None)  # t2028: "cuda:0 bf16[1, 512, 12288]"
  t2034 = prims.reshape(t2028, (1, 512, 32, 3, 128))  # t2034: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t2040 = prims.transpose(t2034, (0, 2, 3, 1, 4))  # t2040: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t2041, t2042, t2043) = ltorch.split(t2040, (1, 1, 1), 2)
    # t2041 = prims.slice_prim(t2040, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2041: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2042 = prims.slice_prim(t2040, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2042: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2043 = prims.slice_prim(t2040, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2043: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t2049 = prims.reshape(t2041, (1, 32, 512, 128))  # t2049: "cuda:0 bf16[1, 32, 512, 128]"
  t2055 = prims.reshape(t2042, (1, 32, 512, 128))  # t2055: "cuda:0 bf16[1, 32, 512, 128]"
  t2061 = prims.reshape(t2043, (1, 32, 512, 128))  # t2061: "cuda:0 bf16[1, 32, 512, 128]"
  t2062 = prims.slice_prim(t2049, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2062: "cuda:0 bf16[1, 32, 512, 128]"
  t2063 = prims.slice_prim(t2062, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2063: "cuda:0 bf16[1, 32, 512, 64]"
  t2064 = prims.slice_prim(t2062, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2064: "cuda:0 bf16[1, 32, 512, 64]"
  t2065 = prims.convert_element_type(t2064, dtypes.float32)  # t2065: "cuda:0 f32[1, 32, 512, 64]"
  t2066 = prims.neg(t2065)  # t2066: "cuda:0 f32[1, 32, 512, 64]"
  t2067 = prims.convert_element_type(t2066, dtypes.bfloat16)  # t2067: "cuda:0 bf16[1, 32, 512, 64]"
  t2069 = prims.cat((t2067, t2063), -1)  # t2069: "cuda:0 bf16[1, 32, 512, 128]"
  t2070 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2070: "cuda:0 f32[1, 32, 512, 128]"
  t2071 = prims.convert_element_type(t2062, dtypes.float32)  # t2071: "cuda:0 f32[1, 32, 512, 128]"
  t2072 = ltorch.mul(t2071, t2070)  # t2072: "cuda:0 f32[1, 32, 512, 128]"
    # t2072 = prims.mul(t2071, t2070)  # t2072: "cuda:0 f32[1, 32, 512, 128]"
  t2073 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2073: "cuda:0 f32[1, 32, 512, 128]"
  t2074 = prims.convert_element_type(t2069, dtypes.float32)  # t2074: "cuda:0 f32[1, 32, 512, 128]"
  t2075 = ltorch.mul(t2074, t2073)  # t2075: "cuda:0 f32[1, 32, 512, 128]"
    # t2075 = prims.mul(t2074, t2073)  # t2075: "cuda:0 f32[1, 32, 512, 128]"
  t2076 = ltorch.add(t2072, t2075, alpha=None)  # t2076: "cuda:0 f32[1, 32, 512, 128]"
    # t2076 = prims.add(t2072, t2075)  # t2076: "cuda:0 f32[1, 32, 512, 128]"
  t2077 = prims.convert_element_type(t2076, dtypes.bfloat16)  # t2077: "cuda:0 bf16[1, 32, 512, 128]"
  t2078 = prims.slice_prim(t2055, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2078: "cuda:0 bf16[1, 32, 512, 128]"
  t2079 = prims.slice_prim(t2078, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2079: "cuda:0 bf16[1, 32, 512, 64]"
  t2080 = prims.slice_prim(t2078, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2080: "cuda:0 bf16[1, 32, 512, 64]"
  t2081 = prims.convert_element_type(t2080, dtypes.float32)  # t2081: "cuda:0 f32[1, 32, 512, 64]"
  t2082 = prims.neg(t2081)  # t2082: "cuda:0 f32[1, 32, 512, 64]"
  t2083 = prims.convert_element_type(t2082, dtypes.bfloat16)  # t2083: "cuda:0 bf16[1, 32, 512, 64]"
  t2085 = prims.cat((t2083, t2079), -1)  # t2085: "cuda:0 bf16[1, 32, 512, 128]"
  t2086 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2086: "cuda:0 f32[1, 32, 512, 128]"
  t2087 = prims.convert_element_type(t2078, dtypes.float32)  # t2087: "cuda:0 f32[1, 32, 512, 128]"
  t2088 = ltorch.mul(t2087, t2086)  # t2088: "cuda:0 f32[1, 32, 512, 128]"
    # t2088 = prims.mul(t2087, t2086)  # t2088: "cuda:0 f32[1, 32, 512, 128]"
  t2089 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2089: "cuda:0 f32[1, 32, 512, 128]"
  t2090 = prims.convert_element_type(t2085, dtypes.float32)  # t2090: "cuda:0 f32[1, 32, 512, 128]"
  t2091 = ltorch.mul(t2090, t2089)  # t2091: "cuda:0 f32[1, 32, 512, 128]"
    # t2091 = prims.mul(t2090, t2089)  # t2091: "cuda:0 f32[1, 32, 512, 128]"
  t2092 = ltorch.add(t2088, t2091, alpha=None)  # t2092: "cuda:0 f32[1, 32, 512, 128]"
    # t2092 = prims.add(t2088, t2091)  # t2092: "cuda:0 f32[1, 32, 512, 128]"
  t2093 = prims.convert_element_type(t2092, dtypes.bfloat16)  # t2093: "cuda:0 bf16[1, 32, 512, 128]"
  t2094 = prims.slice_prim(t2049, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2094: "cuda:0 bf16[1, 32, 512, 0]"
  t2096 = prims.cat((t2077, t2094), -1)  # t2096: "cuda:0 bf16[1, 32, 512, 128]"
  t2097 = prims.slice_prim(t2055, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2097: "cuda:0 bf16[1, 32, 512, 0]"
  t2099 = prims.cat((t2093, t2097), -1)  # t2099: "cuda:0 bf16[1, 32, 512, 128]"
  (t2100, t2101, t2102, t2103) = cudnn_sdpa_fwd(t2096, t2099, t2061, None, 0.0, True, scale=0.08838834764831843)
  t2106 = prims.transpose(t2100, (0, 2, 1, 3))  # t2106: "cuda:0 bf16[1, 512, 32, 128]"
  t2110 = prims.reshape(t2106, (1, 512, 4096))  # t2110: "cuda:0 bf16[1, 512, 4096]"
  t2111 = torch.nn.functional.linear(t2110, t_transformer_h_14_attn_proj_weight, None)  # t2111: "cuda:0 bf16[1, 512, 4096]"
    # t2111 = ltorch.linear(t2110, t_transformer_h_14_attn_proj_weight, None)  # t2111: "cuda:0 bf16[1, 512, 4096]"
      # t2111 = prims.linear(t2110, t_transformer_h_14_attn_proj_weight, None)  # t2111: "cuda:0 bf16[1, 512, 4096]"
  t2112 = prims.convert_element_type(t2111, dtypes.float32)  # t2112: "cuda:0 f32[1, 512, 4096]"
  t2113 = prims.convert_element_type(t2009, dtypes.float32)  # t2113: "cuda:0 f32[1, 512, 4096]"
  t2114 = ltorch.add(t2112, t2113, alpha=None)  # t2114: "cuda:0 f32[1, 512, 4096]"
    # t2114 = prims.add(t2112, t2113)  # t2114: "cuda:0 f32[1, 512, 4096]"
  t2115 = prims.convert_element_type(t2114, dtypes.bfloat16)  # t2115: "cuda:0 bf16[1, 512, 4096]"
  t2116 = prims.convert_element_type(t2115, dtypes.float32)  # t2116: "cuda:0 f32[1, 512, 4096]"
  t2117 = ltorch.mul(t2116, t2116)  # t2117: "cuda:0 f32[1, 512, 4096]"
    # t2117 = prims.mul(t2116, t2116)  # t2117: "cuda:0 f32[1, 512, 4096]"
  t2119 = prims.sum(t2117, (2,))  # t2119: "cuda:0 f32[1, 512]"
  t2120 = prims.broadcast_in_dim(t2119, [1, 512, 1], [0, 1])  # t2120: "cuda:0 f32[1, 512, 1]"
  t2122 = ltorch.true_divide(t2120, 4096.0)  # t2122: "cuda:0 f32[1, 512, 1]"
    # t2122 = prims.div(t2120, 4096.0)  # t2122: "cuda:0 f32[1, 512, 1]"
  t2124 = ltorch.add(t2122, 1e-05, alpha=None)  # t2124: "cuda:0 f32[1, 512, 1]"
    # t2124 = prims.add(t2122, 1e-05)  # t2124: "cuda:0 f32[1, 512, 1]"
  t2125 = prims.rsqrt(t2124)  # t2125: "cuda:0 f32[1, 512, 1]"
  t2126 = prims.broadcast_in_dim(t2125, (1, 512, 4096), (0, 1, 2))  # t2126: "cuda:0 f32[1, 512, 4096]"
  t2127 = ltorch.mul(t2116, t2126)  # t2127: "cuda:0 f32[1, 512, 4096]"
    # t2127 = prims.mul(t2116, t2126)  # t2127: "cuda:0 f32[1, 512, 4096]"
  t2128 = prims.convert_element_type(t2127, dtypes.bfloat16)  # t2128: "cuda:0 bf16[1, 512, 4096]"
  t2129 = prims.broadcast_in_dim(t_transformer_h_14_norm_2_weight, (1, 512, 4096), (2,))  # t2129: "cuda:0 bf16[1, 512, 4096]"
  t2130 = prims.convert_element_type(t2128, dtypes.float32)  # t2130: "cuda:0 f32[1, 512, 4096]"
  t2131 = prims.convert_element_type(t2129, dtypes.float32)  # t2131: "cuda:0 f32[1, 512, 4096]"
  t2132 = ltorch.mul(t2130, t2131)  # t2132: "cuda:0 f32[1, 512, 4096]"
    # t2132 = prims.mul(t2130, t2131)  # t2132: "cuda:0 f32[1, 512, 4096]"
  t2133 = prims.convert_element_type(t2132, dtypes.bfloat16)  # t2133: "cuda:0 bf16[1, 512, 4096]"
  t2134 = torch.nn.functional.linear(t2133, t_transformer_h_14_mlp_fc_1_weight, None)  # t2134: "cuda:0 bf16[1, 512, 11008]"
    # t2134 = ltorch.linear(t2133, t_transformer_h_14_mlp_fc_1_weight, None)  # t2134: "cuda:0 bf16[1, 512, 11008]"
      # t2134 = prims.linear(t2133, t_transformer_h_14_mlp_fc_1_weight, None)  # t2134: "cuda:0 bf16[1, 512, 11008]"
  t2135 = torch.nn.functional.linear(t2133, t_transformer_h_14_mlp_fc_2_weight, None)  # t2135: "cuda:0 bf16[1, 512, 11008]"
    # t2135 = ltorch.linear(t2133, t_transformer_h_14_mlp_fc_2_weight, None)  # t2135: "cuda:0 bf16[1, 512, 11008]"
      # t2135 = prims.linear(t2133, t_transformer_h_14_mlp_fc_2_weight, None)  # t2135: "cuda:0 bf16[1, 512, 11008]"
  t2136 = prims.convert_element_type(t2134, dtypes.float32)  # t2136: "cuda:0 f32[1, 512, 11008]"
  t2137 = prims.neg(t2136)  # t2137: "cuda:0 f32[1, 512, 11008]"
  t2138 = prims.exp(t2137)  # t2138: "cuda:0 f32[1, 512, 11008]"
  t2139 = ltorch.add(1.0, t2138, alpha=None)  # t2139: "cuda:0 f32[1, 512, 11008]"
    # t2139 = prims.add(1.0, t2138)  # t2139: "cuda:0 f32[1, 512, 11008]"
  t2140 = prims.reciprocal(t2139)  # t2140: "cuda:0 f32[1, 512, 11008]"
  t2141 = prims.convert_element_type(t2140, dtypes.bfloat16)  # t2141: "cuda:0 bf16[1, 512, 11008]"
  t2142 = prims.convert_element_type(t2134, dtypes.float32)  # t2142: "cuda:0 f32[1, 512, 11008]"
  t2143 = prims.convert_element_type(t2141, dtypes.float32)  # t2143: "cuda:0 f32[1, 512, 11008]"
  t2144 = ltorch.mul(t2142, t2143)  # t2144: "cuda:0 f32[1, 512, 11008]"
    # t2144 = prims.mul(t2142, t2143)  # t2144: "cuda:0 f32[1, 512, 11008]"
  t2145 = prims.convert_element_type(t2144, dtypes.bfloat16)  # t2145: "cuda:0 bf16[1, 512, 11008]"
  t2146 = prims.convert_element_type(t2145, dtypes.float32)  # t2146: "cuda:0 f32[1, 512, 11008]"
  t2147 = prims.convert_element_type(t2135, dtypes.float32)  # t2147: "cuda:0 f32[1, 512, 11008]"
  t2148 = ltorch.mul(t2146, t2147)  # t2148: "cuda:0 f32[1, 512, 11008]"
    # t2148 = prims.mul(t2146, t2147)  # t2148: "cuda:0 f32[1, 512, 11008]"
  t2149 = prims.convert_element_type(t2148, dtypes.bfloat16)  # t2149: "cuda:0 bf16[1, 512, 11008]"
  t2150 = torch.nn.functional.linear(t2149, t_transformer_h_14_mlp_proj_weight, None)  # t2150: "cuda:0 bf16[1, 512, 4096]"
    # t2150 = ltorch.linear(t2149, t_transformer_h_14_mlp_proj_weight, None)  # t2150: "cuda:0 bf16[1, 512, 4096]"
      # t2150 = prims.linear(t2149, t_transformer_h_14_mlp_proj_weight, None)  # t2150: "cuda:0 bf16[1, 512, 4096]"
  t2151 = prims.convert_element_type(t2150, dtypes.float32)  # t2151: "cuda:0 f32[1, 512, 4096]"
  t2152 = prims.convert_element_type(t2115, dtypes.float32)  # t2152: "cuda:0 f32[1, 512, 4096]"
  t2153 = ltorch.add(t2151, t2152, alpha=None)  # t2153: "cuda:0 f32[1, 512, 4096]"
    # t2153 = prims.add(t2151, t2152)  # t2153: "cuda:0 f32[1, 512, 4096]"
  t2154 = prims.convert_element_type(t2153, dtypes.bfloat16)  # t2154: "cuda:0 bf16[1, 512, 4096]"
  t2155 = prims.convert_element_type(t2154, dtypes.float32)  # t2155: "cuda:0 f32[1, 512, 4096]"
  t2156 = ltorch.mul(t2155, t2155)  # t2156: "cuda:0 f32[1, 512, 4096]"
    # t2156 = prims.mul(t2155, t2155)  # t2156: "cuda:0 f32[1, 512, 4096]"
  t2158 = prims.sum(t2156, (2,))  # t2158: "cuda:0 f32[1, 512]"
  t2159 = prims.broadcast_in_dim(t2158, [1, 512, 1], [0, 1])  # t2159: "cuda:0 f32[1, 512, 1]"
  t2161 = ltorch.true_divide(t2159, 4096.0)  # t2161: "cuda:0 f32[1, 512, 1]"
    # t2161 = prims.div(t2159, 4096.0)  # t2161: "cuda:0 f32[1, 512, 1]"
  t2163 = ltorch.add(t2161, 1e-05, alpha=None)  # t2163: "cuda:0 f32[1, 512, 1]"
    # t2163 = prims.add(t2161, 1e-05)  # t2163: "cuda:0 f32[1, 512, 1]"
  t2164 = prims.rsqrt(t2163)  # t2164: "cuda:0 f32[1, 512, 1]"
  t2165 = prims.broadcast_in_dim(t2164, (1, 512, 4096), (0, 1, 2))  # t2165: "cuda:0 f32[1, 512, 4096]"
  t2166 = ltorch.mul(t2155, t2165)  # t2166: "cuda:0 f32[1, 512, 4096]"
    # t2166 = prims.mul(t2155, t2165)  # t2166: "cuda:0 f32[1, 512, 4096]"
  t2167 = prims.convert_element_type(t2166, dtypes.bfloat16)  # t2167: "cuda:0 bf16[1, 512, 4096]"
  t2168 = prims.broadcast_in_dim(t_transformer_h_15_norm_1_weight, (1, 512, 4096), (2,))  # t2168: "cuda:0 bf16[1, 512, 4096]"
  t2169 = prims.convert_element_type(t2167, dtypes.float32)  # t2169: "cuda:0 f32[1, 512, 4096]"
  t2170 = prims.convert_element_type(t2168, dtypes.float32)  # t2170: "cuda:0 f32[1, 512, 4096]"
  t2171 = ltorch.mul(t2169, t2170)  # t2171: "cuda:0 f32[1, 512, 4096]"
    # t2171 = prims.mul(t2169, t2170)  # t2171: "cuda:0 f32[1, 512, 4096]"
  t2172 = prims.convert_element_type(t2171, dtypes.bfloat16)  # t2172: "cuda:0 bf16[1, 512, 4096]"
  t2173 = torch.nn.functional.linear(t2172, t_transformer_h_15_attn_attn_weight, None)  # t2173: "cuda:0 bf16[1, 512, 12288]"
    # t2173 = ltorch.linear(t2172, t_transformer_h_15_attn_attn_weight, None)  # t2173: "cuda:0 bf16[1, 512, 12288]"
      # t2173 = prims.linear(t2172, t_transformer_h_15_attn_attn_weight, None)  # t2173: "cuda:0 bf16[1, 512, 12288]"
  t2179 = prims.reshape(t2173, (1, 512, 32, 3, 128))  # t2179: "cuda:0 bf16[1, 512, 32, 3, 128]"
  t2185 = prims.transpose(t2179, (0, 2, 3, 1, 4))  # t2185: "cuda:0 bf16[1, 32, 3, 512, 128]"
  (t2186, t2187, t2188) = ltorch.split(t2185, (1, 1, 1), 2)
    # t2186 = prims.slice_prim(t2185, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2186: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2187 = prims.slice_prim(t2185, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2187: "cuda:0 bf16[1, 32, 1, 512, 128]"
    # t2188 = prims.slice_prim(t2185, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2188: "cuda:0 bf16[1, 32, 1, 512, 128]"
  t2194 = prims.reshape(t2186, (1, 32, 512, 128))  # t2194: "cuda:0 bf16[1, 32, 512, 128]"
  t2200 = prims.reshape(t2187, (1, 32, 512, 128))  # t2200: "cuda:0 bf16[1, 32, 512, 128]"
  t2206 = prims.reshape(t2188, (1, 32, 512, 128))  # t2206: "cuda:0 bf16[1, 32, 512, 128]"
  t2207 = prims.slice_prim(t2194, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2207: "cuda:0 bf16[1, 32, 512, 128]"
  t2208 = prims.slice_prim(t2207, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2208: "cuda:0 bf16[1, 32, 512, 64]"
  t2209 = prims.slice_prim(t2207, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2209: "cuda:0 bf16[1, 32, 512, 64]"
  t2210 = prims.convert_element_type(t2209, dtypes.float32)  # t2210: "cuda:0 f32[1, 32, 512, 64]"
  t2211 = prims.neg(t2210)  # t2211: "cuda:0 f32[1, 32, 512, 64]"
  t2212 = prims.convert_element_type(t2211, dtypes.bfloat16)  # t2212: "cuda:0 bf16[1, 32, 512, 64]"
  t2214 = prims.cat((t2212, t2208), -1)  # t2214: "cuda:0 bf16[1, 32, 512, 128]"
  t2215 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2215: "cuda:0 f32[1, 32, 512, 128]"
  t2216 = prims.convert_element_type(t2207, dtypes.float32)  # t2216: "cuda:0 f32[1, 32, 512, 128]"
  t2217 = ltorch.mul(t2216, t2215)  # t2217: "cuda:0 f32[1, 32, 512, 128]"
    # t2217 = prims.mul(t2216, t2215)  # t2217: "cuda:0 f32[1, 32, 512, 128]"
  t2218 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2218: "cuda:0 f32[1, 32, 512, 128]"
  t2219 = prims.convert_element_type(t2214, dtypes.float32)  # t2219: "cuda:0 f32[1, 32, 512, 128]"
  t2220 = ltorch.mul(t2219, t2218)  # t2220: "cuda:0 f32[1, 32, 512, 128]"
    # t2220 = prims.mul(t2219, t2218)  # t2220: "cuda:0 f32[1, 32, 512, 128]"
  t2221 = ltorch.add(t2217, t2220, alpha=None)  # t2221: "cuda:0 f32[1, 32, 512, 128]"
    # t2221 = prims.add(t2217, t2220)  # t2221: "cuda:0 f32[1, 32, 512, 128]"
  t2222 = prims.convert_element_type(t2221, dtypes.bfloat16)  # t2222: "cuda:0 bf16[1, 32, 512, 128]"
  t2223 = prims.slice_prim(t2200, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t2223: "cuda:0 bf16[1, 32, 512, 128]"
  t2224 = prims.slice_prim(t2223, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t2224: "cuda:0 bf16[1, 32, 512, 64]"
  t2225 = prims.slice_prim(t2223, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t2225: "cuda:0 bf16[1, 32, 512, 64]"
  t2226 = prims.convert_element_type(t2225, dtypes.float32)  # t2226: "cuda:0 f32[1, 32, 512, 64]"
  t2227 = prims.neg(t2226)  # t2227: "cuda:0 f32[1, 32, 512, 64]"
  t2228 = prims.convert_element_type(t2227, dtypes.bfloat16)  # t2228: "cuda:0 bf16[1, 32, 512, 64]"
  t2230 = prims.cat((t2228, t2224), -1)  # t2230: "cuda:0 bf16[1, 32, 512, 128]"
  t2231 = prims.broadcast_in_dim(t0, (1, 32, 512, 128), (2, 3))  # t2231: "cuda:0 f32[1, 32, 512, 128]"
  t2232 = prims.convert_element_type(t2223, dtypes.float32)  # t2232: "cuda:0 f32[1, 32, 512, 128]"
  t2233 = ltorch.mul(t2232, t2231)  # t2233: "cuda:0 f32[1, 32, 512, 128]"
    # t2233 = prims.mul(t2232, t2231)  # t2233: "cuda:0 f32[1, 32, 512, 128]"
  t2234 = prims.broadcast_in_dim(t1, (1, 32, 512, 128), (2, 3))  # t2234: "cuda:0 f32[1, 32, 512, 128]"
  t2235 = prims.convert_element_type(t2230, dtypes.float32)  # t2235: "cuda:0 f32[1, 32, 512, 128]"
  t2236 = ltorch.mul(t2235, t2234)  # t2236: "cuda:0 f32[1, 32, 512, 128]"
    # t2236 = prims.mul(t2235, t2234)  # t2236: "cuda:0 f32[1, 32, 512, 128]"
  t2237 = ltorch.add(t2233, t2236, alpha=None)  # t2237: "cuda:0 f32[1, 32, 512, 128]"
    # t2237 = prims.add(t2233, t2236)  # t2237: "cuda:0 f32[1, 32, 512, 128]"
  t2238 = prims.convert_element_type(t2237, dtypes.bfloat16)  # t2238: "cuda:0 bf16[1, 32, 512, 128]"
  t2239 = prims.slice_prim(t2194, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2239: "cuda:0 bf16[1, 32, 512, 0]"
  t2241 = prims.cat((t2222, t2239), -1)  # t2241: "cuda:0 bf16[1, 32, 512, 128]"
  t2242 = prims.slice_prim(t2200, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t2242: "cuda:0 bf16[1, 32, 512, 0]"
  t2244 = prims.cat((t2238, t2242), -1)  # t2244: "cuda:0 bf16[1, 32, 512, 128]"
  (t2245, t2246, t2247, t2248) = cudnn_sdpa_fwd(t2241, t2244, t2206, None, 0.0, True, scale=0.08838834764831843)
  t2251 = prims.transpose(t2245, (0, 2, 1, 3))  # t2251: "cuda:0 bf16[1, 512, 32, 128]"
  t2255 = prims.reshape(t2251, (1, 512, 4096))  # t2255: "cuda:0 bf16[1, 512, 4096]"
  t2256 = torch.nn.functional.linear(t2255, t_transformer_h_15_attn_proj_weight, None)  # t2256: "cuda:0 bf16[1, 512, 4096]"
    # t2256 = ltorch.linear(t2255, t_transformer_h_15_attn_proj_weight, None)  # t2256: "cuda:0 bf16[1, 512, 4096]"
      # t2256 = prims.linear(t2255, t_transformer_h_15_attn_proj_weight, None)  # t2256: "cuda:0 bf16[1, 512, 4096]"
  t2257 = prims.convert_element_type(t2256, dtypes.float32)  # t2257: "cuda:0 f32[1, 512, 4096]"
  t2258 = prims.convert_element_type(t2154, dtypes.float32)  # t2258: "cuda:0 f32[1, 512, 4096]"
  t2259 = ltorch.add(t2257, t2258, alpha=None)  # t2259: "cuda:0 f32[1, 512, 4096]"
    # t2259 = prims.add(t2257, t2258)  # t2259: "cuda:0 f32[1, 512, 4096]"
  t2260 = prims.convert_element_type(t2259, dtypes.bfloat16)  # t2260: "cuda:0 bf16[1, 512, 4096]"
  t2261 = prims.convert_element_type(t2260, dtypes.float32)  # t2261: "cuda:0 f32[1, 512, 4096]"
  t2262 = ltorch.mul(t2261, t2261)  # t2262: "cuda:0 f32[1, 512, 4096]"
    # t2262 = prims.mul(t2261, t2261)  # t2262: "cuda:0 f32[1, 512, 4096]"
  t2264 = prims.sum(t2262, (2,))  # t2264: "cuda:0 f32[1, 512]"
  t2265 = prims.broadcast_in_dim(t2264, [1, 512, 1], [0, 1])  # t2265: "cuda:0 f32[1, 512, 1]"
  t2267 = ltorch.true_divide(t2265, 4096.0)  # t2267: "cuda:0 f32[1, 512, 1]"
    # t2267 = prims.div(t2265, 4096.0)  # t2267: "cuda:0 f32[1, 512, 1]"
  t2269 = ltorch.add(t2267, 1e-05, alpha=None)  # t2269: "cuda:0 f32[1, 512, 1]"
    # t2269 = prims.add(t2267, 1e-05)  # t2269: "cuda:0 f32[1, 512, 1]"
  t2270 = prims.rsqrt(t2269)  # t2270: "cuda:0 f32[1, 512, 1]"
  t2271 = prims.broadcast_in_dim(t2270, (1, 512, 4096), (0, 1, 2))  # t2271: "cuda:0 f32[1, 512, 4096]"
  t2272 = ltorch.mul(t2261, t2271)  # t2272: "cuda:0 f32[1, 512, 4096]"
    # t2272 = prims.mul(t2261, t2271)  # t2272: "cuda:0 f32[1, 512, 4096]"
  t2273 = prims.convert_element_type(t2272, dtypes.bfloat16)  # t2273: "cuda:0 bf16[1, 512, 4096]"
  t2274 = prims.broadcast_in_dim(t_transformer_h_15_norm_2_weight, (1, 512, 4096), (2,))  # t2274: "cuda:0 bf16[1, 512, 4096]"
  t2275 = prims.convert_element_type(t2273, dtypes.float32)  # t2275: "cuda:0 f32[1, 512, 4096]"
  t2276 = prims.convert_element_type(t2274, dtypes.float32)  # t2276: "cuda:0 f32[1, 512, 4096]"
  t2277 = ltorch.mul(t2275, t2276)  # t2277: "cuda:0 f32[1, 512, 4096]"
    # t2277 = prims.mul(t2275, t2276)  # t2277: "cuda:0 f32[1, 512, 4096]"
  t2278 = prims.convert_element_type(t2277, dtypes.bfloat16)  # t2278: "cuda:0 bf16[1, 512, 4096]"
  t2279 = torch.nn.functional.linear(t2278, t_transformer_h_15_mlp_fc_1_weight, None)  # t2279: "cuda:0 bf16[1, 512, 11008]"
    # t2279 = ltorch.linear(t2278, t_transformer_h_15_mlp_fc_1_weight, None)  # t2279: "cuda:0 bf16[1, 512, 11008]"
      # t2279 = prims.linear(t2278, t_transformer_h_15_mlp_fc_1_weight, None)  # t2279: "cuda:0 bf16[1, 512, 11008]"
  t2280 = torch.nn.functional.linear(t2278, t_transformer_h_15_mlp_fc_2_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
    # t2280 = ltorch.linear(t2278, t_transformer_h_15_mlp_fc_2_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
      # t2280 = prims.linear(t2278, t_transformer_h_15_mlp_fc_2_weight, None)  # t2280: "cuda:0 bf16[1, 512, 11008]"
  t2281 = prims.convert_element_type(t2279, dtypes.float32)  # t2281: "cuda:0 f32[1, 512, 11008]"
  t2282 = prims.neg(t2281)  # t2282: "cuda:0 f32[1, 512, 11008]"
  t2283 = prims.exp(t2282)  # t2283: "cuda:0 f32[1, 512, 11008]"
  t2284 = ltorch.add(1.0, t2283, alpha=None)  # t2284: "cuda:0 f32[1, 512, 11008]"
    # t2284 = prims.add(1.0, t2283)  # t2284: "cuda:0 f32[1, 512, 11008]"
  t2285 = prims.reciprocal(t2284)  # t2285: "cuda:0 f32[1, 512, 11008]"
  t2286 = prims.convert_element_type(t2285, dtypes.bfloat16)  # t2286: "cuda:0 bf16[1, 512, 11008]"
  t2287 = prims.convert_element_type(t2279, dtypes.float32)  # t2287: "cuda:0 f32[1, 512, 11008]"
  t2288 = prims.convert_element_type(t2286, dtypes.float32)  # t2288: "cuda:0 f32[1, 512, 11008]"
  t2289 = ltorch.mul(t2287, t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
    # t2289 = prims.mul(t2287, t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"
  t2290 = prims.convert_element_type(t2289, dtypes.bfloat16)  # t2290: "cuda:0 bf16[1, 512, 11008]"
  t2291 = prims.convert_element_type(t2290, dtypes.float32)  # t2291: "cuda:0 f32[1, 512, 11008]"
  t2292 = prims.convert_element_type(t2280, dtypes.float32)  # t2292: "cuda:0 f32[1, 512, 11008]"
  t2293 = ltorch.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
    # t2293 = prims.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"
  t2294 = prims.convert_element_type(t2293, dtypes.bfloat16)  # t2294: "cuda:0 bf16[1, 512, 11008]"
  t2295 = torch.nn.functional.linear(t2294, t_transformer_h_15_mlp_proj_weight, None)  # t2295: "cuda:0 bf16[1, 512, 4096]"
    # t2295 = ltorch.linear(t2294, t_transformer_h_15_mlp_proj_weight, None)  # t2295: "cuda:0 bf16[1, 512, 4096]"
      # t2295 = prims.linear(t2294, t_transformer_h_15_mlp_proj_weight, None)  # t2295: "cuda:0 bf16[1, 512, 4096]"
  t2296 = prims.convert_element_type(t2295, dtypes.float32)  # t2296: "cuda:0 f32[1, 512, 4096]"
  t2297 = prims.convert_element_type(t2260, dtypes.float32)  # t2297: "cuda:0 f32[1, 512, 4096]"
  t2298 = ltorch.add(t2296, t2297, alpha=None)  # t2298: "cuda:0 f32[1, 512, 4096]"
    # t2298 = prims.add(t2296, t2297)  # t2298: "cuda:0 f32[1, 512, 4096]"
  t2299 = prims.convert_element_type(t2298, dtypes.bfloat16)  # t2299: "cuda:0 bf16[1, 512, 4096]"
  t2300 = prims.convert_element_type(t2299, dtypes.float32)  # t2300: "cuda:0 f32[1, 512, 4096]"
  t2301 = ltorch.mul(t2300, t2300)  # t2301: "cuda:0 f32[1, 512, 4096]"
    # t2301 = prims.mul(t2300, t2300)  # t2301: "cuda:0 f32[1, 512, 4096]"
  t2303 = prims.sum(t2301, (2,))  # t2303: "cuda:0 f32[1, 512]"
  t2304 = prims.broadcast_in_dim(t2303, [1, 512, 1], [0, 1])  # t2304: "cuda:0 f32[1, 512, 1]"
  t2306 = ltorch.true_divide(t2304, 4096.0)  # t2306: "cuda:0 f32[1, 512, 1]"
    # t2306 = prims.div(t2304, 4096.0)  # t2306: "cuda:0 f32[1, 512, 1]"
  t2308 = ltorch.add(t2306, 1e-05, alpha=None)  # t2308: "cuda:0 f32[1, 512, 1]"
    # t2308 = prims.add(t2306, 1e-05)  # t2308: "cuda:0 f32[1, 512, 1]"
  t2309 = prims.rsqrt(t2308)  # t2309: "cuda:0 f32[1, 512, 1]"
  t2310 = prims.broadcast_in_dim(t2309, (1, 512, 4096), (0, 1, 2))  # t2310: "cuda:0 f32[1, 512, 4096]"
  t2311 = ltorch.mul(t2300, t2310)  # t2311: "cuda:0 f32[1, 512, 4096]"
    # t2311 = prims.mul(t2300, t2310)  # t2311: "cuda:0 f32[1, 512, 4096]"
  t2312 = prims.convert_element_type(t2311, dtypes.bfloat16)  # t2312: "cuda:0 bf16[1, 512, 4096]"
  t2313 = prims.broadcast_in_dim(t_transformer_ln_f_weight, (1, 512, 4096), (2,))  # t2313: "cuda:0 bf16[1, 512, 4096]"
  t2314 = prims.convert_element_type(t2312, dtypes.float32)  # t2314: "cuda:0 f32[1, 512, 4096]"
  t2315 = prims.convert_element_type(t2313, dtypes.float32)  # t2315: "cuda:0 f32[1, 512, 4096]"
  t2316 = ltorch.mul(t2314, t2315)  # t2316: "cuda:0 f32[1, 512, 4096]"
    # t2316 = prims.mul(t2314, t2315)  # t2316: "cuda:0 f32[1, 512, 4096]"
  t2317 = prims.convert_element_type(t2316, dtypes.bfloat16)  # t2317: "cuda:0 bf16[1, 512, 4096]"
  t2318 = torch.nn.functional.linear(t2317, t_lm_head_weight, None)  # t2318: "cuda:0 bf16[1, 512, 32000]"
    # t2318 = ltorch.linear(t2317, t_lm_head_weight, None)  # t2318: "cuda:0 bf16[1, 512, 32000]"
      # t2318 = prims.linear(t2317, t_lm_head_weight, None)  # t2318: "cuda:0 bf16[1, 512, 32000]"
  return {'output': t2318, 'flat_args': [idx, tos1, t_lm_head_weight, t_sin, t_transformer_h_0_attn_attn_weight, t_transformer_h_0_attn_proj_weight, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t_transformer_h_0_mlp_proj_weight, t_transformer_h_0_norm_1_weight, t_transformer_h_0_norm_2_weight, t_transformer_h_1_attn_attn_weight, t_transformer_h_1_attn_proj_weight, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t_transformer_h_1_mlp_proj_weight, t_transformer_h_1_norm_1_weight, t_transformer_h_1_norm_2_weight, t_transformer_h_2_attn_attn_weight, t_transformer_h_2_attn_proj_weight, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t_transformer_h_2_mlp_proj_weight, t_transformer_h_2_norm_1_weight, t_transformer_h_2_norm_2_weight, t_transformer_h_3_attn_attn_weight, t_transformer_h_3_attn_proj_weight, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t_transformer_h_3_mlp_proj_weight, t_transformer_h_3_norm_1_weight, t_transformer_h_3_norm_2_weight, t_transformer_h_4_attn_attn_weight, t_transformer_h_4_attn_proj_weight, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t_transformer_h_4_mlp_proj_weight, t_transformer_h_4_norm_1_weight, t_transformer_h_4_norm_2_weight, t_transformer_h_5_attn_attn_weight, t_transformer_h_5_attn_proj_weight, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t_transformer_h_5_mlp_proj_weight, t_transformer_h_5_norm_1_weight, t_transformer_h_5_norm_2_weight, t_transformer_h_6_attn_attn_weight, t_transformer_h_6_attn_proj_weight, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t_transformer_h_6_mlp_proj_weight, t_transformer_h_6_norm_1_weight, t_transformer_h_6_norm_2_weight, t_transformer_h_7_attn_attn_weight, t_transformer_h_7_attn_proj_weight, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t_transformer_h_7_mlp_proj_weight, t_transformer_h_7_norm_1_weight, t_transformer_h_7_norm_2_weight, t_transformer_h_8_attn_attn_weight, t_transformer_h_8_attn_proj_weight, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t_transformer_h_8_mlp_proj_weight, t_transformer_h_8_norm_1_weight, t_transformer_h_8_norm_2_weight, t_transformer_h_9_attn_attn_weight, t_transformer_h_9_attn_proj_weight, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t_transformer_h_9_mlp_proj_weight, t_transformer_h_9_norm_1_weight, t_transformer_h_9_norm_2_weight, t_transformer_h_10_attn_attn_weight, t_transformer_h_10_attn_proj_weight, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t_transformer_h_10_mlp_proj_weight, t_transformer_h_10_norm_1_weight, t_transformer_h_10_norm_2_weight, t_transformer_h_11_attn_attn_weight, t_transformer_h_11_attn_proj_weight, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t_transformer_h_11_mlp_proj_weight, t_transformer_h_11_norm_1_weight, t_transformer_h_11_norm_2_weight, t_transformer_h_12_attn_attn_weight, t_transformer_h_12_attn_proj_weight, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t_transformer_h_12_mlp_proj_weight, t_transformer_h_12_norm_1_weight, t_transformer_h_12_norm_2_weight, t_transformer_h_13_attn_attn_weight, t_transformer_h_13_attn_proj_weight, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t_transformer_h_13_mlp_proj_weight, t_transformer_h_13_norm_1_weight, t_transformer_h_13_norm_2_weight, t_transformer_h_14_attn_attn_weight, t_transformer_h_14_attn_proj_weight, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t_transformer_h_14_mlp_proj_weight, t_transformer_h_14_norm_1_weight, t_transformer_h_14_norm_2_weight, t_transformer_h_15_attn_attn_weight, t_transformer_h_15_attn_proj_weight, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t_transformer_h_15_mlp_proj_weight, t_transformer_h_15_norm_1_weight, t_transformer_h_15_norm_2_weight, t_transformer_ln_f_weight, t_transformer_wte_weight], 'flat_output': (t2318,)}, ((idx, t5, t11, t12, t17, t16, t19, t_transformer_h_0_attn_attn_weight, t46, t47, t49, t50, t62, t63, t65, t66, t71, t74, t38, t75, t76, t77, t78, t80, t_transformer_h_0_attn_proj_weight, t86, t95, t96, t101, t100, t103, t_transformer_h_0_mlp_fc_1_weight, t_transformer_h_0_mlp_fc_2_weight, t108, t110, t113, t112, t117, t116, t119, t_transformer_h_0_mlp_proj_weight, t125, t134, t135, t140, t139, t142, t_transformer_h_1_attn_attn_weight, t185, t186, t188, t189, t201, t202, t204, t205, t211, t214, t176, t215, t216, t217, t218, t225, t_transformer_h_1_attn_proj_weight, t231, t240, t241, t246, t245, t248, t_transformer_h_1_mlp_fc_1_weight, t_transformer_h_1_mlp_fc_2_weight, t253, t255, t258, t257, t262, t261, t264, t_transformer_h_1_mlp_proj_weight, t270, t279, t280, t285, t284, t287, t_transformer_h_2_attn_attn_weight, t330, t331, t333, t334, t346, t347, t349, t350, t356, t359, t321, t360, t361, t362, t363, t370, t_transformer_h_2_attn_proj_weight, t376, t385, t386, t391, t390, t393, t_transformer_h_2_mlp_fc_1_weight, t_transformer_h_2_mlp_fc_2_weight, t398, t400, t403, t402, t407, t406, t409, t_transformer_h_2_mlp_proj_weight, t415, t424, t425, t430, t429, t432, t_transformer_h_3_attn_attn_weight, t475, t476, t478, t479, t491, t492, t494, t495, t501, t504, t466, t505, t506, t507, t508, t515, t_transformer_h_3_attn_proj_weight, t521, t530, t531, t536, t535, t538, t_transformer_h_3_mlp_fc_1_weight, t_transformer_h_3_mlp_fc_2_weight, t543, t545, t548, t547, t552, t551, t554, t_transformer_h_3_mlp_proj_weight, t560, t569, t570, t575, t574, t577, t_transformer_h_4_attn_attn_weight, t620, t621, t623, t624, t636, t637, t639, t640, t646, t649, t611, t650, t651, t652, t653, t660, t_transformer_h_4_attn_proj_weight, t666, t675, t676, t681, t680, t683, t_transformer_h_4_mlp_fc_1_weight, t_transformer_h_4_mlp_fc_2_weight, t688, t690, t693, t692, t697, t696, t699, t_transformer_h_4_mlp_proj_weight, t705, t714, t715, t720, t719, t722, t_transformer_h_5_attn_attn_weight, t765, t766, t768, t769, t781, t782, t784, t785, t791, t794, t756, t795, t796, t797, t798, t805, t_transformer_h_5_attn_proj_weight, t811, t820, t821, t826, t825, t828, t_transformer_h_5_mlp_fc_1_weight, t_transformer_h_5_mlp_fc_2_weight, t833, t835, t838, t837, t842, t841, t844, t_transformer_h_5_mlp_proj_weight, t850, t859, t860, t865, t864, t867, t_transformer_h_6_attn_attn_weight, t910, t911, t913, t914, t926, t927, t929, t930, t936, t939, t901, t940, t941, t942, t943, t950, t_transformer_h_6_attn_proj_weight, t956, t965, t966, t971, t970, t973, t_transformer_h_6_mlp_fc_1_weight, t_transformer_h_6_mlp_fc_2_weight, t978, t980, t983, t982, t987, t986, t989, t_transformer_h_6_mlp_proj_weight, t995, t1004, t1005, t1010, t1009, t1012, t_transformer_h_7_attn_attn_weight, t1055, t1056, t1058, t1059, t1071, t1072, t1074, t1075, t1081, t1084, t1046, t1085, t1086, t1087, t1088, t1095, t_transformer_h_7_attn_proj_weight, t1101, t1110, t1111, t1116, t1115, t1118, t_transformer_h_7_mlp_fc_1_weight, t_transformer_h_7_mlp_fc_2_weight, t1123, t1125, t1128, t1127, t1132, t1131, t1134, t_transformer_h_7_mlp_proj_weight, t1140, t1149, t1150, t1155, t1154, t1157, t_transformer_h_8_attn_attn_weight, t1200, t1201, t1203, t1204, t1216, t1217, t1219, t1220, t1226, t1229, t1191, t1230, t1231, t1232, t1233, t1240, t_transformer_h_8_attn_proj_weight, t1246, t1255, t1256, t1261, t1260, t1263, t_transformer_h_8_mlp_fc_1_weight, t_transformer_h_8_mlp_fc_2_weight, t1268, t1270, t1273, t1272, t1277, t1276, t1279, t_transformer_h_8_mlp_proj_weight, t1285, t1294, t1295, t1300, t1299, t1302, t_transformer_h_9_attn_attn_weight, t1345, t1346, t1348, t1349, t1361, t1362, t1364, t1365, t1371, t1374, t1336, t1375, t1376, t1377, t1378, t1385, t_transformer_h_9_attn_proj_weight, t1391, t1400, t1401, t1406, t1405, t1408, t_transformer_h_9_mlp_fc_1_weight, t_transformer_h_9_mlp_fc_2_weight, t1413, t1415, t1418, t1417, t1422, t1421, t1424, t_transformer_h_9_mlp_proj_weight, t1430, t1439, t1440, t1445, t1444, t1447, t_transformer_h_10_attn_attn_weight, t1490, t1491, t1493, t1494, t1506, t1507, t1509, t1510, t1516, t1519, t1481, t1520, t1521, t1522, t1523, t1530, t_transformer_h_10_attn_proj_weight, t1536, t1545, t1546, t1551, t1550, t1553, t_transformer_h_10_mlp_fc_1_weight, t_transformer_h_10_mlp_fc_2_weight, t1558, t1560, t1563, t1562, t1567, t1566, t1569, t_transformer_h_10_mlp_proj_weight, t1575, t1584, t1585, t1590, t1589, t1592, t_transformer_h_11_attn_attn_weight, t1635, t1636, t1638, t1639, t1651, t1652, t1654, t1655, t1661, t1664, t1626, t1665, t1666, t1667, t1668, t1675, t_transformer_h_11_attn_proj_weight, t1681, t1690, t1691, t1696, t1695, t1698, t_transformer_h_11_mlp_fc_1_weight, t_transformer_h_11_mlp_fc_2_weight, t1703, t1705, t1708, t1707, t1712, t1711, t1714, t_transformer_h_11_mlp_proj_weight, t1720, t1729, t1730, t1735, t1734, t1737, t_transformer_h_12_attn_attn_weight, t1780, t1781, t1783, t1784, t1796, t1797, t1799, t1800, t1806, t1809, t1771, t1810, t1811, t1812, t1813, t1820, t_transformer_h_12_attn_proj_weight, t1826, t1835, t1836, t1841, t1840, t1843, t_transformer_h_12_mlp_fc_1_weight, t_transformer_h_12_mlp_fc_2_weight, t1848, t1850, t1853, t1852, t1857, t1856, t1859, t_transformer_h_12_mlp_proj_weight, t1865, t1874, t1875, t1880, t1879, t1882, t_transformer_h_13_attn_attn_weight, t1925, t1926, t1928, t1929, t1941, t1942, t1944, t1945, t1951, t1954, t1916, t1955, t1956, t1957, t1958, t1965, t_transformer_h_13_attn_proj_weight, t1971, t1980, t1981, t1986, t1985, t1988, t_transformer_h_13_mlp_fc_1_weight, t_transformer_h_13_mlp_fc_2_weight, t1993, t1995, t1998, t1997, t2002, t2001, t2004, t_transformer_h_13_mlp_proj_weight, t2010, t2019, t2020, t2025, t2024, t2027, t_transformer_h_14_attn_attn_weight, t2070, t2071, t2073, t2074, t2086, t2087, t2089, t2090, t2096, t2099, t2061, t2100, t2101, t2102, t2103, t2110, t_transformer_h_14_attn_proj_weight, t2116, t2125, t2126, t2131, t2130, t2133, t_transformer_h_14_mlp_fc_1_weight, t_transformer_h_14_mlp_fc_2_weight, t2138, t2140, t2143, t2142, t2147, t2146, t2149, t_transformer_h_14_mlp_proj_weight, t2155, t2164, t2165, t2170, t2169, t2172, t_transformer_h_15_attn_attn_weight, t2215, t2216, t2218, t2219, t2231, t2232, t2234, t2235, t2241, t2244, t2206, t2245, t2246, t2247, t2248, t2255, t_transformer_h_15_attn_proj_weight, t2261, t2270, t2271, t2276, t2275, t2278, t_transformer_h_15_mlp_fc_1_weight, t_transformer_h_15_mlp_fc_2_weight, t2283, t2285, t2288, t2287, t2292, t2291, t2294, t_transformer_h_15_mlp_proj_weight, t2300, t2309, t2310, t2315, t2314, t2317, t_lm_head_weight), (32000, False, False, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0, 2, 0.0, True, 0.08838834764831843, 4096.0, 4096.0))
============================================ GRAPH: _transform_for_operator_executor_execution

cur node 0 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 1 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 2 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 3 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 4 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 5 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 6 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 7 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 8 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 9 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 10 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 11 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 12 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 13 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 14 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 15 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 16 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 17 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 18 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 19 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 20 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 21 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 22 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 23 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 24 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 25 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 26 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 27 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 28 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 29 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 30 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 31 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 32 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 33 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 34 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 35 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 36 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 37 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 38 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 39 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 40 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 41 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 42 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 43 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 44 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 45 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 46 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 47 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 48 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 49 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 50 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 51 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 52 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 53 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 54 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 55 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 56 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 57 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 58 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 59 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 60 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 61 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 62 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 63 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 64 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 65 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 66 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 67 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 68 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 69 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 70 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 71 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 72 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 73 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 74 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 75 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 76 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 77 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 78 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 79 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 80 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 81 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 82 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 83 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 84 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 85 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 86 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 87 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 88 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 89 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 90 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 91 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 92 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 93 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 94 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 95 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 96 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 97 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 98 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 99 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 100 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 101 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 102 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 103 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 104 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 105 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 106 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 107 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 108 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 109 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 110 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 111 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 112 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 113 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 114 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 115 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 116 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 117 group_bsyms len: 1
[Symbol name=unpack_trivial]
[]

cur node 120 group_bsyms len: 1
[Symbol name=embedding]
[t2 = ltorch.reshape(idx, [512])  # t2: "cuda:0 i64[512]"
  # t2 = prims.reshape(idx, (512,))  # t2: "cuda:0 i64[512]", t3 = prims.take(t_transformer_wte_weight, t2, 0)  # t3: "cuda:0 bf16[512, 4096]", t4 = ltorch.reshape(t3, [1, 512, 4096])  # t4: "cuda:0 bf16[1, 512, 4096]"
  # t4 = prims.reshape(t3, (1, 512, 4096))  # t4: "cuda:0 bf16[1, 512, 4096]"]

cur node 1737 group_bsyms len: 1
[Symbol name=return]
[]

cur node 118 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1736 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 119 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 136 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 180 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 200 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 201 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 216 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 131 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 195 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 236 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 280 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 300 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 301 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 316 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 231 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 295 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 336 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 380 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 400 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 401 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 416 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 331 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 395 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 436 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 480 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 500 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 501 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 516 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 431 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 495 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 536 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 580 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 600 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 601 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 616 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 531 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 595 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 636 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 680 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 700 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 701 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 716 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 631 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 695 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 736 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 780 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 800 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 801 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 816 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 731 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 795 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 836 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 880 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 900 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 901 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 916 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 831 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 895 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 936 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 980 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1000 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1001 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1016 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 931 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 995 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1036 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1080 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1100 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1101 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1116 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1031 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1095 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1136 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1180 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1200 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1201 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1216 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1131 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1195 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1236 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1280 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1300 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1301 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1316 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1231 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1295 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1336 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1380 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1400 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1401 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1416 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1331 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1395 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1436 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1480 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1500 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1501 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1516 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1431 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1495 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1536 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1580 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1600 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1601 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1616 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1531 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1595 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1636 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1680 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1700 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1701 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1716 group_bsyms len: 1
[Symbol name=linear]
[]

cur node 1631 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1695 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1731 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 121 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 182 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1665 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 265 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 650 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1165 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1550 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 150 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 665 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1050 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1565 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 165 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 550 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1065 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1450 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 565 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 950 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1465 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 450 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 965 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1350 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 465 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 850 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1365 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 350 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 865 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1250 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 365 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 750 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1265 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1650 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 250 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 765 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1150 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 768 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1153 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1668 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 268 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 653 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1168 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1553 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 153 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 668 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1053 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1568 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 168 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 553 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1068 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1453 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 568 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 953 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1468 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 453 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 968 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1353 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 468 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 853 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1368 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 353 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 868 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1253 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 368 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 753 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1268 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1653 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 253 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 137 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 181 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 208 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 202 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 213 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 217 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 133 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 197 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 237 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 281 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 308 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 302 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 313 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 317 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 233 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 297 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 337 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 381 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 408 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 402 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 413 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 417 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 333 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 397 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 437 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 481 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 508 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 502 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 513 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 517 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 433 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 497 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 537 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 581 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 608 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 602 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 613 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 617 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 533 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 597 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 637 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 681 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 708 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 702 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 713 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 717 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 633 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 697 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 737 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 781 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 808 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 802 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 813 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 817 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 733 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 797 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 837 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 881 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 908 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 902 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 913 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 917 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 833 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 897 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 937 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 981 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1008 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1002 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1013 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1017 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 933 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 997 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1037 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1081 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1108 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1102 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1113 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1117 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1033 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1097 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1137 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1181 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1208 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1202 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1213 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1217 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1133 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1197 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1237 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1281 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1308 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1302 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1313 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1317 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1233 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1297 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1337 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1381 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1408 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1402 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1413 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1417 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1333 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1397 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1437 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1481 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1508 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1502 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1513 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1517 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1433 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1497 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1537 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1581 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1608 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1602 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1613 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1617 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1533 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1597 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1637 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1681 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1708 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1702 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1713 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1717 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1633 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1697 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1733 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 129 group_bsyms len: 1
[Symbol name=mul]
[t13 = prims.mul(t5, t12)  # t13: "cuda:0 f32[1, 512, 4096]"]

cur node 122 group_bsyms len: 1
[Symbol name=mul]
[t6 = prims.mul(t5, t5)  # t6: "cuda:0 f32[1, 512, 4096]"]

cur node 183 group_bsyms len: 1
[Symbol name=add]
[t84 = prims.add(t82, t83)  # t84: "cuda:0 f32[1, 512, 4096]"]

cur node 1667 group_bsyms len: 1
[Symbol name=mul]
[t2233 = prims.mul(t2232, t2231)  # t2233: "cuda:0 f32[1, 32, 512, 128]"]

cur node 267 group_bsyms len: 1
[Symbol name=mul]
[t203 = prims.mul(t202, t201)  # t203: "cuda:0 f32[1, 32, 512, 128]"]

cur node 652 group_bsyms len: 1
[Symbol name=mul]
[t767 = prims.mul(t766, t765)  # t767: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1167 group_bsyms len: 1
[Symbol name=mul]
[t1508 = prims.mul(t1507, t1506)  # t1508: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1552 group_bsyms len: 1
[Symbol name=mul]
[t2072 = prims.mul(t2071, t2070)  # t2072: "cuda:0 f32[1, 32, 512, 128]"]

cur node 152 group_bsyms len: 1
[Symbol name=mul]
[t48 = prims.mul(t47, t46)  # t48: "cuda:0 f32[1, 32, 512, 128]"]

cur node 667 group_bsyms len: 1
[Symbol name=mul]
[t783 = prims.mul(t782, t781)  # t783: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1052 group_bsyms len: 1
[Symbol name=mul]
[t1347 = prims.mul(t1346, t1345)  # t1347: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1567 group_bsyms len: 1
[Symbol name=mul]
[t2088 = prims.mul(t2087, t2086)  # t2088: "cuda:0 f32[1, 32, 512, 128]"]

cur node 167 group_bsyms len: 1
[Symbol name=mul]
[t64 = prims.mul(t63, t62)  # t64: "cuda:0 f32[1, 32, 512, 128]"]

cur node 552 group_bsyms len: 1
[Symbol name=mul]
[t622 = prims.mul(t621, t620)  # t622: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1067 group_bsyms len: 1
[Symbol name=mul]
[t1363 = prims.mul(t1362, t1361)  # t1363: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1452 group_bsyms len: 1
[Symbol name=mul]
[t1927 = prims.mul(t1926, t1925)  # t1927: "cuda:0 f32[1, 32, 512, 128]"]

cur node 567 group_bsyms len: 1
[Symbol name=mul]
[t638 = prims.mul(t637, t636)  # t638: "cuda:0 f32[1, 32, 512, 128]"]

cur node 952 group_bsyms len: 1
[Symbol name=mul]
[t1202 = prims.mul(t1201, t1200)  # t1202: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1467 group_bsyms len: 1
[Symbol name=mul]
[t1943 = prims.mul(t1942, t1941)  # t1943: "cuda:0 f32[1, 32, 512, 128]"]

cur node 452 group_bsyms len: 1
[Symbol name=mul]
[t477 = prims.mul(t476, t475)  # t477: "cuda:0 f32[1, 32, 512, 128]"]

cur node 967 group_bsyms len: 1
[Symbol name=mul]
[t1218 = prims.mul(t1217, t1216)  # t1218: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1352 group_bsyms len: 1
[Symbol name=mul]
[t1782 = prims.mul(t1781, t1780)  # t1782: "cuda:0 f32[1, 32, 512, 128]"]

cur node 467 group_bsyms len: 1
[Symbol name=mul]
[t493 = prims.mul(t492, t491)  # t493: "cuda:0 f32[1, 32, 512, 128]"]

cur node 852 group_bsyms len: 1
[Symbol name=mul]
[t1057 = prims.mul(t1056, t1055)  # t1057: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1367 group_bsyms len: 1
[Symbol name=mul]
[t1798 = prims.mul(t1797, t1796)  # t1798: "cuda:0 f32[1, 32, 512, 128]"]

cur node 352 group_bsyms len: 1
[Symbol name=mul]
[t332 = prims.mul(t331, t330)  # t332: "cuda:0 f32[1, 32, 512, 128]"]

cur node 867 group_bsyms len: 1
[Symbol name=mul]
[t1073 = prims.mul(t1072, t1071)  # t1073: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1252 group_bsyms len: 1
[Symbol name=mul]
[t1637 = prims.mul(t1636, t1635)  # t1637: "cuda:0 f32[1, 32, 512, 128]"]

cur node 367 group_bsyms len: 1
[Symbol name=mul]
[t348 = prims.mul(t347, t346)  # t348: "cuda:0 f32[1, 32, 512, 128]"]

cur node 752 group_bsyms len: 1
[Symbol name=mul]
[t912 = prims.mul(t911, t910)  # t912: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1267 group_bsyms len: 1
[Symbol name=mul]
[t1653 = prims.mul(t1652, t1651)  # t1653: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1652 group_bsyms len: 1
[Symbol name=mul]
[t2217 = prims.mul(t2216, t2215)  # t2217: "cuda:0 f32[1, 32, 512, 128]"]

cur node 252 group_bsyms len: 1
[Symbol name=mul]
[t187 = prims.mul(t186, t185)  # t187: "cuda:0 f32[1, 32, 512, 128]"]

cur node 767 group_bsyms len: 1
[Symbol name=mul]
[t928 = prims.mul(t927, t926)  # t928: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1152 group_bsyms len: 1
[Symbol name=mul]
[t1492 = prims.mul(t1491, t1490)  # t1492: "cuda:0 f32[1, 32, 512, 128]"]

cur node 770 group_bsyms len: 1
[Symbol name=mul]
[t931 = prims.mul(t930, t929)  # t931: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1155 group_bsyms len: 1
[Symbol name=mul]
[t1495 = prims.mul(t1494, t1493)  # t1495: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1670 group_bsyms len: 1
[Symbol name=mul]
[t2236 = prims.mul(t2235, t2234)  # t2236: "cuda:0 f32[1, 32, 512, 128]"]

cur node 270 group_bsyms len: 1
[Symbol name=mul]
[t206 = prims.mul(t205, t204)  # t206: "cuda:0 f32[1, 32, 512, 128]"]

cur node 655 group_bsyms len: 1
[Symbol name=mul]
[t770 = prims.mul(t769, t768)  # t770: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1170 group_bsyms len: 1
[Symbol name=mul]
[t1511 = prims.mul(t1510, t1509)  # t1511: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1555 group_bsyms len: 1
[Symbol name=mul]
[t2075 = prims.mul(t2074, t2073)  # t2075: "cuda:0 f32[1, 32, 512, 128]"]

cur node 155 group_bsyms len: 1
[Symbol name=mul]
[t51 = prims.mul(t50, t49)  # t51: "cuda:0 f32[1, 32, 512, 128]"]

cur node 670 group_bsyms len: 1
[Symbol name=mul]
[t786 = prims.mul(t785, t784)  # t786: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1055 group_bsyms len: 1
[Symbol name=mul]
[t1350 = prims.mul(t1349, t1348)  # t1350: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1570 group_bsyms len: 1
[Symbol name=mul]
[t2091 = prims.mul(t2090, t2089)  # t2091: "cuda:0 f32[1, 32, 512, 128]"]

cur node 170 group_bsyms len: 1
[Symbol name=mul]
[t67 = prims.mul(t66, t65)  # t67: "cuda:0 f32[1, 32, 512, 128]"]

cur node 555 group_bsyms len: 1
[Symbol name=mul]
[t625 = prims.mul(t624, t623)  # t625: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1070 group_bsyms len: 1
[Symbol name=mul]
[t1366 = prims.mul(t1365, t1364)  # t1366: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1455 group_bsyms len: 1
[Symbol name=mul]
[t1930 = prims.mul(t1929, t1928)  # t1930: "cuda:0 f32[1, 32, 512, 128]"]

cur node 570 group_bsyms len: 1
[Symbol name=mul]
[t641 = prims.mul(t640, t639)  # t641: "cuda:0 f32[1, 32, 512, 128]"]

cur node 955 group_bsyms len: 1
[Symbol name=mul]
[t1205 = prims.mul(t1204, t1203)  # t1205: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1470 group_bsyms len: 1
[Symbol name=mul]
[t1946 = prims.mul(t1945, t1944)  # t1946: "cuda:0 f32[1, 32, 512, 128]"]

cur node 455 group_bsyms len: 1
[Symbol name=mul]
[t480 = prims.mul(t479, t478)  # t480: "cuda:0 f32[1, 32, 512, 128]"]

cur node 970 group_bsyms len: 1
[Symbol name=mul]
[t1221 = prims.mul(t1220, t1219)  # t1221: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1355 group_bsyms len: 1
[Symbol name=mul]
[t1785 = prims.mul(t1784, t1783)  # t1785: "cuda:0 f32[1, 32, 512, 128]"]

cur node 470 group_bsyms len: 1
[Symbol name=mul]
[t496 = prims.mul(t495, t494)  # t496: "cuda:0 f32[1, 32, 512, 128]"]

cur node 855 group_bsyms len: 1
[Symbol name=mul]
[t1060 = prims.mul(t1059, t1058)  # t1060: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1370 group_bsyms len: 1
[Symbol name=mul]
[t1801 = prims.mul(t1800, t1799)  # t1801: "cuda:0 f32[1, 32, 512, 128]"]

cur node 355 group_bsyms len: 1
[Symbol name=mul]
[t335 = prims.mul(t334, t333)  # t335: "cuda:0 f32[1, 32, 512, 128]"]

cur node 870 group_bsyms len: 1
[Symbol name=mul]
[t1076 = prims.mul(t1075, t1074)  # t1076: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1255 group_bsyms len: 1
[Symbol name=mul]
[t1640 = prims.mul(t1639, t1638)  # t1640: "cuda:0 f32[1, 32, 512, 128]"]

cur node 370 group_bsyms len: 1
[Symbol name=mul]
[t351 = prims.mul(t350, t349)  # t351: "cuda:0 f32[1, 32, 512, 128]"]

cur node 755 group_bsyms len: 1
[Symbol name=mul]
[t915 = prims.mul(t914, t913)  # t915: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1270 group_bsyms len: 1
[Symbol name=mul]
[t1656 = prims.mul(t1655, t1654)  # t1656: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1655 group_bsyms len: 1
[Symbol name=mul]
[t2220 = prims.mul(t2219, t2218)  # t2220: "cuda:0 f32[1, 32, 512, 128]"]

cur node 255 group_bsyms len: 1
[Symbol name=mul]
[t190 = prims.mul(t189, t188)  # t190: "cuda:0 f32[1, 32, 512, 128]"]

cur node 138 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 210 group_bsyms len: 1
[Symbol name=mul]
[t114 = prims.mul(t112, t113)  # t114: "cuda:0 f32[1, 512, 11008]"]

cur node 203 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 214 group_bsyms len: 1
[Symbol name=mul]
[t118 = prims.mul(t116, t117)  # t118: "cuda:0 f32[1, 512, 11008]"]

cur node 219 group_bsyms len: 1
[Symbol name=add]
[t123 = prims.add(t121, t122)  # t123: "cuda:0 f32[1, 512, 4096]"]

cur node 134 group_bsyms len: 1
[Symbol name=mul]
[t18 = prims.mul(t16, t17)  # t18: "cuda:0 f32[1, 512, 4096]"]

cur node 198 group_bsyms len: 1
[Symbol name=mul]
[t102 = prims.mul(t100, t101)  # t102: "cuda:0 f32[1, 512, 4096]"]

cur node 238 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 283 group_bsyms len: 1
[Symbol name=add]
[t229 = prims.add(t227, t228)  # t229: "cuda:0 f32[1, 512, 4096]"]

cur node 310 group_bsyms len: 1
[Symbol name=mul]
[t259 = prims.mul(t257, t258)  # t259: "cuda:0 f32[1, 512, 11008]"]

cur node 303 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 314 group_bsyms len: 1
[Symbol name=mul]
[t263 = prims.mul(t261, t262)  # t263: "cuda:0 f32[1, 512, 11008]"]

cur node 319 group_bsyms len: 1
[Symbol name=add]
[t268 = prims.add(t266, t267)  # t268: "cuda:0 f32[1, 512, 4096]"]

cur node 234 group_bsyms len: 1
[Symbol name=mul]
[t141 = prims.mul(t139, t140)  # t141: "cuda:0 f32[1, 512, 4096]"]

cur node 298 group_bsyms len: 1
[Symbol name=mul]
[t247 = prims.mul(t245, t246)  # t247: "cuda:0 f32[1, 512, 4096]"]

cur node 338 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 383 group_bsyms len: 1
[Symbol name=add]
[t374 = prims.add(t372, t373)  # t374: "cuda:0 f32[1, 512, 4096]"]

cur node 410 group_bsyms len: 1
[Symbol name=mul]
[t404 = prims.mul(t402, t403)  # t404: "cuda:0 f32[1, 512, 11008]"]

cur node 403 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 414 group_bsyms len: 1
[Symbol name=mul]
[t408 = prims.mul(t406, t407)  # t408: "cuda:0 f32[1, 512, 11008]"]

cur node 419 group_bsyms len: 1
[Symbol name=add]
[t413 = prims.add(t411, t412)  # t413: "cuda:0 f32[1, 512, 4096]"]

cur node 334 group_bsyms len: 1
[Symbol name=mul]
[t286 = prims.mul(t284, t285)  # t286: "cuda:0 f32[1, 512, 4096]"]

cur node 398 group_bsyms len: 1
[Symbol name=mul]
[t392 = prims.mul(t390, t391)  # t392: "cuda:0 f32[1, 512, 4096]"]

cur node 438 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 483 group_bsyms len: 1
[Symbol name=add]
[t519 = prims.add(t517, t518)  # t519: "cuda:0 f32[1, 512, 4096]"]

cur node 510 group_bsyms len: 1
[Symbol name=mul]
[t549 = prims.mul(t547, t548)  # t549: "cuda:0 f32[1, 512, 11008]"]

cur node 503 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 514 group_bsyms len: 1
[Symbol name=mul]
[t553 = prims.mul(t551, t552)  # t553: "cuda:0 f32[1, 512, 11008]"]

cur node 519 group_bsyms len: 1
[Symbol name=add]
[t558 = prims.add(t556, t557)  # t558: "cuda:0 f32[1, 512, 4096]"]

cur node 434 group_bsyms len: 1
[Symbol name=mul]
[t431 = prims.mul(t429, t430)  # t431: "cuda:0 f32[1, 512, 4096]"]

cur node 498 group_bsyms len: 1
[Symbol name=mul]
[t537 = prims.mul(t535, t536)  # t537: "cuda:0 f32[1, 512, 4096]"]

cur node 538 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 583 group_bsyms len: 1
[Symbol name=add]
[t664 = prims.add(t662, t663)  # t664: "cuda:0 f32[1, 512, 4096]"]

cur node 610 group_bsyms len: 1
[Symbol name=mul]
[t694 = prims.mul(t692, t693)  # t694: "cuda:0 f32[1, 512, 11008]"]

cur node 603 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 614 group_bsyms len: 1
[Symbol name=mul]
[t698 = prims.mul(t696, t697)  # t698: "cuda:0 f32[1, 512, 11008]"]

cur node 619 group_bsyms len: 1
[Symbol name=add]
[t703 = prims.add(t701, t702)  # t703: "cuda:0 f32[1, 512, 4096]"]

cur node 534 group_bsyms len: 1
[Symbol name=mul]
[t576 = prims.mul(t574, t575)  # t576: "cuda:0 f32[1, 512, 4096]"]

cur node 598 group_bsyms len: 1
[Symbol name=mul]
[t682 = prims.mul(t680, t681)  # t682: "cuda:0 f32[1, 512, 4096]"]

cur node 638 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 683 group_bsyms len: 1
[Symbol name=add]
[t809 = prims.add(t807, t808)  # t809: "cuda:0 f32[1, 512, 4096]"]

cur node 710 group_bsyms len: 1
[Symbol name=mul]
[t839 = prims.mul(t837, t838)  # t839: "cuda:0 f32[1, 512, 11008]"]

cur node 703 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 714 group_bsyms len: 1
[Symbol name=mul]
[t843 = prims.mul(t841, t842)  # t843: "cuda:0 f32[1, 512, 11008]"]

cur node 719 group_bsyms len: 1
[Symbol name=add]
[t848 = prims.add(t846, t847)  # t848: "cuda:0 f32[1, 512, 4096]"]

cur node 634 group_bsyms len: 1
[Symbol name=mul]
[t721 = prims.mul(t719, t720)  # t721: "cuda:0 f32[1, 512, 4096]"]

cur node 698 group_bsyms len: 1
[Symbol name=mul]
[t827 = prims.mul(t825, t826)  # t827: "cuda:0 f32[1, 512, 4096]"]

cur node 738 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 783 group_bsyms len: 1
[Symbol name=add]
[t954 = prims.add(t952, t953)  # t954: "cuda:0 f32[1, 512, 4096]"]

cur node 810 group_bsyms len: 1
[Symbol name=mul]
[t984 = prims.mul(t982, t983)  # t984: "cuda:0 f32[1, 512, 11008]"]

cur node 803 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 814 group_bsyms len: 1
[Symbol name=mul]
[t988 = prims.mul(t986, t987)  # t988: "cuda:0 f32[1, 512, 11008]"]

cur node 819 group_bsyms len: 1
[Symbol name=add]
[t993 = prims.add(t991, t992)  # t993: "cuda:0 f32[1, 512, 4096]"]

cur node 734 group_bsyms len: 1
[Symbol name=mul]
[t866 = prims.mul(t864, t865)  # t866: "cuda:0 f32[1, 512, 4096]"]

cur node 798 group_bsyms len: 1
[Symbol name=mul]
[t972 = prims.mul(t970, t971)  # t972: "cuda:0 f32[1, 512, 4096]"]

cur node 838 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 883 group_bsyms len: 1
[Symbol name=add]
[t1099 = prims.add(t1097, t1098)  # t1099: "cuda:0 f32[1, 512, 4096]"]

cur node 910 group_bsyms len: 1
[Symbol name=mul]
[t1129 = prims.mul(t1127, t1128)  # t1129: "cuda:0 f32[1, 512, 11008]"]

cur node 903 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 914 group_bsyms len: 1
[Symbol name=mul]
[t1133 = prims.mul(t1131, t1132)  # t1133: "cuda:0 f32[1, 512, 11008]"]

cur node 919 group_bsyms len: 1
[Symbol name=add]
[t1138 = prims.add(t1136, t1137)  # t1138: "cuda:0 f32[1, 512, 4096]"]

cur node 834 group_bsyms len: 1
[Symbol name=mul]
[t1011 = prims.mul(t1009, t1010)  # t1011: "cuda:0 f32[1, 512, 4096]"]

cur node 898 group_bsyms len: 1
[Symbol name=mul]
[t1117 = prims.mul(t1115, t1116)  # t1117: "cuda:0 f32[1, 512, 4096]"]

cur node 938 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 983 group_bsyms len: 1
[Symbol name=add]
[t1244 = prims.add(t1242, t1243)  # t1244: "cuda:0 f32[1, 512, 4096]"]

cur node 1010 group_bsyms len: 1
[Symbol name=mul]
[t1274 = prims.mul(t1272, t1273)  # t1274: "cuda:0 f32[1, 512, 11008]"]

cur node 1003 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1014 group_bsyms len: 1
[Symbol name=mul]
[t1278 = prims.mul(t1276, t1277)  # t1278: "cuda:0 f32[1, 512, 11008]"]

cur node 1019 group_bsyms len: 1
[Symbol name=add]
[t1283 = prims.add(t1281, t1282)  # t1283: "cuda:0 f32[1, 512, 4096]"]

cur node 934 group_bsyms len: 1
[Symbol name=mul]
[t1156 = prims.mul(t1154, t1155)  # t1156: "cuda:0 f32[1, 512, 4096]"]

cur node 998 group_bsyms len: 1
[Symbol name=mul]
[t1262 = prims.mul(t1260, t1261)  # t1262: "cuda:0 f32[1, 512, 4096]"]

cur node 1038 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1083 group_bsyms len: 1
[Symbol name=add]
[t1389 = prims.add(t1387, t1388)  # t1389: "cuda:0 f32[1, 512, 4096]"]

cur node 1110 group_bsyms len: 1
[Symbol name=mul]
[t1419 = prims.mul(t1417, t1418)  # t1419: "cuda:0 f32[1, 512, 11008]"]

cur node 1103 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1114 group_bsyms len: 1
[Symbol name=mul]
[t1423 = prims.mul(t1421, t1422)  # t1423: "cuda:0 f32[1, 512, 11008]"]

cur node 1119 group_bsyms len: 1
[Symbol name=add]
[t1428 = prims.add(t1426, t1427)  # t1428: "cuda:0 f32[1, 512, 4096]"]

cur node 1034 group_bsyms len: 1
[Symbol name=mul]
[t1301 = prims.mul(t1299, t1300)  # t1301: "cuda:0 f32[1, 512, 4096]"]

cur node 1098 group_bsyms len: 1
[Symbol name=mul]
[t1407 = prims.mul(t1405, t1406)  # t1407: "cuda:0 f32[1, 512, 4096]"]

cur node 1138 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1183 group_bsyms len: 1
[Symbol name=add]
[t1534 = prims.add(t1532, t1533)  # t1534: "cuda:0 f32[1, 512, 4096]"]

cur node 1210 group_bsyms len: 1
[Symbol name=mul]
[t1564 = prims.mul(t1562, t1563)  # t1564: "cuda:0 f32[1, 512, 11008]"]

cur node 1203 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1214 group_bsyms len: 1
[Symbol name=mul]
[t1568 = prims.mul(t1566, t1567)  # t1568: "cuda:0 f32[1, 512, 11008]"]

cur node 1219 group_bsyms len: 1
[Symbol name=add]
[t1573 = prims.add(t1571, t1572)  # t1573: "cuda:0 f32[1, 512, 4096]"]

cur node 1134 group_bsyms len: 1
[Symbol name=mul]
[t1446 = prims.mul(t1444, t1445)  # t1446: "cuda:0 f32[1, 512, 4096]"]

cur node 1198 group_bsyms len: 1
[Symbol name=mul]
[t1552 = prims.mul(t1550, t1551)  # t1552: "cuda:0 f32[1, 512, 4096]"]

cur node 1238 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1283 group_bsyms len: 1
[Symbol name=add]
[t1679 = prims.add(t1677, t1678)  # t1679: "cuda:0 f32[1, 512, 4096]"]

cur node 1310 group_bsyms len: 1
[Symbol name=mul]
[t1709 = prims.mul(t1707, t1708)  # t1709: "cuda:0 f32[1, 512, 11008]"]

cur node 1303 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1314 group_bsyms len: 1
[Symbol name=mul]
[t1713 = prims.mul(t1711, t1712)  # t1713: "cuda:0 f32[1, 512, 11008]"]

cur node 1319 group_bsyms len: 1
[Symbol name=add]
[t1718 = prims.add(t1716, t1717)  # t1718: "cuda:0 f32[1, 512, 4096]"]

cur node 1234 group_bsyms len: 1
[Symbol name=mul]
[t1591 = prims.mul(t1589, t1590)  # t1591: "cuda:0 f32[1, 512, 4096]"]

cur node 1298 group_bsyms len: 1
[Symbol name=mul]
[t1697 = prims.mul(t1695, t1696)  # t1697: "cuda:0 f32[1, 512, 4096]"]

cur node 1338 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1383 group_bsyms len: 1
[Symbol name=add]
[t1824 = prims.add(t1822, t1823)  # t1824: "cuda:0 f32[1, 512, 4096]"]

cur node 1410 group_bsyms len: 1
[Symbol name=mul]
[t1854 = prims.mul(t1852, t1853)  # t1854: "cuda:0 f32[1, 512, 11008]"]

cur node 1403 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1414 group_bsyms len: 1
[Symbol name=mul]
[t1858 = prims.mul(t1856, t1857)  # t1858: "cuda:0 f32[1, 512, 11008]"]

cur node 1419 group_bsyms len: 1
[Symbol name=add]
[t1863 = prims.add(t1861, t1862)  # t1863: "cuda:0 f32[1, 512, 4096]"]

cur node 1334 group_bsyms len: 1
[Symbol name=mul]
[t1736 = prims.mul(t1734, t1735)  # t1736: "cuda:0 f32[1, 512, 4096]"]

cur node 1398 group_bsyms len: 1
[Symbol name=mul]
[t1842 = prims.mul(t1840, t1841)  # t1842: "cuda:0 f32[1, 512, 4096]"]

cur node 1438 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1483 group_bsyms len: 1
[Symbol name=add]
[t1969 = prims.add(t1967, t1968)  # t1969: "cuda:0 f32[1, 512, 4096]"]

cur node 1510 group_bsyms len: 1
[Symbol name=mul]
[t1999 = prims.mul(t1997, t1998)  # t1999: "cuda:0 f32[1, 512, 11008]"]

cur node 1503 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1514 group_bsyms len: 1
[Symbol name=mul]
[t2003 = prims.mul(t2001, t2002)  # t2003: "cuda:0 f32[1, 512, 11008]"]

cur node 1519 group_bsyms len: 1
[Symbol name=add]
[t2008 = prims.add(t2006, t2007)  # t2008: "cuda:0 f32[1, 512, 4096]"]

cur node 1434 group_bsyms len: 1
[Symbol name=mul]
[t1881 = prims.mul(t1879, t1880)  # t1881: "cuda:0 f32[1, 512, 4096]"]

cur node 1498 group_bsyms len: 1
[Symbol name=mul]
[t1987 = prims.mul(t1985, t1986)  # t1987: "cuda:0 f32[1, 512, 4096]"]

cur node 1538 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1583 group_bsyms len: 1
[Symbol name=add]
[t2114 = prims.add(t2112, t2113)  # t2114: "cuda:0 f32[1, 512, 4096]"]

cur node 1610 group_bsyms len: 1
[Symbol name=mul]
[t2144 = prims.mul(t2142, t2143)  # t2144: "cuda:0 f32[1, 512, 11008]"]

cur node 1603 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1614 group_bsyms len: 1
[Symbol name=mul]
[t2148 = prims.mul(t2146, t2147)  # t2148: "cuda:0 f32[1, 512, 11008]"]

cur node 1619 group_bsyms len: 1
[Symbol name=add]
[t2153 = prims.add(t2151, t2152)  # t2153: "cuda:0 f32[1, 512, 4096]"]

cur node 1534 group_bsyms len: 1
[Symbol name=mul]
[t2026 = prims.mul(t2024, t2025)  # t2026: "cuda:0 f32[1, 512, 4096]"]

cur node 1598 group_bsyms len: 1
[Symbol name=mul]
[t2132 = prims.mul(t2130, t2131)  # t2132: "cuda:0 f32[1, 512, 4096]"]

cur node 1638 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1683 group_bsyms len: 1
[Symbol name=add]
[t2259 = prims.add(t2257, t2258)  # t2259: "cuda:0 f32[1, 512, 4096]"]

cur node 1710 group_bsyms len: 1
[Symbol name=mul]
[t2289 = prims.mul(t2287, t2288)  # t2289: "cuda:0 f32[1, 512, 11008]"]

cur node 1703 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1714 group_bsyms len: 1
[Symbol name=mul]
[t2293 = prims.mul(t2291, t2292)  # t2293: "cuda:0 f32[1, 512, 11008]"]

cur node 1719 group_bsyms len: 1
[Symbol name=add]
[t2298 = prims.add(t2296, t2297)  # t2298: "cuda:0 f32[1, 512, 4096]"]

cur node 1634 group_bsyms len: 1
[Symbol name=mul]
[t2171 = prims.mul(t2169, t2170)  # t2171: "cuda:0 f32[1, 512, 4096]"]

cur node 1698 group_bsyms len: 1
[Symbol name=mul]
[t2277 = prims.mul(t2275, t2276)  # t2277: "cuda:0 f32[1, 512, 4096]"]

cur node 1734 group_bsyms len: 1
[Symbol name=mul]
[t2316 = prims.mul(t2314, t2315)  # t2316: "cuda:0 f32[1, 512, 4096]"]

cur node 130 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 123 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 184 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1671 group_bsyms len: 1
[Symbol name=add]
[t2237 = prims.add(t2233, t2236)  # t2237: "cuda:0 f32[1, 32, 512, 128]"]

cur node 271 group_bsyms len: 1
[Symbol name=add]
[t207 = prims.add(t203, t206)  # t207: "cuda:0 f32[1, 32, 512, 128]"]

cur node 656 group_bsyms len: 1
[Symbol name=add]
[t771 = prims.add(t767, t770)  # t771: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1171 group_bsyms len: 1
[Symbol name=add]
[t1512 = prims.add(t1508, t1511)  # t1512: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1556 group_bsyms len: 1
[Symbol name=add]
[t2076 = prims.add(t2072, t2075)  # t2076: "cuda:0 f32[1, 32, 512, 128]"]

cur node 156 group_bsyms len: 1
[Symbol name=add]
[t52 = prims.add(t48, t51)  # t52: "cuda:0 f32[1, 32, 512, 128]"]

cur node 671 group_bsyms len: 1
[Symbol name=add]
[t787 = prims.add(t783, t786)  # t787: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1056 group_bsyms len: 1
[Symbol name=add]
[t1351 = prims.add(t1347, t1350)  # t1351: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1571 group_bsyms len: 1
[Symbol name=add]
[t2092 = prims.add(t2088, t2091)  # t2092: "cuda:0 f32[1, 32, 512, 128]"]

cur node 171 group_bsyms len: 1
[Symbol name=add]
[t68 = prims.add(t64, t67)  # t68: "cuda:0 f32[1, 32, 512, 128]"]

cur node 556 group_bsyms len: 1
[Symbol name=add]
[t626 = prims.add(t622, t625)  # t626: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1071 group_bsyms len: 1
[Symbol name=add]
[t1367 = prims.add(t1363, t1366)  # t1367: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1456 group_bsyms len: 1
[Symbol name=add]
[t1931 = prims.add(t1927, t1930)  # t1931: "cuda:0 f32[1, 32, 512, 128]"]

cur node 571 group_bsyms len: 1
[Symbol name=add]
[t642 = prims.add(t638, t641)  # t642: "cuda:0 f32[1, 32, 512, 128]"]

cur node 956 group_bsyms len: 1
[Symbol name=add]
[t1206 = prims.add(t1202, t1205)  # t1206: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1471 group_bsyms len: 1
[Symbol name=add]
[t1947 = prims.add(t1943, t1946)  # t1947: "cuda:0 f32[1, 32, 512, 128]"]

cur node 456 group_bsyms len: 1
[Symbol name=add]
[t481 = prims.add(t477, t480)  # t481: "cuda:0 f32[1, 32, 512, 128]"]

cur node 971 group_bsyms len: 1
[Symbol name=add]
[t1222 = prims.add(t1218, t1221)  # t1222: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1356 group_bsyms len: 1
[Symbol name=add]
[t1786 = prims.add(t1782, t1785)  # t1786: "cuda:0 f32[1, 32, 512, 128]"]

cur node 471 group_bsyms len: 1
[Symbol name=add]
[t497 = prims.add(t493, t496)  # t497: "cuda:0 f32[1, 32, 512, 128]"]

cur node 856 group_bsyms len: 1
[Symbol name=add]
[t1061 = prims.add(t1057, t1060)  # t1061: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1371 group_bsyms len: 1
[Symbol name=add]
[t1802 = prims.add(t1798, t1801)  # t1802: "cuda:0 f32[1, 32, 512, 128]"]

cur node 356 group_bsyms len: 1
[Symbol name=add]
[t336 = prims.add(t332, t335)  # t336: "cuda:0 f32[1, 32, 512, 128]"]

cur node 871 group_bsyms len: 1
[Symbol name=add]
[t1077 = prims.add(t1073, t1076)  # t1077: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1256 group_bsyms len: 1
[Symbol name=add]
[t1641 = prims.add(t1637, t1640)  # t1641: "cuda:0 f32[1, 32, 512, 128]"]

cur node 371 group_bsyms len: 1
[Symbol name=add]
[t352 = prims.add(t348, t351)  # t352: "cuda:0 f32[1, 32, 512, 128]"]

cur node 756 group_bsyms len: 1
[Symbol name=add]
[t916 = prims.add(t912, t915)  # t916: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1271 group_bsyms len: 1
[Symbol name=add]
[t1657 = prims.add(t1653, t1656)  # t1657: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1656 group_bsyms len: 1
[Symbol name=add]
[t2221 = prims.add(t2217, t2220)  # t2221: "cuda:0 f32[1, 32, 512, 128]"]

cur node 256 group_bsyms len: 1
[Symbol name=add]
[t191 = prims.add(t187, t190)  # t191: "cuda:0 f32[1, 32, 512, 128]"]

cur node 771 group_bsyms len: 1
[Symbol name=add]
[t932 = prims.add(t928, t931)  # t932: "cuda:0 f32[1, 32, 512, 128]"]

cur node 1156 group_bsyms len: 1
[Symbol name=add]
[t1496 = prims.add(t1492, t1495)  # t1496: "cuda:0 f32[1, 32, 512, 128]"]

cur node 139 group_bsyms len: 1
[Symbol name=split]
[t23 = prims.slice_prim(t22, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t23: "cuda:0 bf16[1, 32, 1, 512, 128]", t24 = prims.slice_prim(t22, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t24: "cuda:0 bf16[1, 32, 1, 512, 128]", t25 = prims.slice_prim(t22, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t25: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 211 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 204 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 215 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 220 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 135 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 199 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 239 group_bsyms len: 1
[Symbol name=split]
[t156 = prims.slice_prim(t155, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t156: "cuda:0 bf16[1, 32, 1, 512, 128]", t157 = prims.slice_prim(t155, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t157: "cuda:0 bf16[1, 32, 1, 512, 128]", t158 = prims.slice_prim(t155, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t158: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 284 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 311 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 304 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 315 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 320 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 235 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 299 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 339 group_bsyms len: 1
[Symbol name=split]
[t301 = prims.slice_prim(t300, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t301: "cuda:0 bf16[1, 32, 1, 512, 128]", t302 = prims.slice_prim(t300, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t302: "cuda:0 bf16[1, 32, 1, 512, 128]", t303 = prims.slice_prim(t300, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t303: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 384 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 411 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 404 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 415 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 420 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 335 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 399 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 439 group_bsyms len: 1
[Symbol name=split]
[t446 = prims.slice_prim(t445, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t446: "cuda:0 bf16[1, 32, 1, 512, 128]", t447 = prims.slice_prim(t445, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t447: "cuda:0 bf16[1, 32, 1, 512, 128]", t448 = prims.slice_prim(t445, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t448: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 484 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 511 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 504 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 515 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 520 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 435 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 499 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 539 group_bsyms len: 1
[Symbol name=split]
[t591 = prims.slice_prim(t590, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t591: "cuda:0 bf16[1, 32, 1, 512, 128]", t592 = prims.slice_prim(t590, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t592: "cuda:0 bf16[1, 32, 1, 512, 128]", t593 = prims.slice_prim(t590, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t593: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 584 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 611 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 604 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 615 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 620 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 535 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 599 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 639 group_bsyms len: 1
[Symbol name=split]
[t736 = prims.slice_prim(t735, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t736: "cuda:0 bf16[1, 32, 1, 512, 128]", t737 = prims.slice_prim(t735, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t737: "cuda:0 bf16[1, 32, 1, 512, 128]", t738 = prims.slice_prim(t735, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t738: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 684 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 711 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 704 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 715 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 720 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 635 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 699 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 739 group_bsyms len: 1
[Symbol name=split]
[t881 = prims.slice_prim(t880, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t881: "cuda:0 bf16[1, 32, 1, 512, 128]", t882 = prims.slice_prim(t880, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t882: "cuda:0 bf16[1, 32, 1, 512, 128]", t883 = prims.slice_prim(t880, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t883: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 784 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 811 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 804 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 815 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 820 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 735 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 799 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 839 group_bsyms len: 1
[Symbol name=split]
[t1026 = prims.slice_prim(t1025, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1026: "cuda:0 bf16[1, 32, 1, 512, 128]", t1027 = prims.slice_prim(t1025, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1027: "cuda:0 bf16[1, 32, 1, 512, 128]", t1028 = prims.slice_prim(t1025, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1028: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 884 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 911 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 904 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 915 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 920 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 835 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 899 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 939 group_bsyms len: 1
[Symbol name=split]
[t1171 = prims.slice_prim(t1170, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1171: "cuda:0 bf16[1, 32, 1, 512, 128]", t1172 = prims.slice_prim(t1170, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1172: "cuda:0 bf16[1, 32, 1, 512, 128]", t1173 = prims.slice_prim(t1170, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1173: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 984 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1011 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1004 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 1015 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1020 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 935 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 999 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1039 group_bsyms len: 1
[Symbol name=split]
[t1316 = prims.slice_prim(t1315, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1316: "cuda:0 bf16[1, 32, 1, 512, 128]", t1317 = prims.slice_prim(t1315, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1317: "cuda:0 bf16[1, 32, 1, 512, 128]", t1318 = prims.slice_prim(t1315, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1318: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 1084 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1111 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1104 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 1115 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1120 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1035 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1099 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1139 group_bsyms len: 1
[Symbol name=split]
[t1461 = prims.slice_prim(t1460, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1461: "cuda:0 bf16[1, 32, 1, 512, 128]", t1462 = prims.slice_prim(t1460, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1462: "cuda:0 bf16[1, 32, 1, 512, 128]", t1463 = prims.slice_prim(t1460, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1463: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 1184 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1211 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1204 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 1215 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1220 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1135 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1199 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1239 group_bsyms len: 1
[Symbol name=split]
[t1606 = prims.slice_prim(t1605, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1606: "cuda:0 bf16[1, 32, 1, 512, 128]", t1607 = prims.slice_prim(t1605, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1607: "cuda:0 bf16[1, 32, 1, 512, 128]", t1608 = prims.slice_prim(t1605, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1608: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 1284 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1311 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1304 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 1315 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1320 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1235 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1299 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1339 group_bsyms len: 1
[Symbol name=split]
[t1751 = prims.slice_prim(t1750, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1751: "cuda:0 bf16[1, 32, 1, 512, 128]", t1752 = prims.slice_prim(t1750, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1752: "cuda:0 bf16[1, 32, 1, 512, 128]", t1753 = prims.slice_prim(t1750, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1753: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 1384 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1411 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1404 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 1415 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1420 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1335 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1399 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1439 group_bsyms len: 1
[Symbol name=split]
[t1896 = prims.slice_prim(t1895, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1896: "cuda:0 bf16[1, 32, 1, 512, 128]", t1897 = prims.slice_prim(t1895, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1897: "cuda:0 bf16[1, 32, 1, 512, 128]", t1898 = prims.slice_prim(t1895, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1898: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 1484 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1511 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1504 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 1515 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1520 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1435 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1499 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1539 group_bsyms len: 1
[Symbol name=split]
[t2041 = prims.slice_prim(t2040, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2041: "cuda:0 bf16[1, 32, 1, 512, 128]", t2042 = prims.slice_prim(t2040, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2042: "cuda:0 bf16[1, 32, 1, 512, 128]", t2043 = prims.slice_prim(t2040, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2043: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 1584 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1611 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1604 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 1615 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1620 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1535 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1599 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1639 group_bsyms len: 1
[Symbol name=split]
[t2186 = prims.slice_prim(t2185, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t2186: "cuda:0 bf16[1, 32, 1, 512, 128]", t2187 = prims.slice_prim(t2185, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t2187: "cuda:0 bf16[1, 32, 1, 512, 128]", t2188 = prims.slice_prim(t2185, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t2188: "cuda:0 bf16[1, 32, 1, 512, 128]"]

cur node 1684 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1711 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1704 group_bsyms len: 1
[Symbol name=exp]
[]

cur node 1715 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1720 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1635 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1699 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1735 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 132 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 124 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 185 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 218 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1672 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 272 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 657 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1172 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1557 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 157 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 672 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1057 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1572 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 172 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 557 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1072 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1457 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 572 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 957 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1472 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 457 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 972 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1357 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 472 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 857 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1372 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 357 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 872 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1257 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 372 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 757 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1272 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1657 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 257 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 772 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1157 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 140 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 141 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 142 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 212 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 205 group_bsyms len: 1
[Symbol name=add]
[t109 = prims.add(1.0, t108)  # t109: "cuda:0 f32[1, 512, 11008]"]

cur node 282 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 221 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 240 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 241 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 242 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 285 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 318 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 312 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 305 group_bsyms len: 1
[Symbol name=add]
[t254 = prims.add(1.0, t253)  # t254: "cuda:0 f32[1, 512, 11008]"]

cur node 321 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 382 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 340 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 341 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 342 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 385 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 418 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 412 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 405 group_bsyms len: 1
[Symbol name=add]
[t399 = prims.add(1.0, t398)  # t399: "cuda:0 f32[1, 512, 11008]"]

cur node 482 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 421 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 440 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 441 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 442 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 485 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 518 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 512 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 505 group_bsyms len: 1
[Symbol name=add]
[t544 = prims.add(1.0, t543)  # t544: "cuda:0 f32[1, 512, 11008]"]

cur node 521 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 582 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 540 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 541 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 542 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 585 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 618 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 612 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 605 group_bsyms len: 1
[Symbol name=add]
[t689 = prims.add(1.0, t688)  # t689: "cuda:0 f32[1, 512, 11008]"]

cur node 682 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 621 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 640 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 641 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 642 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 685 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 718 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 712 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 705 group_bsyms len: 1
[Symbol name=add]
[t834 = prims.add(1.0, t833)  # t834: "cuda:0 f32[1, 512, 11008]"]

cur node 721 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 782 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 740 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 741 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 742 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 785 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 818 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 812 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 805 group_bsyms len: 1
[Symbol name=add]
[t979 = prims.add(1.0, t978)  # t979: "cuda:0 f32[1, 512, 11008]"]

cur node 882 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 821 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 840 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 841 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 842 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 885 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 918 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 912 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 905 group_bsyms len: 1
[Symbol name=add]
[t1124 = prims.add(1.0, t1123)  # t1124: "cuda:0 f32[1, 512, 11008]"]

cur node 921 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 982 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 940 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 941 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 942 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 985 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1018 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1012 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1005 group_bsyms len: 1
[Symbol name=add]
[t1269 = prims.add(1.0, t1268)  # t1269: "cuda:0 f32[1, 512, 11008]"]

cur node 1082 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1021 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1040 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1041 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1042 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1085 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1118 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1112 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1105 group_bsyms len: 1
[Symbol name=add]
[t1414 = prims.add(1.0, t1413)  # t1414: "cuda:0 f32[1, 512, 11008]"]

cur node 1121 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1182 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1140 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1141 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1142 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1185 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1218 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1212 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1205 group_bsyms len: 1
[Symbol name=add]
[t1559 = prims.add(1.0, t1558)  # t1559: "cuda:0 f32[1, 512, 11008]"]

cur node 1282 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1221 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1240 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1241 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1242 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1285 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1318 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1312 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1305 group_bsyms len: 1
[Symbol name=add]
[t1704 = prims.add(1.0, t1703)  # t1704: "cuda:0 f32[1, 512, 11008]"]

cur node 1321 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1382 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1340 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1341 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1342 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1385 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1418 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1412 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1405 group_bsyms len: 1
[Symbol name=add]
[t1849 = prims.add(1.0, t1848)  # t1849: "cuda:0 f32[1, 512, 11008]"]

cur node 1482 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1421 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1440 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1441 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1442 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1485 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1518 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1512 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1505 group_bsyms len: 1
[Symbol name=add]
[t1994 = prims.add(1.0, t1993)  # t1994: "cuda:0 f32[1, 512, 11008]"]

cur node 1521 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1582 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1540 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1541 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1542 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1585 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1618 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1612 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1605 group_bsyms len: 1
[Symbol name=add]
[t2139 = prims.add(1.0, t2138)  # t2139: "cuda:0 f32[1, 512, 11008]"]

cur node 1682 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1621 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1640 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1641 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1642 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1685 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1718 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1712 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1705 group_bsyms len: 1
[Symbol name=add]
[t2284 = prims.add(1.0, t2283)  # t2284: "cuda:0 f32[1, 512, 11008]"]

cur node 1721 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 125 group_bsyms len: 1
[Symbol name=true_divide]
[t9 = prims.div(t8, 4096.0)  # t9: "cuda:0 f32[1, 512, 1]"]

cur node 193 group_bsyms len: 1
[Symbol name=mul]
[t97 = prims.mul(t86, t96)  # t97: "cuda:0 f32[1, 512, 4096]"]

cur node 186 group_bsyms len: 1
[Symbol name=mul]
[t87 = prims.mul(t86, t86)  # t87: "cuda:0 f32[1, 512, 4096]"]

cur node 1676 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 276 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 674 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1176 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1574 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 174 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 676 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1074 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1576 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 176 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 574 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1076 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1474 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 576 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 974 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1476 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 474 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 976 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1374 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 476 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 874 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1376 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 374 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 876 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1274 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 376 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 774 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1276 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1674 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 274 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 776 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1174 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 173 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 143 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 158 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 175 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 177 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 206 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 229 group_bsyms len: 1
[Symbol name=mul]
[t136 = prims.mul(t125, t135)  # t136: "cuda:0 f32[1, 512, 4096]"]

cur node 222 group_bsyms len: 1
[Symbol name=mul]
[t126 = prims.mul(t125, t125)  # t126: "cuda:0 f32[1, 512, 4096]"]

cur node 273 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 243 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 258 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 275 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 277 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 293 group_bsyms len: 1
[Symbol name=mul]
[t242 = prims.mul(t231, t241)  # t242: "cuda:0 f32[1, 512, 4096]"]

cur node 286 group_bsyms len: 1
[Symbol name=mul]
[t232 = prims.mul(t231, t231)  # t232: "cuda:0 f32[1, 512, 4096]"]

cur node 306 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 329 group_bsyms len: 1
[Symbol name=mul]
[t281 = prims.mul(t270, t280)  # t281: "cuda:0 f32[1, 512, 4096]"]

cur node 322 group_bsyms len: 1
[Symbol name=mul]
[t271 = prims.mul(t270, t270)  # t271: "cuda:0 f32[1, 512, 4096]"]

cur node 373 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 343 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 358 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 375 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 377 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 393 group_bsyms len: 1
[Symbol name=mul]
[t387 = prims.mul(t376, t386)  # t387: "cuda:0 f32[1, 512, 4096]"]

cur node 386 group_bsyms len: 1
[Symbol name=mul]
[t377 = prims.mul(t376, t376)  # t377: "cuda:0 f32[1, 512, 4096]"]

cur node 406 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 429 group_bsyms len: 1
[Symbol name=mul]
[t426 = prims.mul(t415, t425)  # t426: "cuda:0 f32[1, 512, 4096]"]

cur node 422 group_bsyms len: 1
[Symbol name=mul]
[t416 = prims.mul(t415, t415)  # t416: "cuda:0 f32[1, 512, 4096]"]

cur node 473 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 443 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 458 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 475 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 477 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 493 group_bsyms len: 1
[Symbol name=mul]
[t532 = prims.mul(t521, t531)  # t532: "cuda:0 f32[1, 512, 4096]"]

cur node 486 group_bsyms len: 1
[Symbol name=mul]
[t522 = prims.mul(t521, t521)  # t522: "cuda:0 f32[1, 512, 4096]"]

cur node 506 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 529 group_bsyms len: 1
[Symbol name=mul]
[t571 = prims.mul(t560, t570)  # t571: "cuda:0 f32[1, 512, 4096]"]

cur node 522 group_bsyms len: 1
[Symbol name=mul]
[t561 = prims.mul(t560, t560)  # t561: "cuda:0 f32[1, 512, 4096]"]

cur node 573 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 543 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 558 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 575 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 577 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 593 group_bsyms len: 1
[Symbol name=mul]
[t677 = prims.mul(t666, t676)  # t677: "cuda:0 f32[1, 512, 4096]"]

cur node 586 group_bsyms len: 1
[Symbol name=mul]
[t667 = prims.mul(t666, t666)  # t667: "cuda:0 f32[1, 512, 4096]"]

cur node 606 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 629 group_bsyms len: 1
[Symbol name=mul]
[t716 = prims.mul(t705, t715)  # t716: "cuda:0 f32[1, 512, 4096]"]

cur node 622 group_bsyms len: 1
[Symbol name=mul]
[t706 = prims.mul(t705, t705)  # t706: "cuda:0 f32[1, 512, 4096]"]

cur node 673 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 643 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 658 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 675 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 677 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 693 group_bsyms len: 1
[Symbol name=mul]
[t822 = prims.mul(t811, t821)  # t822: "cuda:0 f32[1, 512, 4096]"]

cur node 686 group_bsyms len: 1
[Symbol name=mul]
[t812 = prims.mul(t811, t811)  # t812: "cuda:0 f32[1, 512, 4096]"]

cur node 706 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 729 group_bsyms len: 1
[Symbol name=mul]
[t861 = prims.mul(t850, t860)  # t861: "cuda:0 f32[1, 512, 4096]"]

cur node 722 group_bsyms len: 1
[Symbol name=mul]
[t851 = prims.mul(t850, t850)  # t851: "cuda:0 f32[1, 512, 4096]"]

cur node 773 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 743 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 758 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 775 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 777 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 793 group_bsyms len: 1
[Symbol name=mul]
[t967 = prims.mul(t956, t966)  # t967: "cuda:0 f32[1, 512, 4096]"]

cur node 786 group_bsyms len: 1
[Symbol name=mul]
[t957 = prims.mul(t956, t956)  # t957: "cuda:0 f32[1, 512, 4096]"]

cur node 806 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 829 group_bsyms len: 1
[Symbol name=mul]
[t1006 = prims.mul(t995, t1005)  # t1006: "cuda:0 f32[1, 512, 4096]"]

cur node 822 group_bsyms len: 1
[Symbol name=mul]
[t996 = prims.mul(t995, t995)  # t996: "cuda:0 f32[1, 512, 4096]"]

cur node 873 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 843 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 858 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 875 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 877 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 893 group_bsyms len: 1
[Symbol name=mul]
[t1112 = prims.mul(t1101, t1111)  # t1112: "cuda:0 f32[1, 512, 4096]"]

cur node 886 group_bsyms len: 1
[Symbol name=mul]
[t1102 = prims.mul(t1101, t1101)  # t1102: "cuda:0 f32[1, 512, 4096]"]

cur node 906 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 929 group_bsyms len: 1
[Symbol name=mul]
[t1151 = prims.mul(t1140, t1150)  # t1151: "cuda:0 f32[1, 512, 4096]"]

cur node 922 group_bsyms len: 1
[Symbol name=mul]
[t1141 = prims.mul(t1140, t1140)  # t1141: "cuda:0 f32[1, 512, 4096]"]

cur node 973 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 943 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 958 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 975 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 977 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 993 group_bsyms len: 1
[Symbol name=mul]
[t1257 = prims.mul(t1246, t1256)  # t1257: "cuda:0 f32[1, 512, 4096]"]

cur node 986 group_bsyms len: 1
[Symbol name=mul]
[t1247 = prims.mul(t1246, t1246)  # t1247: "cuda:0 f32[1, 512, 4096]"]

cur node 1006 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 1029 group_bsyms len: 1
[Symbol name=mul]
[t1296 = prims.mul(t1285, t1295)  # t1296: "cuda:0 f32[1, 512, 4096]"]

cur node 1022 group_bsyms len: 1
[Symbol name=mul]
[t1286 = prims.mul(t1285, t1285)  # t1286: "cuda:0 f32[1, 512, 4096]"]

cur node 1073 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1043 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1058 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1075 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1077 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 1093 group_bsyms len: 1
[Symbol name=mul]
[t1402 = prims.mul(t1391, t1401)  # t1402: "cuda:0 f32[1, 512, 4096]"]

cur node 1086 group_bsyms len: 1
[Symbol name=mul]
[t1392 = prims.mul(t1391, t1391)  # t1392: "cuda:0 f32[1, 512, 4096]"]

cur node 1106 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 1129 group_bsyms len: 1
[Symbol name=mul]
[t1441 = prims.mul(t1430, t1440)  # t1441: "cuda:0 f32[1, 512, 4096]"]

cur node 1122 group_bsyms len: 1
[Symbol name=mul]
[t1431 = prims.mul(t1430, t1430)  # t1431: "cuda:0 f32[1, 512, 4096]"]

cur node 1173 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1143 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1158 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1175 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1177 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 1193 group_bsyms len: 1
[Symbol name=mul]
[t1547 = prims.mul(t1536, t1546)  # t1547: "cuda:0 f32[1, 512, 4096]"]

cur node 1186 group_bsyms len: 1
[Symbol name=mul]
[t1537 = prims.mul(t1536, t1536)  # t1537: "cuda:0 f32[1, 512, 4096]"]

cur node 1206 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 1229 group_bsyms len: 1
[Symbol name=mul]
[t1586 = prims.mul(t1575, t1585)  # t1586: "cuda:0 f32[1, 512, 4096]"]

cur node 1222 group_bsyms len: 1
[Symbol name=mul]
[t1576 = prims.mul(t1575, t1575)  # t1576: "cuda:0 f32[1, 512, 4096]"]

cur node 1273 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1243 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1258 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1275 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1277 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 1293 group_bsyms len: 1
[Symbol name=mul]
[t1692 = prims.mul(t1681, t1691)  # t1692: "cuda:0 f32[1, 512, 4096]"]

cur node 1286 group_bsyms len: 1
[Symbol name=mul]
[t1682 = prims.mul(t1681, t1681)  # t1682: "cuda:0 f32[1, 512, 4096]"]

cur node 1306 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 1329 group_bsyms len: 1
[Symbol name=mul]
[t1731 = prims.mul(t1720, t1730)  # t1731: "cuda:0 f32[1, 512, 4096]"]

cur node 1322 group_bsyms len: 1
[Symbol name=mul]
[t1721 = prims.mul(t1720, t1720)  # t1721: "cuda:0 f32[1, 512, 4096]"]

cur node 1373 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1343 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1358 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1375 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1377 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 1393 group_bsyms len: 1
[Symbol name=mul]
[t1837 = prims.mul(t1826, t1836)  # t1837: "cuda:0 f32[1, 512, 4096]"]

cur node 1386 group_bsyms len: 1
[Symbol name=mul]
[t1827 = prims.mul(t1826, t1826)  # t1827: "cuda:0 f32[1, 512, 4096]"]

cur node 1406 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 1429 group_bsyms len: 1
[Symbol name=mul]
[t1876 = prims.mul(t1865, t1875)  # t1876: "cuda:0 f32[1, 512, 4096]"]

cur node 1422 group_bsyms len: 1
[Symbol name=mul]
[t1866 = prims.mul(t1865, t1865)  # t1866: "cuda:0 f32[1, 512, 4096]"]

cur node 1473 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1443 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1458 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1475 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1477 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 1493 group_bsyms len: 1
[Symbol name=mul]
[t1982 = prims.mul(t1971, t1981)  # t1982: "cuda:0 f32[1, 512, 4096]"]

cur node 1486 group_bsyms len: 1
[Symbol name=mul]
[t1972 = prims.mul(t1971, t1971)  # t1972: "cuda:0 f32[1, 512, 4096]"]

cur node 1506 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 1529 group_bsyms len: 1
[Symbol name=mul]
[t2021 = prims.mul(t2010, t2020)  # t2021: "cuda:0 f32[1, 512, 4096]"]

cur node 1522 group_bsyms len: 1
[Symbol name=mul]
[t2011 = prims.mul(t2010, t2010)  # t2011: "cuda:0 f32[1, 512, 4096]"]

cur node 1573 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1543 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1558 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1575 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1577 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 1593 group_bsyms len: 1
[Symbol name=mul]
[t2127 = prims.mul(t2116, t2126)  # t2127: "cuda:0 f32[1, 512, 4096]"]

cur node 1586 group_bsyms len: 1
[Symbol name=mul]
[t2117 = prims.mul(t2116, t2116)  # t2117: "cuda:0 f32[1, 512, 4096]"]

cur node 1606 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 1629 group_bsyms len: 1
[Symbol name=mul]
[t2166 = prims.mul(t2155, t2165)  # t2166: "cuda:0 f32[1, 512, 4096]"]

cur node 1622 group_bsyms len: 1
[Symbol name=mul]
[t2156 = prims.mul(t2155, t2155)  # t2156: "cuda:0 f32[1, 512, 4096]"]

cur node 1673 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1643 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1658 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1675 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1677 group_bsyms len: 1
[Symbol name=cudnn_sdpa_fwd]
[]

cur node 1693 group_bsyms len: 1
[Symbol name=mul]
[t2272 = prims.mul(t2261, t2271)  # t2272: "cuda:0 f32[1, 512, 4096]"]

cur node 1686 group_bsyms len: 1
[Symbol name=mul]
[t2262 = prims.mul(t2261, t2261)  # t2262: "cuda:0 f32[1, 512, 4096]"]

cur node 1706 group_bsyms len: 1
[Symbol name=reciprocal]
[]

cur node 1729 group_bsyms len: 1
[Symbol name=mul]
[t2311 = prims.mul(t2300, t2310)  # t2311: "cuda:0 f32[1, 512, 4096]"]

cur node 1722 group_bsyms len: 1
[Symbol name=mul]
[t2301 = prims.mul(t2300, t2300)  # t2301: "cuda:0 f32[1, 512, 4096]"]

cur node 126 group_bsyms len: 1
[Symbol name=add]
[t10 = prims.add(t9, 1e-05)  # t10: "cuda:0 f32[1, 512, 1]"]

cur node 194 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 187 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 144 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 145 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 151 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 160 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 166 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 159 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 178 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 207 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 230 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 223 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 251 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 244 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 245 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 266 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 259 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 260 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 278 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 294 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 287 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 307 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 330 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 323 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 344 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 345 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 351 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 360 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 366 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 359 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 378 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 394 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 387 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 407 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 430 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 423 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 451 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 444 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 445 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 466 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 459 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 460 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 478 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 494 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 487 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 507 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 530 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 523 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 544 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 545 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 551 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 560 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 566 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 559 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 578 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 594 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 587 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 607 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 630 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 623 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 651 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 644 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 645 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 666 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 659 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 660 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 678 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 694 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 687 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 707 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 730 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 723 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 744 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 745 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 751 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 760 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 766 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 759 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 778 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 794 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 787 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 807 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 830 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 823 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 851 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 844 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 845 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 866 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 859 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 860 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 878 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 894 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 887 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 907 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 930 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 923 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 944 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 945 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 951 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 960 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 966 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 959 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 978 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 994 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 987 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1007 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1030 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1023 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1051 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1044 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1045 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1066 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1059 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1060 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1078 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1094 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1087 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1107 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1130 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1123 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1144 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1145 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1151 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1160 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1166 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1159 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1178 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1194 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1187 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1207 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1230 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1223 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1251 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1244 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1245 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1266 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1259 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1260 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1278 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1294 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1287 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1307 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1330 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1323 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1344 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1345 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1351 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1360 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1366 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1359 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1378 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1394 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1387 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1407 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1430 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1423 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1451 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1444 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1445 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1466 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1459 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1460 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1478 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1494 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1487 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1507 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1530 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1523 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1544 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1545 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1551 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1560 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1566 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1559 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1578 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1594 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1587 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1607 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1630 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1623 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1651 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1644 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1645 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1666 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1659 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1660 group_bsyms len: 1
[Symbol name=slice_prim]
[]

cur node 1678 group_bsyms len: 1
[Symbol name=transpose]
[]

cur node 1694 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1687 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 1707 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1730 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1723 group_bsyms len: 1
[Symbol name=sum]
[]

cur node 127 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 196 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 188 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 149 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 146 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 161 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 164 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 179 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 209 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 232 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 224 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 249 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 246 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 264 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 261 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 279 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 296 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 288 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 309 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 332 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 324 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 349 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 346 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 361 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 364 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 379 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 396 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 388 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 409 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 432 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 424 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 449 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 446 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 464 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 461 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 479 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 496 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 488 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 509 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 532 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 524 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 549 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 546 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 561 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 564 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 579 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 596 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 588 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 609 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 632 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 624 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 649 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 646 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 664 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 661 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 679 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 696 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 688 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 709 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 732 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 724 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 749 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 746 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 761 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 764 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 779 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 796 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 788 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 809 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 832 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 824 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 849 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 846 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 864 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 861 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 879 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 896 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 888 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 909 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 932 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 924 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 949 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 946 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 961 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 964 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 979 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 996 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 988 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1009 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1032 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1024 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1049 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1046 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1064 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1061 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1079 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1096 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1088 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1109 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1132 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1124 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1149 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1146 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1161 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1164 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1179 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1196 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1188 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1209 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1232 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1224 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1249 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1246 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1264 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1261 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1279 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1296 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1288 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1309 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1332 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1324 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1349 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1346 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1361 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1364 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1379 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1396 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1388 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1409 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1432 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1424 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1449 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1446 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1464 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1461 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1479 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1496 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1488 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1509 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1532 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1524 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1549 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1546 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1561 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1564 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1579 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1596 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1588 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1609 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1632 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1624 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1649 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1646 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1664 group_bsyms len: 1
[Symbol name=cat]
[]

cur node 1661 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1679 group_bsyms len: 1
[Symbol name=reshape]
[]

cur node 1696 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1688 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1709 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1732 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1724 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 128 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 189 group_bsyms len: 1
[Symbol name=true_divide]
[t92 = prims.div(t90, 4096.0)  # t92: "cuda:0 f32[1, 512, 1]"]

cur node 154 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 147 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 162 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 169 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 225 group_bsyms len: 1
[Symbol name=true_divide]
[t131 = prims.div(t129, 4096.0)  # t131: "cuda:0 f32[1, 512, 1]"]

cur node 254 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 247 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 269 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 262 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 289 group_bsyms len: 1
[Symbol name=true_divide]
[t237 = prims.div(t235, 4096.0)  # t237: "cuda:0 f32[1, 512, 1]"]

cur node 325 group_bsyms len: 1
[Symbol name=true_divide]
[t276 = prims.div(t274, 4096.0)  # t276: "cuda:0 f32[1, 512, 1]"]

cur node 354 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 347 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 362 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 369 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 389 group_bsyms len: 1
[Symbol name=true_divide]
[t382 = prims.div(t380, 4096.0)  # t382: "cuda:0 f32[1, 512, 1]"]

cur node 425 group_bsyms len: 1
[Symbol name=true_divide]
[t421 = prims.div(t419, 4096.0)  # t421: "cuda:0 f32[1, 512, 1]"]

cur node 454 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 447 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 469 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 462 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 489 group_bsyms len: 1
[Symbol name=true_divide]
[t527 = prims.div(t525, 4096.0)  # t527: "cuda:0 f32[1, 512, 1]"]

cur node 525 group_bsyms len: 1
[Symbol name=true_divide]
[t566 = prims.div(t564, 4096.0)  # t566: "cuda:0 f32[1, 512, 1]"]

cur node 554 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 547 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 562 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 569 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 589 group_bsyms len: 1
[Symbol name=true_divide]
[t672 = prims.div(t670, 4096.0)  # t672: "cuda:0 f32[1, 512, 1]"]

cur node 625 group_bsyms len: 1
[Symbol name=true_divide]
[t711 = prims.div(t709, 4096.0)  # t711: "cuda:0 f32[1, 512, 1]"]

cur node 654 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 647 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 669 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 662 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 689 group_bsyms len: 1
[Symbol name=true_divide]
[t817 = prims.div(t815, 4096.0)  # t817: "cuda:0 f32[1, 512, 1]"]

cur node 725 group_bsyms len: 1
[Symbol name=true_divide]
[t856 = prims.div(t854, 4096.0)  # t856: "cuda:0 f32[1, 512, 1]"]

cur node 754 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 747 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 762 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 769 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 789 group_bsyms len: 1
[Symbol name=true_divide]
[t962 = prims.div(t960, 4096.0)  # t962: "cuda:0 f32[1, 512, 1]"]

cur node 825 group_bsyms len: 1
[Symbol name=true_divide]
[t1001 = prims.div(t999, 4096.0)  # t1001: "cuda:0 f32[1, 512, 1]"]

cur node 854 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 847 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 869 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 862 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 889 group_bsyms len: 1
[Symbol name=true_divide]
[t1107 = prims.div(t1105, 4096.0)  # t1107: "cuda:0 f32[1, 512, 1]"]

cur node 925 group_bsyms len: 1
[Symbol name=true_divide]
[t1146 = prims.div(t1144, 4096.0)  # t1146: "cuda:0 f32[1, 512, 1]"]

cur node 954 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 947 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 962 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 969 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 989 group_bsyms len: 1
[Symbol name=true_divide]
[t1252 = prims.div(t1250, 4096.0)  # t1252: "cuda:0 f32[1, 512, 1]"]

cur node 1025 group_bsyms len: 1
[Symbol name=true_divide]
[t1291 = prims.div(t1289, 4096.0)  # t1291: "cuda:0 f32[1, 512, 1]"]

cur node 1054 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1047 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1069 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1062 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1089 group_bsyms len: 1
[Symbol name=true_divide]
[t1397 = prims.div(t1395, 4096.0)  # t1397: "cuda:0 f32[1, 512, 1]"]

cur node 1125 group_bsyms len: 1
[Symbol name=true_divide]
[t1436 = prims.div(t1434, 4096.0)  # t1436: "cuda:0 f32[1, 512, 1]"]

cur node 1154 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1147 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1162 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1169 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1189 group_bsyms len: 1
[Symbol name=true_divide]
[t1542 = prims.div(t1540, 4096.0)  # t1542: "cuda:0 f32[1, 512, 1]"]

cur node 1225 group_bsyms len: 1
[Symbol name=true_divide]
[t1581 = prims.div(t1579, 4096.0)  # t1581: "cuda:0 f32[1, 512, 1]"]

cur node 1254 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1247 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1269 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1262 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1289 group_bsyms len: 1
[Symbol name=true_divide]
[t1687 = prims.div(t1685, 4096.0)  # t1687: "cuda:0 f32[1, 512, 1]"]

cur node 1325 group_bsyms len: 1
[Symbol name=true_divide]
[t1726 = prims.div(t1724, 4096.0)  # t1726: "cuda:0 f32[1, 512, 1]"]

cur node 1354 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1347 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1362 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1369 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1389 group_bsyms len: 1
[Symbol name=true_divide]
[t1832 = prims.div(t1830, 4096.0)  # t1832: "cuda:0 f32[1, 512, 1]"]

cur node 1425 group_bsyms len: 1
[Symbol name=true_divide]
[t1871 = prims.div(t1869, 4096.0)  # t1871: "cuda:0 f32[1, 512, 1]"]

cur node 1454 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1447 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1469 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1462 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1489 group_bsyms len: 1
[Symbol name=true_divide]
[t1977 = prims.div(t1975, 4096.0)  # t1977: "cuda:0 f32[1, 512, 1]"]

cur node 1525 group_bsyms len: 1
[Symbol name=true_divide]
[t2016 = prims.div(t2014, 4096.0)  # t2016: "cuda:0 f32[1, 512, 1]"]

cur node 1554 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1547 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1562 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1569 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1589 group_bsyms len: 1
[Symbol name=true_divide]
[t2122 = prims.div(t2120, 4096.0)  # t2122: "cuda:0 f32[1, 512, 1]"]

cur node 1625 group_bsyms len: 1
[Symbol name=true_divide]
[t2161 = prims.div(t2159, 4096.0)  # t2161: "cuda:0 f32[1, 512, 1]"]

cur node 1654 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1647 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1669 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1662 group_bsyms len: 1
[Symbol name=neg]
[]

cur node 1689 group_bsyms len: 1
[Symbol name=true_divide]
[t2267 = prims.div(t2265, 4096.0)  # t2267: "cuda:0 f32[1, 512, 1]"]

cur node 1725 group_bsyms len: 1
[Symbol name=true_divide]
[t2306 = prims.div(t2304, 4096.0)  # t2306: "cuda:0 f32[1, 512, 1]"]

cur node 190 group_bsyms len: 1
[Symbol name=add]
[t94 = prims.add(t92, 1e-05)  # t94: "cuda:0 f32[1, 512, 1]"]

cur node 148 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 163 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 226 group_bsyms len: 1
[Symbol name=add]
[t133 = prims.add(t131, 1e-05)  # t133: "cuda:0 f32[1, 512, 1]"]

cur node 248 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 263 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 290 group_bsyms len: 1
[Symbol name=add]
[t239 = prims.add(t237, 1e-05)  # t239: "cuda:0 f32[1, 512, 1]"]

cur node 326 group_bsyms len: 1
[Symbol name=add]
[t278 = prims.add(t276, 1e-05)  # t278: "cuda:0 f32[1, 512, 1]"]

cur node 348 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 363 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 390 group_bsyms len: 1
[Symbol name=add]
[t384 = prims.add(t382, 1e-05)  # t384: "cuda:0 f32[1, 512, 1]"]

cur node 426 group_bsyms len: 1
[Symbol name=add]
[t423 = prims.add(t421, 1e-05)  # t423: "cuda:0 f32[1, 512, 1]"]

cur node 448 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 463 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 490 group_bsyms len: 1
[Symbol name=add]
[t529 = prims.add(t527, 1e-05)  # t529: "cuda:0 f32[1, 512, 1]"]

cur node 526 group_bsyms len: 1
[Symbol name=add]
[t568 = prims.add(t566, 1e-05)  # t568: "cuda:0 f32[1, 512, 1]"]

cur node 548 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 563 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 590 group_bsyms len: 1
[Symbol name=add]
[t674 = prims.add(t672, 1e-05)  # t674: "cuda:0 f32[1, 512, 1]"]

cur node 626 group_bsyms len: 1
[Symbol name=add]
[t713 = prims.add(t711, 1e-05)  # t713: "cuda:0 f32[1, 512, 1]"]

cur node 648 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 663 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 690 group_bsyms len: 1
[Symbol name=add]
[t819 = prims.add(t817, 1e-05)  # t819: "cuda:0 f32[1, 512, 1]"]

cur node 726 group_bsyms len: 1
[Symbol name=add]
[t858 = prims.add(t856, 1e-05)  # t858: "cuda:0 f32[1, 512, 1]"]

cur node 748 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 763 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 790 group_bsyms len: 1
[Symbol name=add]
[t964 = prims.add(t962, 1e-05)  # t964: "cuda:0 f32[1, 512, 1]"]

cur node 826 group_bsyms len: 1
[Symbol name=add]
[t1003 = prims.add(t1001, 1e-05)  # t1003: "cuda:0 f32[1, 512, 1]"]

cur node 848 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 863 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 890 group_bsyms len: 1
[Symbol name=add]
[t1109 = prims.add(t1107, 1e-05)  # t1109: "cuda:0 f32[1, 512, 1]"]

cur node 926 group_bsyms len: 1
[Symbol name=add]
[t1148 = prims.add(t1146, 1e-05)  # t1148: "cuda:0 f32[1, 512, 1]"]

cur node 948 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 963 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 990 group_bsyms len: 1
[Symbol name=add]
[t1254 = prims.add(t1252, 1e-05)  # t1254: "cuda:0 f32[1, 512, 1]"]

cur node 1026 group_bsyms len: 1
[Symbol name=add]
[t1293 = prims.add(t1291, 1e-05)  # t1293: "cuda:0 f32[1, 512, 1]"]

cur node 1048 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1063 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1090 group_bsyms len: 1
[Symbol name=add]
[t1399 = prims.add(t1397, 1e-05)  # t1399: "cuda:0 f32[1, 512, 1]"]

cur node 1126 group_bsyms len: 1
[Symbol name=add]
[t1438 = prims.add(t1436, 1e-05)  # t1438: "cuda:0 f32[1, 512, 1]"]

cur node 1148 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1163 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1190 group_bsyms len: 1
[Symbol name=add]
[t1544 = prims.add(t1542, 1e-05)  # t1544: "cuda:0 f32[1, 512, 1]"]

cur node 1226 group_bsyms len: 1
[Symbol name=add]
[t1583 = prims.add(t1581, 1e-05)  # t1583: "cuda:0 f32[1, 512, 1]"]

cur node 1248 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1263 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1290 group_bsyms len: 1
[Symbol name=add]
[t1689 = prims.add(t1687, 1e-05)  # t1689: "cuda:0 f32[1, 512, 1]"]

cur node 1326 group_bsyms len: 1
[Symbol name=add]
[t1728 = prims.add(t1726, 1e-05)  # t1728: "cuda:0 f32[1, 512, 1]"]

cur node 1348 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1363 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1390 group_bsyms len: 1
[Symbol name=add]
[t1834 = prims.add(t1832, 1e-05)  # t1834: "cuda:0 f32[1, 512, 1]"]

cur node 1426 group_bsyms len: 1
[Symbol name=add]
[t1873 = prims.add(t1871, 1e-05)  # t1873: "cuda:0 f32[1, 512, 1]"]

cur node 1448 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1463 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1490 group_bsyms len: 1
[Symbol name=add]
[t1979 = prims.add(t1977, 1e-05)  # t1979: "cuda:0 f32[1, 512, 1]"]

cur node 1526 group_bsyms len: 1
[Symbol name=add]
[t2018 = prims.add(t2016, 1e-05)  # t2018: "cuda:0 f32[1, 512, 1]"]

cur node 1548 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1563 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1590 group_bsyms len: 1
[Symbol name=add]
[t2124 = prims.add(t2122, 1e-05)  # t2124: "cuda:0 f32[1, 512, 1]"]

cur node 1626 group_bsyms len: 1
[Symbol name=add]
[t2163 = prims.add(t2161, 1e-05)  # t2163: "cuda:0 f32[1, 512, 1]"]

cur node 1648 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1663 group_bsyms len: 1
[Symbol name=convert_element_type]
[]

cur node 1690 group_bsyms len: 1
[Symbol name=add]
[t2269 = prims.add(t2267, 1e-05)  # t2269: "cuda:0 f32[1, 512, 1]"]

cur node 1726 group_bsyms len: 1
[Symbol name=add]
[t2308 = prims.add(t2306, 1e-05)  # t2308: "cuda:0 f32[1, 512, 1]"]

cur node 191 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 227 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 291 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 327 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 391 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 427 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 491 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 527 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 591 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 627 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 691 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 727 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 791 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 827 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 891 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 927 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 991 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1027 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1091 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1127 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1191 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1227 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1291 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1327 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1391 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1427 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1491 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1527 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1591 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1627 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1691 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 1727 group_bsyms len: 1
[Symbol name=rsqrt]
[]

cur node 192 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 228 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 292 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 328 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 392 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 428 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 492 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 528 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 592 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 628 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 692 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 728 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 792 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 828 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 892 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 928 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 992 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1028 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1092 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1128 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1192 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1228 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1292 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1328 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1392 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1428 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1492 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1528 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1592 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1628 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]

cur node 1692 group_bsyms len: 1
[Symbol name=broadcast_in_dim]
[]
