filtered: {'linear': [], 'scaled_dot_product_attention': [thunder.extend.OperatorExecutor('sdpa'), thunder.extend.OperatorExecutor('cudnn')]}
->unpack_trivial
->unpack_trivial
->unpack_trivial
->scaled_dot_product_attention
3 -> cudnn
->scaled_dot_product_attention
4 -> fa3
->add
->python_return
Assigned: {'scaled_dot_product_attention': {3: thunder.extend.OperatorExecutor('cudnn'), 4: thunder.extend.OperatorExecutor('fa3')}}
Input ex: (thunder.extend.OperatorExecutor('nvfuser'), thunder.extend.OperatorExecutor('torchcompile'), thunder.extend.OperatorExecutor('sdpa'), thunder.extend.OperatorExecutor('cudnn'), thunder.extend.OperatorExecutor('torch'), thunder.extend.OperatorExecutor('python'))
================================================================================ Autotune: ================================================================================ Before Autotune Tuning: Optimizing linear
================================================================================ Autotune: ================================================================================ Before Autotune Tuning: Skipping optimization for linear as not requested or not present in computation_trc.
================================================================================ Autotune: ================================================================================ Before Autotune Tuning: Optimizing scaled_dot_product_attention
================================================================================ Autotune: ================================================================================ Before Autotune Tuning: Executors to bench for scaled_dot_product_attention: [thunder.extend.OperatorExecutor('sdpa'), thunder.extend.OperatorExecutor('cudnn'), thunder.extend.OperatorExecutor('torch')]
================================================================================ Autotune: ================================================================================ Before Autotune Tuning: Testing compile data executors: [thunder.extend.OperatorExecutor('sdpa'), thunder.extend.OperatorExecutor('torch'), thunder.extend.OperatorExecutor('nvfuser'), thunder.extend.OperatorExecutor('torchcompile'), thunder.extend.OperatorExecutor('python')]
#FW after trace call
<function trace.<locals>._trace at 0x7cdda07cfe20>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd9ee381f0>
#FW after trace call
<function trace.<locals>._trace at 0x7cdda07cfe20>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd9ee38b80>
#FW after trace call
<function trace.<locals>._trace at 0x7cdda07cfd90>
================================================================================ Autotune: Executors:
================================================================================ Autotune: sdpa -> is operator = True, is fusion = False
================================================================================ Autotune: torch -> is operator = True, is fusion = False
================================================================================ Autotune: nvfuser -> is operator = False, is fusion = True
================================================================================ Autotune: torchcompile -> is operator = False, is fusion = True
================================================================================ Autotune: python -> is operator = True, is fusion = False
================================================================================ Autotune: New forward trace to optimize (strat = OptimizerType.RUNTIME):
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.core.dtypes as dtypes
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  (t7, t8, t9, t10, _, _, t11, t12, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
  t15 = prims.convert_element_type(t7, dtypes.float32)  # t15: "cuda:0 f32[4, 128, 6, 64]"
  t16 = ltorch.add(t14, t15, alpha=None)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = prims.add(t14, t15)  # t16: "cuda:0 f32[4, 128, 6, 64]"
  t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t7, t8, t9, t10, t11, t12), ())
================================================================================ Autotune: Searching best placement for fusion executor = nvfuser
================================================================================ Autotune: Searching best placement for fusion executor = torchcompile
================================================================================ Autotune: Benchmark fw end: Trace = [nvfuser] (time = 0.014675200078636408 ms, mem = 0.002941131591796875 GB)":
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  [t17] = nvFusion0(t0)
    # t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = prims.add(t14, t14)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t0, t1, t2, t3, t4, t5), ())
================================================================================ Autotune: Benchmark fw end: Trace = [nvfuser] (time = 0.014627200085669756 ms, mem = 0.002941131591796875 GB)":
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  [t17] = nvFusion0(t0)
    # t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = prims.add(t14, t14)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t0, t1, t2, t3, t4, t5), ())
================================================================================ Autotune: Caching fw candidate [compile option: None]
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  [t17] = nvFusion0(t0)
    # t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = prims.add(t14, t14)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t0, t1, t2, t3, t4, t5), ())
================================================================================ Autotune: Caching fw candidate [compile option: None]
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  [t17] = nvFusion0(t0)
    # t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = prims.add(t14, t14)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t0, t1, t2, t3, t4, t5), ())
================================================================================ Autotune: End fw time mem pair
================================================================================ Autotune: Benchmark fw end: Trace = [torchcompile] (time = 0.026291199773550034 ms, mem = 0.00331878662109375 GB)":
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  (t7, t8, t9, t10, _, _, t11, t12, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  [t17] = TorchCompile0(t0, t7)
    # t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
    # t15 = prims.convert_element_type(t7, dtypes.float32)  # t15: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = ltorch.add(t14, t15, alpha=None)  # t16: "cuda:0 f32[4, 128, 6, 64]"
      # t16 = prims.add(t14, t15)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t7, t8, t9, t10, t11, t12), ())
================================================================================ Autotune: Benchmark fw end: Trace = [torchcompile] (time = 0.02619839971885085 ms, mem = 0.00331878662109375 GB)":
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  (t7, t8, t9, t10, _, _, t11, t12, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  [t17] = TorchCompile0(t0, t7)
    # t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
    # t15 = prims.convert_element_type(t7, dtypes.float32)  # t15: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = ltorch.add(t14, t15, alpha=None)  # t16: "cuda:0 f32[4, 128, 6, 64]"
      # t16 = prims.add(t14, t15)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t7, t8, t9, t10, t11, t12), ())
================================================================================ Autotune: Caching fw candidate [compile option: None]
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  (t7, t8, t9, t10, _, _, t11, t12, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  [t17] = TorchCompile0(t0, t7)
    # t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
    # t15 = prims.convert_element_type(t7, dtypes.float32)  # t15: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = ltorch.add(t14, t15, alpha=None)  # t16: "cuda:0 f32[4, 128, 6, 64]"
      # t16 = prims.add(t14, t15)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t7, t8, t9, t10, t11, t12), ())
================================================================================ Autotune: Caching fw candidate [compile option: None]
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  (t7, t8, t9, t10, _, _, t11, t12, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  [t17] = TorchCompile0(t0, t7)
    # t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
    # t15 = prims.convert_element_type(t7, dtypes.float32)  # t15: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = ltorch.add(t14, t15, alpha=None)  # t16: "cuda:0 f32[4, 128, 6, 64]"
      # t16 = prims.add(t14, t15)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t7, t8, t9, t10, t11, t12), ())
================================================================================ Autotune: End fw time mem pair
================================================================================ Autotune: New backward trace to optimize (strat = OptimizerType.RUNTIME):
# Constructed by Backward pass
import thunder
import thunder.core.dtypes as dtypes
import thunder.core.prims as prims
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: "Collection"
  # cotangents: "Collection"
  C0, C1, = saved_for_backward
  t18, = cotangents
  query, key, value, t0, t1, t2, t3, t4, t5, t7, t8, t9, t10, t11, t12, = C0
  # C1 (empty sequence)
  t14 = prims.convert_element_type(t18, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
  t15 = prims.convert_element_type(t14, dtypes.bfloat16)  # t15: "cuda:0 bf16[4, 128, 6, 64]"
  t16 = prims.convert_element_type(t14, dtypes.bfloat16)  # t16: "cuda:0 bf16[4, 128, 6, 64]"
  (t17, t19, t20) = sdpafx_scaled_dot_product_efficient_attention_backward(t15, query, key, value, t7, t8, t9, t10, 6, 6, 0.0, False, t11, t12, scale=None)
  (t21, t22, t23) = sdpafx_scaled_dot_product_efficient_attention_backward(t16, query, key, value, t0, t1, t2, t3, 6, 6, 0.0, False, t4, t5, scale=None)
  t24 = prims.convert_element_type(t17, dtypes.float32)  # t24: "cuda:0 f32[4, 128, 6, 64]"
  t25 = prims.convert_element_type(t21, dtypes.float32)  # t25: "cuda:0 f32[4, 128, 6, 64]"
  t26 = prims.add(t24, t25)  # t26: "cuda:0 f32[4, 128, 6, 64]"
  t27 = prims.convert_element_type(t26, dtypes.bfloat16)  # t27: "cuda:0 bf16[4, 128, 6, 64]"
  t28 = prims.convert_element_type(t19, dtypes.float32)  # t28: "cuda:0 f32[4, 128, 6, 64]"
  t29 = prims.convert_element_type(t22, dtypes.float32)  # t29: "cuda:0 f32[4, 128, 6, 64]"
  t30 = prims.add(t28, t29)  # t30: "cuda:0 f32[4, 128, 6, 64]"
  t31 = prims.convert_element_type(t30, dtypes.bfloat16)  # t31: "cuda:0 bf16[4, 128, 6, 64]"
  t32 = prims.convert_element_type(t20, dtypes.float32)  # t32: "cuda:0 f32[4, 128, 6, 64]"
  t33 = prims.convert_element_type(t23, dtypes.float32)  # t33: "cuda:0 f32[4, 128, 6, 64]"
  t34 = prims.add(t32, t33)  # t34: "cuda:0 f32[4, 128, 6, 64]"
  t35 = prims.convert_element_type(t34, dtypes.bfloat16)  # t35: "cuda:0 bf16[4, 128, 6, 64]"
  return (t27, t31, t35)
================================================================================ Autotune: Backward optimization with fw from nvfuser
Current fw cached ctx:
# Constructed by Autotuned transform for execution (took 3006 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3, _, _, t4, t5, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(query, key, value, 0.0, False, scale=None)
  [t17] = nvFusion0(t0)
    # t14 = prims.convert_element_type(t0, dtypes.float32)  # t14: "cuda:0 f32[4, 128, 6, 64]"
    # t16 = prims.add(t14, t14)  # t16: "cuda:0 f32[4, 128, 6, 64]"
    # t17 = prims.convert_element_type(t16, dtypes.bfloat16)  # t17: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t17, 'flat_args': [query, key, value], 'flat_output': (t17,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t0, t1, t2, t3, t4, t5), ())
Options: None
================================================================================ Autotune: Searching best placement for fusion executor = nvfuser
#FW after trace call
<function trace.<locals>._trace at 0x7cdd4a1d77f0>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd4a1d4160>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd4a0201f0>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd4a1d4f70>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd4a1d4160>
TORCH DTYPE t2 torch.int64
TORCH DTYPE t3 torch.int64
TORCH DTYPE t4 torch.int64
TORCH DTYPE t5 torch.int64
#FN EXECUTION FAILED:
# Constructed by Delete Last Used (took 0 milliseconds)
from torch import Tensor
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: "Collection"
  # cotangents: "Collection"
  C0, _, = saved_for_backward
  del saved_for_backward
  t18, = cotangents
  del cotangents
  key, query, t0, t1, t2, t3, t4, t5, value, = C0
  del C0
  (t17, t19, t20) = sdpafx_scaled_dot_product_efficient_attention_backward(t18, query, key, value, t0, t1, t2, t3, 6, 6, 0.0, False, t4, t5, scale=None)
  del t18, query, key, value, t0, t1, t2, t3, t4, t5
  [t24] = nvFusion1(t17)
  t28 = Tensor.to(t19, torch.float32, copy=True)  # t28: "cuda:0 f32[4, 128, 6, 64]"
  del t19
  t32 = Tensor.to(t20, torch.float32, copy=True)  # t32: "cuda:0 f32[4, 128, 6, 64]"
  del t20
  t25 = Tensor.to(t17, torch.float32, copy=True)  # t25: "cuda:0 f32[4, 128, 6, 64]"
  del t17
  t26 = torch.add(t24, t25)  # t26: "cuda:0 f32[4, 128, 6, 64]"
  del t24, t25
  t30 = torch.add(t28, t28)  # t30: "cuda:0 f32[4, 128, 6, 64]"
  del t28
  t34 = torch.add(t32, t32)  # t34: "cuda:0 f32[4, 128, 6, 64]"
  del t32
  t27 = Tensor.to(t26, torch.bfloat16, copy=True)  # t27: "cuda:0 bf16[4, 128, 6, 64]"
  del t26
  t31 = Tensor.to(t30, torch.bfloat16, copy=True)  # t31: "cuda:0 bf16[4, 128, 6, 64]"
  del t30
  t35 = Tensor.to(t34, torch.bfloat16, copy=True)  # t35: "cuda:0 bf16[4, 128, 6, 64]"
  del t34
  return t27, t31, t35
Traceback (most recent call last):
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 505, in benchmark_trace
    t, m, answer = compute_time_cost_ms(executable, executable_str, iters, *input_args)
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 346, in compute_time_cost_ms
    raise e
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 313, in compute_time_cost_ms
    fn(*args)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "thunder.backward_fn_48", line 17, in backward_fn
    (t17, t19, t20) = sdpafx_scaled_dot_product_efficient_attention_backward(t18, query, key, value, t0, t1, t2, t3, 6, 6, 0.0, False, t4, t5, scale=None)
  File "/workspace/workdir/thunder/executors/sdpaex.py", line 453, in _scaled_dot_product_flash_attention_backward_impl
    grads = torch.ops.aten._scaled_dot_product_flash_attention_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1127, in __call__
    return self._op(*args, **(kwargs or {}))
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 79, in __torch_function__
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1127, in __call__
    return self._op(*args, **(kwargs or {}))
RuntimeError: cu_seqlens_q must have dtype int32

#FW after trace call
<function trace.<locals>._trace at 0x7cdd9ee3bc70>
TORCH DTYPE t2 torch.int64
TORCH DTYPE t3 torch.int64
TORCH DTYPE t4 torch.int64
TORCH DTYPE t5 torch.int64
#FN EXECUTION FAILED:
# Constructed by Delete Last Used (took 0 milliseconds)
from torch import Tensor
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: "Collection"
  # cotangents: "Collection"
  C0, _, = saved_for_backward
  del saved_for_backward
  t18, = cotangents
  del cotangents
  key, query, t0, t1, t2, t3, t4, t5, value, = C0
  del C0
  (t17, t19, t20) = sdpafx_scaled_dot_product_efficient_attention_backward(t18, query, key, value, t0, t1, t2, t3, 6, 6, 0.0, False, t4, t5, scale=None)
  del t18, query, key, value, t0, t1, t2, t3, t4, t5
  t24 = Tensor.to(t17, torch.float32, copy=True)  # t24: "cuda:0 f32[4, 128, 6, 64]"
  [t27, t31, t35] = nvFusion1(t17, t24, t19, t20)
  del t17, t24, t19, t20
  return t27, t31, t35
Traceback (most recent call last):
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 505, in benchmark_trace
    t, m, answer = compute_time_cost_ms(executable, executable_str, iters, *input_args)
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 346, in compute_time_cost_ms
    raise e
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 313, in compute_time_cost_ms
    fn(*args)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "thunder.backward_fn_49", line 17, in backward_fn
    (t17, t19, t20) = sdpafx_scaled_dot_product_efficient_attention_backward(t18, query, key, value, t0, t1, t2, t3, 6, 6, 0.0, False, t4, t5, scale=None)
  File "/workspace/workdir/thunder/executors/sdpaex.py", line 453, in _scaled_dot_product_flash_attention_backward_impl
    grads = torch.ops.aten._scaled_dot_product_flash_attention_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1127, in __call__
    return self._op(*args, **(kwargs or {}))
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 79, in __torch_function__
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1127, in __call__
    return self._op(*args, **(kwargs or {}))
RuntimeError: cu_seqlens_q must have dtype int32

#FW after trace call
<function trace.<locals>._trace at 0x7cdd4a0875b0>
TORCH DTYPE t2 torch.int64
TORCH DTYPE t3 torch.int64
TORCH DTYPE t4 torch.int64
TORCH DTYPE t5 torch.int64
#FN EXECUTION FAILED:
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: "Collection"
  # cotangents: "Collection"
  C0, _, = saved_for_backward
  del saved_for_backward
  t18, = cotangents
  del cotangents
  key, query, t0, t1, t2, t3, t4, t5, value, = C0
  del C0
  (t17, t19, t20) = sdpafx_scaled_dot_product_efficient_attention_backward(t18, query, key, value, t0, t1, t2, t3, 6, 6, 0.0, False, t4, t5, scale=None)
  del t18, query, key, value, t0, t1, t2, t3, t4, t5
  [t27, t31, t35] = nvFusion1(t17, t19, t20)
  del t17, t19, t20
  return t27, t31, t35
Traceback (most recent call last):
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 505, in benchmark_trace
    t, m, answer = compute_time_cost_ms(executable, executable_str, iters, *input_args)
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 346, in compute_time_cost_ms
    raise e
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 313, in compute_time_cost_ms
    fn(*args)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "thunder.backward_fn_50", line 16, in backward_fn
    (t17, t19, t20) = sdpafx_scaled_dot_product_efficient_attention_backward(t18, query, key, value, t0, t1, t2, t3, 6, 6, 0.0, False, t4, t5, scale=None)
  File "/workspace/workdir/thunder/executors/sdpaex.py", line 453, in _scaled_dot_product_flash_attention_backward_impl
    grads = torch.ops.aten._scaled_dot_product_flash_attention_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1127, in __call__
    return self._op(*args, **(kwargs or {}))
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 79, in __torch_function__
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1127, in __call__
    return self._op(*args, **(kwargs or {}))
RuntimeError: cu_seqlens_q must have dtype int32

#FW after trace call
<function trace.<locals>._trace at 0x7cdd9ee3aef0>
TORCH DTYPE t2 torch.int64
TORCH DTYPE t3 torch.int64
TORCH DTYPE t4 torch.int64
TORCH DTYPE t5 torch.int64
#FN EXECUTION FAILED:
# Constructed by Delete Last Used (took 0 milliseconds)
from torch import Tensor
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: "Collection"
  # cotangents: "Collection"
  C0, _, = saved_for_backward
  del saved_for_backward
  t18, = cotangents
  del cotangents
  key, query, t0, t1, t2, t3, t4, t5, value, = C0
  del C0
  (t17, t19, t20) = sdpafx_scaled_dot_product_efficient_attention_backward(t18, query, key, value, t0, t1, t2, t3, 6, 6, 0.0, False, t4, t5, scale=None)
  del t18, query, key, value, t0, t1, t2, t3, t4, t5
  t24 = Tensor.to(t17, torch.float32, copy=True)  # t24: "cuda:0 f32[4, 128, 6, 64]"
  del t17
  t28 = Tensor.to(t19, torch.float32, copy=True)  # t28: "cuda:0 f32[4, 128, 6, 64]"
  del t19
  t32 = Tensor.to(t20, torch.float32, copy=True)  # t32: "cuda:0 f32[4, 128, 6, 64]"
  del t20
  t26 = torch.add(t24, t24)  # t26: "cuda:0 f32[4, 128, 6, 64]"
  del t24
  t30 = torch.add(t28, t28)  # t30: "cuda:0 f32[4, 128, 6, 64]"
  del t28
  t34 = torch.add(t32, t32)  # t34: "cuda:0 f32[4, 128, 6, 64]"
  del t32
  t27 = Tensor.to(t26, torch.bfloat16, copy=True)  # t27: "cuda:0 bf16[4, 128, 6, 64]"
  del t26
  t31 = Tensor.to(t30, torch.bfloat16, copy=True)  # t31: "cuda:0 bf16[4, 128, 6, 64]"
  del t30
  t35 = Tensor.to(t34, torch.bfloat16, copy=True)  # t35: "cuda:0 bf16[4, 128, 6, 64]"
  del t34
  return t27, t31, t35
Traceback (most recent call last):
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 505, in benchmark_trace
    t, m, answer = compute_time_cost_ms(executable, executable_str, iters, *input_args)
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 346, in compute_time_cost_ms
    raise e
  File "/workspace/workdir/thunder/backend_optimizer/utils.py", line 313, in compute_time_cost_ms
    fn(*args)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "thunder.backward_fn_51", line 17, in backward_fn
    (t17, t19, t20) = sdpafx_scaled_dot_product_efficient_attention_backward(t18, query, key, value, t0, t1, t2, t3, 6, 6, 0.0, False, t4, t5, scale=None)
  File "/workspace/workdir/thunder/executors/sdpaex.py", line 453, in _scaled_dot_product_flash_attention_backward_impl
    grads = torch.ops.aten._scaled_dot_product_flash_attention_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1127, in __call__
    return self._op(*args, **(kwargs or {}))
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 79, in __torch_function__
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1127, in __call__
    return self._op(*args, **(kwargs or {}))
RuntimeError: cu_seqlens_q must have dtype int32

================================================================================ Autotune: ================================================================================ Before Autotune Tuning: Failed to place sdpa: Failed to get best time placement
Traceback (most recent call last):
  File "/workspace/workdir/thunder/executors/torch_autograd.py", line 458, in split_forward_backward
    primal_trace, fw_extrace, bw_extrace, fw_traces, bw_traces = split()
  File "/workspace/workdir/thunder/executors/torch_autograd.py", line 248, in split
    fw_extrace, bw_extrace = autotune_transform_for_execution(
  File "/workspace/workdir/thunder/executors/passes.py", line 164, in autotune_transform_for_execution
    optimizer_context.optimize()
  File "/workspace/workdir/thunder/backend_optimizer/optimizer.py", line 1124, in optimize
    self.optimizer.optimize()
  File "/workspace/workdir/thunder/backend_optimizer/optimizer.py", line 974, in optimize
    _optimize()
  File "/workspace/workdir/thunder/backend_optimizer/optimizer.py", line 887, in _optimize
    self._search_candidates()
  File "/workspace/workdir/thunder/backend_optimizer/optimizer.py", line 827, in _search_candidates
    _search(ex)
  File "/workspace/workdir/thunder/backend_optimizer/optimizer.py", line 717, in _search
    raise AssertionError("Failed to get best time placement")
AssertionError: Failed to get best time placement

================================================================================ Autotune: ================================================================================ Before Autotune Tuning: Testing compile data executors: [thunder.extend.OperatorExecutor('cudnn'), thunder.extend.OperatorExecutor('torch'), thunder.extend.OperatorExecutor('nvfuser'), thunder.extend.OperatorExecutor('torchcompile'), thunder.extend.OperatorExecutor('python')]
#FW after trace call
<function trace.<locals>._trace at 0x7cdd4a022950>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd5bef48b0>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd5bef48b0>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd4a022830>
#FW after trace call
<function trace.<locals>._trace at 0x7cdd9ee3bb50>
================================================================================ Autotune: Executors:
================================================================================ Autotune: cudnn -> is operator = True, is fusion = False
================================================================================ Autotune: torch -> is operator = True, is fusion = False
================================================================================ Autotune: nvfuser -> is operator = False, is fusion = True
================================================================================ Autotune: torchcompile -> is operator = False, is fusion = True
================================================================================ Autotune: python -> is operator = True, is fusion = False
================================================================================ Autotune: New forward trace to optimize (strat = OptimizerType.RUNTIME):
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.core.dtypes as dtypes
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  (t4, t5, t6, t7) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  t8 = prims.convert_element_type(t0, dtypes.float32)  # t8: "cuda:0 f32[4, 128, 6, 64]"
  t9 = prims.convert_element_type(t4, dtypes.float32)  # t9: "cuda:0 f32[4, 128, 6, 64]"
  t10 = ltorch.add(t8, t9, alpha=None)  # t10: "cuda:0 f32[4, 128, 6, 64]"
    # t10 = prims.add(t8, t9)  # t10: "cuda:0 f32[4, 128, 6, 64]"
  t11 = prims.convert_element_type(t10, dtypes.bfloat16)  # t11: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t11, 'flat_args': [query, key, value], 'flat_output': (t11,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t6, t7), ())
================================================================================ Autotune: Searching best placement for fusion executor = nvfuser
================================================================================ Autotune: Searching best placement for fusion executor = torchcompile
================================================================================ Autotune: Benchmark fw end: Trace = [nvfuser] (time = 0.010593599872663617 ms, mem = 0.002941131591796875 GB)":
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  [t11] = nvFusion0(t0)
    # t8 = prims.convert_element_type(t0, dtypes.float32)  # t8: "cuda:0 f32[4, 128, 6, 64]"
    # t10 = prims.add(t8, t8)  # t10: "cuda:0 f32[4, 128, 6, 64]"
    # t11 = prims.convert_element_type(t10, dtypes.bfloat16)  # t11: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t11, 'flat_args': [query, key, value], 'flat_output': (t11,)}, ((query, key, value, t0, t1, t2, t3, t0, t1, t2, t3), ())
================================================================================ Autotune: Benchmark fw end: Trace = [nvfuser] (time = 0.010647999914363026 ms, mem = 0.002941131591796875 GB)":
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  [t11] = nvFusion0(t0)
    # t8 = prims.convert_element_type(t0, dtypes.float32)  # t8: "cuda:0 f32[4, 128, 6, 64]"
    # t10 = prims.add(t8, t8)  # t10: "cuda:0 f32[4, 128, 6, 64]"
    # t11 = prims.convert_element_type(t10, dtypes.bfloat16)  # t11: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t11, 'flat_args': [query, key, value], 'flat_output': (t11,)}, ((query, key, value, t0, t1, t2, t3, t0, t1, t2, t3), ())
================================================================================ Autotune: Caching fw candidate [compile option: None]
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  [t11] = nvFusion0(t0)
    # t8 = prims.convert_element_type(t0, dtypes.float32)  # t8: "cuda:0 f32[4, 128, 6, 64]"
    # t10 = prims.add(t8, t8)  # t10: "cuda:0 f32[4, 128, 6, 64]"
    # t11 = prims.convert_element_type(t10, dtypes.bfloat16)  # t11: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t11, 'flat_args': [query, key, value], 'flat_output': (t11,)}, ((query, key, value, t0, t1, t2, t3, t0, t1, t2, t3), ())
================================================================================ Autotune: Caching fw candidate [compile option: None]
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  [t11] = nvFusion0(t0)
    # t8 = prims.convert_element_type(t0, dtypes.float32)  # t8: "cuda:0 f32[4, 128, 6, 64]"
    # t10 = prims.add(t8, t8)  # t10: "cuda:0 f32[4, 128, 6, 64]"
    # t11 = prims.convert_element_type(t10, dtypes.bfloat16)  # t11: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t11, 'flat_args': [query, key, value], 'flat_output': (t11,)}, ((query, key, value, t0, t1, t2, t3, t0, t1, t2, t3), ())
================================================================================ Autotune: End fw time mem pair
================================================================================ Autotune: Benchmark fw end: Trace = [torchcompile] (time = 0.018396800477057697 ms, mem = 0.00331878662109375 GB)":
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  (t4, t5, t6, t7) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  [t11] = TorchCompile0(t0, t4)
    # t8 = prims.convert_element_type(t0, dtypes.float32)  # t8: "cuda:0 f32[4, 128, 6, 64]"
    # t9 = prims.convert_element_type(t4, dtypes.float32)  # t9: "cuda:0 f32[4, 128, 6, 64]"
    # t10 = ltorch.add(t8, t9, alpha=None)  # t10: "cuda:0 f32[4, 128, 6, 64]"
      # t10 = prims.add(t8, t9)  # t10: "cuda:0 f32[4, 128, 6, 64]"
    # t11 = prims.convert_element_type(t10, dtypes.bfloat16)  # t11: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t11, 'flat_args': [query, key, value], 'flat_output': (t11,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t6, t7), ())
================================================================================ Autotune: Benchmark fw end: Trace = [torchcompile] (time = 0.01832640040665865 ms, mem = 0.00331878662109375 GB)":
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  (t4, t5, t6, t7) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  [t11] = TorchCompile0(t0, t4)
    # t8 = prims.convert_element_type(t0, dtypes.float32)  # t8: "cuda:0 f32[4, 128, 6, 64]"
    # t9 = prims.convert_element_type(t4, dtypes.float32)  # t9: "cuda:0 f32[4, 128, 6, 64]"
    # t10 = ltorch.add(t8, t9, alpha=None)  # t10: "cuda:0 f32[4, 128, 6, 64]"
      # t10 = prims.add(t8, t9)  # t10: "cuda:0 f32[4, 128, 6, 64]"
    # t11 = prims.convert_element_type(t10, dtypes.bfloat16)  # t11: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t11, 'flat_args': [query, key, value], 'flat_output': (t11,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t6, t7), ())
================================================================================ Autotune: Caching fw candidate [compile option: None]
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(query, key, value):
  # query: "cuda:0 bf16[4, 128, 6, 64]"
  # key: "cuda:0 bf16[4, 128, 6, 64]"
  # value: "cuda:0 bf16[4, 128, 6, 64]"
  (t0, t1, t2, t3) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  (t4, t5, t6, t7) = cudnn_sdpa_fwd(query, key, value, None, 0.0, False, scale=None)
  [t11] = TorchCompile0(t0, t4)
    # t8 = prims.convert_element_type(t0, dtypes.float32)  # t8: "cuda:0 f32[4, 128, 6, 64]"
    # t9 = prims.convert_element_type(t4, dtypes.float32)  # t9: "cuda:0 f32[4, 128, 6, 64]"
    # t10 = ltorch.add(t8, t9, alpha=None)  # t10: "cuda:0 f32[4, 128, 6, 64]"
      # t10 = prims.add(t8, t9)  # t10: "cuda:0 f32[4, 128, 6, 64]"
    # t11 = prims.convert_element_type(t10, dtypes.bfloat16)  # t11: "cuda:0 bf16[4, 128, 6, 64]"
  return {'output': t11, 'flat_args': [query, key, value], 'flat_output': (t11,)}, ((query, key, value, t0, t1, t2, t3, t4, t5, t6, t7), ())
