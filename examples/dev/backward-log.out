============================================ START: LABEL default
============================================ START: computation_trc split_forward_backward
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
============================================ END: computation_trc split_forward_backward
============================================ START: primal_trace sort_data_parallel_syncs
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
============================================ END: primal_trace sort_data_parallel_syncs
============================================ START: augmented_forward_pass
result
t8
env
{'a': VJPDual(primal=t6,
              residuals=((t0, False), None, ([], [t3], [], [t5], [t5, t0]))),
 'result': VJPDual(primal=t7, residuals=((t6, t1), None, ([t1, t6],))),
 't18': VJPDual(primal=t8,
                residuals=((t7, t_proj_weight, None),
                           None,
                           ([t_proj_weight, t7],))),
 't_fc_1_weight': VJPDual(primal=t_fc_1_weight, residuals=()),
 't_fc_2_weight': VJPDual(primal=t_fc_2_weight, residuals=()),
 't_proj_weight': VJPDual(primal=t_proj_weight, residuals=()),
 'x': VJPDual(primal=x, residuals=()),
 'x_fc_1': VJPDual(primal=t0,
                   residuals=((x, t_fc_1_weight, None),
                              None,
                              ([t_fc_1_weight, x],))),
 'x_fc_2': VJPDual(primal=t1,
                   residuals=((x, t_fc_2_weight, None),
                              None,
                              ([t_fc_2_weight, x],)))}
============================================ END: augmented_forward_pass
============================================ START: primal_trace forward_and_backward_from_trace
# Constructed by Augmented forward pass
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
============================================ END: primal_trace forward_and_backward_from_trace
============================================ START: before forward_trc transform_for_execution
# Constructed by Augmented forward pass
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
============================================ END: after forward_trc transform_for_execution
============================================ START: LABEL forward_trc
============================================ START: LABEL backward_trc
============================================ START: before _transform_for_operator_executor_execution
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: "Collection"
  # cotangents: "Collection"
  C0, _, = saved_for_backward
  t9, = cotangents
  x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight, = C0
  t25 = ltorch.reshape(t9, -1, 4096)  # t25: "cuda:0 f32[4096, 4096]"
    # t25 = prims.reshape(t9, (4096, 4096))  # t25: "cuda:0 f32[4096, 4096]"
  t26 = ltorch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
    # t26 = prims.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
  t27 = ltorch.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
    # t27 = prims.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
  t28 = ltorch.reshape(t9, -1, 4096)  # t28: "cuda:0 f32[4096, 4096]"
    # t28 = prims.reshape(t9, (4096, 4096))  # t28: "cuda:0 f32[4096, 4096]"
  t29 = prims.transpose(t28, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"
  t30 = ltorch.reshape(t7, -1, 11008)  # t30: "cuda:0 f32[4096, 11008]"
    # t30 = prims.reshape(t7, (4096, 11008))  # t30: "cuda:0 f32[4096, 11008]"
  t31 = ltorch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
    # t31 = prims.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
  t32 = ltorch.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"
    # t32 = prims.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"
  t33 = ltorch.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"
    # t33 = prims.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"
  t34 = ltorch.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"
    # t34 = prims.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"
  t35 = ltorch.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"
    # t35 = prims.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"
  t36 = ltorch.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"
    # t36 = prims.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"
  t37 = ltorch.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"
    # t37 = prims.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"
  t38 = ltorch.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"
    # t38 = prims.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"
  t39 = ltorch.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"
    # t39 = prims.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"
  t40 = ltorch.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"
    # t40 = prims.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"
  t41 = prims.add(t34, t40)  # t41: "cuda:0 f32[2, 2048, 11008]"
  t42 = ltorch.reshape(t33, -1, 11008)  # t42: "cuda:0 f32[4096, 11008]"
    # t42 = prims.reshape(t33, (4096, 11008))  # t42: "cuda:0 f32[4096, 11008]"
  t43 = ltorch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
    # t43 = prims.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
  t44 = ltorch.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
    # t44 = prims.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
  t45 = ltorch.reshape(t33, -1, 11008)  # t45: "cuda:0 f32[4096, 11008]"
    # t45 = prims.reshape(t33, (4096, 11008))  # t45: "cuda:0 f32[4096, 11008]"
  t46 = prims.transpose(t45, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"
  t47 = ltorch.reshape(x, -1, 4096)  # t47: "cuda:0 f32[4096, 4096]"
    # t47 = prims.reshape(x, (4096, 4096))  # t47: "cuda:0 f32[4096, 4096]"
  t48 = ltorch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
    # t48 = prims.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
  t49 = ltorch.reshape(t41, -1, 11008)  # t49: "cuda:0 f32[4096, 11008]"
    # t49 = prims.reshape(t41, (4096, 11008))  # t49: "cuda:0 f32[4096, 11008]"
  t50 = ltorch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
    # t50 = prims.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
  t51 = ltorch.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
    # t51 = prims.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
  t52 = ltorch.reshape(t41, -1, 11008)  # t52: "cuda:0 f32[4096, 11008]"
    # t52 = prims.reshape(t41, (4096, 11008))  # t52: "cuda:0 f32[4096, 11008]"
  t53 = prims.transpose(t52, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"
  t54 = ltorch.reshape(x, -1, 4096)  # t54: "cuda:0 f32[4096, 4096]"
    # t54 = prims.reshape(x, (4096, 4096))  # t54: "cuda:0 f32[4096, 4096]"
  t55 = ltorch.matmul(t53, t54)  # t55: "cuda:0 f32[11008, 4096]"
    # t55 = prims.matmul(t53, t54)  # t55: "cuda:0 f32[11008, 4096]"
  t56 = prims.add(t44, t51)  # t56: "cuda:0 f32[2, 2048, 4096]"
  return (t56, t55, t48, t31)
============================================ END: before _transform_for_operator_executor_execution
============================================ START: after _transform_for_operator_executor_execution
# Constructed by Transform for operator executor execution (took 1 milliseconds)
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: "Collection"
  # cotangents: "Collection"
  C0, _, = saved_for_backward
  t9, = cotangents
  x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight, = C0
  t25 = ltorch.reshape(t9, -1, 4096)  # t25: "cuda:0 f32[4096, 4096]"
    # t25 = prims.reshape(t9, (4096, 4096))  # t25: "cuda:0 f32[4096, 4096]"
  t26 = torch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
    # t26 = ltorch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
      # t26 = prims.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
  t27 = ltorch.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
    # t27 = prims.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
  t28 = ltorch.reshape(t9, -1, 4096)  # t28: "cuda:0 f32[4096, 4096]"
    # t28 = prims.reshape(t9, (4096, 4096))  # t28: "cuda:0 f32[4096, 4096]"
  t29 = prims.transpose(t28, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"
  t30 = ltorch.reshape(t7, -1, 11008)  # t30: "cuda:0 f32[4096, 11008]"
    # t30 = prims.reshape(t7, (4096, 11008))  # t30: "cuda:0 f32[4096, 11008]"
  t31 = torch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
    # t31 = ltorch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
      # t31 = prims.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
  t32 = ltorch.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"
    # t32 = prims.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"
  t33 = ltorch.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"
    # t33 = prims.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"
  t34 = ltorch.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"
    # t34 = prims.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"
  t35 = ltorch.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"
    # t35 = prims.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"
  t36 = ltorch.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"
    # t36 = prims.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"
  t37 = ltorch.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"
    # t37 = prims.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"
  t38 = ltorch.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"
    # t38 = prims.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"
  t39 = ltorch.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"
    # t39 = prims.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"
  t40 = ltorch.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"
    # t40 = prims.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"
  t41 = prims.add(t34, t40)  # t41: "cuda:0 f32[2, 2048, 11008]"
  t42 = ltorch.reshape(t33, -1, 11008)  # t42: "cuda:0 f32[4096, 11008]"
    # t42 = prims.reshape(t33, (4096, 11008))  # t42: "cuda:0 f32[4096, 11008]"
  t43 = torch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
    # t43 = ltorch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
      # t43 = prims.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
  t44 = ltorch.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
    # t44 = prims.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
  t45 = ltorch.reshape(t33, -1, 11008)  # t45: "cuda:0 f32[4096, 11008]"
    # t45 = prims.reshape(t33, (4096, 11008))  # t45: "cuda:0 f32[4096, 11008]"
  t46 = prims.transpose(t45, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"
  t47 = ltorch.reshape(x, -1, 4096)  # t47: "cuda:0 f32[4096, 4096]"
    # t47 = prims.reshape(x, (4096, 4096))  # t47: "cuda:0 f32[4096, 4096]"
  t48 = torch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
    # t48 = ltorch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
      # t48 = prims.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
  t49 = ltorch.reshape(t41, -1, 11008)  # t49: "cuda:0 f32[4096, 11008]"
    # t49 = prims.reshape(t41, (4096, 11008))  # t49: "cuda:0 f32[4096, 11008]"
  t50 = torch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
    # t50 = ltorch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
      # t50 = prims.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
  t51 = ltorch.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
    # t51 = prims.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
  t52 = ltorch.reshape(t41, -1, 11008)  # t52: "cuda:0 f32[4096, 11008]"
    # t52 = prims.reshape(t41, (4096, 11008))  # t52: "cuda:0 f32[4096, 11008]"
  t53 = prims.transpose(t52, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"
  t54 = ltorch.reshape(x, -1, 4096)  # t54: "cuda:0 f32[4096, 4096]"
    # t54 = prims.reshape(x, (4096, 4096))  # t54: "cuda:0 f32[4096, 4096]"
  t55 = torch.matmul(t53, t54)  # t55: "cuda:0 f32[11008, 4096]"
    # t55 = ltorch.matmul(t53, t54)  # t55: "cuda:0 f32[11008, 4096]"
      # t55 = prims.matmul(t53, t54)  # t55: "cuda:0 f32[11008, 4096]"
  t56 = prims.add(t44, t51)  # t56: "cuda:0 f32[2, 2048, 4096]"
  return (t56, t55, t48, t31)
============================================ GRAPH: _transform_for_operator_executor_execution
graph roots: 0, 1,
traversal nodes:
node ID 0 : [# saved_for_backward: "Collection"]
	parents ids: 
	children ids:  2, 
node ID 1 : [# cotangents: "Collection"]
	parents ids: 
	children ids:  3, 
node ID 2 : [C0, _, = saved_for_backward]
	parents ids:  0, 
	children ids:  4, 
node ID 3 : [t9, = cotangents]
	parents ids:  1, 
	children ids:  8,  5, 
node ID 4 : [x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight, = C0]
	parents ids:  2, 
	children ids:  34,  6,  10,  12,  13,  14,  15,  17,  18,  19,  23,  27,  30, 
node ID 8 : [t28 = ltorch.reshape(t9, -1, 4096)  # t28: "cuda:0 f32[4096, 4096]"
  # t28 = prims.reshape(t9, (4096, 4096))  # t28: "cuda:0 f32[4096, 4096]"]
	parents ids:  3, 
	children ids:  9, 
node ID 5 : [t25 = ltorch.reshape(t9, -1, 4096)  # t25: "cuda:0 f32[4096, 4096]"
  # t25 = prims.reshape(t9, (4096, 4096))  # t25: "cuda:0 f32[4096, 4096]"]
	parents ids:  3, 
	children ids:  6, 
node ID 34 : [t54 = ltorch.reshape(x, -1, 4096)  # t54: "cuda:0 f32[4096, 4096]"
  # t54 = prims.reshape(x, (4096, 4096))  # t54: "cuda:0 f32[4096, 4096]"]
	parents ids:  4, 
	children ids:  35, 
node ID 6 : [t26 = ltorch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
  # t26 = prims.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"]
	parents ids:  4,  5, 
	children ids:  7, 
node ID 10 : [t30 = ltorch.reshape(t7, -1, 11008)  # t30: "cuda:0 f32[4096, 11008]"
  # t30 = prims.reshape(t7, (4096, 11008))  # t30: "cuda:0 f32[4096, 11008]"]
	parents ids:  4, 
	children ids:  11, 
node ID 12 : [t32 = ltorch.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"
  # t32 = prims.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  4,  7, 
	children ids:  14,  15, 
node ID 13 : [t33 = ltorch.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"
  # t33 = prims.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  4,  7, 
	children ids:  25,  22, 
node ID 14 : [t34 = ltorch.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"
  # t34 = prims.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  4,  12, 
	children ids:  21, 
node ID 15 : [t35 = ltorch.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"
  # t35 = prims.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  4,  12, 
	children ids:  16, 
node ID 17 : [t37 = ltorch.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"
  # t37 = prims.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  16,  4, 
	children ids:  18, 
node ID 18 : [t38 = ltorch.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"
  # t38 = prims.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  17,  4, 
	children ids:  19, 
node ID 19 : [t39 = ltorch.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"
  # t39 = prims.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  18,  4, 
	children ids:  20, 
node ID 23 : [t43 = ltorch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
  # t43 = prims.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"]
	parents ids:  4,  22, 
	children ids:  24, 
node ID 27 : [t47 = ltorch.reshape(x, -1, 4096)  # t47: "cuda:0 f32[4096, 4096]"
  # t47 = prims.reshape(x, (4096, 4096))  # t47: "cuda:0 f32[4096, 4096]"]
	parents ids:  4, 
	children ids:  28, 
node ID 30 : [t50 = ltorch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
  # t50 = prims.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"]
	parents ids:  4,  29, 
	children ids:  31, 
node ID 9 : [t29 = prims.transpose(t28, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"]
	parents ids:  8, 
	children ids:  11, 
node ID 35 : [t55 = ltorch.matmul(t53, t54)  # t55: "cuda:0 f32[11008, 4096]"
  # t55 = prims.matmul(t53, t54)  # t55: "cuda:0 f32[11008, 4096]"]
	parents ids:  33,  34, 
	children ids:  37, 
node ID 7 : [t27 = ltorch.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
  # t27 = prims.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  6, 
	children ids:  12,  13, 
node ID 11 : [t31 = ltorch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
  # t31 = prims.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"]
	parents ids:  9,  10, 
	children ids:  37, 
node ID 25 : [t45 = ltorch.reshape(t33, -1, 11008)  # t45: "cuda:0 f32[4096, 11008]"
  # t45 = prims.reshape(t33, (4096, 11008))  # t45: "cuda:0 f32[4096, 11008]"]
	parents ids:  13, 
	children ids:  26, 
node ID 22 : [t42 = ltorch.reshape(t33, -1, 11008)  # t42: "cuda:0 f32[4096, 11008]"
  # t42 = prims.reshape(t33, (4096, 11008))  # t42: "cuda:0 f32[4096, 11008]"]
	parents ids:  13, 
	children ids:  23, 
node ID 21 : [t41 = prims.add(t34, t40)  # t41: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  20,  14, 
	children ids:  32,  29, 
node ID 16 : [t36 = ltorch.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"
  # t36 = prims.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  15, 
	children ids:  17, 
node ID 20 : [t40 = ltorch.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"
  # t40 = prims.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  19, 
	children ids:  21, 
node ID 24 : [t44 = ltorch.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
  # t44 = prims.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  23, 
	children ids:  36, 
node ID 28 : [t48 = ltorch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
  # t48 = prims.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"]
	parents ids:  26,  27, 
	children ids:  37, 
node ID 31 : [t51 = ltorch.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
  # t51 = prims.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  30, 
	children ids:  36, 
node ID 37 : [return (t56, t55, t48, t31)]
	parents ids:  11,  35,  36,  28, 
	children ids: 
node ID 26 : [t46 = prims.transpose(t45, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"]
	parents ids:  25, 
	children ids:  28, 
node ID 32 : [t52 = ltorch.reshape(t41, -1, 11008)  # t52: "cuda:0 f32[4096, 11008]"
  # t52 = prims.reshape(t41, (4096, 11008))  # t52: "cuda:0 f32[4096, 11008]"]
	parents ids:  21, 
	children ids:  33, 
node ID 29 : [t49 = ltorch.reshape(t41, -1, 11008)  # t49: "cuda:0 f32[4096, 11008]"
  # t49 = prims.reshape(t41, (4096, 11008))  # t49: "cuda:0 f32[4096, 11008]"]
	parents ids:  21, 
	children ids:  30, 
node ID 36 : [t56 = prims.add(t44, t51)  # t56: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  24,  31, 
	children ids:  37, 
node ID 33 : [t53 = prims.transpose(t52, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"]
	parents ids:  32, 
	children ids:  35, 

============================================ END: after _transform_for_operator_executor_execution
============================================ START: after fusion_pass
# Constructed by Fusion (took 3 milliseconds)
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: "Collection"
  # cotangents: "Collection"
  C0, _, = saved_for_backward
  t9, = cotangents
  x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight, = C0
  t25 = ltorch.reshape(t9, -1, 4096)  # t25: "cuda:0 f32[4096, 4096]"
    # t25 = prims.reshape(t9, (4096, 4096))  # t25: "cuda:0 f32[4096, 4096]"
  t29 = prims.transpose(t25, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"
  t30 = ltorch.reshape(t7, -1, 11008)  # t30: "cuda:0 f32[4096, 11008]"
    # t30 = prims.reshape(t7, (4096, 11008))  # t30: "cuda:0 f32[4096, 11008]"
  t47 = ltorch.reshape(x, -1, 4096)  # t47: "cuda:0 f32[4096, 4096]"
    # t47 = prims.reshape(x, (4096, 4096))  # t47: "cuda:0 f32[4096, 4096]"
  t26 = torch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
    # t26 = ltorch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
      # t26 = prims.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
  t31 = torch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
    # t31 = ltorch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
      # t31 = prims.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
  t27 = ltorch.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
    # t27 = prims.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
  [t33, t41] = nvFusion0(t0, t1, t27, t3, t5, t6)
    # t32 = prims.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"
    # t33 = prims.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"
    # t34 = prims.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"
    # t35 = prims.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"
    # t36 = prims.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"
    # t37 = prims.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"
    # t38 = prims.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"
    # t39 = prims.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"
    # t40 = prims.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"
    # t41 = prims.add(t34, t40)  # t41: "cuda:0 f32[2, 2048, 11008]"
  t42 = ltorch.reshape(t33, -1, 11008)  # t42: "cuda:0 f32[4096, 11008]"
    # t42 = prims.reshape(t33, (4096, 11008))  # t42: "cuda:0 f32[4096, 11008]"
  t46 = prims.transpose(t42, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"
  t49 = ltorch.reshape(t41, -1, 11008)  # t49: "cuda:0 f32[4096, 11008]"
    # t49 = prims.reshape(t41, (4096, 11008))  # t49: "cuda:0 f32[4096, 11008]"
  t53 = prims.transpose(t49, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"
  t55 = torch.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
    # t55 = ltorch.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
      # t55 = prims.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
  t48 = torch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
    # t48 = ltorch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
      # t48 = prims.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
  t50 = torch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
    # t50 = ltorch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
      # t50 = prims.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
  t43 = torch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
    # t43 = ltorch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
      # t43 = prims.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
  t44 = ltorch.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
    # t44 = prims.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
  t51 = ltorch.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
    # t51 = prims.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
  [t56] = nvFusion1(t44, t51)
    # t56 = prims.add(t44, t51)  # t56: "cuda:0 f32[2, 2048, 4096]"
  return (t56, t55, t48, t31)
============================================ GRAPH: fusion_pass
graph roots: 0, 1,
traversal nodes:
node ID 0 : [# saved_for_backward: "Collection"]
	parents ids: 
	children ids:  2, 
node ID 1 : [# cotangents: "Collection"]
	parents ids: 
	children ids:  3, 
node ID 2 : [C0, _, = saved_for_backward]
	parents ids:  0, 
	children ids:  4, 
node ID 3 : [t9, = cotangents]
	parents ids:  1, 
	children ids:  5, 
node ID 4 : [x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight, = C0]
	parents ids:  2, 
	children ids:  7,  8,  9,  12,  19,  20, 
node ID 5 : [t25 = ltorch.reshape(t9, -1, 4096)  # t25: "cuda:0 f32[4096, 4096]"
  # t25 = prims.reshape(t9, (4096, 4096))  # t25: "cuda:0 f32[4096, 4096]"]
	parents ids:  3, 
	children ids:  9,  6, 
node ID 7 : [t30 = ltorch.reshape(t7, -1, 11008)  # t30: "cuda:0 f32[4096, 11008]"
  # t30 = prims.reshape(t7, (4096, 11008))  # t30: "cuda:0 f32[4096, 11008]"]
	parents ids:  4, 
	children ids:  10, 
node ID 8 : [t47 = ltorch.reshape(x, -1, 4096)  # t47: "cuda:0 f32[4096, 4096]"
  # t47 = prims.reshape(x, (4096, 4096))  # t47: "cuda:0 f32[4096, 4096]"]
	parents ids:  4, 
	children ids:  17,  18, 
node ID 9 : [t26 = torch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
  # t26 = ltorch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
    # t26 = prims.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"]
	parents ids:  4,  5, 
	children ids:  11, 
node ID 12 : [[t33, t41] = nvFusion0(t0, t1, t27, t3, t5, t6)
  # t32 = prims.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"
  # t33 = prims.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"
  # t34 = prims.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"
  # t35 = prims.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"
  # t36 = prims.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"
  # t37 = prims.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"
  # t38 = prims.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"
  # t39 = prims.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"
  # t40 = prims.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"
  # t41 = prims.add(t34, t40)  # t41: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  11,  4, 
	children ids:  13,  15, 
node ID 19 : [t50 = torch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
  # t50 = ltorch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
    # t50 = prims.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"]
	parents ids:  4,  15, 
	children ids:  22, 
node ID 20 : [t43 = torch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
  # t43 = ltorch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
    # t43 = prims.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"]
	parents ids:  4,  13, 
	children ids:  21, 
node ID 6 : [t29 = prims.transpose(t25, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"]
	parents ids:  5, 
	children ids:  10, 
node ID 10 : [t31 = torch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
  # t31 = ltorch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
    # t31 = prims.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"]
	parents ids:  6,  7, 
	children ids:  24, 
node ID 17 : [t55 = torch.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
  # t55 = ltorch.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
    # t55 = prims.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"]
	parents ids:  16,  8, 
	children ids:  24, 
node ID 18 : [t48 = torch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
  # t48 = ltorch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
    # t48 = prims.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"]
	parents ids:  8,  14, 
	children ids:  24, 
node ID 11 : [t27 = ltorch.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
  # t27 = prims.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  9, 
	children ids:  12, 
node ID 13 : [t42 = ltorch.reshape(t33, -1, 11008)  # t42: "cuda:0 f32[4096, 11008]"
  # t42 = prims.reshape(t33, (4096, 11008))  # t42: "cuda:0 f32[4096, 11008]"]
	parents ids:  12, 
	children ids:  20,  14, 
node ID 15 : [t49 = ltorch.reshape(t41, -1, 11008)  # t49: "cuda:0 f32[4096, 11008]"
  # t49 = prims.reshape(t41, (4096, 11008))  # t49: "cuda:0 f32[4096, 11008]"]
	parents ids:  12, 
	children ids:  16,  19, 
node ID 22 : [t51 = ltorch.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
  # t51 = prims.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  19, 
	children ids:  23, 
node ID 21 : [t44 = ltorch.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
  # t44 = prims.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  20, 
	children ids:  23, 
node ID 24 : [return (t56, t55, t48, t31)]
	parents ids:  17,  18,  10,  23, 
	children ids: 
node ID 14 : [t46 = prims.transpose(t42, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"]
	parents ids:  13, 
	children ids:  18, 
node ID 16 : [t53 = prims.transpose(t49, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"]
	parents ids:  15, 
	children ids:  17, 
node ID 23 : [[t56] = nvFusion1(t44, t51)
  # t56 = prims.add(t44, t51)  # t56: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  21,  22, 
	children ids:  24, 

============================================ END: after fusion_pass
============================================ START: after _transform_for_operator_executor_execution (always)
# Constructed by Transform for operator executor execution (took 1 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: "Collection"
  # cotangents: "Collection"
  C0, _, = saved_for_backward
  t9, = cotangents
  x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight, = C0
  t25 = torch.reshape(t9, (-1, 4096))  # t25: "cuda:0 f32[4096, 4096]"
    # t25 = ltorch.reshape(t9, (-1, 4096))  # t25: "cuda:0 f32[4096, 4096]"
      # t25 = prims.reshape(t9, (4096, 4096))  # t25: "cuda:0 f32[4096, 4096]"
  t29 = torch.permute(t25, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"
    # t29 = ltorch.permute(t25, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"
      # t29 = prims.transpose(t25, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"
  t30 = torch.reshape(t7, (-1, 11008))  # t30: "cuda:0 f32[4096, 11008]"
    # t30 = ltorch.reshape(t7, (-1, 11008))  # t30: "cuda:0 f32[4096, 11008]"
      # t30 = prims.reshape(t7, (4096, 11008))  # t30: "cuda:0 f32[4096, 11008]"
  t47 = torch.reshape(x, (-1, 4096))  # t47: "cuda:0 f32[4096, 4096]"
    # t47 = ltorch.reshape(x, (-1, 4096))  # t47: "cuda:0 f32[4096, 4096]"
      # t47 = prims.reshape(x, (4096, 4096))  # t47: "cuda:0 f32[4096, 4096]"
  t26 = torch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
    # t26 = ltorch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
      # t26 = prims.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
  t31 = torch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
    # t31 = ltorch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
      # t31 = prims.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
  t27 = torch.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
    # t27 = ltorch.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
      # t27 = prims.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
  [t33, t41] = nvFusion0(t0, t1, t27, t3, t5, t6)
    # t32 = prims.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"
    # t33 = prims.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"
    # t34 = prims.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"
    # t35 = prims.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"
    # t36 = prims.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"
    # t37 = prims.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"
    # t38 = prims.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"
    # t39 = prims.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"
    # t40 = prims.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"
    # t41 = prims.add(t34, t40)  # t41: "cuda:0 f32[2, 2048, 11008]"
  t42 = torch.reshape(t33, (-1, 11008))  # t42: "cuda:0 f32[4096, 11008]"
    # t42 = ltorch.reshape(t33, (-1, 11008))  # t42: "cuda:0 f32[4096, 11008]"
      # t42 = prims.reshape(t33, (4096, 11008))  # t42: "cuda:0 f32[4096, 11008]"
  t46 = torch.permute(t42, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"
    # t46 = ltorch.permute(t42, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"
      # t46 = prims.transpose(t42, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"
  t49 = torch.reshape(t41, (-1, 11008))  # t49: "cuda:0 f32[4096, 11008]"
    # t49 = ltorch.reshape(t41, (-1, 11008))  # t49: "cuda:0 f32[4096, 11008]"
      # t49 = prims.reshape(t41, (4096, 11008))  # t49: "cuda:0 f32[4096, 11008]"
  t53 = torch.permute(t49, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"
    # t53 = ltorch.permute(t49, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"
      # t53 = prims.transpose(t49, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"
  t55 = torch.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
    # t55 = ltorch.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
      # t55 = prims.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
  t48 = torch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
    # t48 = ltorch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
      # t48 = prims.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
  t50 = torch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
    # t50 = ltorch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
      # t50 = prims.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
  t43 = torch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
    # t43 = ltorch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
      # t43 = prims.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
  t44 = torch.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
    # t44 = ltorch.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
      # t44 = prims.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
  t51 = torch.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
    # t51 = ltorch.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
      # t51 = prims.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
  [t56] = nvFusion1(t44, t51)
    # t56 = prims.add(t44, t51)  # t56: "cuda:0 f32[2, 2048, 4096]"
  return (t56, t55, t48, t31)
============================================ GRAPH: fusion_pass
graph roots: 0, 1,
traversal nodes:
node ID 0 : [# saved_for_backward: "Collection"]
	parents ids: 
	children ids:  2, 
node ID 1 : [# cotangents: "Collection"]
	parents ids: 
	children ids:  3, 
node ID 2 : [C0, _, = saved_for_backward]
	parents ids:  0, 
	children ids:  4, 
node ID 3 : [t9, = cotangents]
	parents ids:  1, 
	children ids:  5, 
node ID 4 : [x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight, = C0]
	parents ids:  2, 
	children ids:  7,  8,  9,  12,  19,  20, 
node ID 5 : [t25 = torch.reshape(t9, (-1, 4096))  # t25: "cuda:0 f32[4096, 4096]"
  # t25 = ltorch.reshape(t9, (-1, 4096))  # t25: "cuda:0 f32[4096, 4096]"
    # t25 = prims.reshape(t9, (4096, 4096))  # t25: "cuda:0 f32[4096, 4096]"]
	parents ids:  3, 
	children ids:  9,  6, 
node ID 7 : [t30 = torch.reshape(t7, (-1, 11008))  # t30: "cuda:0 f32[4096, 11008]"
  # t30 = ltorch.reshape(t7, (-1, 11008))  # t30: "cuda:0 f32[4096, 11008]"
    # t30 = prims.reshape(t7, (4096, 11008))  # t30: "cuda:0 f32[4096, 11008]"]
	parents ids:  4, 
	children ids:  10, 
node ID 8 : [t47 = torch.reshape(x, (-1, 4096))  # t47: "cuda:0 f32[4096, 4096]"
  # t47 = ltorch.reshape(x, (-1, 4096))  # t47: "cuda:0 f32[4096, 4096]"
    # t47 = prims.reshape(x, (4096, 4096))  # t47: "cuda:0 f32[4096, 4096]"]
	parents ids:  4, 
	children ids:  17,  18, 
node ID 9 : [t26 = torch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
  # t26 = ltorch.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"
    # t26 = prims.matmul(t25, t_proj_weight)  # t26: "cuda:0 f32[4096, 11008]"]
	parents ids:  4,  5, 
	children ids:  11, 
node ID 12 : [[t33, t41] = nvFusion0(t0, t1, t27, t3, t5, t6)
  # t32 = prims.mul(t1, t27)  # t32: "cuda:0 f32[2, 2048, 11008]"
  # t33 = prims.mul(t6, t27)  # t33: "cuda:0 f32[2, 2048, 11008]"
  # t34 = prims.mul(t5, t32)  # t34: "cuda:0 f32[2, 2048, 11008]"
  # t35 = prims.mul(t0, t32)  # t35: "cuda:0 f32[2, 2048, 11008]"
  # t36 = prims.neg(t35)  # t36: "cuda:0 f32[2, 2048, 11008]"
  # t37 = prims.mul(t36, t5)  # t37: "cuda:0 f32[2, 2048, 11008]"
  # t38 = prims.mul(t37, t5)  # t38: "cuda:0 f32[2, 2048, 11008]"
  # t39 = prims.mul(t38, t3)  # t39: "cuda:0 f32[2, 2048, 11008]"
  # t40 = prims.neg(t39)  # t40: "cuda:0 f32[2, 2048, 11008]"
  # t41 = prims.add(t34, t40)  # t41: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  11,  4, 
	children ids:  13,  15, 
node ID 19 : [t50 = torch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
  # t50 = ltorch.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"
    # t50 = prims.matmul(t49, t_fc_1_weight)  # t50: "cuda:0 f32[4096, 4096]"]
	parents ids:  4,  15, 
	children ids:  22, 
node ID 20 : [t43 = torch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
  # t43 = ltorch.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"
    # t43 = prims.matmul(t42, t_fc_2_weight)  # t43: "cuda:0 f32[4096, 4096]"]
	parents ids:  4,  13, 
	children ids:  21, 
node ID 6 : [t29 = torch.permute(t25, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"
  # t29 = ltorch.permute(t25, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"
    # t29 = prims.transpose(t25, (1, 0))  # t29: "cuda:0 f32[4096, 4096]"]
	parents ids:  5, 
	children ids:  10, 
node ID 10 : [t31 = torch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
  # t31 = ltorch.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"
    # t31 = prims.matmul(t29, t30)  # t31: "cuda:0 f32[4096, 11008]"]
	parents ids:  6,  7, 
	children ids:  24, 
node ID 17 : [t55 = torch.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
  # t55 = ltorch.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"
    # t55 = prims.matmul(t53, t47)  # t55: "cuda:0 f32[11008, 4096]"]
	parents ids:  16,  8, 
	children ids:  24, 
node ID 18 : [t48 = torch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
  # t48 = ltorch.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"
    # t48 = prims.matmul(t46, t47)  # t48: "cuda:0 f32[11008, 4096]"]
	parents ids:  8,  14, 
	children ids:  24, 
node ID 11 : [t27 = torch.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
  # t27 = ltorch.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"
    # t27 = prims.reshape(t26, (2, 2048, 11008))  # t27: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  9, 
	children ids:  12, 
node ID 13 : [t42 = torch.reshape(t33, (-1, 11008))  # t42: "cuda:0 f32[4096, 11008]"
  # t42 = ltorch.reshape(t33, (-1, 11008))  # t42: "cuda:0 f32[4096, 11008]"
    # t42 = prims.reshape(t33, (4096, 11008))  # t42: "cuda:0 f32[4096, 11008]"]
	parents ids:  12, 
	children ids:  20,  14, 
node ID 15 : [t49 = torch.reshape(t41, (-1, 11008))  # t49: "cuda:0 f32[4096, 11008]"
  # t49 = ltorch.reshape(t41, (-1, 11008))  # t49: "cuda:0 f32[4096, 11008]"
    # t49 = prims.reshape(t41, (4096, 11008))  # t49: "cuda:0 f32[4096, 11008]"]
	parents ids:  12, 
	children ids:  16,  19, 
node ID 22 : [t51 = torch.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
  # t51 = ltorch.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"
    # t51 = prims.reshape(t50, (2, 2048, 4096))  # t51: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  19, 
	children ids:  23, 
node ID 21 : [t44 = torch.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
  # t44 = ltorch.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"
    # t44 = prims.reshape(t43, (2, 2048, 4096))  # t44: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  20, 
	children ids:  23, 
node ID 24 : [return (t56, t55, t48, t31)]
	parents ids:  17,  18,  10,  23, 
	children ids: 
node ID 14 : [t46 = torch.permute(t42, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"
  # t46 = ltorch.permute(t42, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"
    # t46 = prims.transpose(t42, (1, 0))  # t46: "cuda:0 f32[11008, 4096]"]
	parents ids:  13, 
	children ids:  18, 
node ID 16 : [t53 = torch.permute(t49, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"
  # t53 = ltorch.permute(t49, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"
    # t53 = prims.transpose(t49, (1, 0))  # t53: "cuda:0 f32[11008, 4096]"]
	parents ids:  15, 
	children ids:  17, 
node ID 23 : [[t56] = nvFusion1(t44, t51)
  # t56 = prims.add(t44, t51)  # t56: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  21,  22, 
	children ids:  24, 

============================================ END: after _transform_for_operator_executor_execution (always)
---------------------------------------------- all traces
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Augmented forward pass
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
##############################################
# Constructed by Transform for execution (took 2 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t3, t5, t6, t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
##############################################
# Constructed by Update Call Context (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((t0, t1, t7, t_fc_1_weight, t_fc_2_weight, t_proj_weight, x), ())
##############################################
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((t0, t1, t7, t_fc_1_weight, t_fc_2_weight, t_proj_weight, x), ())
##############################################
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Augmented forward pass
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
##############################################
# Constructed by Transform for execution (took 2 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t3, t5, t6, t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
##############################################
# Constructed by Update Call Context (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((t0, t1, t7, t_fc_1_weight, t_fc_2_weight, t_proj_weight, x), ())
##############################################
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((t0, t1, t7, t_fc_1_weight, t_fc_2_weight, t_proj_weight, x), ())
##############################################
---------------------------------------------- ans
tensor([[[-0.0380,  0.1292,  0.0922,  ...,  0.0574, -0.0760, -0.0142],
         [-0.0312, -0.1352,  0.1404,  ..., -0.0036, -0.1777, -0.0775],
         [ 0.0121, -0.0281, -0.1634,  ...,  0.0387, -0.2150,  0.0118],
         ...,
         [-0.1302,  0.0754, -0.1463,  ..., -0.0835, -0.1263,  0.1630],
         [-0.0158,  0.2085,  0.0153,  ..., -0.0273, -0.0947, -0.0970],
         [-0.2236, -0.1944,  0.0894,  ...,  0.0347, -0.0962,  0.1017]],

        [[ 0.0363, -0.1088,  0.1518,  ...,  0.0293,  0.1325,  0.0490],
         [-0.1212, -0.2084,  0.1211,  ..., -0.1555,  0.0875, -0.0580],
         [ 0.1207, -0.0828, -0.0089,  ...,  0.0490,  0.0931,  0.0576],
         ...,
         [-0.0100,  0.0776,  0.1118,  ...,  0.0961,  0.0167,  0.0933],
         [-0.1560,  0.0455, -0.0116,  ...,  0.0028, -0.0157, -0.0022],
         [ 0.3174,  0.0314, -0.0429,  ...,  0.1140,  0.0264,  0.0614]]],
       device='cuda:0', grad_fn=<ThunderFunctionBackward>)
