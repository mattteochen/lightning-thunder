============================================ START: LABEL default
============================================ START: computation_trc split_forward_backward
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
============================================ END: computation_trc split_forward_backward
============================================ START: primal_trace sort_data_parallel_syncs
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
============================================ END: primal_trace sort_data_parallel_syncs
============================================ START: augmented_forward_pass
result
t8
env
{'a': VJPDual(primal=t6,
              residuals=((t0, False), None, ([], [t3], [], [t5], [t5, t0]))),
 'result': VJPDual(primal=t7, residuals=((t6, t1), None, ([t1, t6],))),
 't18': VJPDual(primal=t8,
                residuals=((t7, t_proj_weight, None),
                           None,
                           ([t_proj_weight, t7],))),
 't_fc_1_weight': VJPDual(primal=t_fc_1_weight, residuals=()),
 't_fc_2_weight': VJPDual(primal=t_fc_2_weight, residuals=()),
 't_proj_weight': VJPDual(primal=t_proj_weight, residuals=()),
 'x': VJPDual(primal=x, residuals=()),
 'x_fc_1': VJPDual(primal=t0,
                   residuals=((x, t_fc_1_weight, None),
                              None,
                              ([t_fc_1_weight, x],))),
 'x_fc_2': VJPDual(primal=t1,
                   residuals=((x, t_fc_2_weight, None),
                              None,
                              ([t_fc_2_weight, x],)))}
============================================ END: augmented_forward_pass
============================================ START: primal_trace forward_and_backward_from_trace
# Constructed by Augmented forward pass
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
============================================ END: primal_trace forward_and_backward_from_trace
============================================ START: before forward_trc transform_for_execution
# Constructed by Augmented forward pass
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
============================================ END: after forward_trc transform_for_execution
============================================ START: LABEL forward_trc
============================================ START: before _transform_for_operator_executor_execution
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
============================================ END: before _transform_for_operator_executor_execution
============================================ START: after _transform_for_operator_executor_execution
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
============================================ GRAPH: _transform_for_operator_executor_execution
graph roots: 0, 1, 2, 3,
traversal nodes:
node ID 0 : [# x: "cuda:0 f32[2, 2048, 4096]"]
	parents ids: 
	children ids:  13,  4,  5, 
node ID 1 : [# t_fc_1_weight: "cuda:0 f32[11008, 4096]"]
	parents ids: 
	children ids:  4,  13, 
node ID 2 : [# t_fc_2_weight: "cuda:0 f32[11008, 4096]"]
	parents ids: 
	children ids:  13,  5, 
node ID 3 : [# t_proj_weight: "cuda:0 f32[4096, 11008]"]
	parents ids: 
	children ids:  12,  13, 
node ID 13 : [return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())]
	parents ids:  0,  1,  2,  3,  4,  5,  7,  9,  10,  11,  12, 
	children ids: 
node ID 4 : [t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  0,  1, 
	children ids:  10,  13,  6, 
node ID 5 : [t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  0,  2, 
	children ids:  11,  13, 
node ID 12 : [t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"]
	parents ids:  3,  11, 
	children ids:  13, 
node ID 10 : [t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  9,  4, 
	children ids:  11,  13, 
node ID 6 : [t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  4, 
	children ids:  7, 
node ID 11 : [t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  10,  5, 
	children ids:  12,  13, 
node ID 7 : [t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  6, 
	children ids:  8,  13, 
node ID 8 : [t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
  # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  7, 
	children ids:  9, 
node ID 9 : [t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"]
	parents ids:  8, 
	children ids:  10,  13, 

============================================ END: after _transform_for_operator_executor_execution
============================================ START: after fusion_pass
# Constructed by Fusion (took 1 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t3, t5, t6, t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
============================================ END: after fusion_pass
============================================ START: after _transform_for_operator_executor_execution (always)
# Constructed by Transform for operator executor execution (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t3, t5, t6, t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
============================================ END: after _transform_for_operator_executor_execution (always)
============================================ START: LABEL backward_trc
---------------------------------------------- all traces
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Augmented forward pass
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
##############################################
# Constructed by Transform for execution (took 18 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t3, t5, t6, t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
##############################################
# Constructed by Update Call Context (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((t0, t1, t7, t_fc_1_weight, t_fc_2_weight, t_proj_weight, x), ())
##############################################
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((t0, t1, t7, t_fc_1_weight, t_fc_2_weight, t_proj_weight, x), ())
##############################################
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Dead Code Elimination (took 0 milliseconds)
import thunder
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: "cuda:0 f32[2, 2048, 11008]"
  x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"
    # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: "cuda:0 f32[2, 2048, 11008]"

  # /workspace/pj/lightning-thunder/examples/dev/LLaMAMLP.py:13: 	        x = torch.nn.functional.silu(x_fc_1) * x_fc_2
  a = ltorch.silu(x_fc_1, False)  # a: "cuda:0 f32[2, 2048, 11008]"
    # t9 = prims.neg(x_fc_1)  # t9: "cuda:0 f32[2, 2048, 11008]"
    # t10 = prims.exp(t9)  # t10: "cuda:0 f32[2, 2048, 11008]"
    # t11 = prims.add(1.0, t10)  # t11: "cuda:0 f32[2, 2048, 11008]"
    # t12 = prims.reciprocal(t11)  # t12: "cuda:0 f32[2, 2048, 11008]"
    # a = prims.mul(x_fc_1, t12)  # a: "cuda:0 f32[2, 2048, 11008]"
  result = ltorch.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"
    # result = prims.mul(a, x_fc_2)  # result: "cuda:0 f32[2, 2048, 11008]"

  # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: 	        return F.linear(input, self.weight, self.bias)
  t18 = ltorch.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
    # t18 = prims.linear(result, t_proj_weight, None)  # t18: "cuda:0 f32[2, 2048, 4096]"
  return t18
##############################################
# Constructed by Augmented forward pass
import thunder
import thunder.core.prims as prims
import thunder.torch as ltorch
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
  t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
  t4 = ltorch.add(1.0, t3, alpha=None)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
  t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
  t6 = ltorch.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
  t7 = ltorch.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
##############################################
# Constructed by Transform for execution (took 18 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t3, t5, t6, t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((x, t_fc_1_weight, t_fc_2_weight, t0, t3, t5, t6, t1, t7, t_proj_weight), ())
##############################################
# Constructed by Update Call Context (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((t0, t1, t7, t_fc_1_weight, t_fc_2_weight, t_proj_weight, x), ())
##############################################
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast
def augmented_forward_fn(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: "cuda:0 f32[2, 2048, 4096]"
  # t_fc_1_weight: "cuda:0 f32[11008, 4096]"
  # t_fc_2_weight: "cuda:0 f32[11008, 4096]"
  # t_proj_weight: "cuda:0 f32[4096, 11008]"
  t0 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
    # t0 = ltorch.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
      # t0 = prims.linear(x, t_fc_1_weight, None)  # t0: "cuda:0 f32[2, 2048, 11008]"
  t1 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
    # t1 = ltorch.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
      # t1 = prims.linear(x, t_fc_2_weight, None)  # t1: "cuda:0 f32[2, 2048, 11008]"
  [t7] = nvFusion0(t0, t1)
    # t2 = prims.neg(t0)  # t2: "cuda:0 f32[2, 2048, 11008]"
    # t3 = prims.exp(t2)  # t3: "cuda:0 f32[2, 2048, 11008]"
    # t4 = prims.add(1.0, t3)  # t4: "cuda:0 f32[2, 2048, 11008]"
    # t5 = prims.reciprocal(t4)  # t5: "cuda:0 f32[2, 2048, 11008]"
    # t6 = prims.mul(t0, t5)  # t6: "cuda:0 f32[2, 2048, 11008]"
    # t7 = prims.mul(t6, t1)  # t7: "cuda:0 f32[2, 2048, 11008]"
  t8 = torch.nn.functional.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
    # t8 = ltorch.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
      # t8 = prims.linear(t7, t_proj_weight, None)  # t8: "cuda:0 f32[2, 2048, 4096]"
  return {'output': t8, 'flat_args': [x, t_fc_1_weight, t_fc_2_weight, t_proj_weight], 'flat_output': (t8,)}, ((t0, t1, t7, t_fc_1_weight, t_fc_2_weight, t_proj_weight, x), ())
##############################################
---------------------------------------------- ans
tensor([[[-0.0110,  0.0542,  0.0908,  ..., -0.2110,  0.2082,  0.0331],
         [ 0.4479,  0.0610, -0.0296,  ...,  0.0564, -0.0904, -0.0877],
         [ 0.1233,  0.0146, -0.1153,  ...,  0.1049,  0.0266, -0.0702],
         ...,
         [-0.0202,  0.0180, -0.0293,  ..., -0.0630,  0.1042,  0.0283],
         [-0.0221, -0.0508,  0.1574,  ...,  0.1687, -0.0135,  0.0040],
         [ 0.0555,  0.0216,  0.2707,  ..., -0.0414,  0.1786, -0.2664]],

        [[-0.0476, -0.1409, -0.0704,  ...,  0.0162,  0.0102, -0.0570],
         [-0.1471,  0.0132, -0.2057,  ...,  0.0787, -0.0048, -0.0167],
         [-0.0957, -0.1662, -0.0485,  ...,  0.0173,  0.0265, -0.0916],
         ...,
         [-0.0264, -0.0388, -0.2041,  ...,  0.0679,  0.0027, -0.0122],
         [ 0.0222,  0.0553, -0.2055,  ...,  0.0905,  0.1831,  0.0558],
         [ 0.0816, -0.0930,  0.0024,  ..., -0.1418, -0.0122, -0.0344]]],
       device='cuda:0', grad_fn=<ThunderFunctionBackward>)
